<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.SI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.SI</link>


<item>
<title>Leveraging the Power of AI and Social Interactions to Restore Trust in Public Polls</title>
<link>https://arxiv.org/abs/2511.07593</link>
<guid>https://arxiv.org/abs/2511.07593</guid>
<content:encoded><![CDATA[
<div> polling, crowdsourced data, social networks, credibility, social interactions

Summary:
The paper discusses the challenges of credibility in crowdsourced data collected on social networks and explores the use of social interactions to enhance reliability. Traditional polling methods have seen a decline in engagement, while social networks face credibility issues due to dishonest participants. The study uses AI-based graph analysis to detect ineligible participation in a polling task by analyzing social interactions within a network of honest and dishonest actors. By focusing on the structure of social interaction graphs without considering content, the study aims to restore credibility in data collection. Experiments on real-world datasets show promising results in detecting ineligibility across diverse participation patterns, achieving an accuracy of over 90% in some scenarios. The research highlights the potential of leveraging social interactions to improve the trustworthiness of crowdsourced data in social science research. 

<br /><br />Summary: <div>
arXiv:2511.07593v1 Announce Type: new 
Abstract: The emergence of crowdsourced data has significantly reshaped social science, enabling extensive exploration of collective human actions, viewpoints, and societal dynamics. However, ensuring safe, fair, and reliable participation remains a persistent challenge. Traditional polling methods have seen a notable decline in engagement over recent decades, raising concerns about the credibility of collected data. Meanwhile, social and peer-to-peer networks have become increasingly widespread, but data from these platforms can suffer from credibility issues due to fraudulent or ineligible participation. In this paper, we explore how social interactions can help restore credibility in crowdsourced data collected over social networks. We present an empirical study to detect ineligible participation in a polling task through AI-based graph analysis of social interactions among imperfect participants composed of honest and dishonest actors. Our approach focuses solely on the structure of social interaction graphs, without relying on the content being shared. We simulate different levels and types of dishonest behavior among participants who attempt to propagate the task within their social networks. We conduct experiments on real-world social network datasets, using different eligibility criteria and modeling diverse participation patterns. Although structural differences in social interaction graphs introduce some performance variability, our study achieves promising results in detecting ineligibility across diverse social and behavioral profiles, with accuracy exceeding 90% in some configurations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiLoMix: Robust High- and Low-Frequency Graph Learning Framework for Mixing Address Association</title>
<link>https://arxiv.org/abs/2511.07759</link>
<guid>https://arxiv.org/abs/2511.07759</guid>
<content:encoded><![CDATA[
<div> Graph-based models, Mixing address association, Label noise, Label scarcity, Weak supervised learning <br />
Summary: <br />
HiLoMix is a novel graph-based learning framework designed for mixing address association in transaction networks. It addresses challenges of label noise and scarcity by constructing a Heterogeneous Attributed Mixing Interaction Graph (HAMIG) and applying frequency-aware graph contrastive learning. The framework utilizes weak supervised learning to handle noisy labels and trains high-pass and low-pass Graph Neural Networks (GNNs) using unsupervised contrastive signals and confidence-based supervision. A stacking framework is employed to fuse predictions from multiple models, enhancing generalization and robustness. Experimental results show that HiLoMix outperforms existing methods in accurately associating mixing addresses, making it an effective tool in detecting illicit transactions. <br /> <div>
arXiv:2511.07759v1 Announce Type: new 
Abstract: As mixing services are increasingly being exploited by malicious actors for illicit transactions, mixing address association has emerged as a critical research task. A range of approaches have been explored, with graph-based models standing out for their ability to capture structural patterns in transaction networks. However, these approaches face two main challenges: label noise and label scarcity, leading to suboptimal performance and limited generalization. To address these, we propose HiLoMix, a graph-based learning framework specifically designed for mixing address association. First, we construct the Heterogeneous Attributed Mixing Interaction Graph (HAMIG) to enrich the topological structure. Second, we introduce frequency-aware graph contrastive learning that captures complementary structural signals from high- and low-frequency graph views. Third, we employ weak supervised learning that assigns confidence-based weighting to noisy labels. Then, we jointly train high-pass and low-pass GNNs using both unsupervised contrastive signals and confidence-based supervision to learn robust node representations. Finally, we adopt a stacking framework to fuse predictions from multiple heterogeneous models, further improving generalization and robustness. Experimental results demonstrate that HiLoMix outperforms existing methods in mixing address association.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Value Expressions in Social Media Posts</title>
<link>https://arxiv.org/abs/2511.08453</link>
<guid>https://arxiv.org/abs/2511.08453</guid>
<content:encoded><![CDATA[
<div> Keywords: value alignment, sociotechnical systems, large-language models, Schwartz value system, social media data

Summary: 
The study focuses on the measurement of human values expressed in social media posts, a crucial aspect of value alignment in sociotechnical systems. Utilizing the Schwartz value system as a comprehensive framework, the researchers develop a method to measure value expressions at scale. Through the analysis of over 32,000 social media posts and annotations from human raters, low inter-rater agreement is observed. The study suggests that value expression is subjective and personalized, leading to the construction of a personalization architecture for classifying value expressions. Results show that a system accounting for individual differences produces more consistent predictions of value expressions compared to human raters. This research provides valuable insights and methods for understanding and measuring human values in the context of social media data.
<br /><br />Summary: <div>
arXiv:2511.08453v1 Announce Type: new 
Abstract: The value alignment of sociotechnical systems has become a central debate but progress in this direction requires the measurement of the expressions of values. While the rise of large-language models offer new possible opportunities for measuring expressions of human values (e.g., humility or equality) in social media data, there remain both conceptual and practical challenges in operationalizing value expression in social media posts: what value system and operationalization is most applicable, and how do we actually measure them? In this paper, we draw on the Schwartz value system as a broadly encompassing and theoretically grounded set of basic human values, and introduce a framework for measuring Schwartz value expressions in social media posts at scale. We collect 32,370 ground truth value expression annotations from N=1,079 people on 5,211 social media posts representative of real users' feeds. We observe low levels of inter-rater agreement between people, and low agreement between human raters and LLM-based methods. Drawing on theories of interpretivism - that different people will have different subjective experiences of the same situation - we argue that value expression is (partially) in the eye of the beholder. In response, we construct a personalization architecture for classifying value expressions. We find that a system that explicitly models these differences yields predicted value expressions that people agree with more than they agree with other people. These results contribute new methods and understanding for the measurement of human values in social media data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations</title>
<link>https://arxiv.org/abs/2511.07734</link>
<guid>https://arxiv.org/abs/2511.07734</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian optimization, graph-structured domains, Gaussian process surrogates, low-rank spectral representations, global search

Summary: 
This paper introduces a scalable framework for global optimization over graphs using Bayesian optimization. The framework utilizes low-rank spectral representations to build Gaussian process surrogates from sparse structural observations on graph-structured domains. By jointly inferring graph structure and node representations through learnable embeddings, the method enables efficient global search and principled uncertainty estimation even with limited data. The approach achieves faster convergence and improved optimization performance compared to existing methods, overcoming challenges posed by the discrete and combinatorial nature of graphs. Theoretical analysis establishes conditions for accurate recovery of underlying graph structure under different sampling regimes. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework in achieving faster convergence and improved optimization performance. <div>
arXiv:2511.07734v1 Announce Type: cross 
Abstract: Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Political Text at Scale with Online Tensor LDA</title>
<link>https://arxiv.org/abs/2511.07809</link>
<guid>https://arxiv.org/abs/2511.07809</guid>
<content:encoded><![CDATA[
<div> Keywords: topic modeling, Tensor Latent Dirichlet Allocation, scalable, social media, #MeToo movement

Summary: 
This paper introduces a scalable topic modeling method called Tensor Latent Dirichlet Allocation (TLDA) that can handle billions of documents. The method guarantees identifiable and recoverable parameters, sample complexity, and computational and memory efficiency, outperforming previous parallelized Latent Dirichlet Allocation (LDA) methods. By providing an open-source, GPU-based implementation, TLDA allows for the analysis of very large text datasets, like over a billion documents. The scalability of TLDA enables social scientists to conduct large-scale studies on subjects like the #MeToo movement and social media conversations about election fraud in the 2020 presidential election. The method facilitates in-depth analysis and provides insights into real-time discourse on significant societal topics.<br /><br />Summary: <div>
arXiv:2511.07809v1 Announce Type: cross 
Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Weisfeiler-Lehman Kernels to Subgraphs</title>
<link>https://arxiv.org/abs/2412.02181</link>
<guid>https://arxiv.org/abs/2412.02181</guid>
<content:encoded><![CDATA[
<div> Subgraph representation learning; graph neural networks; Weisfeiler-Lehman kernel; structural information; neighborhood sampling <br />
<br />
Summary: <br />
Subgraph representation learning has become a popular method for solving real-world problems, but current graph neural networks struggle with capturing complex interactions within and between subgraphs. To address this, the WLKS approach introduces a Weisfeiler-Lehman kernel that is specialized for subgraphs by applying the WL algorithm on induced k-hop neighborhoods. By combining kernels across different k-hop levels, WLKS can capture richer structural information without the need for neighborhood sampling. In experiments across various datasets, WLKS outperformed existing approaches on five datasets while also reducing training time significantly, showcasing a balance between expressiveness and efficiency in subgraph representation learning. <div>
arXiv:2412.02181v3 Announce Type: replace-cross 
Abstract: Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more expressive and efficient alternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for subgraphs by applying the WL algorithm on induced $k$-hop neighborhoods. We combine kernels across different $k$-hop levels to capture richer structural information that is not fully encoded in existing models. Our approach can balance expressiveness and efficiency by eliminating the need for neighborhood sampling. In experiments on eight real-world and synthetic benchmarks, WLKS significantly outperforms leading approaches on five datasets while reducing training time, ranging from 0.01x to 0.25x compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effects of higher-order interactions and homophily on information access inequality</title>
<link>https://arxiv.org/abs/2506.00156</link>
<guid>https://arxiv.org/abs/2506.00156</guid>
<content:encoded><![CDATA[
<div> Keywords: socio-technical systems, hypergraphs, hyperedge homophily, nonlinear social contagion, information access <br />
Summary: 
The article introduces the $\texttt{H3}$ model for hypergraphs with hyperedge homophily, hyperedge size-dependent property, and tunable degree distribution. It also presents a model for nonlinear social contagion with asymmetric transmission between in-group and out-group nodes. The study shows that the interaction between social contagion dynamics and hyperedge homophily can significantly influence group-level differences in information access. The findings highlight the importance of considering hyperedge homophily in shaping interaction patterns and suggest targeted interventions at specific hyperedge sizes to reduce inequality in socio-technical systems. The research emphasizes the need for a higher-order perspective in designing socio-technical systems, focusing on dynamics-informed interventions embedded in platform architecture. <br /><br />Summary: <div>
arXiv:2506.00156v2 Announce Type: replace-cross 
Abstract: The spread of information through socio-technical systems determines which individuals are the first to gain access to opportunities and insights. Yet, the pathways through which information flows can be skewed, leading to systematic differences in access across social groups. These inequalities remain poorly characterized in settings involving nonlinear social contagion and higher-order interactions that exhibit homophily. We introduce a enerative model for hypergraphs with hyperedge homophily, a hyperedge size-dependent property, and tunable degree distribution, called the $\texttt{H3}$ model, along with a model for nonlinear social contagion that incorporates asymmetric transmission between in-group and out-group nodes. Using stochastic simulations of a social contagion process on hypergraphs from the $\texttt{H3}$ model and diverse empirical datasets, we show that the interaction between social contagion dynamics and hyperedge homophily -- an effect unique to higher-order networks due to its dependence on hyperedge size -- can critically shape group-level differences in information access. By emphasizing how hyperedge homophily shapes interaction patterns, our findings underscore the need to rethink socio-technical system design through a higher-order perspective and suggest that dynamics-informed, targeted interventions at specific hyperedge sizes, embedded in a platform architecture, offer a powerful lever for reducing inequality.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Food as Soft Power: Taiwanese Gastrodiplomacy on Social Media and Algorithmic Suppression</title>
<link>https://arxiv.org/abs/2511.05729</link>
<guid>https://arxiv.org/abs/2511.05729</guid>
<content:encoded><![CDATA[
<div> dataset, bubble tea, Instagram, Taiwanese cuisine, soft power
Summary: 
This study analyzes how Taiwanese gastrodiplomacy is represented on social media platforms, specifically focusing on bubble tea. The researchers collected data from Instagram posts over five years, revealing that bubble tea is a dominant symbol of Taiwanese cuisine on the platform. However, there is evidence of Instagram suppressing posts mentioning Taiwan, indicating potential vulnerabilities in Taiwan's gastrodiplomatic strategy. The study also observes a significant decrease in engagement following a political event in Taiwan. These findings highlight the role of digital platforms in facilitating or hindering gastrodiplomacy and cultural diplomacy, emphasizing the importance of algorithmic transparency. The study provides insights for nations looking to leverage soft power through food diplomacy and for researchers studying algorithmic suppression. 
<br /><br />Summary: <div>
arXiv:2511.05729v1 Announce Type: new 
Abstract: Social media platforms have become pivotal for projecting national identity and soft power in an increasingly digital world. This study examines the digital manifestation of Taiwanese gastrodiplomacy by focusing on bubble tea -- a culturally iconic beverage -- leveraging a dataset comprising 107,169 posts from the popular lifestyle social media platform Instagram. Including 315,279,227 engagements, 4,756,320 comments, and 8,097,260,651 views over five full years (2020--2024), we investigate how social media facilitates discussion about Taiwanese cuisine and contributes to Taiwan's digital soft power. Our analysis reveals that bubble tea consistently emerges as the dominant representation of Taiwanese cuisine across Meta's Instagram channels. However, this dominance also indicates vulnerability in gastrodiplomatic strategy compared to other countries. Additionally, we find evidence that Instagram suppresses bubble tea posts mentioning Taiwan by 1,200\% -- roughly a twelve--fold decrease in exposure -- relative to posts without such mentions. Crucially, we observe a significant drop in the number of posts, views, and engagement following Lai's inauguration in May 2024. This study ultimately contributes to understanding how digital platforms can enable or disable gastrodiplomacy, soft power, and cultural diplomacy while highlighting the need for greater algorithmic transparency. By noting Taiwan's bubble tea's digital engagement and footprint, critical insights are brought for nations seeking to leverage soft power through gastronomic means in a politicized digital era and researchers trying to better understand algorithmic suppression.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Enterprise Big Data Microservice Architecture Based on Domain-Driven Design (DDD)</title>
<link>https://arxiv.org/abs/2511.05880</link>
<guid>https://arxiv.org/abs/2511.05880</guid>
<content:encoded><![CDATA[
<div> Keywords: digitization, enterprise big data processing platform, microservice architecture, Domain Driven Design, scalability

Summary:
This article introduces a solution for enterprise big data processing platforms by utilizing a microservice architecture based on Domain Driven Design. Traditional monolithic architectures struggle to handle the growing complexity of business demands and data volume, resulting in limited scalability and efficiency. By decomposing the core business logic into independent microservices, the platform gains flexibility and scalability. The article also presents an automated data collection process utilizing microservices and a dynamic scheduling algorithm to efficiently allocate tasks to Docker nodes. Real-time monitoring ensures data collection accuracy and efficiency. Implementation and testing of the platform confirm significant enhancements in scalability, data quality, and collection efficiency.<br /><br />Summary: <div>
arXiv:2511.05880v1 Announce Type: new 
Abstract: With the rapid advancement of digitization and intelligence, enterprise big data processing platforms have become increasingly important in data management. However, traditional monolithic architectures, due to their high coupling, are unable to cope with increasingly complex demands in the face of business expansion and increased data volume, resulting in limited platform scalability and decreased data collection efficiency. This article proposes a solution for enterprise big data processing platform based on microservice architecture, based on the concept of Domain Driven Design (DDD). Through in-depth analysis of business requirements, the functional and non functional requirements of the platform in various scenarios were determined, and the DDD method was used to decompose the core business logic into independent microservice modules, enabling data collection, parsing, cleaning, and visualization functions to be independently developed, deployed, and upgraded, thereby improving the flexibility and scalability of the system. This article also designs an automated data collection process based on microservices and proposes an improved dynamic scheduling algorithm to efficiently allocate data collection tasks to Docker nodes, and monitor the collection progress and service status in real time to ensure the accuracy and efficiency of data collection. Through the implementation and testing of the platform, it has been verified that the enterprise big data processing platform based on microservice architecture has significantly improved scalability, data quality, and collection efficiency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Construction and Evolutionary Analysis of a Game Model for Supply Chain Finance Funding Based on Blockchain Technology</title>
<link>https://arxiv.org/abs/2511.05891</link>
<guid>https://arxiv.org/abs/2511.05891</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain finance, blockchain technology, information asymmetry, credit transmission chains, evolutionary game theory

Summary:
Supply chain finance has addressed capital challenges in domestic enterprises by facilitating financing services based on core enterprise credit. However, challenges like information asymmetry and inefficient credit transmission hinder growth in China. This paper proposes a supply chain finance framework integrating blockchain technology to enhance efficiency. It outlines participating entities, their relationships, and constructs a financing game model using evolutionary game theory. The model analyzes equilibrium points and stability, exploring strategies adopted by small and medium-sized enterprises, key players, and financing entities. The impact of blockchain technology on supply chain finance transactions is also assessed, highlighting its influence on completing transactions successfully. <div>
arXiv:2511.05891v1 Announce Type: new 
Abstract: The current surge in supply chain finance has significantly alleviated the "capital challenges" faced by domestic related enterprises, enabling enterprises upstream and subsequent stages of the industrial chain to achieve effective circulation of financing services in the supply chain based on the credit of core enterprises. By gathering essential information from the heart of the supply chain, supply chain financing enables efficient resource distribution and aids all stakeholders in making well-informed choices. However, supply chain finance in China still faces numerous obstacles, such as information asymmetry and inefficient credit transmission chains, hindering its long-term development. This paper designs an operational framework for supply chain finance incorporating blockchain technology, clearly defines the participating entities, and analyzes their business relationships. Based upon evolutionary game theory, a supply chain finance financing game model incorporating blockchain technology is constructed. A comparative analysis of the model's equilibrium points and their stability is conducted. The choices of evolutionary equilibrium strategies adopted by small and medium-sized enterprises, key players, and financing entities within this framework are explored, and the influence of blockchain technology on the prerequisites for completing supply chain finance transactions is investigated.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research On CODP Localization Decision Model Of Automotive Supply Chain Based On Delayed Manufacturing Strategy</title>
<link>https://arxiv.org/abs/2511.05899</link>
<guid>https://arxiv.org/abs/2511.05899</guid>
<content:encoded><![CDATA[
<div> delayed response manufacturing, automotive manufacturing, production system, order response node configuration, cost control

Summary:
The traditional manufacturing model is insufficient to meet the current demands for personalized products and quick deliveries in the automotive industry. To address this, a production system based on delayed response manufacturing strategy is proposed. This system focuses on order response nodes configuration in the production process to optimize resource allocation and market response. The model considers structural cost changes, dynamic unit manufacturing cost variations, and intermediate inventory costs at different stages. It incorporates delivery time constraints to enhance practicality. By using function fitting, simulation analysis, and mathematical modeling tools, the study systematically describes total cost changes and validates the model through actual enterprise cases. The findings provide a decision-making framework for automobile manufacturers to achieve flexibility in production while controlling costs in response to changing demands. The research also offers practical insights for implementation strategies and system optimization in the future.<br /><br />Summary: <div>
arXiv:2511.05899v1 Announce Type: new 
Abstract: Under the market background of increasingly personalized product demand and compressed response cycle, the traditional manufacturing model with standardized mass production as the core has been difficult to meet the dual expectations of customers for differentiation and fast delivery. In order to improve the efficiency of resource allocation and market response, automobile manufacturers need to build a production system that takes into account cost and flexibility. Based on the delayed response manufacturing strategy, this study built an order response node configuration model suitable for automotive manufacturing scenarios, focusing on the positioning of order driven intervention points in the production process. The model comprehensively considers the structural cost changes brought by process adjustment, the dynamic characteristics of the changes of unit manufacturing cost and intermediate inventory cost at different stages with the location of nodes, and introduces delivery time constraints to embed time factors into the inventory decision logic to enhance the practicality of the model and the adaptation of realistic constraints. In terms of solution methods, this paper adopts function fitting and simulation analysis methods, combined with mathematical modeling tools, systematically describes the change trend of total cost, and verifies the rationality and effectiveness of the model structure and solution through actual enterprise cases. The research results provide a theoretical basis and decision support for automobile manufacturing enterprises to realize the synergy of flexible production and cost control in the environment of variable demand, and also provide an empirical reference for the implementation path and system optimization of subsequent relevant strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role and Mechanism of Deep Statistical Machine Learning In Biological Target Screening and Immune Microenvironment Regulation of Asthma</title>
<link>https://arxiv.org/abs/2511.05904</link>
<guid>https://arxiv.org/abs/2511.05904</guid>
<content:encoded><![CDATA[
<div> phosphodiesterase 4, phosphodiesterase 7, natural inhibitors, computer aided drug design, xanthine oxidase <br />
Summary:<br /> 
This study focused on screening natural inhibitors of phosphodiesterase 4 (PDE4) and phosphodiesterase 7 (PDE7) using a combination of computer-aided drug design (CADD) and deep learning methods. The potential inhibitors were verified through enzyme activity experiments and enzyme-linked immunoassays. Dual-target inhibitors for these enzymes are sought for treating inflammatory diseases with minimal adverse reactions. Additionally, the development of natural inhibitors for xanthine oxidase (XO) was explored using pharmacophore and molecular docking technologies. Sixteen potential natural inhibitors of PDE4/7 were identified and their binding stability was confirmed through molecular dynamics simulation. This study provides a foundation for establishing an efficient dual-target inhibitor screening system and discovering lead compounds for novel XO inhibitors. <br /> <div>
arXiv:2511.05904v1 Announce Type: new 
Abstract: As an important source of small molecule drugs, natural products show remarkable biological activities with their rich types and unique structures. However, due to the limited number of samples and structural complexity, the rapid discovery of lead compounds is limited. Therefore, in this study, natural inhibitors of phosphodiesterase 4 (PDE4) and Phosphodiesterase 7 (PDE7) were screened by combining computer aided drug design (CADD) technology and deep learning method, and their activities were verified by enzyme activity experiment and enzymo-linked immunoassay. These two enzymes have important application potential in the treatment of inflammatory diseases such as chronic obstructive pulmonary disease and asthma, but PDE4 inhibitors may cause adverse reactions, so it is particularly important to develop both effective and safe dual-target inhibitors. In addition, as a potential target of hyperuricemia, the development of natural inhibitors of xanthine oxidase (X0) is also of great value. We used pharmacophore technology for virtual screening, combined with molecular docking technology to improve accuracy, and finally selected 16 potential natural inhibitors of PDE4/7, and verified their binding stability through molecular dynamics simulation. The results of this study laid a foundation for establishing an efficient dual-target inhibitor screening system and exploring the lead compounds of novel X0 inhibitors.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing AI Manipulation Risks in Brazilian YouTube Climate Discourse</title>
<link>https://arxiv.org/abs/2511.06091</link>
<guid>https://arxiv.org/abs/2511.06091</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, YouTube, audience engagement, persuasive strategies, generative language models 

Summary: 
Climate change presents a global threat that requires evidence-based policies and a thorough understanding of public perception on platforms like YouTube. This study focuses on climate-related discourse on Brazilian YouTube to investigate the psychological traits driving audience engagement, their impact on content popularity, and their potential use in designing persuasive synthetic campaigns. The researchers also release a large dataset of Brazilian YouTube videos and user comments on climate change, including annotations of persuasive strategies, theory-of-mind categorizations, and content creator typologies. This dataset can support future research on digital climate communication and the ethical risks associated with algorithmically amplified narratives and generative media. <div>
arXiv:2511.06091v1 Announce Type: new 
Abstract: Climate change poses a global threat to public health, food security, and economic stability. Addressing it requires evidence-based policies and a nuanced understanding of how the threat is perceived by the public, particularly within visual social media, where narratives quickly evolve through voices of individuals, politicians, NGOs, and institutions. This study investigates climate-related discourse on YouTube within the Brazilian context, a geopolitically significant nation in global environmental negotiations. Through three case studies, we examine (1) which psychological content traits most effectively drive audience engagement, (2) the extent to which these traits influence content popularity, and (3) whether such insights can inform the design of persuasive synthetic campaigns--such as climate denialism--using recent generative language models. Another contribution of this work is the release of a large publicly available dataset of 226K Brazilian YouTube videos and 2.7M user comments on climate change. The dataset includes fine-grained annotations of persuasive strategies, theory-of-mind categorizations in user responses, and typologies of content creators. This resource can help support future research on digital climate communication and the ethical risk of algorithmically amplified narratives and generative media.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperEF 2.0: Spectral Hypergraph Coarsening via Krylov Subspace Expansion and Resistance-based Local Clustering</title>
<link>https://arxiv.org/abs/2511.06600</link>
<guid>https://arxiv.org/abs/2511.06600</guid>
<content:encoded><![CDATA[
<div> Framework, Spectral coarsening, Hypergraphs, Effective resistances, Clustering 

Summary: 
- HyperEF 2.0 is a framework for spectral coarsening and clustering of large-scale hypergraphs through hyperedge effective resistances.
- It leverages expanded Krylov subspace for improved approximation accuracy of effective resistances.
- A resistance-based local clustering scheme merges small isolated nodes into nearby clusters for balanced clusters with improved conductance.
- HyperEF 2.0 integrates resistance-based hyperedge weighting and community detection into a multilevel hypergraph partitioning tool.
- Extensive experiments on VLSI benchmarks show HyperEF 2.0 effectively coarsens hypergraphs, maintaining structural properties and delivering better solution quality. It outperforms state-of-the-art methods like HyperEF and HyperSF and achieves smaller cut sizes compared to hMETIS, SpecPart, MedPart, and KaHyPar. Additionally, HyperEF 2.0 achieves up to a 4.5x speedup over HyperSF, demonstrating superior efficiency and partitioning quality. 

<br /><br />Summary: <div>
arXiv:2511.06600v1 Announce Type: new 
Abstract: This paper introduces HyperEF 2.0, a scalable framework for spectral coarsening and clustering of large-scale hypergraphs through hyperedge effective resistances, aiming to decompose hypergraphs into multiple node clusters with a small number of inter-cluster hyperedges. Building on the recent HyperEF framework, our approach offers three primary contributions. Specifically, first, by leveraging the expanded Krylov subspace exploiting both clique and star expansions of hyperedges, we can significantly improve the approximation accuracy of effective resistances. Second, we propose a resistance-based local clustering scheme for merging small isolated nodes into nearby clusters, yielding more balanced clusters with substantially improved conductance. Third, the proposed HyperEF 2.0 enables the integration of resistance-based hyperedge weighting and community detection into a multilevel hypergraph partitioning tool, achieving state-of-the-art performance. Extensive experiments on real-world VLSI benchmarks show that HyperEF 2.0 can more effectively coarsen hypergraphs without compromising their structural properties, while delivering much better solution quality (e.g. conductance) than the state-of-the-art hypergraph coarsening methods, such as HyperEF and HyperSF. Moreover, compared to leading hypergraph partitioners such as hMETIS, SpecPart, MedPart, and KaHyPar, our framework consistently achieves smaller cut sizes. In terms of runtime, HyperEF 2.0 attains up to a 4.5x speedup over the latest flow-based local clustering algorithm, HyperSF, demonstrating both superior efficiency and partitioning quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Centrality: Understanding Urban Street Network Typologies Through Intersection Patterns</title>
<link>https://arxiv.org/abs/2511.06747</link>
<guid>https://arxiv.org/abs/2511.06747</guid>
<content:encoded><![CDATA[
<div> Intersection classification, clustering algorithms, San Francisco Bay Area, road network patterns, typologies <br />
Summary: 
This study introduces a novel metric for classifying intersections in the San Francisco Bay Area based on angles formed, identifying grid, orthogonal, and organic city typologies. The research fills gaps in previous studies by focusing on detailed characterizations within a single large urban region and considering intersection-level geometric angles. The study's approach utilizes clustering algorithms in machine learning to classify intersections and differentiate cities based on street and intersection patterns. The typologies generated, grid, orthogonal, and organic, offer valuable insights for city planners and policymakers. These typologies can inform various strategies tailored to each city's road network complexities, including evacuation plans, traffic signage placements, and traffic signal control. <br /><br /> <div>
arXiv:2511.06747v1 Announce Type: new 
Abstract: The structure of road networks plays a pivotal role in shaping transportation dynamics. It also provides insights into how drivers experience city streets and helps uncover each urban environment's unique characteristics and challenges. Consequently, characterizing cities based on their road network patterns can facilitate the identification of similarities and differences, informing collaborative traffic management strategies, particularly at a regional scale. While previous studies have investigated global network patterns for cities, they have often overlooked detailed characterizations within a single large urban region. Additionally, most existing research uses metrics like degree, centrality, orientation, etc., and misses the nuances of street networks at the intersection level, specifically the geometric angles formed by links at intersections, which could offer a more refined feature for characterization. To address these gaps, this study examines over 100 cities in the San Francisco Bay Area. We introduce a novel metric for classifying intersections, distinguishing between different types of 3-way and 4-way intersections based on the angles formed at the intersections. Through the application of clustering algorithms in machine learning, we have identified three distinct typologies - grid, orthogonal, and organic cities - within the San Francisco Bay Area. We demonstrate the effectiveness of the metric in capturing the differences between cities based on street and intersection patterns. The typologies generated in this study could offer valuable support for city planners and policymakers in crafting a range of practical strategies tailored to the complexities of each city's road network, covering aspects such as evacuation plans, traffic signage placements, and traffic signal control.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CGLE: Class-label Graph Link Estimator for Link Prediction</title>
<link>https://arxiv.org/abs/2511.06982</link>
<guid>https://arxiv.org/abs/2511.06982</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Link Prediction, Class-label Graph Link Estimator, Semantic Information, Multi-Layer Perceptron

Summary:
Class-label Graph Link Estimator (CGLE) is introduced to enhance Graph Neural Network models by utilizing semantic information at the class level for link prediction tasks. CGLE constructs a class-conditioned link probability matrix based on ground-truth labels or pseudo-labels from clustering. This matrix is combined with structural link embeddings from a GNN and processed by a Multi-Layer Perceptron for prediction. CGLE's efficient preprocessing stage ensures no impact on the computational complexity of the underlying GNN model. Experimental results demonstrate significant performance improvements over strong baselines like NCN and NCNC, particularly achieving over 10% HR@100 improvement on homophilous datasets and over 4% MRR improvement on sparse heterophilous graphs. The integration of global semantic priors through CGLE provides a promising approach to link prediction without the need for complex model architectures.

Summary:<br /><br />Graph Neural Network, Link Prediction, Class-label Graph Link Estimator, Semantic Information, Multi-Layer Perceptron<br />CGLE enhances GNN models by incorporating class-level semantic information for link prediction tasks. It constructs a class-conditioned link probability matrix using ground-truth or pseudo-labels and combines it with structural embeddings for prediction. Despite its efficiency, CGLE outperforms baseline models like NCN and NCNC, achieving significant improvements on various datasets. This approach demonstrates the effectiveness of leveraging global semantic priors in link prediction tasks, presenting a compelling alternative to complex model architectures. <div>
arXiv:2511.06982v1 Announce Type: new 
Abstract: Link prediction is a pivotal task in graph mining with wide-ranging applications in social networks, recommendation systems, and knowledge graph completion. However, many leading Graph Neural Network (GNN) models often neglect the valuable semantic information aggregated at the class level. To address this limitation, this paper introduces CGLE (Class-label Graph Link Estimator), a novel framework designed to augment GNN-based link prediction models. CGLE operates by constructing a class-conditioned link probability matrix, where each entry represents the probability of a link forming between two node classes. This matrix is derived from either available ground-truth labels or from pseudo-labels obtained through clustering. The resulting class-based prior is then concatenated with the structural link embedding from a backbone GNN, and the combined representation is processed by a Multi-Layer Perceptron (MLP) for the final prediction. Crucially, CGLE's logic is encapsulated in an efficient preprocessing stage, leaving the computational complexity of the underlying GNN model unaffected. We validate our approach through extensive experiments on a broad suite of benchmark datasets, covering both homophilous and sparse heterophilous graphs. The results show that CGLE yields substantial performance gains over strong baselines such as NCN and NCNC, with improvements in HR@100 of over 10 percentage points on homophilous datasets like Pubmed and DBLP. On sparse heterophilous graphs, CGLE delivers an MRR improvement of over 4% on the Chameleon dataset. Our work underscores the efficacy of integrating global, data-driven semantic priors, presenting a compelling alternative to the pursuit of increasingly complex model architectures. Code to reproduce our findings is available at: https://github.com/data-iitd/cgle-icdm2025.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Past-aware game-theoretic centrality in complex contagion dynamics</title>
<link>https://arxiv.org/abs/2511.07157</link>
<guid>https://arxiv.org/abs/2511.07157</guid>
<content:encoded><![CDATA[
<div> centrality measures, game theory, collaboration, influence maximization, contagion dynamics  
Summary:  
- The paper introduces past-aware game-theoretic centrality, a method that considers the collaborative contribution of nodes in a network, including uncertain and certain collaborators.  
- It extends a general framework for computing standard game-theoretic centrality to the past-aware scenario.  
- A new heuristic is developed for influence maximization problems in complex contagion dynamics, focusing on processes that require reinforcement from multiple neighbors to spread.  
- An explicit formula for past-aware centrality score is derived, enabling scalable algorithms to identify the most influential nodes.  
- The proposed approach outperforms the standard greedy approach in both efficiency and solution quality for most cases.  

<br /><br />Summary: <div>
arXiv:2511.07157v1 Announce Type: new 
Abstract: In this paper, we introduce past-aware game-theoretic centrality, a class of centrality measures that captures the collaborative contribution of nodes in a network, accounting for both uncertain and certain collaborators. A general framework for computing standard game-theoretic centrality is extended to the past-aware case. As an application, we develop a new heuristic for different versions of the influence maximization problems in complex contagion dynamics, which models processes requiring reinforcement from multiple neighbors to spread. A computationally efficient explicit formula for the corresponding past-aware centrality score is derived, leading to scalable algorithms for identifying the most influential nodes, which in most cases outperform the standard greedy approach in both efficiency and solution quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deception Decoder: Proposing a Human-Focused Framework for Identifying AI-Generated Content on Social Media</title>
<link>https://arxiv.org/abs/2511.05555</link>
<guid>https://arxiv.org/abs/2511.05555</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, misinformation, disinformation, Deception Decoder, social media <br />
<br />
Summary: 
Generative AI (GenAI) is a significant threat to the integrity of information in the public sphere, particularly on social media platforms. The focus of research has been on automated detection methods, but concerns about false positives and biases persist. This dissertation presents the Deception Decoder, a user-friendly framework to identify AI-generated misinformation and disinformation in text, image, and video. The framework was developed through a comparative synthesis of existing models and refined with a focus group session. Initial testing shows promising results, but further research is needed to ensure its effectiveness across user groups over time. The human-centered approach of the Deception Decoder offers a different perspective on combating the spread of misleading content online. <br /><br />Summary: <div>
arXiv:2511.05555v1 Announce Type: cross 
Abstract: Generative AI (GenAI) poses a substantial threat to the integrity of information within the contemporary public sphere, which increasingly relies on social media platforms as intermediaries for news consumption. At present, most research efforts are directed toward automated and machine learning-based detection methods, despite growing concerns regarding false positives, social and political biases, and susceptibility to circumvention. This dissertation instead adopts a human-centred approach. It proposes the Deception Decoder; a multimodal, systematic, and topological framework designed to support general users in identifying AI-generated misinformation and disinformation across text, image, and video. The framework was developed through a comparative synthesis of existing models, supplemented by a content analysis of GenAI-video, and refined through a small-scale focus group session. While initial testing indicates promising improvements, further research is required to confirm its generalisability across user groups, and sustained effectiveness over time.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network and Risk Analysis of Surety Bonds</title>
<link>https://arxiv.org/abs/2511.05691</link>
<guid>https://arxiv.org/abs/2511.05691</guid>
<content:encoded><![CDATA[
<div> Surety bonds, contractor network, risk propagation, Friedkin-Johnsen model, network effects <br />
Summary: <br />
- The study focuses on surety bonds in the context of a contractor network, where project failures can occur due to incomplete obligations propagating through the network.
- A network approach is taken, modeling the contractor network as a directed graph to understand risk propagation.
- By extending the Friedkin-Johnsen model and introducing a stochastic process, the study simulates principal failures across the network.
- The theoretical analysis suggests that considering network effects leads to an increase in both average risk and right-tail risk for the surety organization.
- Data from an insurance company validates the findings, showing a 2% higher exposure when network effects are taken into account. <br /> <div>
arXiv:2511.05691v1 Announce Type: cross 
Abstract: Surety bonds are financial agreements between a contractor (principal) and obligee (project owner) to complete a project. However, most large-scale projects involve multiple contractors, creating a network and introducing the possibility of incomplete obligations to propagate and result in project failures. Typical models for risk assessment assume independent failure probabilities within each contractor. However, we take a network approach, modeling the contractor network as a directed graph where nodes represent contractors and project owners and edges represent contractual obligations with associated financial records. To understand risk propagation throughout the contractor network, we extend the celebrated Friedkin-Johnsen model and introduce a stochastic process to simulate principal failures across the network. From a theoretical perspective, we show that under natural monotonicity conditions on the contractor network, incorporating network effects leads to increases in both the average risk and the tail probability mass of the loss distribution (i.e. larger right-tail risk) for the surety organization. We further use data from a partnering insurance company to validate our findings, estimating an approximately 2% higher exposure when accounting for network effects.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design and Implementation of Data Acquisition and Analysis System for Programming Debugging Process Based On VS Code Plug-In</title>
<link>https://arxiv.org/abs/2511.05825</link>
<guid>https://arxiv.org/abs/2511.05825</guid>
<content:encoded><![CDATA[
<div> debugging, programming, data acquisition, analysis, behavior<br />
<br />
Summary: This paper presents a data acquisition and analysis system for programming debugging process using a VS Code plug-in to enhance students' debugging ability training. The system captures real-time debugging behavior in the local editor, uploads data to a platform database for monitoring and feedback, and offers precise guidance to teachers. It introduces a debugging behavior analysis model based on abstract syntax tree, node annotation, sequence recognition, and cluster analysis to track the context of students' debugging process and identify key features accurately. The system supports multi-file and multi-task debugging scenarios, improving the capturing ability of debugging data and refining data analysis. Practical teaching tests verify the system's feasibility, stability, and effectiveness in supporting procedural evaluation in programming debugging teaching, offering a new direction for debugging behavior analysis research. <div>
arXiv:2511.05825v1 Announce Type: cross 
Abstract: In order to meet the needs of students' programming debugging ability training, this paper designs and implements a data acquisition and analysis system for programming debugging process based on VS Code plug-in, which aims to solve the limitation of traditional assessment methods that are difficult to fully evaluate students' debugging ability. The system supports a variety of programming languages, integrates debugging tasks and data acquisition functions, captures students' debugging behavior in the local editor in real time, and uploads the data to the platform database to realize the whole process monitoring and feedback, provides accurate debugging guidance for teachers, and improves the teaching effect. In terms of data analysis, the system proposed a debugging behavior analysis model based on abstract syntax tree, combined with node annotation, sequence recognition and cluster analysis and other technologies, to automatically track the context of students' debugging process and accurately identify key features in the debugging path. Through this tool, the system realizes the intelligent identification and labeling of the debugging direction and behavior pattern, and improves the refinement level of debugging data analysis. In this research system, a complex debugging scenario of multi-file and multi-task is introduced into the debugging problem design, which optimizes the multi-dimensional capturing ability of debugging data and lays a foundation for accurate debugging behavior analysis. Through several practical teaching tests, the feasibility and stability of the system are verified, which proves that it can effectively support procedural evaluation in programming debugging teaching, and provides a new direction for debugging behavior analysis research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</title>
<link>https://arxiv.org/abs/2511.06402</link>
<guid>https://arxiv.org/abs/2511.06402</guid>
<content:encoded><![CDATA[
<div> Keywords: Sugar dating, social media, content detection, transformer-based framework, class imbalance<br />
Summary: <br />
The study focuses on detecting sugar dating-related content on social media, which raises concerns about intimate relationships' commercialization and normalization of transactional relationships. SugarTextNet, a transformer-based framework, is introduced to identify such posts by capturing subtle linguistic cues and addressing class imbalance. Context-Aware Focal Loss, a tailored loss function, enhances minority-class detection. Evaluation on a dataset of 3,067 Chinese social media posts shows superior performance compared to traditional and deep learning models. Ablation studies confirm the importance of each component in the framework. The research underscores the significance of domain-specific modeling for sensitive content detection and offers a robust solution for content moderation on mainstream social media platforms. <br /><br />Summary: <div>
arXiv:2511.06402v1 Announce Type: cross 
Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms</title>
<link>https://arxiv.org/abs/2511.06448</link>
<guid>https://arxiv.org/abs/2511.06448</guid>
<content:encoded><![CDATA[
<div> Keywords: collective financial fraud, large language model agents, MultiAgentFraudBench, fraud scenarios, mitigation strategies

Summary: 
The study examines the risks of collective financial fraud in large multi-agent systems driven by large language model agents. It investigates how agents can collaborate in fraudulent activities, the amplification of risks through collaboration, and factors influencing fraud success. A benchmark called MultiAgentFraudBench is introduced to simulate realistic online fraud scenarios. Key factors impacting fraud success, such as interaction depth and activity level, are analyzed. Mitigation strategies proposed include incorporating content warnings in fraudulent communications, using LLMs to monitor and block potentially malicious agents, and promoting group resilience through societal-level information sharing. The study also observes that malicious agents can adapt to interventions. The research underscores the real-world threats posed by multi-agent financial fraud and suggests practical steps to address them. The code for the study is available on GitHub at https://github.com/zheng977/MutiAgent4Fraud. 

<br /><br />Summary: <div>
arXiv:2511.06448v1 Announce Type: cross 
Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity</title>
<link>https://arxiv.org/abs/2511.06568</link>
<guid>https://arxiv.org/abs/2511.06568</guid>
<content:encoded><![CDATA[
<div> Keywords: link prediction, fairness, graph machine learning, bias mitigation, demographic parity

Summary:<br /><br />Link prediction is a crucial task in graph machine learning, but biases in predictions can worsen societal inequalities. Existing fairness measures based on demographic parity may overlook disparities across subgroups. Furthermore, demographic parity may not be suitable for ranking tasks like link prediction. In this study, the limitations of current fairness evaluations are formalized, and a new framework is proposed for a more comprehensive assessment. A post-processing method combined with separate link predictors is suggested as an effective way to mitigate bias and achieve optimal fairness-utility trade-offs. This approach improves fairness in link prediction tasks and sets new standards in addressing bias in machine learning models. <div>
arXiv:2511.06568v1 Announce Type: cross 
Abstract: Link prediction is a fundamental task in graph machine learning with applications, ranging from social recommendation to knowledge graph completion. Fairness in this setting is critical, as biased predictions can exacerbate societal inequalities. Prior work adopts a dyadic definition of fairness, enforcing fairness through demographic parity between intra-group and inter-group link predictions. However, we show that this dyadic framing can obscure underlying disparities across subgroups, allowing systemic biases to go undetected. Moreover, we argue that demographic parity does not meet desired properties for fairness assessment in ranking-based tasks such as link prediction. We formalize the limitations of existing fairness evaluations and propose a framework that enables a more expressive assessment. Additionally, we propose a lightweight post-processing method combined with decoupled link predictors that effectively mitigates bias and achieves state-of-the-art fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Collaborative Model for Improving Information Sharing among Cancer Care Groups using Software Engineering Principles</title>
<link>https://arxiv.org/abs/2511.06885</link>
<guid>https://arxiv.org/abs/2511.06885</guid>
<content:encoded><![CDATA[
<div> GitHub, cancer, information sharing, software engineering, care groups
Summary:
The article discusses the challenges in cancer treatment due to delays and miscommunication among care groups. It proposes a model for information sharing based on software engineering principles, inspired by GitHub's version control system. By involving all stakeholders, including patient caretakers and administrators, the model aims to reduce delays and improve coordination in cancer case management. Using AnyLogic simulation software, the model shows that principles from software engineering can be applied to enhance collaboration and information sharing. This approach can lead to early diagnosis, improve treatment outcomes, and increase the chances of patient survival in cancer care. <div>
arXiv:2511.06885v1 Announce Type: cross 
Abstract: Effective treatment of cancer requires early diagnosis which involves the patient's awareness of the early signs and symptoms, leading to a consultation with a health provider, who would then promptly refer the patient for confirmation of the diagnosis and thereafter treatment. However, this is not always the case because of delays arising from limited skilled manpower and health information management systems that are neither integrated nor organized in their design hence leading to information gap among care groups. Existing methods focus on using accumulated data to support decision making, enhancing the sharing of secondary data while others exclude some critical stakeholders like patient caretakers and administrators thus, leaving an information gap that creates delays and miscommunication during case management. We however notice some similarities between cancer treatment and software engineering information management especially when progress history needs to be maintained (versioning).
  We analyze the similarities and propose a model for information sharing among cancer care groups using the software engineering principles approach. We model for reducing delays and improving coordination among care groups in cancer case management. Model design was guided by software engineering principles adopted in GitHub version control system for bug fixing in open-source code projects. Any-Logic simulation software was used to mimic the model realism in a virtual environment. Results show that bug resolution principles from software engineering and GitHub version control system can be adopted to coordinate collaboration and information sharing among care groups in a cancer case management environment while involving all stakeholders to improve care treatment outcomes, ensure early diagnosis and increase patient's survival chances.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization</title>
<link>https://arxiv.org/abs/2511.06937</link>
<guid>https://arxiv.org/abs/2511.06937</guid>
<content:encoded><![CDATA[
<div> Diffusion models, ReFiT framework, Reinforcement learning, recommendation systems, collaborative filtering<br />
Summary:<br />
ReFiT introduces a new framework for recommender systems that combines diffusion models with Reinforcement Learning (RL) fine-tuning. Unlike previous RL approaches, ReFiT uses a task-aligned design to optimize recommendation quality. By formulating the denoising trajectory as a Markov decision process (MDP) and integrating a collaborative signal-aware reward function, ReFiT maximizes exact log-likelihood of user-item interactions. Experimental results on real-world datasets show that ReFiT outperforms competitors by up to 36.3% in sequential recommendation, exhibits linear complexity in user/item numbers, and generalizes well across different recommendation scenarios. The framework's source code and datasets are publicly available for further research. <br /> <div>
arXiv:2511.06937v1 Announce Type: cross 
Abstract: Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Policy Effects through Opinion Dynamics and Network Sampling</title>
<link>https://arxiv.org/abs/2501.08150</link>
<guid>https://arxiv.org/abs/2501.08150</guid>
<content:encoded><![CDATA[
<div> social dynamics, policy effectiveness, network topology, opinion change, policymaker decisions

Summary: 
This paper explores how social dynamics influence public opinion towards new policies and how policymakers can quantify these effects. By considering different scenarios of policy revelation within a population network, the study examines how opinions evolve through discussions and debates. The research evaluates the impact of network topology and social interactions on policy beliefs, using the Wasserstein distance as a measure. Through numerical analyses on both simulated and real-life network datasets, the paper aims to provide policymakers with a quantitative framework for assessing policy effectiveness amidst complex network structures and resource constraints. <div>
arXiv:2501.08150v2 Announce Type: replace 
Abstract: In the process of enacting or introducing a new policy, policymakers frequently consider the population's responses. These considerations are critical for effective governance. There are numerous methods to gauge the ground sentiment from a subset of the population; examples include surveys or listening to various feedback channels. Many conventional approaches implicitly assume that opinions are static; however, in reality, the population will discuss and debate these new policies among themselves, and reform new opinions in the process. In this paper, we pose the following questions: Can we quantify the effect of these social dynamics on the broader opinion towards a new policy? Given some information about the relationship network that underlies the population, how does overall opinion change post-discussion? We investigate three different settings in which the policy is revealed: respondents who do not know each other, groups of respondents who all know each other, and respondents chosen randomly. By controlling who the policy is revealed to, we control the degree of discussion among the population. We quantify how these factors affect the changes in policy beliefs via the Wasserstein distance between the empirically observed data post-discussion and its distribution pre-discussion. We also provide several numerical analyses based on generated network and real-life network datasets. Our work aims to address the challenges associated with network topology and social interactions, and provide policymakers with a quantitative lens to assess policy effectiveness in the face of resource constraints and network complexities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Cross-type Homophily of Heterogeneous Graphs: Understanding and Unleashing</title>
<link>https://arxiv.org/abs/2501.14600</link>
<guid>https://arxiv.org/abs/2501.14600</guid>
<content:encoded><![CDATA[
<div> Homophily, network science, graph neural networks, heterogeneous graphs, Cross-Type Homophily Ratio<br />
Summary:<br />
The study focuses on homophily in heterogeneous graphs and its impact on graph neural networks. Traditional homophily metrics are inadequate for heterogeneous graphs due to diverse node types. The Cross-Type Homophily Ratio (CHR) is introduced to measure homophily based on similarity across different node types. Cross-Type Homophily-guided Graph Editing (CTHGE) optimizes cross-type connectivity in heterogeneous graph neural networks (HGNNs) using CHR. Experiments on five datasets validate CTHGE's effectiveness, showing over 25% improvement in HGNN performance on node classification tasks. This approach provides valuable insights into the role of cross-type homophily in learning from heterogeneous graphs.<br /> <div>
arXiv:2501.14600v2 Announce Type: replace 
Abstract: Homophily, the tendency of similar nodes to connect, is a fundamental phenomenon in network science and a critical factor in the performance of graph neural networks (GNNs). While existing studies primarily explore homophily in homogeneous graphs, where nodes share the same type, real-world networks are often more accurately modeled as heterogeneous graphs (HGs) with diverse node types and intricate cross-type interactions. This structural diversity complicates the analysis of homophily, as traditional homophily metrics fail to account for distinct label spaces across node types. To address this limitation, we introduce the Cross-Type Homophily Ratio (CHR), a novel metric that quantifies homophily based on the similarity of target information across different node types. Additionally, we propose Cross-Type Homophily-guided Graph Editing (CTHGE), a novel method for improving heterogeneous graph neural networks (HGNNs) performance by optimizing cross-type connectivity using Cross-Type Homophily Ratio. Extensive experiments on five HG datasets with nine HGNNs validate the effectiveness of CTHGE, which delivers a maximum relative performance improvement of over 25% for HGNNs on node classification tasks, offering a fresh perspective on cross-type homophily in HGs learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindVote: When AI Meets the Wild West of Social Media Opinion</title>
<link>https://arxiv.org/abs/2505.14422</link>
<guid>https://arxiv.org/abs/2505.14422</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large Language Models, Public Opinion, Social Media, MindVote

Summary:
The article introduces MindVote, a benchmark for predicting public opinion distribution using social media discourse. It addresses the limitations of current benchmarks that rely on structured surveys by incorporating the dynamic, context-rich nature of social media environments. MindVote is built from naturalistic polls from Reddit and Weibo, covering 23 topics with annotations for platform, topical, and temporal context. The benchmark evaluates 15 Large Language Models (LLMs) for their ability to predict public opinion in authentic social media settings. By shifting the evaluation paradigm from surveys to social media data, MindVote provides a more ecologically valid framework for testing LLMs and advancing the development of socially intelligent AI systems. <div>
arXiv:2505.14422v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used as scalable tools for pilot testing, predicting public opinion distributions before deploying costly surveys. To serve as effective pilot testing tools, the performance of these LLMs is typically benchmarked against their ability to reproduce the outcomes of past structured surveys. This evaluation paradigm, however, is misaligned with the dynamic, context-rich social media environments where public opinion is increasingly formed and expressed. By design, surveys strip away the social, cultural, and temporal context that shapes public opinion, and LLM benchmarks built on this paradigm inherit these critical limitations. To bridge this gap, we introduce MindVote, the first benchmark for public opinion distribution prediction grounded in authentic social media discourse. MindVote is constructed from 3,918 naturalistic polls sourced from Reddit and Weibo, spanning 23 topics and enriched with detailed annotations for platform, topical, and temporal context. Using this benchmark, we conduct a comprehensive evaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework to move beyond survey-based evaluations and advance the development of more socially intelligent AI systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Prosocial Behavior Theory in LLM Agents under Policy-Induced Inequities</title>
<link>https://arxiv.org/abs/2505.15857</link>
<guid>https://arxiv.org/abs/2505.15857</guid>
<content:encoded><![CDATA[
<div> Simulation framework, prosocial behavior, large language models, social conditions, fairness-based punishment

Summary:
ProSim is introduced as a simulation framework to model prosocial behavior in Large Language Model (LLM) agents in various social contexts. The study conducted three progressive studies to evaluate prosocial alignment. Firstly, LLM agents exhibit human-like prosocial behavior and respond to normative policy interventions. Secondly, agents engage in fairness-based third-party punishment and respond to variations in inequity magnitude and enforcement cost. Thirdly, policy-induced inequities suppress prosocial behavior and contribute to norm erosion in social networks. This research enhances understanding of how institutional dynamics influence the development, decline, and spread of prosocial norms in societies driven by autonomous agents.<br /><br />Summary: <div>
arXiv:2505.15857v2 Announce Type: replace 
Abstract: As large language models (LLMs) increasingly operate as autonomous agents in social contexts, evaluating their capacity for prosocial behavior is both theoretically and practically critical. However, existing research has primarily relied on static, economically framed paradigms, lacking models that capture the dynamic evolution of prosociality and its sensitivity to structural inequities. To address these gaps, we introduce ProSim, a simulation framework for modeling the prosocial behavior in LLM agents across diverse social conditions. We conduct three progressive studies to assess prosocial alignment. First, we demonstrate that LLM agents can exhibit human-like prosocial behavior across a broad range of real-world scenarios and adapt to normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate norm erosion through social networks. These findings advance prosocial behavior theory by elucidating how institutional dynamics shape the emergence, decay, and diffusion of prosocial norms in agent-driven societies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curse of Shared Knowledge: Recursive Belief Reasoning in a Coordination Game with Imperfect Information</title>
<link>https://arxiv.org/abs/2008.08849</link>
<guid>https://arxiv.org/abs/2008.08849</guid>
<content:encoded><![CDATA[
<div> Common Knowledge, Shared Knowledge, Coordination, Belief Attribution, Experiment<br />
Summary:<br />
- Common knowledge is crucial for successful group coordination.
- Humans often rely on attributing beliefs and intentions to infer shared knowledge.
- However, attributing shared knowledge is limited in depth, leading to coordination failures.
- Three experiments with 802 participants examined the distinction between common knowledge and nth-order shared knowledge.
- The results reveal that participants struggle to differentiate between common knowledge and shared knowledge, often prioritizing coordination even at shallow depths of shared knowledge despite significant penalties. <div>
arXiv:2008.08849v2 Announce Type: replace-cross 
Abstract: Common knowledge is a necessary condition for safe group coordination. When common knowledge can not be obtained, humans routinely use their ability to attribute beliefs and intentions in order to infer what is known. But such shared knowledge attributions are limited in depth and therefore prone to coordination failures, because any finite-order knowledge attribution allows for an even higher order attribution that may change what is known by whom. In three separate experiments we investigate to which degree human participants (N=802) are able to recognize the difference between common knowledge and nth-order shared knowledge. We use a new two-person coordination game with imperfect information that is able to cast the recursive game structure and higher-order uncertainties into a simple, everyday-like setting. Our results show that participants have a very hard time accepting the fact that common knowledge is not reducible to shared knowledge. Instead, participants try to coordinate even at the shallowest depths of shared knowledge and in spite of huge payoff penalties.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ginzburg--Landau Functionals in the Large-Graph Limit</title>
<link>https://arxiv.org/abs/2408.00422</link>
<guid>https://arxiv.org/abs/2408.00422</guid>
<content:encoded><![CDATA[
<div> Keywords: Ginzburg-Landau functionals, large-graph limits, graph clustering, graphons, total-variation functional

Summary: 
Ginzburg-Landau (GL) functionals on graphs, a relaxation of graph-cut functionals, are studied in the context of large-graph limits. By viewing graphs as nonlocal kernels, the authors analyze GL functionals on sequences of growing graphs converging to graphons. The graph GL functional converges to a continuous and nonlocal graphon GL functional. Relationships between sharp-interface limits of the functionals and a nonlocal total-variation functional are explored. The limiting GL functional is expressed using Young measures, providing a probabilistic interpretation in the large-graph limit. Additionally, examples of GL minimizers for various graphon families are determined to aid in understanding the graphon GL functional.

<br /><br />Summary: Ginzburg-Landau functionals on graphs, a relaxation of graph-cut functionals, are studied in the context of large-graph limits. By viewing graphs as nonlocal kernels, the authors analyze GL functionals on sequences of growing graphs converging to graphons. The graph GL functional converges to a continuous and nonlocal graphon GL functional. Relationships between sharp-interface limits of the functionals and a nonlocal total-variation functional are explored. The limiting GL functional is expressed using Young measures, providing a probabilistic interpretation in the large-graph limit. Additionally, examples of GL minimizers for various graphon families are determined to aid in understanding the graphon GL functional. <div>
arXiv:2408.00422v2 Announce Type: replace-cross 
Abstract: Ginzburg--Landau (GL) functionals on graphs, which are relaxations of graph-cut functionals on graphs, have yielded a variety of insights in image segmentation and graph clustering. In this paper, we study large-graph limits of GL functionals by taking a functional-analytic view of graphs as nonlocal kernels. For a graph $W_n$ with $n$ nodes, the corresponding graph GL functional $\GL^{W_n}_\ep$ is an energy for functions on $W_n$. We minimize GL functionals on sequences of growing graphs that converge to functions called graphons. For such sequences of graphs, we show that the graph GL functional $\Gamma$-converges to a continuous and nonlocal functional that we call the \emph{graphon GL functional}. We also investigate the sharp-interface limits of the graph GL and graphon GL functionals, and we relate these limits to a nonlocal total-variation (TV) functional. We express the limiting GL functional in terms of Young measures and thereby obtain a probabilistic interpretation of the variational problem in the large-graph limit. Finally, to develop intuition about the graphon GL functional, we determine the GL minimizer for several example families of graphons.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Vulnerabilities With Agent Personas</title>
<link>https://arxiv.org/abs/2511.04697</link>
<guid>https://arxiv.org/abs/2511.04697</guid>
<content:encoded><![CDATA[
<div> Keywords: Disinformation campaigns, Agent-based simulation, Large Language Models, Information responses, Mental schemas

Summary:
Large Language Models (LLMs) are used in an agent-based simulation to model responses to misinformation in different populations. Agent personas from various professions and mental schemas are created to evaluate reactions to news headlines. The study shows that LLM-generated agents closely align with ground-truth labels and human predictions, supporting their use in analyzing information responses. Mental schemas have a greater impact on how agents interpret misinformation compared to professional backgrounds. This research validates the use of LLMs as agents in analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.<br /><br />Summary: Large Language Models are employed in an agent-based simulation to study responses to misinformation across various populations. The study validates the use of LLMs as proxies for analyzing information responses and highlights the influence of mental schemas on misinformation interpretation, surpassing the impact of professional backgrounds. This research lays the foundation for further exploration of trust, polarization, and susceptibility to deceptive content in intricate social systems. <div>
arXiv:2511.04697v1 Announce Type: new 
Abstract: Disinformation campaigns can distort public perception and destabilize institutions. Understanding how different populations respond to information is crucial for designing effective interventions, yet real-world experimentation is impractical and ethically challenging. To address this, we develop an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. We construct agent personas spanning five professions and three mental schemas, and evaluate their reactions to news headlines. Our findings show that LLM-generated agents align closely with ground-truth labels and human predictions, supporting their use as proxies for studying information responses. We also find that mental schemas, more than professional background, influence how agents interpret misinformation. This work provides a validation of LLMs to be used as agents in an agent-based model of an information network for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Constrained Private Decentralized Online Personalized Mean Estimation</title>
<link>https://arxiv.org/abs/2511.04702</link>
<guid>https://arxiv.org/abs/2511.04702</guid>
<content:encoded><![CDATA[
<div> consensus-based algorithm, privacy constraint, collaborative personalized mean estimation, differential privacy, communication restriction

Summary:
The article addresses the problem of collaborative personalized mean estimation under a privacy constraint in an environment where agents receive data from unknown distributions. A consensus-based algorithm is proposed to protect data privacy while achieving convergence. The theoretical analysis shows that collaboration leads to faster convergence compared to a fully local approach. This advantage is observed under an oracle decision rule and with specific constraints on privacy level and connectivity between agents. The study highlights the benefits of private collaboration in an online setting with communication restrictions. Numerical results support the theoretical guarantee of faster-than-local convergence provided by the proposed algorithm. <div>
arXiv:2511.04702v1 Announce Type: new 
Abstract: We consider the problem of communication-constrained collaborative personalized mean estimation under a privacy constraint in an environment of several agents continuously receiving data according to arbitrary unknown agent-specific distributions. A consensus-based algorithm is studied under the framework of differential privacy in order to protect each agent's data. We give a theoretical convergence analysis of the proposed consensus-based algorithm for any bounded unknown distributions on the agents' data, showing that collaboration provides faster convergence than a fully local approach where agents do not share data, under an oracle decision rule and under some restrictions on the privacy level and the agents' connectivity, which illustrates the benefit of private collaboration in an online setting under a communication restriction on the agents. The theoretical faster-than-local convergence guarantee is backed up by several numerical results.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</title>
<link>https://arxiv.org/abs/2511.04712</link>
<guid>https://arxiv.org/abs/2511.04712</guid>
<content:encoded><![CDATA[
<div> Attribute-augmented conductance, neural community search, deep learning, community detection, graph optimization <br />
Summary: <br />
- The paper introduces NCSAC, a novel approach that integrates deep learning with rule-based constraints to enhance community search.
- NCSAC combines structural proximity, attribute similarity, and reinforcement learning to identify locally dense communities.
- It proposes the concept of attribute-augmented conductance to refine candidate communities effectively.
- Extensive experiments on real-world graphs demonstrate the superiority of NCSAC in terms of accuracy, efficiency, and scalability.
- The proposed solution outperforms state-of-the-art methods, achieving significant F1-score improvements ranging from 5.3% to 42.4%. <br /> <div>
arXiv:2511.04712v1 Announce Type: new 
Abstract: Identifying locally dense communities closely connected to the user-initiated query node is crucial for a wide range of applications. Existing approaches either solely depend on rule-based constraints or exclusively utilize deep learning technologies to identify target communities. Therefore, an important question is proposed: can deep learning be integrated with rule-based constraints to elevate the quality of community search? In this paper, we affirmatively address this question by introducing a novel approach called Neural Community Search via Attribute-augmented Conductance, abbreviated as NCSAC. Specifically, NCSAC first proposes a novel concept of attribute-augmented conductance, which harmoniously blends the (internal and external) structural proximity and the attribute similarity. Then, NCSAC extracts a coarse candidate community of satisfactory quality using the proposed attribute-augmented conductance. Subsequently, NCSAC frames the community search as a graph optimization task, refining the candidate community through sophisticated reinforcement learning techniques, thereby producing high-quality results. Extensive experiments on six real-world graphs and ten competitors demonstrate the superiority of our solutions in terms of accuracy, efficiency, and scalability. Notably, the proposed solution outperforms state-of-the-art methods, achieving an impressive F1-score improvement ranging from 5.3\% to 42.4\%. For reproducibility purposes, the source code is available at https://github.com/longlonglin/ncsac.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoo of Centralities: Encyclopedia of Node Metrics in Complex Networks</title>
<link>https://arxiv.org/abs/2511.05122</link>
<guid>https://arxiv.org/abs/2511.05122</guid>
<content:encoded><![CDATA[
<div> Keywords: Centrality, Network Science, Complex Systems, Centrality Measures, Centrality Zoo <br />
Summary: 
Centrality is a crucial concept in network science, used to understand the structure and dynamics of various complex systems. However, the lack of a universal definition has led to the development of numerous centrality measures, creating a "zoo" of metrics each capturing node importance in different ways. This diversity has resulted in challenges such as discoverability, redundancy, naming conflicts, validation, and accessibility. To address these issues, a catalog of over 400 centrality measures has been compiled, with detailed descriptions and references to original sources. The Centrality Zoo website offers an interactive platform for exploring, comparing, and implementing these measures. This effort represents a systematic and comprehensive approach to organizing and presenting centrality measures, providing a valuable resource for researchers in the field.<br /><br />Summary: <div>
arXiv:2511.05122v1 Announce Type: new 
Abstract: Centrality is a fundamental concept in network science, providing critical insights into the structure and dynamics of complex systems such as social, transportation, biological and financial networks. Despite its extensive use, there is no universally accepted definition of centrality, leading to the development of a vast array of distinct centrality measures. These measures have grown so numerous that they resemble a 'zoo', each representing a unique approach to capturing node importance within a network. However, the increasing number of metrics being developed has led to several challenges, including issues of discoverability, redundancy, naming conflicts, validation and accessibility. This work aims to address these challenges by providing a comprehensive catalog of over 400 centrality measures, along with clear descriptions and references to original sources. While not exhaustive, this compilation represents the most extensive and systematic effort to date in organizing and presenting centrality measures. We also encourage readers to explore and contribute to the Centrality Zoo website at https://centralityzoo.github.io/, which provides an interactive platform for discovering, comparing and implementing centrality measures.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperation Under Network-Constrained Communication</title>
<link>https://arxiv.org/abs/2511.05290</link>
<guid>https://arxiv.org/abs/2511.05290</guid>
<content:encoded><![CDATA[
<div> communication, cooperation, distributed games, network topology, Prisoner's Dilemma

Summary:
In this paper, the focus is on studying cooperation in distributed games with network-constrained communication. A sufficient condition for cooperative equilibrium is derived based on the network topology delaying communication between agents. Each player deploys agents at various locations, engaging in local Prisoner's Dilemma interactions. The derived condition hinges on the network diameter and the number of locations, exploring scenarios of instantaneous, delayed, and proportionally delayed communication. The study also delves into scale-free communication networks where the network diameter grows sub-linearly with the number of locations. These analyses provide insights into how communication latency and network design influence the emergence of distributed cooperation. <div>
arXiv:2511.05290v1 Announce Type: cross 
Abstract: In this paper, we study cooperation in distributed games under network-constrained communication. Building on the framework of Monderer and Tennenholtz (1999), we derive a sufficient condition for cooperative equilibrium in settings where communication between agents is delayed by the underlying network topology. Each player deploys an agent at every location, and local interactions follow a Prisoner's Dilemma structure. We derive a sufficient condition that depends on the network diameter and the number of locations, and analyze extreme cases of instantaneous, delayed, and proportionally delayed communication. We also discuss the asymptotic case of scale-free communication networks, in which the network diameter grows sub-linearly in the number of locations. These insights clarify how communication latency and network design jointly determine the emergence of distributed cooperation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Listening Between the Lines: Decoding Podcast Narratives with Language Modeling</title>
<link>https://arxiv.org/abs/2511.05310</link>
<guid>https://arxiv.org/abs/2511.05310</guid>
<content:encoded><![CDATA[
<div> Podcasts, narrative frames, automated analysis, BERT model, discourse trends <br />
<br />
Summary: This paper explores the use of narrative frames in analyzing podcast data to understand how they influence public opinion. The conversational nature of podcasts poses a challenge for automated analysis, as existing models struggle to capture subtle narrative cues. To address this, the study develops a fine-tuned BERT model that links narrative frames to specific entities mentioned in conversations, enhancing the analysis of podcast narratives at scale. The novel frame-labeling methodology aligns closely with human judgment for messy, conversational data. The analysis reveals a systematic relationship between discussed topics and how they are presented, providing a robust framework for studying influence in digital media. This research contributes to a better understanding of how podcasts shape contemporary discourse and offers a new approach to analyzing narrative structures in digital media.<br /> <div>
arXiv:2511.05310v1 Announce Type: cross 
Abstract: Podcasts have become a central arena for shaping public opinion, making them a vital source for understanding contemporary discourse. Their typically unscripted, multi-themed, and conversational style offers a rich but complex form of data. To analyze how podcasts persuade and inform, we must examine their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant challenge for automated analysis. We show that existing large language models, typically trained on more structured text such as news articles, struggle to capture the subtle cues that human listeners rely on to identify narrative frames. As a result, current approaches fall short of accurately analyzing podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that explicitly links narrative frames to specific entities mentioned in the conversation, effectively grounding the abstract frame in concrete details. Our approach then uses these granular frame labels and correlates them with high-level topics to reveal broader discourse trends. The primary contributions of this paper are: (i) a novel frame-labeling methodology that more closely aligns with human judgment for messy, conversational data, and (ii) a new analysis that uncovers the systematic relationship between what is being discussed (the topic) and how it is being presented (the frame), offering a more robust framework for studying influence in digital media.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting narrative signals from public discourse: a network-based approach</title>
<link>https://arxiv.org/abs/2411.00702</link>
<guid>https://arxiv.org/abs/2411.00702</guid>
<content:encoded><![CDATA[
<div> extract, analyze, political narratives, digital textual corpora, graph-based formalism <br />
Summary: 
The article introduces a graph-based formalism and machine-guided method for extracting and analyzing political narratives from digital textual corpora using Abstract Meaning Representation (AMR). It emphasizes the importance of narratives in understanding societal issues such as polarization and misinformation. The proposed method involves extracting the meaning of sentences in the corpus using AMR and filtering graph representations for actors, relationships, events, and perspectivization. These core narrative signals are then reconstructed to reveal underlying political narratives through a combination of distant and close reading. The approach is demonstrated through a case study on State of the European Union addresses from 2010 to 2023. The method aims to provide researchers with a systematic way to analyze and surface political narratives from public discourse. <br /><br /> <div>
arXiv:2411.00702v2 Announce Type: replace-cross 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, transcripts of parliamentary debates, and political manifestos on party websites. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors and their relationships, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that allude to larger political narratives. By systematically analyzing and re-assembling these signals into networks that guide the researcher to the relevant parts of the text, the underlying narratives can be reconstructed through a combination of distant and close reading. A case study of State of the European Union addresses (2010 -- 2023) demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Equitable AI: Evaluating Cultural Expressiveness in LLMs for Latin American Contexts</title>
<link>https://arxiv.org/abs/2511.04090</link>
<guid>https://arxiv.org/abs/2511.04090</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Latin America, Bias, Dataset, Cultural Awareness

Summary:
This paper examines the bias in Artificial Intelligence (AI) systems towards economically advanced regions, which marginalize economically developing regions like Latin America due to imbalanced datasets. The dominance of English over Spanish, Portuguese, and indigenous languages in AI systems perpetuates biases and frames Latin American perspectives through a Western lens. To address this issue, the authors introduce a culturally aware dataset rooted in Latin American history and socio-political contexts to challenge Eurocentric models. Evaluating six language models on questions testing cultural context awareness, they find disparities in capturing Latin American perspectives and significant sentiment misalignment. Fine-tuning Mistral-7B with the culturally aware dataset improves its cultural expressiveness by 42.9%, advancing equitable AI development. The authors advocate for equitable AI development by prioritizing datasets that reflect Latin American history, indigenous knowledge, and diverse languages, while emphasizing community-centered approaches to amplify marginalized voices.

Summary: <div>
arXiv:2511.04090v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems often reflect biases from economically advanced regions, marginalizing contexts in economically developing regions like Latin America due to imbalanced datasets. This paper examines AI representations of diverse Latin American contexts, revealing disparities between data from economically advanced and developing regions. We highlight how the dominance of English over Spanish, Portuguese, and indigenous languages such as Quechua and Nahuatl perpetuates biases, framing Latin American perspectives through a Western lens. To address this, we introduce a culturally aware dataset rooted in Latin American history and socio-political contexts, challenging Eurocentric models. We evaluate six language models on questions testing cultural context awareness, using a novel Cultural Expressiveness metric, statistical tests, and linguistic analyses. Our findings show that some models better capture Latin American perspectives, while others exhibit significant sentiment misalignment (p < 0.001). Fine-tuning Mistral-7B with our dataset improves its cultural expressiveness by 42.9%, advancing equitable AI development. We advocate for equitable AI by prioritizing datasets that reflect Latin American history, indigenous knowledge, and diverse languages, while emphasizing community-centered approaches to amplify marginalized voices.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI Tools</title>
<link>https://arxiv.org/abs/2511.04453</link>
<guid>https://arxiv.org/abs/2511.04453</guid>
<content:encoded><![CDATA[
<div> APIs, GitHub stars, Hacker News, machine learning models, open-source projects
<br />
Summary: 
Social news platforms, particularly Hacker News, play a crucial role in the launch of open-source projects, but quantifying their immediate impact can be challenging. A reproducible demonstration system has been developed to track the relationship between Hacker News exposure and GitHub star growth for AI and LLM tools. Analyzing 138 repository launches from 2024-2025, the study shows significant effects: repositories typically gain 121 stars within 24 hours, 189 stars within 48 hours, and 289 stars within a week of being featured on Hacker News. Machine learning models and non-linear approaches are employed to identify key predictors of viral growth, highlighting the importance of posting timing. Contrary to expectations, the "Show HN" tag does not offer a statistical advantage after accounting for other factors. The study's automated framework allows for quick data collection, model training, and visualization generation, making it easily reproducible and extendable to other platforms. <div>
arXiv:2511.04453v1 Announce Type: new 
Abstract: Social news platforms have become key launch outlets for open-source projects, especially Hacker News (HN), though quantifying their immediate impact remains challenging. This paper presents a reproducible demonstration system that tracks how HN exposure translates into GitHub star growth for AI and LLM tools. Built entirely on public APIs, our pipeline analyzes 138 repository launches from 2024-2025 and reveals substantial launch effects: repositories gain an average of 121 stars within 24 hours, 189 stars within 48 hours, and 289 stars within a week of HN exposure. Through machine learning models (Elastic Net) and non-linear approaches (Gradient Boosting), we identify key predictors of viral growth. Posting timing appears as key factor--launching at optimal hours can mean hundreds of additional stars--while the "Show HN" tag shows no statistical advantage after controlling for other factors. The demonstration completes in under five minutes on standard hardware, automatically collecting data, training models, and generating visualizations through single-file scripts. This makes our findings immediately reproducible and the framework easily be extended to other platforms, providing both researchers and developers with actionable insights into launch dynamics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLM-based agents for social science research: insights from citation network simulations</title>
<link>https://arxiv.org/abs/2511.03758</link>
<guid>https://arxiv.org/abs/2511.03758</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, CiteAgent framework, citation networks, LLM-SE, LLM-LE

Summary: The study focuses on exploring the social attributes of Large Language Models (LLMs) in social simulation using the CiteAgent framework. The framework generates citation networks based on human behavior simulation with LLM-based agents, replicating real-world citation network phenomena like power-law distribution and citational distortion. Two research paradigms, LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment), are introduced to analyze citation network phenomena and challenge existing theories. The study extends traditional science of science studies through idealized social experiments, providing insights for real-world academic environments. Through rigorous analyses and realistic simulations, the potential of LLMs in advancing science of science research in social science is demonstrated. <br /><br />Summary: The study introduces the CiteAgent framework for social simulation using LLMs, replicating real-world citation network phenomena. It establishes research paradigms LLM-SE and LLM-LE to analyze and challenge existing theories, extending the scope of science of science studies. <div>
arXiv:2511.03758v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) demonstrates their potential to encapsulate the logic and patterns inherent in human behavior simulation by leveraging extensive web data pre-training. However, the boundaries of LLM capabilities in social simulation remain unclear. To further explore the social attributes of LLMs, we introduce the CiteAgent framework, designed to generate citation networks based on human-behavior simulation with LLM-based agents. CiteAgent successfully captures predominant phenomena in real-world citation networks, including power-law distribution, citational distortion, and shrinking diameter. Building on this realistic simulation, we establish two LLM-based research paradigms in social science: LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment). These paradigms facilitate rigorous analyses of citation network phenomena, allowing us to validate and challenge existing theories. Additionally, we extend the research scope of traditional science of science studies through idealized social experiments, with the simulation experiment results providing valuable insights for real-world academic environments. Our work demonstrates the potential of LLMs for advancing science of science research in social science.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistics of leaves in growing random trees</title>
<link>https://arxiv.org/abs/2511.04085</link>
<guid>https://arxiv.org/abs/2511.04085</guid>
<content:encoded><![CDATA[
<div> leaves, vertex, degree, distribution, trees
Summary:
- The study focuses on the role of leaves in graph structure, particularly in sparsely connected settings where they are prevalent.
- It introduces the concept of leaf degree, the number of leaves a vertex is connected to, and analyzes the associated leaf degree distribution.
- The research investigates the leaf degree distribution of random recursive trees (RRTs) and trees grown using a leaf-based preferential attachment mechanism.
- In RRTs, the leaf degree distribution decays factorially, contrasting with the purely geometric degree distribution.
- A one-parameter leaf-based growth model is proposed where new vertices attach to existing vertices based on their leaf degree, leading to different types of leaf degree distributions based on the attachment rate parameter a. 
<br /><br />Summary: <div>
arXiv:2511.04085v1 Announce Type: cross 
Abstract: Leaves, i.e., vertices of degree one, can play a significant role in graph structure, especially in sparsely connected settings in which leaves often constitute the largest fraction of vertices. We consider a leaf-based counterpart of the degree, namely, the leaf degree -- the number of leaves a vertex is connected to -- and the associated leaf degree distribution, analogous to the degree distribution. We determine the leaf degree distribution of random recursive trees (RRTs) and trees grown via a leaf-based preferential attachment mechanism that we introduce. The RRT leaf degree distribution decays factorially, in contrast with its purely geometric degree distribution. In the one-parameter leaf-based growth model, each new vertex attaches to an existing vertex with rate $\ell$ + a, where $\ell$ is the leaf degree of the existing vertex, and a > 0. The leaf degree distribution has a powerlaw tail when 0 < a < 1 and an exponential tail (with algebraic prefactor) for a > 1. The critical case of a = 1 has a leaf degree distribution with stretched exponential tail. We compute a variety of additional characteristics in these models and conjecture asymptotic equivalence of degree and leaf degree powerlaw tail exponent in the scale free regime. We highlight several avenues of possible extension for future studies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Turing Test Reveals Systematic Differences Between Human and AI Language</title>
<link>https://arxiv.org/abs/2511.04195</link>
<guid>https://arxiv.org/abs/2511.04195</guid>
<content:encoded><![CDATA[
<div> validation, language models, human-likeness, calibration, text generation

Summary:<br /><br />
The paper introduces a computational Turing test to assess the realism of Large Language Models (LLMs) in generating human-like text. It combines aggregate metrics and linguistic features to evaluate LLMs' performance in approximating human language within specific datasets. The study compares nine LLMs using various calibration strategies on social media platforms. The findings challenge existing assumptions, showing that even calibrated LLMs are distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models perform poorly compared to base models, and increasing model size does not improve human-likeness. The research highlights a trade-off between optimizing for human-likeness and maintaining semantic fidelity. These results provide a valuable framework for validating and calibrating LLM simulations while cautioning about their current limitations in capturing human communication. <div>
arXiv:2511.04195v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-training</title>
<link>https://arxiv.org/abs/2407.03953</link>
<guid>https://arxiv.org/abs/2407.03953</guid>
<content:encoded><![CDATA[
<div> Transformer-based, PGT, graph pre-training, inductive ability, scalability <br />
<br />
Summary: 
The article introduces a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer) aiming to develop a general graph pre-trained model with inductive ability for web-scale graphs. The framework utilizes a masked autoencoder architecture with two pre-training tasks: reconstructing node features and local structures. Unlike traditional approaches, they propose a novel strategy that uses the decoder for feature augmentation. Tested on the ogbn-papers100M dataset and Tencent's online game data, the framework showcases state-of-the-art performance, scalability, and efficiency. It can pre-train on real-world graphs with millions of nodes and billions of edges and generalize effectively across diverse downstream tasks. The framework's ability to make predictions for unseen new nodes and graphs highlights its potential for industrial applications with large-scale graph data. <br /> <div>
arXiv:2407.03953v4 Announce Type: replace-cross 
Abstract: Graph pre-training has been concentrated on graph-level tasks involving small graphs (e.g., molecular graphs) or learning node representations on a fixed graph. Extending graph pre-trained models to web-scale graphs with billions of nodes in industrial scenarios, while avoiding negative transfer across graphs or tasks, remains a challenge. We aim to develop a general graph pre-trained model with inductive ability that can make predictions for unseen new nodes and even new graphs. In this work, we introduce a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer). Based on the masked autoencoder architecture, we design two pre-training tasks: one for reconstructing node features and the other for reconstructing local structures. Unlike the original autoencoder architecture where the pre-trained decoder is discarded, we propose a novel strategy that utilizes the decoder for feature augmentation. Our framework, tested on the publicly available ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges, achieves state-of-the-art performance, showcasing scalability and efficiency. We have deployed our framework on Tencent's online game data, confirming its capability to pre-train on real-world graphs with over 540 million nodes and 12 billion edges and to generalize effectively across diverse static and dynamic downstream tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who is the root in a syntactic dependency structure?</title>
<link>https://arxiv.org/abs/2501.15188</link>
<guid>https://arxiv.org/abs/2501.15188</guid>
<content:encoded><![CDATA[
<div> Keywords: syntactic structure, unsupervised methods, root vertex, centrality scores, network science

Summary:
The article discusses the challenges in determining the syntactic structure of sentences, specifically in guessing the correct direction of edges in a syntactic dependency tree. The authors propose using an ensemble of centrality scores, incorporating both spatial and non-spatial factors, to identify the root vertex in the structure. They hypothesize that the root vertex is a crucial and central element in the dependency tree, which is supported by their findings that root vertices tend to have high centrality. Novel scoring methods that consider the position of a vertex and its neighbors perform best in identifying the root. The study contributes to a better understanding of the concept of rootness in syntactic structures from a network science perspective. Overall, the research provides theoretical and empirical foundations for a universal notion of rootness in syntactic dependency trees. 

<br /><br />Summary: <div>
arXiv:2501.15188v3 Announce Type: replace-cross 
Abstract: The syntactic structure of a sentence can be described as a tree that indicates the syntactic relationships between words. In spite of significant progress in unsupervised methods that retrieve the syntactic structure of sentences, guessing the right direction of edges is still a challenge. As in a syntactic dependency structure edges are oriented away from the root, the challenge of guessing the right direction can be reduced to finding an undirected tree and the root. The limited performance of current unsupervised methods demonstrates the lack of a proper understanding of what a root vertex is from first principles. We consider an ensemble of centrality scores, some that only take into account the free tree (non-spatial scores) and others that take into account the position of vertices (spatial scores). We test the hypothesis that the root vertex is an important or central vertex of the syntactic dependency structure. We confirm the hypothesis in the sense that root vertices tend to have high centrality and that vertices of high centrality tend to be roots. The best performance in guessing the root is achieved by novel scores that only take into account the position of a vertex and that of its neighbours. We provide theoretical and empirical foundations towards a universal notion of rootness from a network science perspective.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Trust in Crowdsourced Data</title>
<link>https://arxiv.org/abs/2511.03016</link>
<guid>https://arxiv.org/abs/2511.03016</guid>
<content:encoded><![CDATA[
<div> Trust management practices for crowdsourced data are examined across various platforms, including Volunteered Geographic Information, Wiki Ecosystems, and Social Media. Strengths identified include automated moderation and community validation, while limitations involve data influx and elite dominance. Proposed solutions include AI tools, transparent reputation metrics, decentralised moderation, structured engagement, and a "soft power" strategy. Keywords: crowdsourced data, trust management, automated moderation, community validation, AI tools.<br /><br />Summary: This study explores trust management practices in crowdsourced data platforms. It identifies strengths like automated moderation and community validation, as well as limitations such as rapid data influx and elite dominance. Proposed solutions involve using AI tools, transparent reputation metrics, decentralised moderation, structured community engagement, and a "soft power" strategy to improve data reliability and distribute decision-making authority equitably. <div>
arXiv:2511.03016v1 Announce Type: new 
Abstract: Crowdsourced data supports real-time decision-making but faces challenges like misinformation, errors, and contributor power concentration. This study systematically examines trust management practices across platforms categorised as Volunteered Geographic Information, Wiki Ecosystems, Social Media, Mobile Crowdsensing, and Specialised Review and Environmental Crowdsourcing. Identified strengths include automated moderation and community validation, while limitations involve rapid data influx, niche oversight gaps, opaque trust metrics, and elite dominance. Proposed solutions incorporate advanced AI tools, transparent reputation metrics, decentralised moderation, structured community engagement, and a ``soft power'' strategy, aiming to equitably distribute decision-making authority and enhance overall data reliability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Citations: Measuring Idea-level Knowledge Diffusion from Research to Journalism and Policy-making</title>
<link>https://arxiv.org/abs/2511.03378</link>
<guid>https://arxiv.org/abs/2511.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: social science knowledge, diffusion, media effects theories, cross-domain, contextualized meanings<br />
Summary:<br />
- The study introduces a text-based approach to measure the diffusion of social science knowledge between research, journalism, and policy-making domains.<br />
- Focuses on media effects theories in communication science, analyzing 72,703 documents from 2000-2019 mentioning these ideas.<br />
- Varies significantly in diffusion patterns and dynamics among different ideas, some diffusing across domains while others do not.<br />
- Compares contextualized meanings across domains, finding larger distances between research and policy compared to research and journalism.<br />
- Observes semantic convergence over time, particularly for practically oriented ideas, showing ideas shifting roles across domains from theories in research to applied use in policy.<br /> <div>
arXiv:2511.03378v1 Announce Type: new 
Abstract: Despite the importance of social science knowledge for various stakeholders, measuring its diffusion into different domains remains a challenge. This study uses a novel text-based approach to measure the idea-level diffusion of social science knowledge from the research domain to the journalism and policy-making domains. By doing so, we expand the detection of knowledge diffusion beyond the measurements of direct references. Our study focuses on media effects theories as key research ideas in the field of communication science. Using 72,703 documents (2000-2019) from three domains (i.e., research, journalism, and policy-making) that mention these ideas, we count the mentions of these ideas in each domain, estimate their domain-specific contexts, and track and compare differences across domains and over time. Overall, we find that diffusion patterns and dynamics vary considerably between ideas, with some ideas diffusing between other domains, while others do not. Based on the embedding regression approach, we compare contextualized meanings across domains and find that the distances between research and policy are typically larger than between research and journalism. We also find that ideas largely shift roles across domains - from being the theories themselves in research to sense-making in news to applied, administrative use in policy. Over time, we observe semantic convergence mainly for ideas that are practically oriented. Our results characterize the cross-domain diffusion patterns and dynamics of social science knowledge at the idea level, and we discuss the implications for measuring knowledge diffusion beyond citations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A local eigenvector centrality</title>
<link>https://arxiv.org/abs/2511.03608</link>
<guid>https://arxiv.org/abs/2511.03608</guid>
<content:encoded><![CDATA[
<div> local eigenvector centrality, global connectivity, prominent community structures, network analysis, centrality assessment

Summary:
local eigenvector centrality is introduced as a new measure that combines both local and global connectivity, incorporating prominent eigengaps and the associated eigenspectrum to detect centrality that reflects community structures. In contact networks with clear community structures, it identifies distinct distributions compared to eigenvector centrality applied on isolated communities and PageRank. Discrepancies between the measures highlight nodes not conforming to local structures. References to PageRank help mitigate localisation effects of eigenvector measures. For networks without well-defined communities like city road networks, local eigenvector centrality identifies both local hubs and globally connected nodes, demonstrating its ability to capture both local and global significance in network analysis. <div>
arXiv:2511.03608v1 Announce Type: new 
Abstract: Eigenvector centrality is an established measure of global connectivity, from which the importance and influence of nodes can be inferred. We introduce a local eigenvector centrality that incorporates both local and global connectivity. This new measure references prominent eigengaps and combines their associated eigenspectrum, via the Euclidean norm, to detect centrality that reflects the influence of prominent community structures. In contact networks, with clearly defined community structures, local eigenvector centrality is shown to identify similar but distinct distributions to eigenvector centrality applied on each community in isolation and PageRank. Discrepancies between the two eigenvector measures highlight nodes and communities that do not conform to their defined local structures, e.g. nodes with more connections outside of their defined community than within it. While reference to PageRank's centrality assessment enables a mitigation strategy for localisation effects inherent in eigenvector-based measures. In networks without clearly defined communities, such as city road networks, local eigenvector centrality is shown to identify both locally prominent and globally connected hubs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2511.03251</link>
<guid>https://arxiv.org/abs/2511.03251</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Mixture-of-Experts, prompt-based learning, generalization, scalability

Summary:
GMoPE (Graph Mixture of Prompt-Experts) is a novel framework that combines the Mixture-of-Experts architecture with prompt-based learning for graphs. It addresses challenges such as negative transfer, scalability issues, and high adaptation costs by allowing experts to specialize in distinct subdomains and dynamically contribute to predictions. A soft orthogonality constraint across prompt vectors promotes diversity and prevents expert collapse, while a prompt-only fine-tuning strategy reduces spatiotemporal complexity during transfer. The framework outperforms existing baselines and achieves performance comparable to full parameter fine-tuning, requiring only a fraction of the adaptation overhead. GMoPE provides a scalable and efficient solution for advancing generalizable graph foundation models. 

<br /><br />Summary: GMoPE integrates Mixture-of-Experts with prompt-based learning for graphs to improve generalization, scalability, and adaptation costs. Experts specialize in distinct subdomains, with a soft orthogonality constraint promoting diversity and balance. A prompt-only fine-tuning strategy reduces complexity during transfer, leading to superior performance compared to baselines and similar results to full parameter fine-tuning with lower overhead. GMoPE offers a principled and scalable framework for enhancing generalizable and efficient graph foundation models. <div>
arXiv:2511.03251v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots</title>
<link>https://arxiv.org/abs/2511.03286</link>
<guid>https://arxiv.org/abs/2511.03286</guid>
<content:encoded><![CDATA[
<div> Keywords: global digital platforms, multiagent transition systems, centralised, decentralised, federated, grassroots

Summary:<br /><br />Global digital platforms, serving billions of people, can be classified into four classes based on the cardinality of essential agents in atomic transactions-based multiagent transition systems: Centralised (one server), Decentralised (finite bootstrap nodes), Federated (infinite servers), and Grassroots (universal agents). The article presents a formal framework for analyzing these platforms, using a global social network as an example to demonstrate the different classes and their specifications. The text also explores other global platforms like currencies, sharing economy apps, and AI. Furthermore, it discusses how grassroots platforms, which encompass all agents as essential, form a distinct class within this classification framework. This mathematical approach provides a structured way to categorize and analyze various global platforms, both existing and potential, based on their multiagent atomic transactions and essential agent configurations. <br />Summary: <div>
arXiv:2511.03286v1 Announce Type: cross 
Abstract: Global digital platforms are software systems designed to serve entire populations, with some already serving billions of people. We propose atomic transactions-based multiagent transition systems and protocols as a formal framework to study them; introduce essential agents -- minimal sets of agents the removal of which makes communication impossible; and show that the cardinality of essential agents partitions all global platforms into four classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we provide centralised, decentralised, federated, and grassroots specifications via multiagent atomic transactions, and prove they satisfy basic correctness properties. We discuss informally additional global platforms -- currencies, ``sharing economy'' apps, AI, and more. While this may be the first characterisation of centralised, decentralised, and federated global platforms, grassroots platforms have been formally defined previously, but using different notions. Here, we prove that their original definition implies that all agents are essential, placing grassroots platforms in a distinct class within the broader formal context that includes all global platforms. This work provides the first mathematical framework for classifying any global platform -- existing or imagined -- by providing a multiagent atomic-transactions specification of it and determining the cardinality of the minimal set of essential agents in the ensuing multiagent protocol. It thus
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond</title>
<link>https://arxiv.org/abs/2511.03434</link>
<guid>https://arxiv.org/abs/2511.03434</guid>
<content:encoded><![CDATA[
<div> Trust Models, Inter-Agent Protocols, LLM-powered AI agents, Protocol Design, Comparative Study<br />
Summary:<br />
The article examines trust models in inter-agent protocol design as the "agentic web" continues to grow with AI agents transacting autonomously. Various protocols such as Google's A2A and Ethereum's ERC-8004 have crystallized the shift towards protocol-driven trust. The study evaluates trust models including Brief, Claim, Proof, Stake, Reputation, and Constraint, noting their assumptions, vulnerabilities, and trade-offs. It highlights fragilities specific to LLM-powered agents and emphasizes the need for proof and stake mechanisms to gate high-impact actions. Recommendations include hybrid trust models to mitigate reputation gaming and ensure safe and scalable agent economies. The article also evaluates protocols under metrics like security, privacy, latency/cost, and social robustness, providing actionable design guidelines for interoperable systems. <br />Summary: <div>
arXiv:2511.03434v1 Announce Type: cross 
Abstract: As the "agentic web" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges</title>
<link>https://arxiv.org/abs/2403.04468</link>
<guid>https://arxiv.org/abs/2403.04468</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, imbalance, noise, privacy, out-of-distribution scenarios

Summary:
This comprehensive survey focuses on addressing real-world challenges faced by Graph Neural Networks (GNNs). These challenges include data distribution imbalance, noisy data, privacy concerns, and generalization for out-of-distribution scenarios. The paper reviews existing GNN models and their solutions to these issues, emphasizing the need to enhance the reliability and robustness of GNN models in practical scenarios. By dissecting how various strategies address the challenges, such as data balancing techniques, noise reduction methods, privacy-preserving mechanisms, and strategies for handling out-of-distribution scenarios, the survey provides insights into the advancements in the field. Lastly, it discusses promising directions and future perspectives for research in improving GNN models for real-world applications. 

<br /><br />Summary: <div>
arXiv:2403.04468v2 Announce Type: replace-cross 
Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Model for Human Mobility Generation in Natural Disasters</title>
<link>https://arxiv.org/abs/2511.01928</link>
<guid>https://arxiv.org/abs/2511.01928</guid>
<content:encoded><![CDATA[
<div> physics-informed prompt, physics-guided alignment, cross-disaster generalization, cross-city generalization, meta-learning framework 
Summary: 
The article introduces a model called UniDisMob for generating human mobility patterns in natural disaster scenarios. The model aims to be universal, capable of adapting to new disaster types and different cities. Two main challenges addressed are the diversity of disaster types and the heterogeneity among cities. To tackle these challenges, the model incorporates physics-informed prompt and physics-guided alignment to capture common patterns in mobility changes after various disasters. Additionally, a meta-learning framework is utilized to extract universal patterns across multiple cities and account for city-specific features. Experimental results across different cities and disaster scenarios show that the proposed method outperforms existing approaches by more than 13% on average. <div>
arXiv:2511.01928v1 Announce Type: new 
Abstract: Human mobility generation in disaster scenarios plays a vital role in resource allocation, emergency response, and rescue coordination. During disasters such as wildfires and hurricanes, human mobility patterns often deviate from their normal states, which makes the task more challenging. However, existing works usually rely on limited data from a single city or specific disaster, significantly restricting the model's generalization capability in new scenarios. In fact, disasters are highly sudden and unpredictable, and any city may encounter new types of disasters without prior experience. Therefore, we aim to develop a one-for-all model for mobility generation that can generalize to new disaster scenarios. However, building a universal framework faces two key challenges: 1) the diversity of disaster types and 2) the heterogeneity among different cities. In this work, we propose a unified model for human mobility generation in natural disasters (named UniDisMob). To enable cross-disaster generalization, we design physics-informed prompt and physics-guided alignment that leverage the underlying common patterns in mobility changes after different disasters to guide the generation process. To achieve cross-city generalization, we introduce a meta-learning framework that extracts universal patterns across multiple cities through shared parameters and captures city-specific features via private parameters. Extensive experiments across multiple cities and disaster scenarios demonstrate that our method significantly outperforms state-of-the-art baselines, achieving an average performance improvement exceeding 13%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Notes are Vulnerable to Rater Bias and Manipulation</title>
<link>https://arxiv.org/abs/2511.02615</link>
<guid>https://arxiv.org/abs/2511.02615</guid>
<content:encoded><![CDATA[
<div> algorithm, bias, manipulation, moderation, reliability

Summary: 
The article evaluates the Community Notes algorithm used for crowdsourced moderation on social media platforms. It identifies challenges such as rater bias and manipulation that can impact the effectiveness of the system. The algorithm was found to suppress helpful notes and be sensitive to biases like polarization and in-group preferences. It was also discovered that a small percentage of bad raters could strategically censor reliable information by targeting helpful notes. These findings raise concerns about the reliability and trustworthiness of community-driven moderation systems. The study emphasizes the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking. 

<br /><br />Summary: <div>
arXiv:2511.02615v1 Announce Type: new 
Abstract: Social media platforms increasingly rely on crowdsourced moderation systems like Community Notes to combat misinformation at scale. However, these systems face challenges from rater bias and potential manipulation, which may undermine their effectiveness. Here we systematically evaluate the Community Notes algorithm using simulated data that models realistic rater and note behaviors, quantifying error rates in publishing helpful versus unhelpful notes. We find that the algorithm suppresses a substantial fraction of genuinely helpful notes and is highly sensitive to rater biases, including polarization and in-group preferences. Moreover, a small minority (5--20\%) of bad raters can strategically suppress targeted helpful notes, effectively censoring reliable information. These findings suggest that while community-driven moderation may offer scalability, its vulnerability to bias and manipulation raises concerns about reliability and trustworthiness, highlighting the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedback dynamics in Politics: The interplay between sentiment and engagement</title>
<link>https://arxiv.org/abs/2511.02663</link>
<guid>https://arxiv.org/abs/2511.02663</guid>
<content:encoded><![CDATA[
<div> adaptation, political communication, sentiment dynamics, social media, feedback mechanisms <br />
Summary:<br />
- The study analyzes how politicians adjust the sentiment of their messages based on public engagement in political communication on social media platforms. 
- Over 1.5 million tweets from Members of Parliament in the UK, Spain, and Greece in 2021 were used to identify sentiment dynamics through a linear model. 
- Results show a closed-loop behavior, where engagement with positive and negative messages influences the sentiment of future posts. 
- Opposition members tend to be more responsive to negative engagement, while government officials are more influenced by positive signals. 
- The findings provide a quantitative understanding of behavioral adaptation in online politics, demonstrating how feedback mechanisms can shed light on the self-reinforcing dynamics seen in social media discourse. <br /> <div>
arXiv:2511.02663v1 Announce Type: new 
Abstract: We investigate feedback mechanisms in political communication by testing whether politicians adapt the sentiment of their messages in response to public engagement. Using over 1.5 million tweets from Members of Parliament in the United Kingdom, Spain, and Greece during 2021, we identify sentiment dynamics through a simple yet interpretable linear model. The analysis reveals a closed-loop behavior: engagement with positive and negative messages influences the sentiment of subsequent posts. Moreover, the learned coefficients highlight systematic differences across political roles: opposition members are more reactive to negative engagement, whereas government officials respond more to positive signals. These results provide a quantitative, control-oriented view of behavioral adaptation in online politics, showing how feedback principles can explain the self-reinforcing dynamics that emerge in social media discourse.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance</title>
<link>https://arxiv.org/abs/2412.00621</link>
<guid>https://arxiv.org/abs/2412.00621</guid>
<content:encoded><![CDATA[
<div> vulnerabilities, Large Language Models, scam detection, adversarial examples, dataset

Summary:
Large Language Models (LLMs) face vulnerabilities when detecting scams, particularly when dealing with adversarial scam messages. A comprehensive dataset was created with detailed labels for scam messages, including both original and adversarial examples. The dataset expanded traditional scam detection categories to include more specific scam types. Adversarial examples exploited LLM vulnerabilities, resulting in a high misclassification rate. The performance of LLMs on these adversarial scam messages was evaluated, and strategies to enhance their robustness were proposed.<br /><br />Summary: <div>
arXiv:2412.00621v1 Announce Type: cross 
Abstract: Can we trust Large Language Models (LLMs) to accurately predict scam? This paper investigates the vulnerabilities of LLMs when facing adversarial scam messages for the task of scam detection. We addressed this issue by creating a comprehensive dataset with fine-grained labels of scam messages, including both original and adversarial scam messages. The dataset extended traditional binary classes for the scam detection task into more nuanced scam types. Our analysis showed how adversarial examples took advantage of vulnerabilities of a LLM, leading to high misclassification rate. We evaluated the performance of LLMs on these adversarial scam messages and proposed strategies to improve their robustness.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Story and essential meaning dynamics in Bangladesh's July 2024 Student-People's Uprising</title>
<link>https://arxiv.org/abs/2511.01865</link>
<guid>https://arxiv.org/abs/2511.01865</guid>
<content:encoded><![CDATA[
<div> YouTube comments, news coverage, emotional dynamics, public perception, political unrest

Summary:
During the July 2024 Student-People's Uprising in Bangladesh, analysis of over 50,000 YouTube comments revealed a dominance of negative sentiment correlating with the number of protest deaths. Comments reflected a landscape of power, aggression, and danger, with touches of hope and moral conviction. Topic discourse evolved, shifting between political conflict, media flow, student violence, social resistance, and digital movement. Sentiment shifts indicated an increase in happiness after the second internet blackout, with more positive words like 'victory' and 'peace' being used. The allotaxonometric analysis showed a shift from protest to justice, indicating a progression in public perception during the movement. <div>
arXiv:2511.01865v1 Announce Type: cross 
Abstract: News media serves a crucial role in disseminating information and shaping public perception, especially during periods of political unrest. Using over 50,0000 YouTube comments on news coverage from July 16 to August 6, 2024, we investigate the emotional dynamics and evolving discourse of public perception during the July 2024 Student-People's Uprising in Bangladesh. Through integrated analyses of sentiment, emotion, topic, lexical discourse, timeline progression, sentiment shifts, and allotaxonometry, we show how negative sentiment dominated during the movement. We find a negative correlation between comment happiness and number of protest deaths $(r = -0.45,\p = 0.00)$. Using an ousiometer to measure essential meaning, we find public responses reflect a landscape of power, aggression, and danger, alongside persistent expressions of hope, moral conviction, and empowerment through goodnesses. Topic discourse progressed during the movement, with peaks in `Political Conflict', `Media Flow', and `Student Violence' during crisis surges, while topics like `Social Resistance' and `Digital Movement' persisted amid repression. Sentiment shifts reveal that after the second internet blackout, average happiness increased, driven by the more frequent use of positive words such as `victory', `peace' and `freedom' and a decrease in negative terms such as `death' and `lies'. Finally, through allotaxonometric analysis, we observe a clear shift from protest to justice.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expertise and confidence explain how social influence evolves along intellective tasks</title>
<link>https://arxiv.org/abs/2011.07168</link>
<guid>https://arxiv.org/abs/2011.07168</guid>
<content:encoded><![CDATA[
<div> Keywords: interpersonal influence, expertise, social confidence, cognitive dynamical model, deep neural networks

Summary:
Interpersonal influence in small groups undertaking intellective tasks is influenced by individuals' expertise and social confidence. Low performers tend to underestimate the expertise of high-performing teammates. Three hypotheses are introduced and supported by empirical and theoretical evidence. A cognitive dynamical model based on transactive memory systems, social comparison, and confidence heuristics describes how individuals adjust interpersonal influence over time. The model accurately predicts individuals' influence and provides analytical results for identically performing individuals. A novel approach using deep neural networks on a text embedding model accurately predicts individuals' influence based on message contents, times, and correctness. Experiments show the effectiveness of the proposed models compared to traditional methods. The neural networks model is the most accurate, while the dynamical model is the most interpretable for predicting influence. <br /><br />Summary: <div>
arXiv:2011.07168v2 Announce Type: replace 
Abstract: Discovering the antecedents of individuals' influence in collaborative environments is an important, practical, and challenging problem. In this paper, we study interpersonal influence in small groups of individuals who collectively execute a sequence of intellective tasks. We observe that along an issue sequence with feedback, individuals with higher expertise and social confidence are accorded higher interpersonal influence. We also observe that low-performing individuals tend to underestimate their high-performing teammate's expertise. Based on these observations, we introduce three hypotheses and present empirical and theoretical support for their validity. We report empirical evidence on longstanding theories of transactive memory systems, social comparison, and confidence heuristics on the origins of social influence. We propose a cognitive dynamical model inspired by these theories to describe the process by which individuals adjust interpersonal influences over time. We demonstrate the model's accuracy in predicting individuals' influence and provide analytical results on its asymptotic behavior for the case with identically performing individuals. Lastly, we propose a novel approach using deep neural networks on a pre-trained text embedding model for predicting the influence of individuals. Using message contents, message times, and individual correctness collected during tasks, we are able to accurately predict individuals' self-reported influence over time. Extensive experiments verify the accuracy of the proposed models compared to baselines such as structural balance and reflected appraisal model. While the neural networks model is the most accurate, the dynamical model is the most interpretable for influence prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Behavioural Analysis of Credulous Twitter Users</title>
<link>https://arxiv.org/abs/2101.10782</link>
<guid>https://arxiv.org/abs/2101.10782</guid>
<content:encoded><![CDATA[
<div> Keywords: Twitter, social media, fake news, bots, user behavior

Summary:
Using Twitter data, this study focuses on identifying "credulous" users who follow bots and are susceptible to spreading false information. By analyzing features of user behavior, the study successfully identifies credulous users using lightweight features. The research highlights differences in interaction patterns between credulous and non-credulous users, with credulous users amplifying content posted by bots. Detecting these credulous users is crucial in combating the dissemination of spam, propaganda, and unreliable information on social media platforms. The study emphasizes the importance of understanding user behavior to address the issue of false news spreading rapidly through social media.<br /><br />Summary: <div>
arXiv:2101.10782v2 Announce Type: replace 
Abstract: Thanks to platforms such as Twitter and Facebook, people can know facts and events that otherwise would have been silenced. However, social media significantly contribute also to fast spreading biased and false news while targeting specific segments of the population. We have seen how false information can be spread using automated accounts, known as bots. Using Twitter as a benchmark, we investigate behavioural attitudes of so called `credulous' users, i.e., genuine accounts following many bots. Leveraging our previous work, where supervised learning is successfully applied to single out credulous users, we improve the classification task with a detailed features' analysis and provide evidence that simple and lightweight features are crucial to detect such users. Furthermore, we study the differences in the way credulous and not credulous users interact with bots and discover that credulous users tend to amplify more the content posted by bots and argue that their detection can be instrumental to get useful information on possible dissemination of spam content, propaganda, and, in general, little or no reliable information.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title>
<link>https://arxiv.org/abs/2410.18803</link>
<guid>https://arxiv.org/abs/2410.18803</guid>
<content:encoded><![CDATA[
<div> Keywords: domain reliability, Wikipedia, language-agnostic model, credibility, verification

Summary: 
- The article introduces a language-agnostic model that assesses the reliability of web domains as sources in Wikipedia across multiple language editions.
- The model evaluates domain reliability within articles on various controversial topics like Climate Change, COVID-19, History, Media, and Biology.
- Crafted features expressing domain usage across articles help predict domain reliability.
- The model achieves an F1 Macro score of around 0.80 for high-resource languages and 0.65 for mid-resource languages.
- The time the domain remains present in articles (permanence) emerges as a highly predictive feature.
- Maintaining consistent model performance across languages of varying resource levels is a challenge.
- Adapting models from higher-resource languages can enhance performance.
- The findings can support Wikipedia editors in verifying citations and provide insights for other user-generated content communities.

Summary:<br /><br />The article introduces a language-agnostic model for assessing domain reliability in Wikipedia across multiple language editions, evaluating it on articles spanning various controversial topics and achieving high predictive accuracy, influenced significantly by the permanence of domain presence. The challenge of maintaining model performance across languages of differing resource levels is noted, with potential improvements through adapting models from richer language datasets, offering valuable support to Wikipedia editors and other user-generated content platforms in verifying information sources. <div>
arXiv:2410.18803v3 Announce Type: replace 
Abstract: Over the last few years, verifying the credibility of information sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of web domains as sources in references across multiple language editions of Wikipedia. Utilizing editing activity data, the model evaluates domain reliability within different articles of varying controversiality, such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts domain reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65, while the performance of low-resource languages varies. In all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. We believe these findings can assist Wikipedia editors in their ongoing efforts to verify citations and may offer useful insights for other user-generated content communities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Small Acts Scale: Ethical Thresholds in Network Diffusion</title>
<link>https://arxiv.org/abs/2511.00329</link>
<guid>https://arxiv.org/abs/2511.00329</guid>
<content:encoded><![CDATA[
<div> Keywords: ethical evaluation, networked environments, social graph, platform design, responsibility<br />
Summary:<br />
This article introduces a model that considers the diffusion of public acts in networked, platform-mediated environments, moving beyond dyadic evaluations. The model examines how an initiating act spreads across a social graph based on factors such as exposure, salience, compliance, and depth. It identifies a network multiplier that quantifies the impact of these factors on the diffusion of acts. By analyzing common platform design levers like reach, ranking, and friction, the model shows how these levers can affect downstream responsibility in various scenarios, such as pandemic mitigation and platform amplification of norms. The model also highlights a threshold that distinguishes subcritical, critical, and supercritical regimes in the diffusion process. Overall, this research provides insights into how platform design can influence the spread of actions and the associated ethical implications. <br /><br /> <div>
arXiv:2511.00329v1 Announce Type: new 
Abstract: Much ethical evaluation treats actions dyadically: one agent acts on one recipient. In networked, platform-mediated environments, this lens misses how public acts diffuse. We introduce a minimal message-passing model in which an initiating act with baseline valence w spreads across a social graph with exposure b, per-hop salience $alpha$, compliance $q$, and depth (horizon) d. The model yields a closed-form \emph{network multiplier} relative to the dyadic baseline and identifies a threshold at r=b.alpha.q=1 separating subcritical (saturating), critical (linear), and supercritical (geometric) regimes. We show how common platform design levers -- reach and fan-out (affecting b), ranking and context (affecting alpha), share mechanics and friction (affecting q), and time-bounds (affecting d) -- systematically change expected downstream responsibility Applications include pandemic mitigation and vaccination externalities, as well as platform amplification of prosocial and harmful norms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-centrality: A Network Centrality Measure Based on Minimum Energy Control for Laplacian Dynamics</title>
<link>https://arxiv.org/abs/2511.00339</link>
<guid>https://arxiv.org/abs/2511.00339</guid>
<content:encoded><![CDATA[
<div> centrality, network, dynamics, optimal control theory, U-centrality <br />
Summary: <br />
The article introduces a dynamic, task-aware centrality framework based on optimal control theory. Traditional centrality measures often do not consider network dynamics but this new U-centrality measure takes into account both system dynamics and specific operational objectives. By formulating a control problem on minimum energy control of average opinion using Laplacian dynamics, U-centrality quantifies a node's ability to unify agents' state. It combines aspects of degree centrality and current-flow closeness centrality, offering a versatile tool for network analysis in dynamic environments. U-centrality interpolates between known centrality measures and reveals a node's importance over different time scales, making it a valuable addition to the field of network analysis. <div>
arXiv:2511.00339v1 Announce Type: new 
Abstract: Network centrality is a foundational concept for quantifying the importance of nodes within a network. Many traditional centrality measures--such as degree and betweenness centrality--are purely structural and often overlook the dynamics that unfold across the network. However, the notion of a node's importance is inherently context-dependent and must reflect both the system's dynamics and the specific objectives guiding its operation. Motivated by this perspective, we propose a dynamic, task-aware centrality framework rooted in optimal control theory. By formulating a problem on minimum energy control of average opinion based on Laplacian dynamics and focusing on the variance of terminal state, we introduce a novel centrality measure--termed U-centrality--that quantifies a node's ability to unify the agents' state. We demonstrate that U-centrality interpolates between known measures: it aligns with degree centrality in the short-time horizon and converges to a new centrality over longer time scales which is closely related to current-flow closeness centrality. This work bridges structural and dynamical approaches to centrality, offering a principled, versatile tool for network analysis in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion Dynamics: A Comprehensive Overview</title>
<link>https://arxiv.org/abs/2511.00401</link>
<guid>https://arxiv.org/abs/2511.00401</guid>
<content:encoded><![CDATA[
<div> Opinion dynamics, social interactions, interdisciplinary, unified framework, models<br />
<br />
Summary: <br />
- This survey reviews models of opinion dynamics across disciplines in a unified framework, categorizing them based on properties. <br />
- It analyzes convergence properties, viral marketing implications, and user characteristics in these models. <br />
- Examining final configuration and convergence time, the study delves into consensus vs polarized outcomes. <br />
- Algorithmic, complexity, and combinatorial results are reviewed in the context of viral marketing strategies. <br />
- Node characteristics like stubbornness and activeness are explored for their impact on diffusion outcomes. <br /> <div>
arXiv:2511.00401v1 Announce Type: new 
Abstract: Opinion dynamics, the evolution of individuals through social interactions, is an important area of research with applications ranging from politics to marketing. Due to its interdisciplinary relevance, studies of opinion dynamics remain fragmented across computer science, mathematics, the social sciences, and physics, and often lack shared frameworks. This survey bridges these gaps by reviewing well-known models of opinion dynamics within a unified framework and categorizing them into distinct classes based on their properties. Furthermore, the key findings on these models are covered in three parts: convergence properties, viral marketing, and user characteristics. We first analyze the final configuration (consensus vs polarized) and convergence time for each model. We then review the main algorithmic, complexity, and combinatorial results in the context of viral marketing. Finally, we explore how node characteristics, such as stubbornness, activeness, or neutrality, shape diffusion outcomes. By unifying terminology, methods, and challenges across disciplines, this paper aims to foster cross-disciplinary collaboration and accelerate progress in understanding and harnessing opinion dynamics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework Based on Graph Cellular Automata for Similarity Evaluation in Urban Spatial Networks</title>
<link>https://arxiv.org/abs/2511.00768</link>
<guid>https://arxiv.org/abs/2511.00768</guid>
<content:encoded><![CDATA[
<div> framework, graph cellular automata, similarity evaluation, urban spatial networks, network resonance

Summary:

The study introduces GCA-Sim, a framework based on graph cellular automata for assessing similarity in urban spatial networks. It utilizes multiple submodels to measure similarity by analyzing divergence between value distributions during an information evolution process. The research identifies network resonance, where certain propagation rules amplify differences in network signals. Through an enhanced differentiable logic-gate network, various submodels inducing network resonance are learned. Evaluation on city-level and district-level road networks demonstrates superior performance compared to existing methods. The framework reveals that planning-led street networks exhibit less internal homogeneity than organically developed ones. Different morphological categories contribute equally to similarity evaluation. Additionally, the study observes that degree, a basic topological signal, aligns more with land value and related variables over iterations. <div>
arXiv:2511.00768v1 Announce Type: new 
Abstract: Measuring similarity in urban spatial networks is key to understanding cities as complex systems. Yet most existing methods are not tailored for spatial networks and struggle to differentiate them effectively. We propose GCA-Sim, a similarity-evaluation framework based on graph cellular automata. Each submodel measures similarity by the divergence between value distributions recorded at multiple stages of an information evolution process. We find that some propagation rules magnify differences among network signals; we call this "network resonance." With an improved differentiable logic-gate network, we learn several submodels that induce network resonance. We evaluate similarity through clustering performance on fifty city-level and fifty district-level road networks. The submodels in this framework outperform existing methods, with Silhouette scores above 0.9. Using the best submodel, we further observe that planning-led street networks are less internally homogeneous than organically grown ones; morphological categories from different domains contribute with comparable importance; and degree, as a basic topological signal, becomes increasingly aligned with land value and related variables over iterations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deciphering Scientific Collaboration in Biomedical LLM Research: Dynamics, Institutional Participation, and Resource Disparities</title>
<link>https://arxiv.org/abs/2511.00818</link>
<guid>https://arxiv.org/abs/2511.00818</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, biomedical research, collaboration diversity, network analysis, research performance

Summary: 
1. Large language models (LLMs) are reshaping scientific collaboration in biomedical research, with a focus on the evolving diversity of collaborations over time.
2. Collaboration diversity has increased steadily, with a decrease in Computer Science and Artificial Intelligence authors, indicating lower technical barriers for biomedical investigators.
3. Central institutions such as Stanford University and Harvard Medical School, along with bridging disciplines like Medicine and Computer Science, anchor collaboration networks in LLM-related biomedical research.
4. Biomedical research resources are strongly linked to research performance, with high-performing resource-constrained institutions collaborating more with top institutions.
5. The study highlights a complex landscape where democratizing trends coexist with a resource-driven hierarchy, emphasizing the critical role of strategic collaboration in the field.

<br /><br />Summary: <div>
arXiv:2511.00818v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly transforming biomedical discovery and clinical innovation, yet their impact extends far beyond algorithmic revolution-LLMs are restructuring how scientific collaboration occurs, who participates, and how resources shape innovation. Despite this profound transformation, how this rapid technological shift is reshaping the structure and equity of scientific collaboration in biomedical LLM research remains largely unknown. By analyzing 5,674 LLM-related biomedical publications from PubMed, we examine how collaboration diversity evolves over time, identify institutions and disciplines that anchor and bridge collaboration networks, and assess how resource disparities underpin research performance. We find that collaboration diversity has grown steadily, with a decreasing share of Computer Science and Artificial Intelligence authors, suggesting that LLMs are lowering technical barriers for biomedical investigators. Network analysis reveals central institutions, including Stanford University and Harvard Medical School, and bridging disciplines such as Medicine and Computer Science that anchor collaborations in this field. Furthermore, biomedical research resources are strongly linked to research performance, with high-performing resource-constrained institutions exhibiting larger collaboration volume with the top 1% most connected institutions in the network. Together, these findings reveal a complex landscape, where democratizing trends coexist with a persistent, resource-driven hierarchy, highlighting the critical role of strategic collaboration in this evolving field.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single-Tokenomics: How Farcaster's Pluralistic Incentives Reshape Social Networking</title>
<link>https://arxiv.org/abs/2511.00827</link>
<guid>https://arxiv.org/abs/2511.00827</guid>
<content:encoded><![CDATA[
<div> token-based rewards, platform dynamics, user behaviors, tokenomics design, social interactions  

Summary:  
This paper presents empirical analysis on the impact of diverse token-based reward mechanisms on platform dynamics and user behaviors using data from the Farcaster blockchain-based social network. The study found that different tokenomics designs influence participation rates and wealth concentration patterns. Inter-community tipping mitigates echo chambers among non-following pairs. However, token rewards may not always enhance content quality and can lead to asymmetric network growth. While token rewards boost content creation and follower acquisition, they may also encourage strategic optimization. The findings shed light on the challenges of aligning economic incentives with authentic social value in social platforms integrating cryptocurrencies. <div>
arXiv:2511.00827v1 Announce Type: new 
Abstract: This paper presents the first empirical analysis of how diverse token-based reward mechanisms impact platform dynamics and user behaviors. For this, we gather a unique, large-scale dataset from Farcaster. This blockchain-based, decentralized social network incorporates multiple incentive mechanisms spanning platform-native rewards, third-party token programs, and peer-to-peer tipping. Our dataset captures token transactions and social interactions from 574,829 wallet-linked users, representing 64.25% of the platform's user base. Our socioeconomic analyses reveal how different tokenomics design shape varying participation rates (7.6%--70%) and wealth concentration patterns (Gini 0.72--0.94), whereas inter-community tipping (51--75% of all tips) is 1.3--2x more frequent among non-following pairs, thereby mitigating echo chambers. Our causal analyses further uncover several critical trade-offs: (1) while most token rewards boost content creation, they often fail to enhance -- sometimes undermining -- content quality; (2) token rewards increase follower acquisition but show neutral or negative effects on outbound following, suggesting potential asymmetric network growth; (3) repeated algorithmic rewards demonstrate strong cumulative effects that may encourage strategic optimization. Our findings advance understanding of cryptocurrency integration in social platforms and highlight challenges in aligning economic incentives with authentic social value.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Employee Verification Mechanisms Alter Cultural Signals in Employer Reviews?</title>
<link>https://arxiv.org/abs/2511.01086</link>
<guid>https://arxiv.org/abs/2511.01086</guid>
<content:encoded><![CDATA[
<div> Keywords: online reviews, Glassdoor, Blind, verification, workplace culture

Summary:
Online reviews play a significant role in shaping perceptions of products and workplaces, with employer reviews combining narratives and ratings that reflect organizational culture. Platforms like Glassdoor allow for fully anonymous reviews, while Blind requires verification of employment. Research indicates that verified reviews may be more trustworthy, but verification can also impact authenticity when expectations are not met. Using the Competing Values Framework and the CultureBERT model, a study analyzed over 300k ratings on both platforms. Blind reviews tend to emphasize clan and hierarchy, while Glassdoor reviews skew positive and highlight clan and market cultures. Verification alone does not eliminate bias but alters how culture is portrayed. Job seekers using different platforms may receive conflicting signals about workplace culture, influencing their application decisions and job fit. <br /><br />Summary: Online reviews on Glassdoor and Blind, with and without verification, offer insights into workplace culture but may present different perspectives to job seekers, impacting their decision-making process. <div>
arXiv:2511.01086v1 Announce Type: new 
Abstract: Online reviews shape impressions across products and workplaces. Employer reviews combine narratives and ratings that reflect culture. Glassdoor permits fully anonymous posts; Blind requires employment verification while preserving anonymity. We ask how verification changes reviews. Evidence suggests verified reviews can be more trustworthy, yet verification can also erode authenticity when expectations are unmet. We use the Competing Values Framework (clan, adhocracy, hierarchy, market) and the CultureBERT model by Koch and Pasch, 2023 to over 300k ratings. We find that Blind reviews emphasize clan and hierarchy while Glassdoor skews positive and highlights clan and market. Verification on its own does not remove bias but shifts how culture is represented. Job seekers using different platforms receive systematically different signals about workplace culture, affecting application decisions and job-matching.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEEP: A Discourse Evolution Engine for Predictions about Social Movements</title>
<link>https://arxiv.org/abs/2511.01142</link>
<guid>https://arxiv.org/abs/2511.01142</guid>
<content:encoded><![CDATA[
<div> transformer-based, multivariate time series, Discourse Evolution Engine, social movements, prediction
<br />
SMART's DEEP tool uses transformer-based technology to predict the volume of future articles/posts and the emotions expressed in social movements related to the UN's Sustainable Development Goals. Developed by a multidisciplinary team, the tool provides probabilistic forecasts with uncertainty estimates, aiding editorial planning and strategic decision-making. DEEP is evaluated using a case study of the #MeToo movement, analyzing a longitudinal dataset of Reddit posts and news articles from September 2024 to June 2025. The dataset, comprising 433K Reddit posts and 121K news articles, will be publicly released for research upon publication of the paper.
<br /><br />Summary: SMART's DEEP tool, utilizing transformer-based technology, predicts future article/post volume and emotions in social movements supporting the UN's SDGs. Developed by a diverse team, the tool provides probabilistic forecasts aiding decision-making. Evaluated with a case study of #MeToo movement, a dataset of 433K Reddit posts and 121K news articles from 2024 to 2025 is analyzed and will be publicly released for research. <div>
arXiv:2511.01142v1 Announce Type: new 
Abstract: Numerous social movements (SMs) around the world help support the UN's Sustainable Development Goals (SDGs). Understanding how key events shape SMs is key to the achievement of the SDGs. We have developed SMART (Social Media Analysis & Reasoning Tool) to track social movements related to the SDGs. SMART was designed by a multidisciplinary team of AI researchers, journalists, communications scholars and legal experts. This paper describes SMART's transformer-based multivariate time series Discourse Evolution Engine for Predictions about Social Movements (DEEP) to predict the volume of future articles/posts and the emotions expressed. DEEP outputs probabilistic forecasts with uncertainty estimates, providing critical support for editorial planning and strategic decision-making. We evaluate DEEP with a case study of the #MeToo movement by creating a novel longitudinal dataset (433K Reddit posts and 121K news articles) from September 2024 to June 2025 that will be publicly released for research purposes upon publication of this paper.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks</title>
<link>https://arxiv.org/abs/2511.01228</link>
<guid>https://arxiv.org/abs/2511.01228</guid>
<content:encoded><![CDATA[
<div> node importance ranking, graph data analysis, causal representation learning, node embeddings, generalization capability <br />
Summary: <br />
The paper proposes the Influence-aware Causal Autoencoder Network (ICAN), a novel framework for node importance ranking in graph data analysis. ICAN leverages causal representation learning to extract robust node embeddings that are causally related to node importance. By training exclusively on synthetic networks, ICAN eliminates the need to rely on the topology of target networks, improving generalization capability across diverse real-world graphs. The framework introduces an influence-aware causal representation learning module within an autoencoder architecture and utilizes a causal ranking loss and unified optimization framework to jointly optimize reconstruction and ranking objectives. Experimental results on benchmark datasets demonstrate that ICAN outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability. <div>
arXiv:2511.01228v1 Announce Type: new 
Abstract: Node importance ranking is a fundamental problem in graph data analysis. Existing approaches typically rely on node features derived from either traditional centrality measures or advanced graph representation learning methods, which depend directly on the target network's topology. However, this reliance on structural information raises privacy concerns and often leads to poor generalization across different networks. In this work, we address a key question: Can we design a node importance ranking model trained exclusively on synthetic networks that is effectively appliable to real-world networks, eliminating the need to rely on the topology of target networks and improving both practicality and generalizability? We answer this question affirmatively by proposing the Influence-aware Causal Autoencoder Network (ICAN), a novel framework that leverages causal representation learning to get robust, invariant node embeddings for cross-network ranking tasks. Firstly, ICAN introduces an influence-aware causal representation learning module within an autoencoder architecture to extract node embeddings that are causally related to node importance. Moreover, we introduce a causal ranking loss and design a unified optimization framework that jointly optimizes the reconstruction and ranking objectives, enabling mutual reinforcement between node representation learning and ranking optimization. This design allows ICAN, trained on synthetic networks, to generalize effectively across diverse real-world graphs. Extensive experiments on multiple benchmark datasets demonstrate that ICAN consistently outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polarization and echo chambers in Reddit's political discourse</title>
<link>https://arxiv.org/abs/2510.27467</link>
<guid>https://arxiv.org/abs/2510.27467</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, polarization, echo chambers, political debate, social media

Summary:
The study examines political debate on Reddit during the 2016 US presidential election, challenging the assumption that the platform is less polarized than others. By analyzing ideologically distinct communities based on user interactions and news consumption, the researchers identify Democratic, Conservative, and Banned subreddit clusters. They find clear polarization, intensified cross-group engagement during election periods, alignment of Banned and Conservative content, and reduced linguistic diversity within groups. The study characterizes Reddit as a polarized environment with echo chambers, emphasizing the importance of network validation for identifying behavioral and interaction patterns on social media platforms. <br /><br />Summary: The study on Reddit during the 2016 US presidential election reveals polarization, increased cross-group engagement during elections, alignment of certain subreddit clusters, and reduced linguistic diversity within groups, depicting Reddit as a polarized environment with echo chambers that necessitates network validation. <div>
arXiv:2510.27467v1 Announce Type: cross 
Abstract: Political debate nowadays takes place mainly on online social media, with election periods amplifying ideological engagement. Reddit is generally considered more resistant to polarization and echo chamber effects than platforms like Twitter or Facebook. Here, we challenge this assumption through a case study across the 2016 US presidential election. We use statistical validation techniques to extract ideologically distinct communities of subreddits, in terms of their contributing user base and news consumption, which we use to analyze the dynamics of political debate. We thus reveal clear polarization in both interaction-based and topic-based communities, with clusters of Democratic, Conservative, and Banned subreddits. Election periods intensify cross-group engagement, align Banned and Conservative content, and reduce linguistic diversity within groups. Overall we characterize Reddit as a polarized environment marked by the presence of echo chambers, highlighting network validation as a key method for identifying behavioral and interaction patterns on online social media.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Dip: a validated generalization of the Dip Test</title>
<link>https://arxiv.org/abs/2511.01705</link>
<guid>https://arxiv.org/abs/2511.01705</guid>
<content:encoded><![CDATA[
<div> Detection, multimodality, empirical distributions, statistics, data analysis  
Summary:  
- Multimodality detection in empirical distributions is crucial in various fields for analyzing data patterns.  
- Hartigan's Dip Test is a traditional method for testing multimodality but is limited by sample size and requires lookup tables.  
- The Z-Dip method, a standardized extension of the Dip Test, eliminates sample size dependence by comparing observed Dip values to simulated null distributions.  
- A universal decision threshold for Z-Dip is calibrated through simulation and bootstrap resampling, offering a consistent criterion for multimodality detection.  
- A downsampling-based approach is proposed to reduce residual sample-size effects in large datasets.  
<br /><br />Summary: <div>
arXiv:2511.01705v1 Announce Type: cross 
Abstract: Detecting multimodality in empirical distributions is a fundamental problem in statistics and data analysis, with applications ranging from clustering to social science. Hartigan's Dip Test is a classical nonparametric procedure for testing unimodality versus multimodality, but its interpretation is hindered by strong dependence on sample size and the need for lookup tables. We introduce the Z-Dip, a standardized extension of the Dip Test that removes sample-size dependence by comparing observed Dip values to simulated null distributions. We calibrate a universal decision threshold for Z-Dip via simulation and bootstrap resampling, providing a unified criterion for multimodality detection. In the final section, we also propose a downsampling-based approach to further mitigate residual sample-size effects in very large datasets. Lookup tables and software implementations are made available for efficient use in practice.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Community Detection Over Streaming Bipartite Networks (Technical Report)</title>
<link>https://arxiv.org/abs/2411.01424</link>
<guid>https://arxiv.org/abs/2411.01424</guid>
<content:encoded><![CDATA[
<div> Keywords: bipartite graph, community detection, streaming networks, butterfly motif, structural cohesiveness

Summary: 
Community Detection over Streaming Bipartite Network (CD-SBN) is proposed to identify dense subgraphs in bipartite streaming graphs. The study focuses on the butterfly motif in the bipartite graph and defines a novel community structure called $(k,r,\sigma)$-bitruss. The problem aims to retrieve communities with user-specific query keywords and high structural cohesiveness. Efficient algorithms are developed to handle snapshot and continuous CD-SBN queries by employing pruning strategies and a hierarchical synopsis. The approach's performance is demonstrated through experiments on real and synthetic streaming bipartite networks. The CD-SBN approach addresses the challenge of identifying qualified communities in bipartite streaming networks, offering a valuable solution for various real-world applications. 

<br /><br />Summary: <div>
arXiv:2411.01424v2 Announce Type: replace 
Abstract: The streaming bipartite graph is widely used to model the dynamic relationship between two types of entities in various real-world applications, including movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of the community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel $(k,r,\sigma)$-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of $(k,r,\sigma)$-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. We develop efficient algorithms to answer snapshot and continuous CD-SBN queries by traversing the synopsis and applying pruning strategies. With extensive experiments, we demonstrate the performance of our CD-SBN approach on real/synthetic streaming bipartite networks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering the Sociodemographic Fabric of Reddit</title>
<link>https://arxiv.org/abs/2502.05049</link>
<guid>https://arxiv.org/abs/2502.05049</guid>
<content:encoded><![CDATA[
<div> Keywords: sociodemographic composition, online platforms, Reddit, Naive Bayes, user self-disclosures

Summary: 
- The study focuses on understanding the sociodemographic makeup of online platforms, specifically Reddit, using a framework that relies on user-provided data.
- Over 850,000 user self-declarations of age, gender, and partisan affiliation are used to train models for sociodemographic inference, showcasing the effectiveness of simple probabilistic models like Naive Bayes.
- The approach outperforms more complex alternatives, improving classification performance by up to 19% in ROC AUC and maintaining quantification error below 15%.
- The models are well-calibrated and interpretable, allowing for uncertainty estimation and analysis of subreddit-level feature importance.
- The study advocates for a more ethical and transparent approach to computational social science, emphasizing the importance of grounding sociodemographic analysis in user-provided data rather than researcher assumptions. 

<br /><br />Summary: <div>
arXiv:2502.05049v2 Announce Type: replace 
Abstract: Understanding the sociodemographic composition of online platforms is essential for accurately interpreting digital behavior and its societal implications. Yet, current methods often lack the transparency and reliability required, risking misrepresenting social identities and distorting our understanding of digital society. Here, we introduce a principled framework for sociodemographic inference on Reddit that leverages over 850,000 user self-declarations of age, gender, and partisan affiliation. By training models on sparse user activity signals from this extensive, self-disclosed dataset, we demonstrate that simple probabilistic models, such as Naive Bayes, outperform more complex embedding-based alternatives. Our approach improves classification performance over the state of the art by up to 19% in ROC AUC and maintains quantification error below 15%. The models produce well-calibrated and interpretable outputs, enabling uncertainty estimation and subreddit-level feature importance analysis. More broadly, this work advocates for a shift toward more ethical and transparent computational social science by grounding sociodemographic analysis in user-provided data rather than researcher assumptions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating hashtag dynamics with networked groups of generative agents</title>
<link>https://arxiv.org/abs/2510.26832</link>
<guid>https://arxiv.org/abs/2510.26832</guid>
<content:encoded><![CDATA[
<div> Keywords: Networked environments, narrative interactions, Large Language Models, agent-based modeling, social rewards 

Summary:
The study explores how group communication around narrative media affects belief formation and consensus or polarization. Using a framework of Large Language Model (LLM) agents, the researchers simulated narrative interactions to understand how they evolve in networked communication settings. Benchmarking the simulations against human behaviors in an experiment and Twitter hashtags, the study found that LLMs can replicate human-like coherence in controlled environments but struggle with more complex or politically sensitive narratives. This suggests that incorporating background knowledge and social context in narrative generation may require structured prompting. The results highlight the importance of understanding how information embedded in narratives influences beliefs and behavior in networked environments, shedding light on the mechanisms behind belief formation and group dynamics. Overall, the study underscores the need for careful consideration when using LLMs in narrative communication settings to ensure accurate representation of social interactions. 

<br /><br />Summary: <div>
arXiv:2510.26832v1 Announce Type: new 
Abstract: Networked environments shape how information embedded in narratives influences individual and group beliefs and behavior. This raises key questions about how group communication around narrative media impacts belief formation and how such mechanisms contribute to the emergence of consensus or polarization. Language data from generative agents offer insight into how naturalistic forms of narrative interactions (such as hashtag generation) evolve in response to social rewards within networked communication settings. To investigate this, we developed an agent-based modeling and simulation framework composed of networks of interacting Large Language Model (LLM) agents. We benchmarked the simulations of four state-of-the-art LLMs against human group behaviors observed in a prior network experiment (Study 1) and against naturally occurring hashtags from Twitter (Study 2). Quantitative metrics of network coherence (e.g., entropy of a group's responses) reveal that while LLMs can approximate human-like coherence in sanitized domains (Study 1's experimental data), effective integration of background knowledge and social context in more complex or politically sensitive narratives likely requires careful and structured prompting.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Online Sports Fan Communities Becoming More Offensive? A Quantitative Review of Topics, Trends, and Toxicity of r/PremierLeague</title>
<link>https://arxiv.org/abs/2510.27003</link>
<guid>https://arxiv.org/abs/2510.27003</guid>
<content:encoded><![CDATA[
<div> community, sports fans, Premier League, online, toxicity

Summary:
The study focuses on the online community of sports fans, particularly those of the Premier League, analyzing over 1.1 million comments on the r/PremierLeague subreddit from 2013-2022. The research delves into sentiment, topics, and toxicity within these discussions, noting a rise in negative sentiment and toxicity over time. The subreddit has become a platform for users to discuss societal issues such as racism, the COVID-19 pandemic, and political tensions, reflecting the broader conversations happening within the fan community. The surge in popularity of online sports communities has led to more diverse discussions but has also raised concerns about the increase in negative interactions. This study provides insight into the landscape of online discussions among sports fans and highlights the need for further understanding and monitoring of online fan communities. 

<br /><br />Summary: <div>
arXiv:2510.27003v1 Announce Type: new 
Abstract: Online communities for sports fans have surged in popularity, with Reddit's r/PremierLeague emerging as a focal point for fans of one of the globe's most celebrated sports leagues. This boom has helped the Premier League make significant inroads into the US market, increasing viewership and sparking greater interest in its matches. Despite the league's broad appeal, there's still a notable gap in understanding its online fan community. Therefore, we analyzed a substantial dataset of over 1.1 million comments posted from 2013-2022 on r/PremierLeague. Our study delves into the sentiment, topics, and toxicity of these discussions, tracking trends over time, aiming to map out the conversation landscape. The rapid expansion has brought more diverse discussions, but also a worrying rise in negative sentiment and toxicity. Additionally, the subreddit has become a venue for users to voice frustrations about broader societal issues like racism, the COVID-19 pandemic, and political tensions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disrupting Networks: Amplifying Social Dissensus via Opinion Perturbation and Large Language Models</title>
<link>https://arxiv.org/abs/2510.27152</link>
<guid>https://arxiv.org/abs/2510.27152</guid>
<content:encoded><![CDATA[
<div> social networks, targeted content injection, disruption, Friedkin-Johnsen model, reinforcement learning

Summary:
Using the Friedkin-Johnsen model, the study explores disrupting social networks through targeted content injection. Simple variants of the model are ineffective in perturbing the network, but extensions show promise in achieving disruption. By altering individual opinions, disruption can be maximized. A reinforcement learning framework is developed to fine-tune a Large Language Model (LLM) for generating disruption-oriented text. Experiments on both synthetic and real-world data demonstrate that tuned LLMs can approach theoretical disruption limits. These findings have implications for content moderation, adversarial information campaigns, and regulation of generative models. 

<br /><br />Summary: <div>
arXiv:2510.27152v1 Announce Type: new 
Abstract: We study how targeted content injection can strategically disrupt social networks. Using the Friedkin-Johnsen (FJ) model, we utilize a measure of social dissensus and show that (i) simple FJ variants cannot significantly perturb the network, (ii) extending the model enables valid graph structures where disruption at equilibrium exceeds the initial state, and (iii) altering an individual's inherent opinion can maximize disruption. Building on these insights, we design a reinforcement learning framework to fine-tune a Large Language Model (LLM) for generating disruption-oriented text. Experiments on synthetic and real-world data confirm that tuned LLMs can approach theoretical disruption limits. Our findings raise important considerations for content moderation, adversarial information campaigns, and generative model regulation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Optimal Intervention for Rumor Dynamics on Networks: Node-Level, Time-Varying, and Resource-Constrained</title>
<link>https://arxiv.org/abs/2510.27165</link>
<guid>https://arxiv.org/abs/2510.27165</guid>
<content:encoded><![CDATA[
<div> Framework, Rumor propagation, Social networks, Optimal intervention, Misinformation management<br />
Summary: <br />
The article introduces a new framework for addressing rumor propagation in social networks. Traditional static, centrality-based approaches are ineffective in combating the spread of misinformation. The proposed framework utilizes a node-level, time-varying optimal intervention strategy to allocate resources based on the evolving diffusion state of the network. By solving a resource-constrained optimal control problem tied to the network structure, the framework successfully reduces both the infection peak and cumulative infection area compared to uniform and static centrality-based allocations. The approach follows a stage-aware law, focusing early resources on influential hubs to prevent rapid spread and later resources on peripheral nodes to eliminate residual transmission. This integrated approach combines global efficiency with adaptability, providing a scalable and interpretable method for managing misinformation and crisis response. <div>
arXiv:2510.27165v1 Announce Type: new 
Abstract: Rumor propagation in social networks undermines social stability and public trust, calling for interventions that are both effective and resource-efficient. We develop a node-level, time-varying optimal intervention framework that allocates limited resources according to the evolving diffusion state. Unlike static, centrality-based heuristics, our approach derives control weights by solving a resource-constrained optimal control problem tightly coupled to the network structure. Across synthetic and real-world networks, the method consistently lowers both the infection peak and the cumulative infection area relative to uniform and centrality-based static allocations. Moreover, it reveals a stage-aware law: early resources prioritize influential hubs to curb rapid spread, whereas later resources shift to peripheral nodes to eliminate residual transmission. By integrating global efficiency with fine-grained adaptability, the framework offers a scalable and interpretable paradigm for misinformation management and crisis response.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meritocracy versus Matthew-effect: Two underlying network formation mechanisms of online social platforms</title>
<link>https://arxiv.org/abs/2510.27339</link>
<guid>https://arxiv.org/abs/2510.27339</guid>
<content:encoded><![CDATA[
<div> Keyword: online social networks, network formation mechanisms, social power distribution, content creators, empirical data <br />
Summary: 
The article discusses the evolving landscape of online social networks, particularly the shift from connection-based platforms like Facebook to content-based platforms such as TikTok and Instagram. It highlights the significant inequality in social power distribution observed in content-based platforms compared to traditional ones. The authors propose two network formation mechanisms - meritocracy-based and Matthew-effect-based - to explain the distinct patterns in social power distribution. The models are validated through theoretical and numerical analysis, showcasing their ability to replicate essential statistical features and match empirical data. The study also suggests a hybrid model for platforms like academic collaboration networks, which exhibit characteristics between traditional and emerging social networks. Understanding the formation mechanisms of online social networks offers valuable insights into the evolution of content ecosystems and the behavior of content creators on these platforms. <br /><br />Summary: <div>
arXiv:2510.27339v1 Announce Type: new 
Abstract: With the rapid development of the internet industry, online social networks have come to play an increasingly significant role in everyday life. In recent years, content-based emerging platforms such as TikTok, Instagram, and Bilibili have diverged fundamentally in their underlying logic from traditional connection-based social platforms like Facebook and LinkedIn. Empirical data on follower counts and follower-count-based rankings reveal that the distribution of social power varies significantly across different types of platforms, with content-based platforms exhibiting notably greater inequality. Here we propose two fundamental network formation mechanisms: a meritocracy-based model and a Matthew-effect-based model, designed to capture the formation logic underlying traditional and emerging social networks, respectively. Through theoretical and numerical analysis, we demonstrate that both models replicate salient statistical features of social networks including scale-free and small-world property, while also closely match empirical patterns on the relationship between in-degrees and in-degree rankings, thereby capturing the distinctive distributions of social power in respective platforms. Moreover, networks such as academic collaboration networks, where the distribution of social power usually lies between that of traditional and emerging platorms, can be interpreted through a hybrid of the two proposed mechanisms. Deconstructing the formation mechanisms of online social networks offers valuable insights into the evolution of the content ecosystems and the behavioral patterns of content creators on online social platforms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Communities: A Mixed-Methods and Community-Driven Evaluation of Cultural Sensitivity in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2510.27361</link>
<guid>https://arxiv.org/abs/2510.27361</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image models, cultural sensitivity, misrepresentation, evaluation methodology, minority groups<br />
Summary:<br />
The paper explores the issue of cultural sensitivity in text-to-image (T2I) models, which often reflect Western cultural norms and perpetuate misrepresentation of minority groups. Through a mixed-methods community-based evaluation methodology developed in collaboration with individuals from 19 countries, the study uncovers the complexities of assessing cultural sensitivity in T2I models. Quantitative scores and qualitative inquiries reveal both convergence and disagreement within and across communities, shedding light on the downstream consequences of misrepresentation. The research highlights how training data influenced by unequal power relations distort representations and emphasizes the need for extensive assessments with high resource requirements. The paper offers actionable recommendations for stakeholders and suggests pathways to investigate the sources, mechanisms, and impacts of cultural (mis)representation in T2I models.<br /> 
Summary: <div>
arXiv:2510.27361v1 Announce Type: new 
Abstract: Evidence shows that text-to-image (T2I) models disproportionately reflect Western cultural norms, amplifying misrepresentation and harms to minority groups. However, evaluating cultural sensitivity is inherently complex due to its fluid and multifaceted nature. This paper draws on a state-of-the-art review and co-creation workshops involving 59 individuals from 19 different countries. We developed and validated a mixed-methods community-based evaluation methodology to assess cultural sensitivity in T2I models, which embraces first-person methods. Quantitative scores and qualitative inquiries expose convergence and disagreement within and across communities, illuminate the downstream consequences of misrepresentation, and trace how training data shaped by unequal power relations distort depictions. Extensive assessments are constrained by high resource requirements and the dynamic nature of culture, a tension we alleviate through a context-based and iterative methodology. The paper provides actionable recommendations for stakeholders, highlighting pathways to investigate the sources, mechanisms, and impacts of cultural (mis)representation in T2I models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Demographics: Behavioural Segmentation and Spatial Analytics to Enhance Visitor Experience at The British Museum</title>
<link>https://arxiv.org/abs/2510.27542</link>
<guid>https://arxiv.org/abs/2510.27542</guid>
<content:encoded><![CDATA[
<div> visitor behaviour, data science methods, British Museum, satisfaction, visitor types

Summary:
This study analyzes visitor behavior at The British Museum using data science methods such as audio guide usage logs and TripAdvisor reviews. Through the analysis of 42,000 visitor journeys and over 50,000 reviews, the study identifies key drivers of satisfaction, segments visitors into four types (Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers), examines tour engagement, models spatial navigation, and investigates room popularity. The analysis reveals high drop-off rates in tour usage, with accessibility and proximity playing a significant role in shaping visitor paths more than thematic organization. Room popularity is found to be more influenced by physical accessibility than curatorial content. The study proposes practical strategies for improving engagement and flow, providing a scalable framework for visitor-centered, data-informed museum planning. <br /><br />Summary: <div>
arXiv:2510.27542v1 Announce Type: new 
Abstract: This study explores visitor behaviour at The British Museum using data science methods applied to novel sources, including audio guide usage logs and TripAdvisor reviews. Analysing 42,000 visitor journeys and over 50,000 reviews, we identify key drivers of satisfaction, segment visitors by behavioural patterns, examine tour engagement, model spatial navigation, and investigate room popularity. Behavioural clustering uncovered four distinct visitor types: Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers, each characterised by different levels of engagement and movement. Tour usage analysis revealed high drop-off rates and variation in completion rates across different language groups. Spatial flow modelling revealed that accessibility and proximity, particularly aversion to stairs, shaped visitor paths more than thematic organisation. Room popularity was more strongly predicted by physical accessibility than curatorial content. We propose practical strategies for improving engagement and flow, offering a scalable framework for visitor-centred, data-informed museum planning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Detection on Model Explanation Graphs for Explainable AI</title>
<link>https://arxiv.org/abs/2510.27655</link>
<guid>https://arxiv.org/abs/2510.27655</guid>
<content:encoded><![CDATA[
<div> Feature-attribution methods, SHAP, LIME, Modules of Influence, model explanation graph, community detection<br />
<br />
Summary:
Modules of Influence (MoI) is a new framework for explaining individual predictions by identifying sets of features that work together to affect outcomes. The framework constructs a model explanation graph from per-instance attributions and applies community detection to find feature modules that jointly influence predictions. By quantifying the relationship between these modules and bias, redundancy, and causality patterns, MoI enhances model debugging and uncovers correlated feature groups. The framework also localizes bias exposure to specific modules, improving the understanding of model predictions. Stability and synergy metrics, a reference implementation, and evaluation protocols have been released to benchmark module discovery in explainable artificial intelligence (XAI). Overall, MoI offers a comprehensive approach to understanding and interpreting the behavior of machine learning models. <br /><br /> <div>
arXiv:2510.27655v1 Announce Type: new 
Abstract: Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison</title>
<link>https://arxiv.org/abs/2510.26899</link>
<guid>https://arxiv.org/abs/2510.26899</guid>
<content:encoded><![CDATA[
<div> Keywords: Grokipedia, Wikipedia, AI-generated, biases, encyclopedic content

Summary: 
The study compares 382 article pairs between Grokipedia and Wikipedia to examine the similarities and differences in form and content. Results show strong semantic and stylistic alignment between the two platforms, with Grokipedia producing longer but less diverse articles. Additionally, Grokipedia has fewer references per word and variable structural depth compared to Wikipedia, indicating a divergence in editorial norms. The findings suggest that AI-generated content mirrors Wikipedia's scope of information but favors narrative expansion over citation-based verification. This raises concerns around transparency, provenance, and knowledge governance in the age of automated text generation. Ultimately, while Grokipedia aims to provide "truthful" entries, the study highlights the challenges in escaping biases and limitations inherent in human-edited platforms like Wikipedia.

Summary: <div>
arXiv:2510.26899v1 Announce Type: cross 
Abstract: The launch of Grokipedia, an AI-generated encyclopedia developed by Elon Musk's xAI, was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce "truthful" entries via the large language model Grok. Yet whether an AI-driven alternative can escape the biases and limitations of human-edited platforms remains unclear. This study undertakes a large-scale computational comparison of 382 matched article pairs between Grokipedia and Wikipedia. Using metrics across lexical richness, readability, structural organization, reference density, and semantic similarity, we assess how closely the two platforms align in form and substance. The results show that while Grokipedia exhibits strong semantic and stylistic alignment with Wikipedia, it typically produces longer but less lexically diverse articles, with fewer references per word and more variable structural depth. These findings suggest that AI-generated encyclopedic content currently mirrors Wikipedia's informational scope but diverges in editorial norms, favoring narrative expansion over citation-based verification. The implications highlight new tensions around transparency, provenance, and the governance of knowledge in an era of automated text generation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2510.27195</link>
<guid>https://arxiv.org/abs/2510.27195</guid>
<content:encoded><![CDATA[
<div> AI systems; social intelligence; deception detection; Multimodal Large Language Models; MIVA<br />
<br />
Summary: 
The article discusses the challenge of automatic deception detection in dynamic, multi-party conversations and the potential of Multimodal Large Language Models (MLLMs) in addressing this issue. A new task, Multimodal Interactive Veracity Assessment (MIVA), is introduced along with a dataset derived from the social deduction game Werewolf. Despite the impressive capabilities of MLLMs, such as GPT-4o, in visual and textual understanding, they struggle to reliably distinguish truth from falsehood. The analysis of failure modes suggests that these models have difficulty grounding language in visual social cues and may be overly conservative in their alignment. This highlights the need for novel approaches to enhancing the perceptiveness and trustworthiness of AI systems. <div>
arXiv:2510.27195v1 Announce Type: cross 
Abstract: As AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Regional Disparities in Discounted Grocery Products</title>
<link>https://arxiv.org/abs/2510.27493</link>
<guid>https://arxiv.org/abs/2510.27493</guid>
<content:encoded><![CDATA[
<div> sale, retail, food waste, geographic context, sustainability

Summary: 
- Food waste contributes significantly to greenhouse gas emissions, with the retail sector playing a crucial role in mediating product flows.
- The study analyzes data from Denmark's largest retail group to understand the variations in near-expiry product sales across different geographic contexts.
- By employing a dual-clustering approach, the research identifies regional retail store clusters and their spatial separation using shortest-path distances along the street network.
- The study reveals that stores in rural areas are more likely to put meat and dairy products on sale compared to metropolitan regions, while metropolitan areas tend to favor convenience products with balanced nutritional profiles but less favorable environmental impacts.
- The findings emphasize the need for region-specific sustainability strategies to tackle food waste and reduce greenhouse gas emissions. 

<br /><br />Summary: <div>
arXiv:2510.27493v1 Announce Type: cross 
Abstract: Food waste represents a major challenge to global climate resilience, accounting for almost 10\% of annual greenhouse gas emissions. The retail sector is a critical player, mediating product flows between producers and consumers, where supply chain inefficiencies can shape which items are put on sale. Yet how these dynamics vary across geographic contexts remains largely unexplored. Here, we analyze data from Denmark's largest retail group on near-expiry products put on sale. We uncover the geospatial variations using a dual-clustering approach. We identify multi-scale spatial relationships in retail organization by correlating store clustering -- measured using shortest-path distances along the street network -- with product clustering based on promotion co-occurrence patterns. Using a bipartite network approach, we identify three regional store clusters, and use percolation thresholds to corroborate the scale of their spatial separation. We find that stores in rural communities put meat and dairy products on sale up to 2.2 times more frequently than metropolitan areas. In contrast, we find that metropolitan and capital regions lean toward convenience products, which have more balanced nutritional profiles but less favorable environmental impacts. By linking geographic context to retail inventory, we provide evidence that reducing food waste requires interventions tailored to local retail dynamics, highlighting the importance of region-specific sustainability strategies.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representing Classical Compositions through Implication-Realization Temporal-Gestalt Graphs</title>
<link>https://arxiv.org/abs/2510.27530</link>
<guid>https://arxiv.org/abs/2510.27530</guid>
<content:encoded><![CDATA[
<div> Graph-based approach, Implication-Realization model, Temporal Gestalt theory, cognitive music analysis, computational musicology <br />
Summary: This study introduces a graph-based computational approach to analyze musical compositions incorporating the Implication-Realization model and Temporal Gestalt theory. By segmenting melodies into perceptual units and annotating them with I-R patterns, the approach captures structural and cognitive aspects of music perception. Dynamic Time Warping and k-nearest neighbors graphs are used to model relationships between segments. Nodes in the graphs are labeled with melodic expectancy values to encode tension and resolution. The Weisfeiler-Lehman graph kernel shows significant distinctions in intra- and inter-graph structures, while multidimensional scaling confirms structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate capturing stylistic and structural features beyond composer identity. Overall, this graph-based method provides a structured and cognitively informed framework for computational music analysis, offering a deeper understanding of musical structure and style through listener perception. <br /><br /> <div>
arXiv:2510.27530v1 Announce Type: cross 
Abstract: Understanding the structural and cognitive underpinnings of musical compositions remains a key challenge in music theory and computational musicology. While traditional methods focus on harmony and rhythm, cognitive models such as the Implication-Realization (I-R) model and Temporal Gestalt theory offer insight into how listeners perceive and anticipate musical structure. This study presents a graph-based computational approach that operationalizes these models by segmenting melodies into perceptual units and annotating them with I-R patterns. These segments are compared using Dynamic Time Warping and organized into k-nearest neighbors graphs to model intra- and inter-segment relationships.
  Each segment is represented as a node in the graph, and nodes are further labeled with melodic expectancy values derived from Schellenberg's two-factor I-R model-quantifying pitch proximity and pitch reversal at the segment level. This labeling enables the graphs to encode both structural and cognitive information, reflecting how listeners experience musical tension and resolution.
  To evaluate the expressiveness of these graphs, we apply the Weisfeiler-Lehman graph kernel to measure similarity between and within compositions. Results reveal statistically significant distinctions between intra- and inter-graph structures. Segment-level analysis via multidimensional scaling confirms that structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate that these representations capture stylistic and structural features that extend beyond composer identity.
  These findings highlight the potential of graph-based methods as a structured, cognitively informed framework for computational music analysis, enabling a more nuanced understanding of musical structure and style through the lens of listener perception.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sybil-Resistant Service Discovery for Agent Economies</title>
<link>https://arxiv.org/abs/2510.27554</link>
<guid>https://arxiv.org/abs/2510.27554</guid>
<content:encoded><![CDATA[
<div> HTTP services, cryptocurrency payments, TraceRank, reputation propagation, semantic search 
Summary: 
x402 introduces a system that allows HTTP services to accept cryptocurrency payments and uses TraceRank, a reputation-weighted ranking algorithm, to determine the reliability of services. Payment transactions serve as endorsements, influencing the reputation of services based on transaction value and recency. This approach promotes services preferred by high-reputation users over those with high transaction volume. By combining TraceRank with semantic search, the system provides high-quality results in response to natural language queries. The reputation propagation mechanism helps in resisting Sybil attacks, ensuring that spam services with low-reputation payers rank lower than legitimate services with high-reputation payers. The system aims to create a search method for x402 enabled services that is unbiased and outperforms purely volume-based or semantic methods. 
<br /><br />Summary: <div>
arXiv:2510.27554v1 Announce Type: cross 
Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social learning moderates the tradeoffs between efficiency, stability, and equity in group foraging</title>
<link>https://arxiv.org/abs/2510.27683</link>
<guid>https://arxiv.org/abs/2510.27683</guid>
<content:encoded><![CDATA[
<div> social learning, collective search, information sharing, area-restricted search, foraging

Summary:
The study explores how social learning impacts collective search behavior in foraging scenarios. The model combines social learning with area-restricted search and considers three behavioral modes: exploration, exploitation, and targeted walk. A parameter, $\rho$, regulates the balance between exploration and exploitation at the group level. Results show a trade-off between group efficiency, temporal variability, and agent equity in resource distribution based on $\rho$. Optimal group efficiency occurs at intermediate $\rho$ values balancing exploration and exploitation. At high $\rho$, agent equality is high, but efficiency decreases. Introducing negative rewards demonstrates how social learning can mitigate risk. <div>
arXiv:2510.27683v1 Announce Type: cross 
Abstract: Social learning shapes collective search by influencing how individuals use peer information. Empirical and computational studies show that optimal information sharing that is neither too localized nor too diffuse, can enhance resource detection and coordination. Building on these insights, we develop a randomized search model that integrates social learning with area-restricted search (ARS) to investigate how communication distance affects collective foraging. The model includes three behavioral modes: exploration, exploitation, and targeted walk, which are governed by a single parameter, $\rho$, that balances exploration and exploitation at the group level. We quantify how $\rho$ influences group efficiency ($\eta$), temporal variability/burstiness ($B$), and agent variability/equity in resource distribution ($\sigma$), revealing a clear trade-off among these outcomes. When $\rho \to 0$, agents explore independently, maximizing collective exploration. As $\rho$ increases, individuals preferentially exploit patches discovered by others: $\eta$ first rises and then declines, while $B$ shows the opposite trend. Group efficiency is optimized at interior $\rho$ values that balance exploration and exploitation. At the largest $\rho$, equality among agents is highest, but efficiency declines and burstiness is maximized too. Finally, by introducing negative rewards, we examine how social learning mitigates risk.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Modeling of Information Diffusion on Networks with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2411.09100</link>
<guid>https://arxiv.org/abs/2411.09100</guid>
<content:encoded><![CDATA[
<div> likelihood-based approach, edge weights, General Linear Threshold model, Influence Maximization, information diffusion paths<br />
Summary:<br />
The paper focuses on the estimation of edge weights in information spread models through networks, specifically under the General Linear Threshold (GLT) model. It addresses the need for accurate estimators of edge weights in probabilistic diffusion models and establishes the asymptotic properties of the estimator for the GLT model. The study includes deriving conditions for edge weight identifiability, finite sample error bounds, and asymptotic normality of the estimator. Additionally, the paper explores the application of the GLT model in the context of the Influence Maximization (IM) problem, optimizing node selections to maximize information spread. Through experiments on synthetic and real-world networks, the GLT model and estimation framework show improved spread estimation, node activation prediction, and quality of IM problem solutions. <div>
arXiv:2411.09100v2 Announce Type: replace 
Abstract: Modeling information spread through a network is one of the key problems of network analysis, with applications in a wide array of areas such as marketing and public health. Most approaches assume that the spread is governed by some probabilistic diffusion model, often parameterized by the strength of connections between network members (edge weights), highlighting the need for methods that can accurately estimate them. Multiple prior works suggest such estimators for particular diffusion models; however, most of them lack a rigorous statistical analysis that would establish the asymptotic properties of the estimator and allow for uncertainty quantification. In this paper, we develop a likelihood-based approach to estimate edge weights from the observed information diffusion paths under the proposed General Linear Threshold (GLT) model, a broad class of discrete-time information diffusion models that includes both the well-known linear threshold (LT) and independent cascade (IC) models. We first derive necessary and sufficient conditions that make the edge weights identifiable under this model. Then, we derive a finite sample error bound for the estimator and demonstrate that it is asymptotically normal under mild conditions. We conclude by studying the GLT model in the context of the Influence Maximization (IM) problem, that is, the task of selecting a subset of $k$ nodes to start the diffusion, so that the average information spread is maximized. Extensive experiments on synthetic and real-world networks demonstrate that the flexibility of the proposed class of GLT models, coupled with the proposed estimation and inference framework for its parameters, can significantly improve estimation of spread from a given subset of nodes, prediction of node activation, and the quality of the IM problem solutions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Media Coverage of War Victims: Journalistic Biases in Reporting on Israel and Gaza</title>
<link>https://arxiv.org/abs/2510.06453</link>
<guid>https://arxiv.org/abs/2510.06453</guid>
<content:encoded><![CDATA[
<div> media bias, asymmetrical warfare, Israeli victims, Palestinian victims, journalistic biases

Summary: 
Western media coverage of the war against Gaza in 2023 showed three systematic biases. Firstly, Israeli victims were portrayed as individual human beings, while Palestinian victims were depicted as undifferentiated collectives. Secondly, a false balance was created by equating Israeli and Palestinian suffering, even when there were no new events involving Israeli victims. Lastly, when reporting on the number of victims, journalists cast doubt on the credibility of information regarding Palestinian suffering, undermining readers' trust. These biases were prevalent in Western outlets like The New York Times, BBC, and CNN but were minimal in Al Jazeera's coverage of the conflict. This analysis sheds light on the role of media bias in shaping perceptions of asymmetrical warfare and highlights the need for more objective reporting in conflict-ridden regions. 

<br /><br />Summary: <div>
arXiv:2510.06453v2 Announce Type: replace 
Abstract: October 7th 2023 marked the start of a war against Gaza, which is considered one of the most devastating wars in modern history and has led to a stark attitudinal divide within and between countries. To investigate the role of media bias in reporting on this asymmetrical warfare, we analyzed over 14,000 news articles published during the first year of war in three Western (The New York Times, BBC, CNN) and one non-Western English-language outlets (Al Jazeera English). Exploring the media narratives concerning Israeli and Palestinian victims experiencing hardship, we found three systematic biases in Western media. 1) Compared to Palestinian victims, represented mainly as undifferentiated collectives, Israeli victims were more likely to be portrayed as identifiable individual human beings. 2) Despite the striking difference in all forms of hardship (casualties, displacement, etc.), Western journalists created a false balance, equating Israeli and Palestinian suffering, by persistently referring back to the 7th of October massacre, even in the absence of new events involving Israeli victims. 3) When reporting on numbers of Palestinian (vs. Israeli) victims, journalists used language that casts doubt about the credibility of the information and the reputation of the source providing it, thereby selectively undermining the reader's trust in the information regarding Palestinian suffering. Together, our analysis reveals a series of systematic journalistic biases in high-profile Western media that are absent or greatly reduced in Al Jazeera.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flex-GAD : Flexible Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.25809</link>
<guid>https://arxiv.org/abs/2510.25809</guid>
<content:encoded><![CDATA[
<div> Framework, unsupervised, anomaly detection, attributed networks, node level<br />
Summary:<br />
- Flex-GAD is an unsupervised framework designed for detecting anomalous nodes in attributed networks, incorporating two encoders to capture structural and attribute information.<br />
- It integrates a community-based GCN encoder to model intra-community and inter-community relationships, ensuring structural consistency, and a standard attribute encoder for descriptive features.<br />
- The framework utilizes a self-attention-based representation fusion module to combine diverse representations, allowing adaptive weighting for effective integration of information from different encoders.<br />
- Flex-GAD outperforms the previous best-performing method, GAD-NR, by achieving an average AUC improvement of 7.98% across seven real-world attributed graphs, showcasing its effectiveness and flexibility.<br />
- Additionally, Flex-GAD significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average, making it a powerful and efficient tool for graph anomaly detection. <br /> 
Summary: <div>
arXiv:2510.25809v1 Announce Type: new 
Abstract: Detecting anomalous nodes in attributed networks, where each node is associated with both structural connections and descriptive attributes, is essential for identifying fraud, misinformation, and suspicious behavior in domains such as social networks, academic citation graphs, and e-commerce platforms. We propose Flex-GAD, a novel unsupervised framework for graph anomaly detection at the node level. Flex-GAD integrates two encoders to capture complementary aspects of graph data. The framework incorporates a novel community-based GCN encoder to model intra-community and inter-community information into node embeddings, thereby ensuring structural consistency, along with a standard attribute encoder. These diverse representations are fused using a self-attention-based representation fusion module, which enables adaptive weighting and effective integration of the encoded information. This fusion mechanism allows automatic emphasis of the most relevant node representation across different encoders. We evaluate Flex-GAD on seven real-world attributed graphs with varying sizes, node degrees, and attribute homogeneity. Flex-GAD achieves an average AUC improvement of 7.98% over the previously best-performing method, GAD-NR, demonstrating its effectiveness and flexibility across diverse graph structures. Moreover, it significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average across seven benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signed Graph Unlearning</title>
<link>https://arxiv.org/abs/2510.26092</link>
<guid>https://arxiv.org/abs/2510.26092</guid>
<content:encoded><![CDATA[
<div> privacy-preserving mechanisms, signed networks, graph unlearning, SGU, model performance<br />
<br />
Summary: 
The article discusses the importance of privacy-preserving mechanisms in signed networks on social media platforms. It introduces SGU (Signed Graph Unlearning), a framework specifically designed for signed networks. SGU incorporates a new partition paradigm and algorithm that preserve edge sign information during partitioning, ensuring structural balance across partitions. Existing graph unlearning methods designed for unsigned networks do not consider edge sign information, leading to structural imbalance and decreased model performance in signed networks. SGU outperforms baselines in both model performance and unlearning efficiency, making it a state-of-the-art solution for managing sensitive and dynamic user interactions in signed networks. <div>
arXiv:2510.26092v1 Announce Type: new 
Abstract: The proliferation of signed networks in contemporary social media platforms necessitates robust privacy-preserving mechanisms. Graph unlearning, which aims to eliminate the influence of specific data points from trained models without full retraining, becomes particularly critical in these scenarios where user interactions are sensitive and dynamic. Existing graph unlearning methodologies are exclusively designed for unsigned networks and fail to account for the unique structural properties of signed graphs. Their naive application to signed networks neglects edge sign information, leading to structural imbalance across subgraphs and consequently degrading both model performance and unlearning efficiency. This paper proposes SGU (Signed Graph Unlearning), a graph unlearning framework specifically for signed networks. SGU incorporates a new graph unlearning partition paradigm and a novel signed network partition algorithm that preserve edge sign information during partitioning and ensure structural balance across partitions. Compared with baselines, SGU achieves state-of-the-art results in both model performance and unlearning efficiency.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating and Experimenting with Social Media Mobilization Using LLM Agents</title>
<link>https://arxiv.org/abs/2510.26494</link>
<guid>https://arxiv.org/abs/2510.26494</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, political mobilization, agent-based simulation, voter turnout, peer influence

Summary: 
An agent-based simulation framework was developed to study the impact of mobilization messages on voter turnout in online social networks. The framework integrates real U.S. Census demographic distributions, authentic Twitter network topology, and large language model (LLM) agents with varying political sophistication. Simulated agents interact over realistic social network structures, receiving personalized feeds and updating their engagement behaviors and voting intentions. The simulation replicates patterns observed in field experiments, showing stronger mobilization effects under social message treatments and peer spillovers. This framework allows for testing counterfactual designs and sensitivity analyses in political mobilization research, bridging the gap between field experiments and computational modeling. The code and data for the simulation are available on GitHub for reproducibility and further research. 

<br /><br />Summary: <div>
arXiv:2510.26494v1 Announce Type: new 
Abstract: Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\texttt{GPT-4.1}, \texttt{GPT-4.1-Mini}, or \texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis</title>
<link>https://arxiv.org/abs/2510.26172</link>
<guid>https://arxiv.org/abs/2510.26172</guid>
<content:encoded><![CDATA[
<div> Keywords: social media mining, large language models, heterogeneous data, visualization, multi-modal integration

Summary:<br />
The article discusses the challenges of analyzing heterogeneous data from social media platforms and introduces SIA (Social Insight Agents), an automated system that leverages large language models to analyze raw inputs such as text, network, and behavioral data. SIA connects different data types through coordinated agent flows guided by a taxonomy linking insight types with appropriate mining and visualization techniques. The system includes a data coordinator to unify tabular, textual, and network data for coherent analysis. SIA's interactive interface allows users to track, validate, and refine the agent's reasoning, supporting adaptability and trustworthiness. Expert-centered case studies and quantitative evaluations demonstrate that SIA effectively uncovers diverse insights from social media while facilitating collaboration between humans and agents in complex analytical tasks. <div>
arXiv:2510.26172v1 Announce Type: cross 
Abstract: Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning</title>
<link>https://arxiv.org/abs/2504.10456</link>
<guid>https://arxiv.org/abs/2504.10456</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Learning Networks, Federated Learning, Privacy-preserving, Model personalization, Explainable AI<br />
Summary: 
This study proposes a framework that integrates Federated Learning with Social Learning Networks to predict future interactions among students in multiple classrooms. By leveraging Federated Learning, the approach allows collaborative model training while preserving data privacy. Model personalization techniques are utilized to adapt the model to individual classroom characteristics. The results demonstrate the effectiveness of the approach in capturing both shared and classroom-specific representations of student interactions. Additionally, explainable AI techniques are employed to interpret model predictions, identifying key factors influencing link formation across classrooms. This collaborative and distributed machine learning framework provides insights into the drivers of social learning interactions in a privacy-preserving manner. <div>
arXiv:2504.10456v2 Announce Type: replace 
Abstract: Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merit Network Telescope: Processing and Initial Insights from Nearly 20 Years of Darknet Traffic for Cybersecurity Research</title>
<link>https://arxiv.org/abs/2510.25050</link>
<guid>https://arxiv.org/abs/2510.25050</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet traffic, network telescope, threat activity, scanning behavior, denial-of-service campaigns

Summary:
An initial longitudinal analysis of unsolicited Internet traffic collected between 2005 and 2025 by a network telescope operated by Merit Network reveals global threat activity through scanning and backscatter traffic. The dataset allows insights into large-scale probing behavior, data outages, and ongoing denial-of-service campaigns. A coarse-to-fine methodology is used for processing the extensive archive, extracting general insights through a metadata sub-pipeline and detailed analysis with a packet header sub-pipeline. Long-term trends and recurring traffic spikes are observed, some attributed to Internet-wide scanning events and others linked to DoS activities. General observations from 2006-2024 are presented, with a focused analysis of traffic characteristics in 2024. <div>
arXiv:2510.25050v1 Announce Type: new 
Abstract: This paper presents an initial longitudinal analysis of unsolicited Internet traffic collected between 2005 and 2025 by one of the largest and most persistent network telescopes in the United States, operated by Merit Network. The dataset provides a unique view into global threat activity as observed through scanning and backscatter traffic, key indicators of large-scale probing behavior, data outages, and ongoing denial-of-service (DoS) campaigns. To process this extensive archive, coarse-to-fine methodology is adopted in which general insights are first extracted through a resource-efficient metadata sub-pipeline, followed by a more detailed packet header sub-pipeline for finer-grained analysis. The methodology establishes two sub-pipelines to enable scalable processing of nearly two decades of telescope data and supports multi-level exploration of traffic dynamics. Initial insights highlight long-term trends and recurring traffic spikes, some attributable to Internet-wide scanning events and others likely linked to DoS activities.We present general observations spanning 2006-2024, with a focused analysis of traffic characteristics during 2024.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMM-Fact: A Multimodal, Multi-Domain Fact-Checking Dataset with Multi-Level Retrieval Difficulty</title>
<link>https://arxiv.org/abs/2510.25120</link>
<guid>https://arxiv.org/abs/2510.25120</guid>
<content:encoded><![CDATA[
<div> multimodal, fact-checking, benchmark, misinformation, evidence  
Summary:  
The article introduces MMM-Fact, a new benchmark for fact-checking that addresses the limitations of existing resources. MMM-Fact includes 125,449 fact-checked statements from 1995 to 2025, covering multiple domains and providing multimodal evidence such as text, images, videos, and tables. Each statement is tagged with a retrieval-difficulty tier to support fairness in evaluation. The dataset uses a three-class veracity scheme (true/false/not enough information) and enables tasks in veracity prediction, explainable fact-checking, evidence aggregation, and longitudinal analysis. Baseline models using mainstream LLMs show that MMM-Fact is more challenging than previous benchmarks, especially as evidence complexity increases. Overall, MMM-Fact offers a realistic and scalable benchmark for transparent and reliable fact-checking that goes beyond simple evidence-based reasoning.  
Summary: <div>
arXiv:2510.25120v1 Announce Type: new 
Abstract: Misinformation and disinformation demand fact checking that goes beyond simple evidence-based reasoning. Existing benchmarks fall short: they are largely single modality (text-only), span short time horizons, use shallow evidence, cover domains unevenly, and often omit full articles -- obscuring models' real-world capability. We present MMM-Fact, a large-scale benchmark of 125,449 fact-checked statements (1995--2025) across multiple domains, each paired with the full fact-check article and multimodal evidence (text, images, videos, tables) from four fact-checking sites and one news outlet. To reflect verification effort, each statement is tagged with a retrieval-difficulty tier -- Basic (1--5 sources), Intermediate (6--10), and Advanced (>10) -- supporting fairness-aware evaluation for multi-step, cross-modal reasoning. The dataset adopts a three-class veracity scheme (true/false/not enough information) and enables tasks in veracity prediction, explainable fact-checking, complex evidence aggregation, and longitudinal analysis. Baselines with mainstream LLMs show MMM-Fact is markedly harder than prior resources, with performance degrading as evidence complexity rises. MMM-Fact offers a realistic, scalable benchmark for transparent, reliable, multimodal fact-checking.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Emotional Co-occurrence Patterns Revealed by Network Analysis of Social Media</title>
<link>https://arxiv.org/abs/2510.25204</link>
<guid>https://arxiv.org/abs/2510.25204</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion network, social media, crisis events, network theory, emotion co-occurrence

Summary: 
- The study explores emotion interactions as an emotion network in social media during crisis events and normal times, using a network theory-based computational approach.
- Large-scale Japanese social media data from the past decade covering earthquakes and COVID-19 vaccination periods is analyzed.
- Emotion links, represented by link strength, particularly those associated with Tension, are found to be significantly strengthened during earthquake and pre-vaccination periods.
- The stability of emotion network structure across different situations and over time at the population level is revealed.
- The research challenges the context-based assumption of emotion co-occurrence and provides deeper insights into the intrinsic structure of emotions. 
<br /><br />Summary: <div>
arXiv:2510.25204v1 Announce Type: new 
Abstract: Examining emotion interactions as an emotion network in social media offers key insights into human psychology, yet few studies have explored how fluctuations in such emotion network evolve during crises and normal times. This study proposes a novel computational approach grounded in network theory, leveraging large-scale Japanese social media data spanning varied crisis events (earthquakes and COVID-19 vaccination) and non-crisis periods over the past decade. Our analysis identifies and evaluates links between emotions through the co-occurrence of emotion-related concepts (words), revealing a stable structure of emotion network across situations and over time at the population level. We find that some emotion links (represented as link strength) such as emotion links associated with Tension are significantly strengthened during earthquake and pre-vaccination periods. However, the rank of emotion links remains highly intact. These findings challenge the assumption that emotion co-occurrence is context-based and offer a deeper understanding of emotions' intrinsic structure. Moreover, our network-based framework offers a systematic, scalable method for analyzing emotion co-occurrence dynamics, opening new avenues for psychological research using large-scale textual data.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Correlation in Graphs by Counting Bounded Degree Motifs</title>
<link>https://arxiv.org/abs/2510.25289</link>
<guid>https://arxiv.org/abs/2510.25289</guid>
<content:encoded><![CDATA[
<div> graphs, correlation, hypothesis testing, motifs, networks

Summary:
- The paper focuses on detecting correlation between two Erds-Rnyi graphs, treating it as a hypothesis testing problem.
- A polynomial-time test is developed using bounded degree motifs, proving its effectiveness for any constant correlation coefficient when the edge connecting probability is at least n^-2/3.
- Results show that the test works for any constant correlation, overcoming previous limitations.
- The use of bounded degree motifs, common in real networks, makes the proposed test both intuitive and scalable.
- Validation on synthetic and real co-citation networks confirms the effectiveness of this approach in capturing correlation signals. 

<br /><br />Summary: <div>
arXiv:2510.25289v1 Announce Type: new 
Abstract: Correlation analysis is a fundamental step for extracting meaningful insights from complex datasets. In this paper, we investigate the problem of detecting correlation between two Erd\H{o}s-R\'enyi graphs $G(n,p)$, formulated as a hypothesis testing problem: under the null hypothesis, the two graphs are independent, while under the alternative hypothesis, they are correlated. We develop a polynomial-time test by counting bounded degree motifs and prove its effectiveness for any constant correlation coefficient $\rho$ when the edge connecting probability satisfies $p\ge n^{-2/3}$. Our results overcome the limitation requiring $\rho \ge \sqrt{\alpha}$, where $\alpha\approx 0.338$ is the Otter's constant, extending it to any constant $\rho$. Methodologically, bounded degree motifs -- ubiquitous in real networks -- make the proposed statistic both natural and scalable. We also validate our method on synthetic and real co-citation networks, further confirming that this simple motif family effectively captures correlation signals and exhibits strong empirical performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YTLive: A Dataset of Real-World YouTube Live Streaming Sessions</title>
<link>https://arxiv.org/abs/2510.24769</link>
<guid>https://arxiv.org/abs/2510.24769</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube Live, live streaming, viewer behavior, dataset, temporal viewing patterns

Summary:
The article introduces the YTLive dataset, a public dataset focused on YouTube Live, containing over 507,000 records from 12,156 live streams. The dataset tracks concurrent viewer counts at five-minute intervals, providing insights into viewer behavior in real-time. An initial analysis of the dataset reveals that viewer counts are higher and more stable on weekends, particularly in the afternoon. Shorter streams attract larger and more consistent audiences, while longer streams exhibit slower growth and greater variability. These findings have implications for adaptive streaming, resource allocation, and Quality of Experience (QoE) modeling in live streaming platforms. YTLive serves as a valuable resource for researchers and industry professionals looking to study and improve live streaming systems. The dataset is available on GitHub for further analysis and research purposes. 

<br /><br />Summary: <div>
arXiv:2510.24769v1 Announce Type: cross 
Abstract: Live streaming plays a major role in today's digital platforms, supporting entertainment, education, social media, etc. However, research in this field is limited by the lack of large, publicly available datasets that capture real-time viewer behavior at scale. To address this gap, we introduce YTLive, a public dataset focused on YouTube Live. Collected through the YouTube Researcher Program over May and June 2024, YTLive includes more than 507000 records from 12156 live streams, tracking concurrent viewer counts at five-minute intervals along with precise broadcast durations. We describe the dataset design and collection process and present an initial analysis of temporal viewing patterns. Results show that viewer counts are higher and more stable on weekends, especially during afternoon hours. Shorter streams attract larger and more consistent audiences, while longer streams tend to grow slowly and exhibit greater variability. These insights have direct implications for adaptive streaming, resource allocation, and Quality of Experience (QoE) modeling. YTLive offers a timely, open resource to support reproducible research and system-level innovation in live streaming. The dataset is publicly available at github.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction</title>
<link>https://arxiv.org/abs/2510.25348</link>
<guid>https://arxiv.org/abs/2510.25348</guid>
<content:encoded><![CDATA[
<div> Keywords: information cascade prediction, social networks, e-commerce dataset, temporal modeling, state-of-the-art performance

Summary:
- The article addresses the limitations of current information cascade popularity prediction works.
- It introduces a time-ordered splitting strategy to avoid temporal leakage in evaluation.
- A new dataset, Taoke, is presented with rich promoter/product attributes and ground-truth purchase conversions.
- The CasTemp framework is developed, utilizing temporal walks, neighbor selection, and GRU-based encoding with time-aware attention for efficient cascade dynamics modeling.
- CasTemp achieves state-of-the-art performance on four datasets with a significant speedup, particularly excelling in predicting second-stage popularity conversions. 

<br /><br />Summary: <div>
arXiv:2510.25348v1 Announce Type: cross 
Abstract: Information cascade popularity prediction is a key problem in analyzing content diffusion in social networks. However, current related works suffer from three critical limitations: (1) temporal leakage in current evaluation--random cascade-based splits allow models to access future information, yielding unrealistic results; (2) feature-poor datasets that lack downstream conversion signals (e.g., likes, comments, or purchases), which limits more practical applications; (3) computational inefficiency of complex graph-based methods that require days of training for marginal gains. We systematically address these challenges from three perspectives: task setup, dataset construction, and model design. First, we propose a time-ordered splitting strategy that chronologically partitions data into consecutive windows, ensuring models are evaluated on genuine forecasting tasks without future information leakage. Second, we introduce Taoke, a large-scale e-commerce cascade dataset featuring rich promoter/product attributes and ground-truth purchase conversions--capturing the complete diffusion lifecycle from promotion to monetization. Third, we develop CasTemp, a lightweight framework that efficiently models cascade dynamics through temporal walks, Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based encoding with time-aware attention. Under leak-free evaluation, CasTemp achieves state-of-the-art performance across four datasets with orders-of-magnitude speedup. Notably, it excels at predicting second-stage popularity conversions--a practical task critical for real-world applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph clustering using Ricci curvature: an edge transport perspective</title>
<link>https://arxiv.org/abs/2412.15695</link>
<guid>https://arxiv.org/abs/2412.15695</guid>
<content:encoded><![CDATA[
<div> Ricci flow; hypergraphs; probability measures; community detection; weighting<br />
<br />
Summary: 
The paper introduces a novel method for extending Ricci flow to hypergraphs by defining probability measures on edges and transporting them on the line expansion. This new approach results in a new weighting on edges that improves community detection. A comparison with a similar Ricci flow defined on clique expansion shows enhanced sensitivity to hypergraph structure, particularly with large hyperedges. The two methods, when used together, create a powerful and interpretable framework for community detection in hypergraphs. <div>
arXiv:2412.15695v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a novel method for extending Ricci flow to hypergraphs by defining probability measures on the edges and transporting them on the line expansion. This approach yields a new weighting on the edges, which proves particularly effective for community detection. We extensively compare this method with a similar notion of Ricci flow defined on the clique expansion, demonstrating its enhanced sensitivity to the hypergraph structure, especially in the presence of large hyperedges. The two methods are complementary and together form a powerful and highly interpretable framework for community detection in hypergraphs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global YouTube Trending Dataset (2022-2025): Three Years of Platform-Curated, Cross-National Trends in Digital Culture</title>
<link>https://arxiv.org/abs/2510.23645</link>
<guid>https://arxiv.org/abs/2510.23645</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube, Trending, dataset, video, global attention

Summary: 
The article discusses the retirement of YouTube's public "Trending" pages on July 1, 2025, and the implications of this change on video discovery. A three-year archival dataset of YouTube Trending videos from July 1, 2022, to June 30, 2025, is presented, providing a valuable resource for studying global attention and cultural salience. The dataset includes 446,971 snapshots from 104 countries, encompassing 78.4 million video entries with associated metadata. Each record contains core identifiers and content metadata, allowing for in-depth analysis of digital culture and platform governance. The article outlines the data collection methodology, schema design, coverage, and descriptive statistics for both global and U.S. trending videos. Ethical safeguards were also implemented throughout the data collection process. This dataset offers a unique opportunity to study temporal dynamics in content popularity and platform influence on a global scale.<br /><br />Summary: <div>
arXiv:2510.23645v1 Announce Type: new 
Abstract: On July 1, 2025, YouTube retired its decade-long public "Trending" pages, ending platform-curated, non-personalized video discovery. The Trending list had long served as a vital lens into algorithmic influence, cultural diffusion, and crisis communication globally, offering a rare "ground-truth" reference to study global attention and cultural salience. We present a three-year archival dataset of YouTube Trending videos, collected from July 1, 2022, to June 30, 2025, with four daily snapshots for each of the 104 countries. The dataset includes 446,971 snapshots, each capturing up to 200 trending videos, encompassing 78.4 million video entries (726,627 unique videos) and associated metadata. Each record includes core identifiers (snapshot time, country, rank) and content metadata (video ID, channel ID, title, description, tags, publication date, category, channel name, language, live status, views, and comments). Unlike previous datasets with limited geographic scope or short timeframes, our non-personalized data provides exceptional cross-national and longitudinal coverage for studying digital culture, platform governance, and temporal dynamics in content popularity. We document the data collection methodology, schema design, coverage, descriptive statistics for both global and U.S. trending videos, and the ethical safeguards implemented throughout.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamming Graph Metrics: A Multi-Scale Framework for Structural Redundancy and Uniqueness in Graphs</title>
<link>https://arxiv.org/abs/2510.23646</link>
<guid>https://arxiv.org/abs/2510.23646</guid>
<content:encoded><![CDATA[
<div> reachability tensor, graph centrality, Hamming Graph Metrics, multi-scale connectivity patterns, structural uniqueness

Summary: The paper introduces Hamming Graph Metrics (HGM), a framework to quantify the structural uniqueness of multi-scale connectivity patterns in graphs. HGM represents a graph using an exact-$k$ reachability tensor $\mathcal{B}G$, capturing shortest-path distances. The framework ensures permutation invariance and defines a true metric, the tensor Hamming distance, on labeled graphs. It offers Lipschitz stability to edge perturbations. The paper develops per-scale spectral analysis, summary statistics for structural dissimilarity, graph-to-graph comparison using the metric, and various analytic properties such as extremal characterizations and stability bounds. This framework enables a deeper understanding of network resilience and function by capturing the unique connectivity patterns present in graphs.<br /><br /> <div>
arXiv:2510.23646v1 Announce Type: new 
Abstract: Traditional graph centrality measures effectively quantify node importance but fail to capture the structural uniqueness of multi-scale connectivity patterns -- critical for understanding network resilience and function. This paper introduces \emph{Hamming Graph Metrics (HGM)}, a framework that represents a graph by its exact-$k$ reachability tensor $\mathcal{B}G\in{0,1}^{N\times N\times D}$ with slices $(\mathcal{B}G){:,:,1}=A$ and, for $k\ge 2$, $(\mathcal{B}G){:,:,k}=\mathbf{1}!\left[\sum{t=1}^{k} A^t>0\right]-\mathbf{1}!\left[\sum_{t=1}^{k-1} A^t>0\right]$ (shortest-path distance exactly $k$). Guarantees. (i) \emph{Permutation invariance}: $d_{\mathrm{HGM}}(\pi(G),\pi(H))=d_{\mathrm{HGM}}(G,H)$ for all vertex relabelings $\pi$; (ii) the \emph{tensor Hamming distance} $d_{\mathrm{HGM}}(G,H):=|,\mathcal{B}G-\mathcal{B}H,|{1}=\sum{i,j,k}\mathbf{1}!\big[(\mathcal{B}G){ijk}\neq(\mathcal{B}H){ijk}\big]$ is a \emph{true metric} on labeled graphs; and (iii) \emph{Lipschitz stability} to edge perturbations with explicit degree-dependent constants (see Graph-to-Graph Comparison'' $\to$ Tensor Hamming metric''; ``Stability to edge perturbations''; Appendix A). We develop: (1) \emph{per-scale spectral analysis} via classical MDS on double-centered Hamming matrices $D^{(k)}$, yielding spectral coordinates and explained variances; (2) \emph{summary statistics} for node-wise and graph-level structural dissimilarity; (3) \emph{graph-to-graph comparison} via the metric above; and (4) \emph{analytic properties} including extremal characterizations, multi-scale limits, and stability bounds.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection</title>
<link>https://arxiv.org/abs/2510.23648</link>
<guid>https://arxiv.org/abs/2510.23648</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, BERT, semantic embeddings, max pooling, user-level representations
Summary:
- The article discusses the challenge of detecting automated accounts on platforms like Twitter.
- Existing methods combine text, metadata, and user relationship info in graph-based frameworks.
- Many models rely heavily on explicit user-user relationship data, limiting their applicability.
- The proposed multimodal framework integrates textual features with user metadata and employs graph-based reasoning without needing follower-following data.
- Transformer-based models extract deep embeddings from tweets, aggregated using max pooling for user-level representations, combined with behavioral features, and passed through a GraphSAGE model for local and global behavior patterns.
<br /><br />Summary: <div>
arXiv:2510.23648v1 Announce Type: new 
Abstract: Detecting automated accounts (bots) among genuine users on platforms like Twitter remains a challenging task due to the evolving behaviors and adaptive strategies of such accounts. While recent methods have achieved strong detection performance by combining text, metadata, and user relationship information within graph-based frameworks, many of these models heavily depend on explicit user-user relationship data. This reliance limits their applicability in scenarios where such information is unavailable. To address this limitation, we propose a novel multimodal framework that integrates detailed textual features with enriched user metadata while employing graph-based reasoning without requiring follower-following data. Our method uses transformer-based models (e.g., BERT) to extract deep semantic embeddings from tweets, which are aggregated using max pooling to form comprehensive user-level representations. These are further combined with auxiliary behavioral features and passed through a GraphSAGE model to capture both local and global patterns in user behavior. Experimental results on the Cresci-15, Cresci-17, and PAN 2019 datasets demonstrate the robustness of our approach, achieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting its effectiveness against increasingly sophisticated bot strategies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title>
<link>https://arxiv.org/abs/2510.23662</link>
<guid>https://arxiv.org/abs/2510.23662</guid>
<content:encoded><![CDATA[
<div> Keywords: urban sensing, human mobility data, spatiotemporal model, self-supervised learning, dynamic graph

Summary: 
The paper introduces the General-purpose and Dynamic Human Mobility Embedding (GDHME) framework for urban sensing using large-scale human mobility data collected from base stations. The framework employs self-supervised learning to capture the latent semantics behind mobility behavior and supports various urban sensing tasks. In the first stage, GDHME treats people and regions as nodes in a dynamic graph, capturing evolving node representations in continuous time. An autoregressive self-supervised task guides the learning of general-purpose node embeddings. In the second stage, these representations are utilized for various tasks. Offline experiments demonstrate the framework's ability to automatically learn valuable node features from vast data. The framework was used to deploy the JiuTian ChuanLiu Big Model at the 2023 China Mobile Worldwide Partner Conference. <div>
arXiv:2510.23662v1 Announce Type: new 
Abstract: As a window for urban sensing, human mobility contains rich spatiotemporal information that reflects both residents' behavior preferences and the functions of urban areas. The analysis of human mobility has attracted the attention of many researchers. However, existing methods often address specific tasks from a particular perspective, leading to insufficient modeling of human mobility and limited applicability of the learned knowledge in various downstream applications. To address these challenges, this paper proposes to push massive amounts of human mobility data into a spatiotemporal model, discover latent semantics behind mobility behavior and support various urban sensing tasks. Specifically, a large-scale and widely covering human mobility data is collected through the ubiquitous base station system and a framework named General-purpose and Dynamic Human Mobility Embedding (GDHME) for urban sensing is introduced. The framework follows the self-supervised learning idea and contains two major stages. In stage 1, GDHME treats people and regions as nodes within a dynamic graph, unifying human mobility data as people-region-time interactions. An encoder operating in continuous-time dynamically computes evolving node representations, capturing dynamic states for both people and regions. Moreover, an autoregressive self-supervised task is specially designed to guide the learning of the general-purpose node embeddings. In stage 2, these representations are utilized to support various tasks. To evaluate the effectiveness of our GDHME framework, we further construct a multi-task urban sensing benchmark. Offline experiments demonstrate GDHME's ability to automatically learn valuable node features from vast amounts of data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu Big Model, a system that has been presented at the 2023 China Mobile Worldwide Partner Conference.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting sub-populations in online health communities: A mixed-methods exploration of breastfeeding messages in BabyCenter Birth Clubs</title>
<link>https://arxiv.org/abs/2510.23692</link>
<guid>https://arxiv.org/abs/2510.23692</guid>
<content:encoded><![CDATA[
<div> stress, parental, breastfeeding, anxiety, topics

Summary:
- The study examines parental stress through analyzing BabyCenter birth club users' posts.
- Parents seek advice and share experiences in various venues to alleviate stress.
- The analysis covers 5.43 million posts from 331,843 BabyCenter users.
- Anxiety-related terms in posts steadily increased from April 2017 to January 2024.
- Breastfeeding topics, particularly on sleep and work/daycare, were dominant among users compared to other birth club content. 

<br /><br />Summary: <div>
arXiv:2510.23692v1 Announce Type: new 
Abstract: Parental stress is a nationwide health crisis according to the U.S. Surgeon General's 2024 advisory. To allay stress, expecting parents seek advice and share experiences in a variety of venues, from in-person birth education classes and parenting groups to virtual communities, for example, BabyCenter, a moderated online forum community with over 4 million members in the United States alone. In this study, we aim to understand how parents talk about pregnancy, birth, and parenting by analyzing 5.43M posts and comments from the April 2017--January 2024 cohort of 331,843 BabyCenter "birth club" users (that is, users who participate in due date forums or "birth clubs" based on their babies' due dates). Using BERTopic to locate breastfeeding threads and LDA to summarize themes, we compare documents in breastfeeding threads to all other birth-club content. Analyzing time series of word rank, we find that posts and comments containing anxiety-related terms increased steadily from April 2017 to January 2024. We used an ensemble of topic models to identify dominant breastfeeding topics within birth clubs, and then explored trends among all user content versus those who posted in threads related to breastfeeding topics. We conducted Latent Dirichlet Allocation (LDA) topic modeling to identify the most common topics in the full population, as well as within the subset breastfeeding population. We find that the topic of sleep dominates in content generated by the breastfeeding population, as well anxiety-related and work/daycare topics that are not predominant in the full BabyCenter birth club dataset.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach</title>
<link>https://arxiv.org/abs/2510.24077</link>
<guid>https://arxiv.org/abs/2510.24077</guid>
<content:encoded><![CDATA[
<div> social return, social media, predictive model, Indian tourists, future trip planning

Summary:<br />
This paper explores how the social return, measured through social media responses, of recent trips influences the future travel decisions of Indian tourists. A multi-model framework was developed to create a predictive machine learning model that considers social media usage, trip-related factors, and future trip-planning behavior. Data collected from a survey of Indian tourists was cleaned and analyzed using oversampling and Monte Carlo cross-validation techniques. The results indicate a 75% accuracy in predicting the impact of social return on future trip plans. The findings have significant implications for the domestic tourism sector in India and suggest directions for future research on social media, destination marketing, smart tourism, and heritage tourism.<br />Summary: <div>
arXiv:2510.24077v1 Announce Type: new 
Abstract: With the surge of domestic tourism in India and the influence of social media on young tourists, this paper aims to address the research question on how "social return" - responses received on social media sharing - of recent trip details can influence decision-making for short-term future travels. The paper develops a multi-model framework to build a predictive machine learning model that establishes a relationship between a traveler's social return, various social media usage, trip-related factors, and her future trip-planning behavior. The primary data was collected via a survey from Indian tourists. After data cleaning, the imbalance in the data was addressed using a robust oversampling method, and the reliability of the predictive model was ensured by applying a Monte Carlo cross-validation technique. The results suggest at least 75% overall accuracy in predicting the influence of social return on changing the future trip plan. Moreover, the model fit results provide crucial practical implications for the domestic tourism sector in India with future research directions concerning social media, destination marketing, smart tourism, heritage tourism, etc.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation</title>
<link>https://arxiv.org/abs/2510.24251</link>
<guid>https://arxiv.org/abs/2510.24251</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social graphs, Graphia, reinforcement learning, dynamic graph generation

Summary:
Graphia is a framework that utilizes social graph data as supervision for training large language models (LLMs) through reinforcement learning. It focuses on predicting interactions and generating edges in social networks. The framework is evaluated in two settings: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG), showing significant improvements in alignment metrics for destination selection, edge generation, and network properties replication. Graphia also supports counterfactual simulation to simulate behavioral shifts under different incentives. The results demonstrate that social graphs can effectively guide LLM-based simulations, bridging the gap between individual agent behaviors and network dynamics. The code for Graphia is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2510.24251v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in simulating human-like social behaviors. Social graphs provide high-quality supervision signals that encode both local interactions and global network structure, yet they remain underutilized for LLM training. To address this gap, we propose Graphia, the first general LLM-based social graph simulation framework that leverages graph data as supervision for LLM post-training via reinforcement learning. With GNN-based structural rewards, Graphia trains specialized agents to predict whom to interact with (destination selection) and how to interact (edge generation), followed by designed graph generation pipelines. We evaluate Graphia under two settings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with our proposed node-wise interaction alignment metrics; and Inductive Dynamic Graph Generation (IDGG), a macro-level task with our proposed metrics for aligning emergent network properties. On three real-world networks, Graphia improves micro-level alignment by 6.1% in the composite destination selection score, 12% in edge classification accuracy, and 27.9% in edge content BERTScore over the strongest baseline. For macro-level alignment, it achieves 41.11% higher structural similarity and 32.98% better replication of social phenomena such as power laws and echo chambers. Graphia also supports counterfactual simulation, generating plausible behavioral shifts under platform incentives. Our results show that social graphs can serve as high-quality supervision signals for LLM post-training, closing the gap between agent behaviors and network dynamics for LLM-based simulation. Code is available at https://github.com/Ji-Cather/Graphia.git.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewarding Engagement and Personalization in Popularity-Based Rankings Amplifies Extremism and Polarization</title>
<link>https://arxiv.org/abs/2510.24354</link>
<guid>https://arxiv.org/abs/2510.24354</guid>
<content:encoded><![CDATA[
<div> Ranking algorithms, extremism, polarization, online platforms, mechanism <br />
Summary: 
- The article explores how ranking algorithms on online platforms can amplify extremism and polarization.
- Users are more likely to engage with items displayed higher in the ranking, leading to a preference for like-minded content.
- Individuals with more extreme views tend to be more active on these platforms.
- Popular items receive higher rankings, further reinforcing extremist content consumption.
- The study formalizes this mechanism in a dynamical model, validated through simulations and experiments with human participants. <br /><br />Summary: <div>
arXiv:2510.24354v1 Announce Type: new 
Abstract: Despite extensive research, the mechanisms through which online platforms shape extremism and polarization remain poorly understood. We identify and test a mechanism, grounded in empirical evidence, that explains how ranking algorithms can amplify both phenomena. This mechanism is based on well-documented assumptions: (i) users exhibit position bias and tend to prefer items displayed higher in the ranking, (ii) users prefer like-minded content, (iii) users with more extreme views are more likely to engage actively, and (iv) ranking algorithms are popularity-based, assigning higher positions to items that attract more clicks. Under these conditions, when platforms additionally reward \emph{active} engagement and implement \emph{personalized} rankings, users are inevitably driven toward more extremist and polarized news consumption. We formalize this mechanism in a dynamical model, which we evaluate by means of simulations and interactive experiments with hundreds of human participants, where the rankings are updated dynamically in response to user activity.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance of Overlapping Network Nodes in Influence Spreading</title>
<link>https://arxiv.org/abs/2510.24360</link>
<guid>https://arxiv.org/abs/2510.24360</guid>
<content:encoded><![CDATA[
<div> circle structures, influence spreading, overlapping nodes, network analysis, complex contagion

Summary:
- The study focuses on analyzing networks with circle structures and the role of overlapping nodes in influence spreading processes.
- Three node metrics are used to quantify the roles of nodes: In-Centrality, Out-Centrality, and Betweenness Centrality.
- Overlapping nodes consistently exhibit greater influence than non-overlapping nodes at each stage of the spreading process.
- The criteria used to define circles shape the overlapping effects, highlighting the strategic importance of overlapping nodes in spreading dynamics.
- Largest circles not only reflect node-level attributes but also topological importance, distinguishing between local attribute-driven circles and global community structures.

<br /><br />Summary: <div>
arXiv:2510.24360v1 Announce Type: new 
Abstract: In complex networks there are overlapping substructures or "circles" that consist of nodes belonging to multiple cohesive subgroups. Yet the role of these overlapping nodes in influence spreading processes remains underexplored. In the present study, we analyse networks with circle structures using a probabilistic influence spreading model for processes of simple and complex contagion. We quantify the roles of nodes using three metrics, i.e., In-Centrality, Out-Centrality, and Betweenness Centrality that represent the susceptibility, spreading power, and mediatory role of nodes, respectively, and find that at each stage of the spreading process the overlapping nodes consistently exhibit greater influence than the non-overlapping ones. Furthermore, we observe that the criteria to define circles shape the overlapping effects. When we restrict our analysis to only largest circles, we find that circles reflect not only node-level attributes but also of topological importance. These findings clarify the distinction between local attribute-driven circles and global community structures, thus highlighting the strategic importanc of overlapping nodes in spreading dynamics. This provides foundation for future research on overlapping nodes in both circles and communities.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Emergent Topological Properties in Socio-Economic Networks through Learning Heterogeneity</title>
<link>https://arxiv.org/abs/2510.24107</link>
<guid>https://arxiv.org/abs/2510.24107</guid>
<content:encoded><![CDATA[
<div> Keywords: individualized learning rates, structural dynamics, emergent phenomena, network architecture, socioeconomic networks 

Summary: 
This paper introduces a dual-learning framework that considers individualized learning rates for agents and a network rewiring rate, reflecting real-world cognitive diversity and structural adaptability. Through a simulation model based on the Prisoner's Dilemma and Quantal Response Equilibrium, the study examines how variations in learning rates impact the formation of large-scale network structures. Lower and more evenly distributed learning rates tend to promote scale-free networks, while higher or more diverse learning rates result in core-periphery topologies. Analysis of topological metrics such as scale-free exponents, Estrada heterogeneity, and assortativity demonstrates that the speed and variability of learning play a crucial role in shaping system rationality and network architecture. This research provides a comprehensive framework for exploring how individual learnability and structural adaptability influence the development of diverse topologies in socioeconomic networks, offering valuable insights into adaptive behavior, systemic organization, and resilience. 

<br /><br />Summary: <div>
arXiv:2510.24107v1 Announce Type: cross 
Abstract: Understanding how individual learning behavior and structural dynamics interact is essential to modeling emergent phenomena in socioeconomic networks. While bounded rationality and network adaptation have been widely studied, the role of heterogeneous learning rates both at the agent and network levels remains under explored. This paper introduces a dual-learning framework that integrates individualized learning rates for agents and a rewiring rate for the network, reflecting real-world cognitive diversity and structural adaptability.
  Using a simulation model based on the Prisoner's Dilemma and Quantal Response Equilibrium, we analyze how variations in these learning rates affect the emergence of large-scale network structures. Results show that lower and more homogeneously distributed learning rates promote scale-free networks, while higher or more heterogeneously distributed learning rates lead to the emergence of core-periphery topologies. Key topological metrics including scale-free exponents, Estrada heterogeneity, and assortativity reveal that both the speed and variability of learning critically shape system rationality and network architecture. This work provides a unified framework for examining how individual learnability and structural adaptability drive the formation of socioeconomic networks with diverse topologies, offering new insights into adaptive behavior, systemic organization, and resilience.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research</title>
<link>https://arxiv.org/abs/2510.24337</link>
<guid>https://arxiv.org/abs/2510.24337</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Large Language Models, content analysis, communication research, automation, best practices

Summary: 
Generative Large Language Models (gLLMs) like ChatGPT are proving to be valuable tools in communication research, surpassing human coders in coding tasks and offering efficiency and cost-effectiveness. However, integrating gLLMs into quantitative content analysis poses several challenges that researchers must address. These include developing a suitable codebook, creating effective prompts, selecting the appropriate model, fine-tuning parameters, refining iteratively, and validating reliability. Optional steps for enhancing performance may also be necessary. This paper seeks to bridge the gap by providing a best-practice guide to help communication researchers navigate these challenges and ensure the quality standards of validity, reliability, reproducibility, and research ethics are met. The goal is to make gLLM-based content analysis more accessible to a wider audience within the communication research realm. 

<br /><br />Summary: <div>
arXiv:2510.24337v1 Announce Type: cross 
Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly being used in communication research for content analysis. Studies show that gLLMs can outperform both crowd workers and trained coders, such as research assistants, on various coding tasks relevant to communication science, often at a fraction of the time and cost. Additionally, gLLMs can decode implicit meanings and contextual information, be instructed using natural language, deployed with only basic programming skills, and require little to no annotated data beyond a validation dataset - constituting a paradigm shift in automated content analysis. Despite their potential, the integration of gLLMs into the methodological toolkit of communication research remains underdeveloped. In gLLM-assisted quantitative content analysis, researchers must address at least seven critical challenges that impact result quality: (1) codebook development, (2) prompt engineering, (3) model selection, (4) parameter tuning, (5) iterative refinement, (6) validation of the model's reliability, and optionally, (7) performance enhancement. This paper synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide to navigate these challenges. Our goal is to make gLLM-based content analysis more accessible to a broader range of communication researchers and ensure adherence to established disciplinary quality standards of validity, reliability, reproducibility, and research ethics.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pair Approximation Meets Reality: Diffusion of Innovation in Organizational Networks within the biased-independence q-Voter Model</title>
<link>https://arxiv.org/abs/2510.24447</link>
<guid>https://arxiv.org/abs/2510.24447</guid>
<content:encoded><![CDATA[
<div> Keywords: collective adaptation, biased-independence $q$-voter model, agent-based modeling, social influence, organizational networks

Summary: 
The study introduces the biased-independence $q$-voter model, a generalization of the $q$-voter model, to analyze collective adaptation processes influenced by individual decisions and social factors. The model incorporates conformity and independent choice parameters, with the engagement parameter breaking the symmetry between two options. The pair approximation (PA) for an asymmetric $q$-voter model is developed and validated on empirical organizational networks. The results demonstrate discontinuous phase transitions and irreversible hysteresis in adoption dynamics, reflecting path-dependent behaviors. Surprisingly, the PA performs well in predicting outcomes on empirical networks, showcasing its potential as a computationally efficient tool for exploring decision-making processes and collective actions. <div>
arXiv:2510.24447v1 Announce Type: cross 
Abstract: Collective adaptation, whether in innovation adoption, pro-environmental or organizational change, emerges from the interplay between individual decisions and social influence. Agent-based modeling provides a useful tool for studying such processes. Here, we introduce the biased-independence $q$-voter model, a generalization of the $q$-voter model with independence, one of the most popular agent-based models of opinion dynamics. In our model, individuals choose between two options, adopt or not adopt, under the competing influences of conformity and independent choice. Independent choice between two options is determined by an engagement parameter, inspired by earlier agent-based model of eco-innovation diffusion. When the engagement parameter equals $0.5$, the model reduces to the original $q$-voter model with independence; values different from $0.5$ break the symmetry between the two options. To place our study in a broader context, we briefly review asymmetric versions of the $q$-voter model proposed to date. The novelty of this work goes beyond introducing a generalized model: we develop the pair approximation (PA) for an asymmetric $q$-voter model and, for the first time, validate it on empirical organizational networks. Our results show that the interplay of social influence, independence, and option preference generates discontinuous phase transitions and irreversible hysteresis, reflecting path-dependent adoption dynamics. Surprisingly, the PA agrees well with Monte Carlo simulations on some empirical networks, even small ones, highlighting its potential as a computationally efficient bridge between individual decision-making and collective actions.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-based Anchor Embedding for Efficient Exact Subgraph Matching</title>
<link>https://arxiv.org/abs/2502.00031</link>
<guid>https://arxiv.org/abs/2502.00031</guid>
<content:encoded><![CDATA[
<div> Keywords: subgraph matching, deep learning, graph neural network, exact matching, query efficiency

Summary:
The article introduces a novel approach called the Graph Neural Network-based Anchor Embedding Framework (GNN-AE) for exact subgraph matching queries. Unlike traditional methods that rely on online creation of auxiliary structures for each query, GNN-AE indexes small feature subgraphs in the data graph offline. This approach utilizes GNNs to efficiently retrieve high-quality candidates for graph isomorphism tests. By leveraging anchored subgraphs and anchored paths, the exact subgraph matching problem is transformed into a search problem in the embedding space. Additionally, a parallel matching growth algorithm and a cost-based DFS query planning method are developed to enhance query efficiency. Experimental results on real-world and synthetic datasets demonstrate the superiority of GNN-AE over baseline methods, achieving up to 1-2 orders of magnitude improvement in efficiency, particularly surpassing exploration-based baselines.<br /><br />Summary: <div>
arXiv:2502.00031v5 Announce Type: replace 
Abstract: Subgraph matching query is a fundamental problem in graph data management and has a variety of real-world applications. Several recent works utilize deep learning (DL) techniques to process subgraph matching queries. Most of them find approximate subgraph matching results without accuracy guarantees. Unlike these DL-based inexact subgraph matching methods, we propose a learning-based exact subgraph matching framework, called \textit{graph neural network (GNN)-based anchor embedding framework} (GNN-AE). In contrast to traditional exact subgraph matching methods that rely on creating auxiliary summary structures online for each specific query, our method indexes small feature subgraphs in the data graph offline and uses GNNs to perform graph isomorphism tests for these indexed feature subgraphs to efficiently obtain high-quality candidates. To make a tradeoff between query efficiency and index storage cost, we use two types of feature subgraphs, namely anchored subgraphs and anchored paths. Based on the proposed techniques, we transform the exact subgraph matching problem into a search problem in the embedding space. Furthermore, to efficiently retrieve all matches, we develop a parallel matching growth algorithm and design a cost-based DFS query planning method to further improve the matching growth algorithm. Extensive experiments on 6 real-world and 3 synthetic datasets indicate that GNN-AE is more efficient than the baselines, especially outperforming the exploration-based baseline methods by up to 1--2 orders of magnitude.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heaven &amp; Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus</title>
<link>https://arxiv.org/abs/2510.21950</link>
<guid>https://arxiv.org/abs/2510.21950</guid>
<content:encoded><![CDATA[
<div> Conservation-law, scale laws, tie policies, tight bounds, asynchronous updates <br />
Summary: The study focuses on Heaven-Hell dynamics, a model for network consensus, particularly examining systems with a single uniform hub. They establish a one-step convergence threshold based on the per-node inbound hub weight, providing robustness to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. The research introduces scale laws and operational refinements to ensure the threshold's reliability across various scenarios. Key contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are formalized in Coq, with experiments on different network topologies confirming the accuracy and effectiveness of the proposed refinements. <br /><br />Summary: <div>
arXiv:2510.21950v1 Announce Type: new 
Abstract: We study Heaven-Hell dynamics, a model for network consensus. A known result establishes an exact one-step convergence threshold for systems with a single uniform hub: the per-node inbound hub weight W suffices if and only if W >= maxrest, the maximum non-hub inbound mass. We develop scale laws and operational refinements that make this threshold robust to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. Our contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds improving on classical worst-case guarantees, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are mechanized in Coq, with experiments on rings, grids, scale-free graphs, and heterogeneous weighted graphs validating tightness and gap closures
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups</title>
<link>https://arxiv.org/abs/2510.21984</link>
<guid>https://arxiv.org/abs/2510.21984</guid>
<content:encoded><![CDATA[
<div> experiment, online communication, large language models, social cohesion, AI assistance
Summary: 
An online experiment with 557 participants explored the impact of AI-driven messaging assistance on social cohesion in online discussions. The study found that subtle linguistic style shifts mediated by AI assistance can influence the collective structures of discussion groups. Personalized AI assistance led users to segregate into like-minded groups, while group-focused assistance incorporating group members' viewpoints promoted cohesion through more receptive exchanges. The findings highlight the potential for AI-mediated communication to support social cohesion in diverse groups, emphasizing the importance of thoughtful personalization design. <div>
arXiv:2510.21984v1 Announce Type: new 
Abstract: Social cohesion is difficult to sustain in societies marked by opinion diversity, particularly in online communication. As large language model (LLM)-driven messaging assistance becomes increasingly embedded in these contexts, it raises critical questions about its societal impact. We present an online experiment with 557 participants who engaged in multi-round discussions on politically controversial topics while freely reconfiguring their discussion groups. In some conditions, participants received real-time message suggestions generated by an LLM, either personalized to the individual or adapted to their group context. We find that subtle shifts in linguistic style during communication, mediated by AI assistance, can scale up to reshape collective structures. While individual-focused assistance leads users to segregate into like-minded groups, relational assistance that incorporates group members' stances enhances cohesion through more receptive exchanges. These findings demonstrate that AI-mediated communication can support social cohesion in diverse groups, but outcomes critically depend on how personalization is designed.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Short-Video Diplomacy: Topic and Sentiment Analysis of China-US Relations on Douyin and TikTok</title>
<link>https://arxiv.org/abs/2510.22415</link>
<guid>https://arxiv.org/abs/2510.22415</guid>
<content:encoded><![CDATA[
<div> Keywords: China-U.S. relations, social media, sentiment analysis, regulation, regional factors

Summary:
This study explores discussions about China-U.S. relations on the social media platforms Douyin and TikTok, owned by ByteDance, in China and the US. Through analyzing 4,040 videos and 338,209 user comments, key themes such as economic strength, technological interdependence, cultural values, and responses to global challenges were identified. Emotional differences between the two countries were evident in these discussions. The Chinese government's new regulation requiring social media accounts to disclose geolocation information led to an investigation of changes in sentiment towards the US in mainland China. By linking socioeconomic factors such as GDP per capita, minority index, and internet penetration rate with online discussions, this study sheds light on how regional and economic factors influence Chinese views of the US. These findings are valuable for understanding China-U.S. relations and informing policy decisions.<br /><br />Summary: <div>
arXiv:2510.22415v1 Announce Type: new 
Abstract: We examine discussions surrounding China-U.S. relations on the Chinese and American social media platforms \textit{Douyin} and \textit{TikTok}. Both platforms, owned by \textit{ByteDance}, operate under different regulatory and cultural environments, providing a unique perspective for analyzing China-U.S. public discourse. This study analyzed 4,040 videos and 338,209 user comments to assess the public discussions and sentiments on social media regarding China-U.S. relations. Through topic clustering and sentiment analysis, we identified key themes, including economic strength, technological and industrial interdependence, cultural cognition and value pursuits, and responses to global challenges. There are significant emotional differences between China and the US on various themes. Since April 2022, the Chinese government has implemented a new regulation requiring all social media accounts to disclose their provincial-level geolocation information. Utilizing this publicly available data, along with factors such as GDP per capita, minority index, and internet penetration rate, we investigate the changes in sentiment towards the U.S. in mainland China. This study links socioeconomic indicators with online discussions, deeply analyzing how regional and economic factors influence Chinese comments on their views of the US, providing important insights for China-U.S. relationship research and policy making.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Discrete-time Model of Information Diffusion on Social Networks Considering Users Behavior</title>
<link>https://arxiv.org/abs/2510.22501</link>
<guid>https://arxiv.org/abs/2510.22501</guid>
<content:encoded><![CDATA[
<div> SDIR model, online social networks, user behavior, mean-field approximation, edge-deletion problem
<br />
Summary:
In this paper, the SDIR model is introduced as an extension of the classic SIR epidemic framework for online social networks, explicitly capturing user behavior. The model includes a new state, D, representing delayed information spread. Dynamical equations are derived using mean-field approximation, and convergence and stability conditions are studied. An approximation algorithm for the edge-deletion problem is proposed to minimize information diffusion impact by identifying approximate solutions. The SDIR model offers a comprehensive framework for analyzing information spread dynamics in online social networks, considering user interactions and delay effects. <div>
arXiv:2510.22501v1 Announce Type: new 
Abstract: In this paper, we introduce the SDIR (Susceptible-Delayable-Infected-Recovered) model, an extension of the classical SIR epidemic framework, to provide a more explicit characterization of user behavior in online social networks. The newly merged state D (delayable) represents users who have received the information but delayed its spreading and may eventually choose not to share it at all. Based on the mean-field approximation method, we derive the dynamical equations of the model and investigate its convergence and stability conditions. Under these conditions, we further propose an approximation algorithm for the edge-deletion problem, aiming to minimize the influence of information diffusion by identifying approximate solutions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of Network Topology and Vaccination Strategies on HPV Dynamics: A Simulation Study Using the SeCoNet Growth Model</title>
<link>https://arxiv.org/abs/2510.22644</link>
<guid>https://arxiv.org/abs/2510.22644</guid>
<content:encoded><![CDATA[
<div> Age-based; ring-based; centrality-based; vaccination strategies; human papillomavirus

Summary:
Degree, betweenness, and percolation centrality-based vaccination strategies are most effective in reducing peak incidence of human papillomavirus. Ring vaccination is most successful in lowering cumulative incidence among females. Network topology, including average degree, power-law exponent, average shortest path length, and clustering, significantly influences vaccination effectiveness. Higher average degree hinders vaccination outcomes, while a higher power-law exponent, longer average shortest path length, and stronger clustering improve results. Incorporating network structure into HPV vaccination program design is crucial for achieving optimal outcomes. 

<br /><br />Summary: <div>
arXiv:2510.22644v1 Announce Type: new 
Abstract: This study examines how contact network topology influences the effectiveness of vaccination programs in the context of human papillomavirus (HPV) transmission. Using the SeCoNet sexual contact network growth model, we evaluate age based, ring based, and several centrality based vaccination strategies across the overall, male, and female cohorts, focusing on peak incidence, timing of peak prevalence, and cumulative incidence. The simulations show that degree, betweenness, and percolation centrality based strategies are generally the most effective, while ring vaccination achieves the greatest reduction in cumulative incidence among females. Network topology also plays a critical role: higher average degree reduces vaccination effectiveness, whereas higher power-law exponent, longer average shortest path length, and stronger clustering improve vaccination outcomes. The results highlight the importance of incorporating network structure into the design of HPV vaccination programs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Search in Attributed Networks using Dominance Relationships and Random Walks</title>
<link>https://arxiv.org/abs/2510.22850</link>
<guid>https://arxiv.org/abs/2510.22850</guid>
<content:encoded><![CDATA[
<div> algorithm, community search, attributed networks, structural connectivity, attribute similarity

Summary:
- The paper introduces a novel algorithm that combines hop-based and random-walk-based methods to find high-quality communities in attributed networks.
- It uses the domination score to measure the influence of nodes based on their attributes and incorporates $k$-core extraction for strong structural cohesion within communities.
- By considering both network structure and node attributes, the algorithm identifies cohesive communities with meaningful attribute similarities.
- The algorithm was tested on real-world datasets and proved effective in efficiently identifying well-connected communities.
- It is suitable for applications such as social network analysis and recommendation systems. 

<br /><br />Summary: <div>
arXiv:2510.22850v1 Announce Type: new 
Abstract: Community search in attributed networks poses a dual challenge: balancing structural connectivity -- the network's topological properties -- and attribute similarity -- the shared characteristics of nodes. This paper introduces a novel algorithm that integrates hop-based and random-walk-based methods to identify high-quality communities, effectively addressing this balance. Our approach employs the concept of the domination score to quantify the influence of nodes based on their attributes, followed by $k$-core extraction to ensure strong structural cohesion within the communities. By considering both the network structure and node attributes, the algorithm identifies communities that are not only well-connected, but also share meaningful attribute similarities. We evaluated the algorithm on large real-world datasets, demonstrating its ability to efficiently identify cohesive communities, making it suitable for applications such as social network analysis and recommendation systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Political Discourse with Sentence-BERT and BERTopic</title>
<link>https://arxiv.org/abs/2510.22904</link>
<guid>https://arxiv.org/abs/2510.22904</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, political discourse, topic evolution, Moral Foundations Theory, Twitter activity<br />
Summary: <br />
This study explores the impact of social media on political discourse by analyzing Twitter activity during the 117th U.S. Congress. The research introduces a unique framework that combines BERTopic-based topic modeling with Moral Foundations Theory to track the evolution and moral dimensions of political topics over time. The study finds that while overarching themes remain stable, more specific topics tend to fade quickly, limiting their long-term influence. The analysis also shows that moral values, particularly Care and Loyalty, play a crucial role in the persistence of topics, with partisan differences leading to varied moral framing strategies. Overall, this work offers a scalable and interpretable approach to understanding how moral values drive topic evolution on social media, contributing to the field of social network analysis and computational political discourse. <br /> <div>
arXiv:2510.22904v1 Announce Type: new 
Abstract: Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2510.22017</link>
<guid>https://arxiv.org/abs/2510.22017</guid>
<content:encoded><![CDATA[
<div> trust-aware RL algorithm, resource allocation, institutional trust, fairness, humanitarian engineering

Summary: 
- The study focuses on the development of a trust-aware RL algorithm for resource allocation in communities, with a focus on humanitarian engineering.
- The research explores how incorporating institutional trust into RL algorithms can lead to more successful policies, especially when organizational goals are uncertain.
- Results show that more conservative trust estimates result in increased fairness and average community trust, albeit at the cost of organization success.
- An intervention strategy is proposed to prevent unfair outcomes by implementing a quota system to ensure the organization serves enough community members, leading to improved fairness and trust in some cases, but at the expense of organization success.
- The work highlights the significance of institutional trust in algorithm design and implementation, and uncovers a trade-off between organization success and community well-being. 

<br /><br />Summary: <div>
arXiv:2510.22017v1 Announce Type: cross 
Abstract: Many governmental bodies are adopting AI policies for decision-making. In particular, Reinforcement Learning has been used to design policies that citizens would be expected to follow if implemented. Much RL work assumes that citizens follow these policies, and evaluate them with this in mind. However, we know from prior work that without institutional trust, citizens will not follow policies put in place by governments. In this work, we develop a trust-aware RL algorithm for resource allocation in communities. We consider the case of humanitarian engineering, where the organization is aiming to distribute some technology or resource to community members. We use a Deep Deterministic Policy Gradient approach to learn a resource allocation that fits the needs of the organization. Then, we simulate resource allocation according to the learned policy, and model the changes in institutional trust of community members. We investigate how this incorporation of institutional trust affects outcomes, and ask how effectively an organization can learn policies if trust values are private. We find that incorporating trust into RL algorithms can lead to more successful policies, specifically when the organization's goals are less certain. We find more conservative trust estimates lead to increased fairness and average community trust, though organization success suffers. Finally, we explore a strategy to prevent unfair outcomes to communities. We implement a quota system by an external entity which decreases the organization's utility when it does not serve enough community members. We find this intervention can improve fairness and trust among communities in some cases, while decreasing the success of the organization. This work underscores the importance of institutional trust in algorithm design and implementation, and identifies a tension between organization success and community well-being.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Larger holes as narrower degree distributions in complex networks</title>
<link>https://arxiv.org/abs/2510.22720</link>
<guid>https://arxiv.org/abs/2510.22720</guid>
<content:encoded><![CDATA[
<div> loops, degree distributions, scale-free networks, connectivity, random networks  
Summary: 
The study explores the relationship between degree distributions and loop lengths in various networks, including scale-free networks, Erdos-Renyi random graphs, and regular networks. It is observed that networks with narrower degree distributions tend to have longer shortest loops, which is considered a universal property in random networks. This suggests that enhancing longer loops, specifically of O(log N) length, can improve the robustness of connectivity in networks. The findings indicate that increasing loop lengths can help decrease the variance of degree distributions and enhance connectivity resilience, particularly in scale-free networks containing shorter loops like triangles. This research contributes to understanding the structural properties of networks and how loop enhancement can impact network connectivity. <div>
arXiv:2510.22720v1 Announce Type: cross 
Abstract: Although the analysis of loops is not so much because of the complications, it has already been found that heuristically enhancing loops decreases the variance of degree distributions for improving the robustness of connectivity. While many real scale-free networks are known to contain shorter loops such as triangles, it remains to investigate the distributions of longer loops in more wide class of networks. We find a relation between narrower degree distributions and longer loops in investigating the lengths of the shortest loops in various networks with continuously changing degree distributions, including three typical types of scale-free networks, classical Erd\"os-R\'enyi random graphs, and regular networks. In particular, we show that narrower degree distributions contain longer shortest loops, as a universal property in a wide class of random networks. We suggest that the robustness of connectivity is enhanced by constructing long loops of O(log N).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications</title>
<link>https://arxiv.org/abs/2510.23235</link>
<guid>https://arxiv.org/abs/2510.23235</guid>
<content:encoded><![CDATA[
<div> Low-pass graph filters, signal processing, parametric graph families, Riemannian interpolation, normal coordinates<br />
Summary:<br />
The article proposes a new algorithm for interpolating low-pass graph filters using Riemannian interpolation on the Grassmann manifold, reducing computational costs for parametric graph families. An error bound estimate for subspace interpolation is derived, and two potential applications are discussed. Firstly, the temporal evolution of node features can be translated to evolving graph topology by adjusting network homophily. Secondly, a dot product graph family can be induced from a static graph to enhance message passing schemes for node classification through filter interpolation. <div>
arXiv:2510.23235v1 Announce Type: cross 
Abstract: Low-pass graph filters are fundamental for signal processing on graphs and other non-Euclidean domains. However, the computation of such filters for parametric graph families can be prohibitively expensive as computation of the corresponding low-frequency subspaces, requires the repeated solution of an eigenvalue problem. We suggest a novel algorithm of low-pass graph filter interpolation based on Riemannian interpolation in normal coordinates on the Grassmann manifold. We derive an error bound estimate for the subspace interpolation and suggest two possible applications for induced parametric graph families. First, we argue that the temporal evolution of the node features may be translated to the evolving graph topology via a similarity correction to adjust the homophily degree of the network. Second, we suggest a dot product graph family induced by a given static graph which allows to infer improved message passing scheme for node classification facilitated by the filter interpolation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger together? The homophily trap in networks</title>
<link>https://arxiv.org/abs/2412.20158</link>
<guid>https://arxiv.org/abs/2412.20158</guid>
<content:encoded><![CDATA[
<div> homophily, diversity, inequalities, minority groups, network structure
Summary:
Homophily, the tendency to connect with similar others, can have both positive and negative effects on social networks. While it can foster a sense of belonging and shared values, it can also hinder diversity and exacerbate inequalities. In this study, the concept of homophily traps is introduced, where increased homophily among minorities can lead to reduced structural opportunities within a network. The research reveals that homophily traps occur when a minority group's size is below 25% of the network, resulting in lower visibility and opportunities for the minority. This highlights the importance of group size in determining the impact of homophily on structural outcomes in networks. By understanding these dynamics, we gain insights into the underlying processes that contribute to group inequality in social networks.<br /><br />Summary: <div>
arXiv:2412.20158v2 Announce Type: replace 
Abstract: While homophily -- the tendency to link with similar others -- may nurture a sense of belonging and shared values, it can also hinder diversity and widen inequalities. Here, we unravel this trade-off analytically, revealing homophily traps for minority groups: scenarios where increased homophilic interaction among minorities negatively affects their structural opportunities within a network. We demonstrate that homophily traps arise when minority size falls below 25% of a network, at which point homophily comes at the expense of lower structural visibility for the minority group. Our work reveals that social groups require a critical size to benefit from homophily without incurring structural costs, providing insights into core processes underlying the emergence of group inequality in networks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics on Signed Graphs and Graphons</title>
<link>https://arxiv.org/abs/2505.04472</link>
<guid>https://arxiv.org/abs/2505.04472</guid>
<content:encoded><![CDATA[
<div> Keywords: graphon theory, opinion dynamics, negative interactions, repelling model, opposing model

Summary:
In this paper, the authors utilize graphon theory to analyze opinion dynamics on large undirected networks. They focus on models that incorporate negative interactions among individuals, allowing for opinions to diverge. By considering repelling and opposing models of negative interactions, defined on signed graphons, the authors establish the existence and uniqueness of solutions to the initial value problem. The study demonstrates that the graphon dynamics serve as a reliable approximation for dynamics on large graphs converging to a graphon. The findings are applicable to large random graphs sampled according to a graphon (W-random graphs), supported by a novel convergence result under broad assumptions. <div>
arXiv:2505.04472v2 Announce Type: replace 
Abstract: In this paper, we make use of graphon theory to study opinion dynamics on large undirected networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individuals, whose opinions can thus grow apart. We consider both the repelling and the opposing models of negative interactions, which have been studied in the literature. We define the repelling and the opposing dynamics on signed graphons and we show that their initial value problem solutions exist and are unique. We then show that, in a suitable sense, the graphon dynamics is a good approximation of the dynamics on large graphs that converge to a graphon. This result applies to large random graphs that are sampled according to a graphon (W-random graphs), for which we provide a new convergence result under very general assumptions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Research Interest Similarity with Transition Probabilities</title>
<link>https://arxiv.org/abs/2409.18240</link>
<guid>https://arxiv.org/abs/2409.18240</guid>
<content:encoded><![CDATA[
<div> Keywords: paper similarity, author similarity, citation network, random walkers, classification tasks

Summary: 
The study introduces a novel family of measures for paper and author similarity based on the likelihood of papers being retrieved during literature searches following citations. This browsing process is modeled as a walk in a citation network, utilizing transition probabilities of random walkers. The proposed measures are continuous, symmetric, and applicable to any citation network. Validation tests compared these measures against existing alternatives, demonstrating the superiority of the basic transition probability measure in classifying papers and predicting future co-authors across various analysis scales. The study also explores leveraging publication-level data to approximate the research interest similarity among individual scientists. A Python package accompanying the paper implements all tested metrics.


<br /><br />Summary: <div>
arXiv:2409.18240v2 Announce Type: replace-cross 
Abstract: We introduce a family of paper and author similarity measures based on the concept that papers are more similar if they are more likely to be retrieved during a literature search following backward and forward citations. Since this browsing process resembles a walk in a citation network, we operationalize the concept using the transition probability (TP) of random walkers. The proposed measures are continuous, symmetric, and can be implemented on any citation network. We conduct validation tests of the TP concept and other extant alternatives to gauge which metric can classify papers and predict future co-authors most consistently across different scales of analysis (co-authorships, journals, and disciplines). Our results show that the proposed basic TP measure outperforms alternative metrics such as personalized PageRank and the Node2vec machine-learning technique in classification tasks at various scales. Additionally, we discuss how publication-level data can be leveraged to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package that implements all the tested metrics.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approximate-Master-Equation Formulation of the Watts Threshold Model on Hypergraphs</title>
<link>https://arxiv.org/abs/2503.04020</link>
<guid>https://arxiv.org/abs/2503.04020</guid>
<content:encoded><![CDATA[
<div> Keywords: behavioral dynamics, opinion dynamics, social networks, hypergraphs, Watts threshold model <br />
Summary: 
The article explores behavioral and opinion dynamics on social networks, considering interactions in groups rather than just pairs. It extends the Watts threshold model (WTM) from graphs to hypergraphs, allowing interactions between any number of individuals. Using approximate master equations (AMEs), the model is accurately represented in continuous time. The high-dimensional system is reduced to three coupled differential equations for computational efficiency. A cascade condition is derived to predict large spreading events. The model is applied to empirical networks, a French primary school contact network, and a computer-science coauthorship hypergraph, showing accuracy in simulation. Future research incorporating structural correlations is suggested for improved real-world network modeling. <br /><br />Summary: <div>
arXiv:2503.04020v3 Announce Type: replace-cross 
Abstract: In traditional models of behavioral or opinion dynamics on social networks, researchers suppose that all interactions occur between pairs of individuals. However, in reality, social interactions also occur in groups of three or more individuals. A common way to incorporate such polyadic interactions is to study dynamical processes on hypergraphs. In a hypergraph, interactions can occur between any number of the individuals in a network. The Watts threshold model (WTM) is a well-known model of a simplistic social spreading process. Very recently, Chen et al. extended the WTM from dyadic networks (i.e., graphs) to polyadic networks (i.e., hypergraphs). In the present paper, we extend their discrete-time model to continuous time using approximate master equations (AMEs). By using AMEs, we are able to model the system with very high accuracy. We then reduce the high-dimensional AME system to a system of three coupled differential equations without any detectable loss of accuracy. This much lower-dimensional system is more computationally efficient to solve numerically and is also easier to interpret. We linearize the reduced AME system and calculate a cascade condition, which allows us to determine when a large spreading event occurs. We then apply our model to a social contact network of a French primary school and to a hypergraph of computer-science coauthorships. We find that the AME system is accurate in modelling the polyadic WTM on these empirical networks; however, we expect that future work that incorporates structural correlations between nearby nodes and groups into the model for the dynamics will lead to more accurate theory for real-world networks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Simulations with Large Language Model Risk Utopian Illusion</title>
<link>https://arxiv.org/abs/2510.21180</link>
<guid>https://arxiv.org/abs/2510.21180</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation, human behavior, large language models (LLMs), social cognitive biases, social desirability bias

Summary: 
The study explores how large language models (LLMs) simulate human behavior in social contexts. Through chatroom-style interactions, the researchers analyzed LLMs across five linguistic dimensions to identify social cognitive biases. The findings reveal that LLMs do not accurately replicate authentic human behavior, exhibiting biases like social role bias, primacy effect, and positivity bias. These biases lead to the creation of "Utopian" societies that lack the complexity and variability of real human interactions. The study highlights the importance of developing socially grounded LLMs that can capture the diverse nature of human social behavior. This systematic framework provides a valuable tool for assessing the behavior of LLMs in social simulations and emphasizes the need for more nuanced representations of human behavior in artificial intelligence models. 

<br /><br />Summary: <div>
arXiv:2510.21180v1 Announce Type: cross 
Abstract: Reliable simulation of human behavior is essential for explaining, predicting, and intervening in our society. Recent advances in large language models (LLMs) have shown promise in emulating human behaviors, interactions, and decision-making, offering a powerful new lens for social science studies. However, the extent to which LLMs diverge from authentic human behavior in social contexts remains underexplored, posing risks of misinterpretation in scientific studies and unintended consequences in real-world applications. Here, we introduce a systematic framework for analyzing LLMs' behavior in social simulation. Our approach simulates multi-agent interactions through chatroom-style conversations and analyzes them across five linguistic dimensions, providing a simple yet effective method to examine emergent social cognitive biases. We conduct extensive experiments involving eight representative LLMs across three families. Our findings reveal that LLMs do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it, shaped by the social desirability bias. In particular, LLMs show social role bias, primacy effect, and positivity bias, resulting in "Utopian" societies that lack the complexity and variability of real human interactions. These findings call for more socially grounded LLMs that capture the diversity of human social behavior.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shift Bribery over Social Networks</title>
<link>https://arxiv.org/abs/2510.21200</link>
<guid>https://arxiv.org/abs/2510.21200</guid>
<content:encoded><![CDATA[
<div> bribery, social influence, networks, complexity landscape, shift strategy<br />
Summary:<br />
In shift bribery over networks, a briber aims to influence voters in a social network to promote their preferred candidate by paying them to change their rankings. This study considers the impact of social influence within the network, where bribed voters can influence their neighbors, amplifying persuasion effects. The problem complexity is shown to be challenging, being NP-complete and W[2]-hard under various parameters. However, some positive results include algorithms for specific network structures such as complete graphs, path graphs, transitive tournaments, and cluster graphs. Additionally, fixed-parameter tractable algorithms are developed based on treewidth and cluster vertex deletion number parameters. Overall, this research provides a comprehensive analysis of the complexity landscape of shift bribery in social networks, offering insights into the feasibility and challenges of influencing voters through network propagation. <br /> <div>
arXiv:2510.21200v1 Announce Type: cross 
Abstract: In shift bribery, a briber seeks to promote his preferred candidate by paying voters to raise their ranking. Classical models of shift bribery assume voters act independently, overlooking the role of social influence. However, in reality, individuals are social beings and are often represented as part of a social network, where bribed voters may influence their neighbors, thereby amplifying the effect of persuasion. We study Shift bribery over Networks, where voters are modeled as nodes in a directed weighted graph, and arcs represent social influence between them. In this setting, bribery is not confined to directly targeted voters its effects can propagate through the network, influencing neighbors and amplifying persuasion. Given a budget and individual cost functions for shifting each voter's preference toward a designated candidate, the goal is to determine whether a shift strategy exists within budget that ensures the preferred candidate wins after both direct and network-propagated influence takes effect. We show that the problem is NP-Complete even with two candidates and unit costs, and W[2]-hard when parameterized by budget or maximum degree. On the positive side, we design polynomial-time algorithms for complete graphs under plurality and majority rules and path graphs for uniform edge weights, linear-time algorithms for transitive tournaments for two candidates, linear cost functions and uniform arc weights, and pseudo-polynomial algorithms for cluster graphs. We further prove the existence of fixed-parameter tractable algorithms with treewidth as parameter for two candidates, linear cost functions and uniform arc weights and pseudo-FPT with cluster vertex deletion number for two candidates and uniform arc weights. Together, these results give a detailed complexity landscape for shift bribery in social networks.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World-POI: Global Point-of-Interest Data Enriched from Foursquare and OpenStreetMap as Tabular and Graph Data</title>
<link>https://arxiv.org/abs/2510.21342</link>
<guid>https://arxiv.org/abs/2510.21342</guid>
<content:encoded><![CDATA[
<div> dataset, points of interest, Foursquare, OpenStreetMap, metadata
Summary: 
This data paper presents a methodology for integrating Foursquare and OpenStreetMap datasets to create a comprehensive dataset of points of interest (POIs). The Foursquare dataset provides a baseline of commercial POIs, while the OpenStreetMap dataset offers enriched metadata. By combining these datasets, a 1 TB dataset is created, with filtered releases available for practical use. Record linkage is achieved through name similarity scores and spatial distances, allowing for the identification of high-confidence matches representing real businesses. A graph-based representation of POIs is constructed using this filtered dataset, enabling advanced spatial analyses and a variety of downstream applications. <div>
arXiv:2510.21342v1 Announce Type: cross 
Abstract: Recently, Foursquare released a global dataset with more than 100 million points of interest (POIs), each representing a real-world business on its platform. However, many entries lack complete metadata such as addresses or categories, and some correspond to non-existent or fictional locations. In contrast, OpenStreetMap (OSM) offers a rich, user-contributed POI dataset with detailed and frequently updated metadata, though it does not formally verify whether a POI represents an actual business. In this data paper, we present a methodology that integrates the strengths of both datasets: Foursquare as a comprehensive baseline of commercial POIs and OSM as a source of enriched metadata. The combined dataset totals approximately 1 TB. While this full version is not publicly released, we provide filtered releases with adjustable thresholds that reduce storage needs and make the data practical to download and use across domains. We also provide step-by-step instructions to reproduce the full 631 GB build. Record linkage is achieved by computing name similarity scores and spatial distances between Foursquare and OSM POIs. These measures identify and retain high-confidence matches that correspond to real businesses in Foursquare, have representations in OSM, and show strong name similarity. Finally, we use this filtered dataset to construct a graph-based representation of POIs enriched with attributes from both sources, enabling advanced spatial analyses and a range of downstream applications.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains</title>
<link>https://arxiv.org/abs/2510.21452</link>
<guid>https://arxiv.org/abs/2510.21452</guid>
<content:encoded><![CDATA[
<div> SSCs, software supply chains, threat detection, socio-technical models, adaptive.

Summary:
Software supply chains (SSCs) are complex systems that are vulnerable to attacks, necessitating targeted and adaptive threat detection strategies. Current approaches mainly focus on technical aspects, but understanding socio-technical dynamics is crucial for effective threat detection. By analyzing the XZ Utils attack, where malicious actors exploited social trust within the project, the importance of monitoring both technical and social data for identifying suspicious behavior is highlighted. The proposed research vision involves using socio-technical models to support adaptive threat detection in SSCs, leveraging trends in data to inform targeted vulnerability assessment. Key challenges include techniques for developer and software analysis, decentralized adaptation, and the establishment of a test bed for SSC security research. This holistic approach aims to enhance the security of software supply chains by integrating technical and social considerations in threat detection mechanisms.<br /><br />Summary: <div>
arXiv:2510.21452v1 Announce Type: cross 
Abstract: Software supply chains (SSCs) are complex systems composed of dynamic, heterogeneous technical and social components which collectively achieve the production and maintenance of software artefacts. Attacks on SSCs are increasing, yet pervasive vulnerability analysis is challenging due to their complexity. Therefore, threat detection must be targeted, to account for the large and dynamic structure, and adaptive, to account for its change and diversity. While current work focuses on technical approaches for monitoring supply chain dependencies and establishing component controls, approaches which inform threat detection through understanding the socio-technical dynamics are lacking. We outline a position and research vision to develop and investigate the use of socio-technical models to support adaptive threat detection of SSCs. We motivate this approach through an analysis of the XZ Utils attack whereby malicious actors undermined the maintainers' trust via the project's GitHub and mailing lists. We highlight that monitoring technical and social data can identify trends which indicate suspicious behaviour to then inform targeted and intensive vulnerability assessment. We identify challenges and research directions to achieve this vision considering techniques for developer and software analysis, decentralised adaptation and the need for a test bed for software supply chain security research.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Another Hour on TikTok: ID sampling to obtain a complete slice of TikTok</title>
<link>https://arxiv.org/abs/2504.13279</link>
<guid>https://arxiv.org/abs/2504.13279</guid>
<content:encoded><![CDATA[
<div> TikTok, platform, posts, metadata, statistics
Summary:
- The study focuses on analyzing TikTok, a popular platform with significant global impact, utilizing a method to extract a representative sample of posts.
- The method enables collection of metadata, video media, and comments from a vast portion of TikTok, providing crucial statistics about the platform.
- An estimated 269 million posts were produced on the observed day, highlighting the platform's extensive content creation.
- Approximately 18% of videos on TikTok feature children, reflecting a significant presence of younger users on the platform.
- Notably, at least 0.5% of posts contain artificial intelligence-generated content, indicating the integration of AI technology into TikTok's content creation. 

<br /><br />Summary: <div>
arXiv:2504.13279v4 Announce Type: replace 
Abstract: TikTok is now a massive platform, and has a deep impact on global events. Despite preliminary studies, issues remain in determining fundamental characteristics of the platform. We develop a method to extract a representative sample of >99% of posts from a given time range on TikTok, and use it to collect all posts from a full hour on the platform, alongside all posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media, and comments from a close-to-complete slice of TikTok, and report the critical statistics of the platform. Notably, we estimate a total of 269 million posts produced on the day we looked at, that 18% of videos on the platform feature children, and that at least 0.5% of posts contain artificial intelligence-generated content.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-confidence opinion models with random-time interactions</title>
<link>https://arxiv.org/abs/2409.15148</link>
<guid>https://arxiv.org/abs/2409.15148</guid>
<content:encoded><![CDATA[
<div> random-time interactions, bounded-confidence model, renewal processes, interevent-time distributions, transient dynamics

Summary:
Random-time interactions in bounded-confidence models on networks are studied in this paper, allowing agents to interact at random times using renewal processes to determine event times following arbitrary interevent-time distributions (ITDs). Connections between random-time and deterministic-time interaction BCMs are established, with a focus on the impact of ITDs on transient dynamics. Markovian ITDs result in consistent statistical properties, while non-Markovian ITDs show dependence on the type of ITD even with the same mean. Numerical analysis of models with various ITDs on different networks reveals differences in expected order-parameter values and convergence times for transient and steady-state dynamics. This research provides insights into the effects of random-time interactions in opinion dynamics models and highlights the importance of considering randomness in social interactions. 

<br /><br />Summary: <div>
arXiv:2409.15148v2 Announce Type: replace-cross 
Abstract: In models of opinion dynamics, agents interact with each other and can change their opinions as a result of those interactions. One type of opinion model is a bounded-confidence model (BCM), in which opinions take continuous values and interacting agents compromise their opinions with each other if their opinions are sufficiently similar. In studies of BCMs, researchers typically assume that interactions between agents occur at deterministic times. This assumption neglects an inherent element of randomness in social interactions, and it is desirable to account for it. In this paper, we study BCMs on networks and allow agents to interact at random times. To incorporate random-time interactions, we use renewal processes to determine social-interaction event times, which can follow arbitrary interevent-time distributions (ITDs). We establish connections between these random-time-interaction BCMs and deterministic-time-interaction BCMs. We analyze the quantitative impact of ITDs on the transient dynamics of BCMs and derive approximate master equations for the time-dependent expectations of the BCM dynamics. We find that BCMs with Markovian ITDs have consistent statistical properties (in particular, they have the same expected time-dependent opinions) when the ITDs have the same mean but that the statistical properties of BCMs with non-Markovian ITDs depend on the type of ITD even when the ITDs have the same mean. Additionally, we numerically examine the transient and steady-state dynamics of our models with various ITDs on different networks and compare their expected order-parameter values and expected convergence times.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A QUBO Framework for Team Formation</title>
<link>https://arxiv.org/abs/2503.23209</link>
<guid>https://arxiv.org/abs/2503.23209</guid>
<content:encoded><![CDATA[
<div> team formation problem, experts, skills, cost functions, QUBO

Summary:
The article introduces the unified TeamFormation formulation, addressing the team formation problem by optimizing the coverage of required skills while minimizing expert costs. Three variants of TeamFormation are formulated using QUBO, with different cost functions considered. Two general-purpose solution methods are evaluated, demonstrating that QUBO-based solutions are at least as effective as established baselines. Additionally, the article highlights the use of graph neural networks in QUBO-based solutions, enabling the learning of expert and skill representations for transfer learning. This approach allows for efficient application of node embeddings from one problem instance to another, showcasing the potential for enhanced problem-solving capabilities in team formation scenarios. <div>
arXiv:2503.23209v2 Announce Type: replace-cross 
Abstract: The team formation problem assumes a set of experts and a task, where each expert has a set of skills and the task requires some skills. The objective is to find a set of experts that maximizes coverage of the required skills while simultaneously minimizing the costs associated with the experts. Different definitions of cost have traditionally led to distinct problem formulations and algorithmic solutions. We introduce the unified TeamFormation formulation that captures all cost definitions for team formation problems that balance task coverage and expert cost. Specifically, we formulate three TeamFormation variants with different cost functions using quadratic unconstrained binary optimization (QUBO), and we evaluate two distinct general-purpose solution methods. We show that solutions based on the QUBO formulations of TeamFormation problems are at least as good as those produced by established baselines. Furthermore, we show that QUBO-based solutions leveraging graph neural networks can effectively learn representations of experts and skills to enable transfer learning, allowing node embeddings from one problem instance to be efficiently applied to another.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Risks of Industry Influence in Tech Research</title>
<link>https://arxiv.org/abs/2510.19894</link>
<guid>https://arxiv.org/abs/2510.19894</guid>
<content:encoded><![CDATA[
<div> Keywords: information technologies, public health, technology policy, industry influence, scientific research

Summary: 
Emerging information technologies have a significant impact on public health, political institutions, social dynamics, and the environment. It is crucial to understand these effects to shape evidence-based technology policies that minimize harm and maximize benefits. However, the data necessary for scientific progress in this field are controlled by the same industry that may be subject to regulation, raising concerns about potential industry influence on the scientific record. Technology companies also play a major role in funding research in this area, further complicating the issue. The unique challenges faced by science in technology research necessitate strengthening existing safeguards and developing new ones to ensure the integrity of scientific understanding. Addressing these challenges is essential for maintaining the credibility and objectivity of research in the technology sector. 

<br /><br />Summary: <div>
arXiv:2510.19894v1 Announce Type: new 
Abstract: Emerging information technologies like social media, search engines, and AI can have a broad impact on public health, political institutions, social dynamics, and the natural world. It is critical to develop a scientific understanding of these impacts to inform evidence-based technology policy that minimizes harm and maximizes benefits. Unlike most other global-scale scientific challenges, however, the data necessary for scientific progress are generated and controlled by the same industry that might be subject to evidence-based regulation. Moreover, technology companies historically have been, and continue to be, a major source of funding for this field. These asymmetries in information and funding raise significant concerns about the potential for undue industry influence on the scientific record. In this Perspective, we explore how technology companies can influence our scientific understanding of their products. We argue that science faces unique challenges in the context of technology research that will require strengthening existing safeguards and constructing wholly new ones.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Feature Importance for Online Content Moderation</title>
<link>https://arxiv.org/abs/2510.19882</link>
<guid>https://arxiv.org/abs/2510.19882</guid>
<content:encoded><![CDATA[
<div> predictive performance, user behaviour, moderation intervention, feature selection, user characteristics
Summary: 
- The study aims to understand how user characteristics relate to different responses to moderation interventions. 
- 753 features were analyzed to predict changes in user behavior on Reddit after a major moderation intervention. 
- A greedy feature selection approach was used to identify the most informative features for predicting changes in user activity, toxicity, and participation diversity. 
- Certain features were found to be consistently predictive across all tasks, while others were task-specific or of limited use. 
- Changes in user activity and toxicity were easier to predict than changes in participation diversity, indicating the complexity of post-moderation user behavior. 
- The results suggest the importance of tailoring moderation strategies to both user traits and the specific goals of the intervention. 
- This research lays the groundwork for developing accurate systems to predict user reactions to moderation interventions. 
- Effective moderation strategies should take into account the diversity of user responses and adjust accordingly. 
- The findings highlight the need for personalized and adaptive moderation approaches in online communities. 
Summary: <div>
arXiv:2510.19882v1 Announce Type: cross 
Abstract: Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of "quantification", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drug-disease networks and drug repurposing</title>
<link>https://arxiv.org/abs/2510.19948</link>
<guid>https://arxiv.org/abs/2510.19948</guid>
<content:encoded><![CDATA[
<div> prediction, drug-disease associations, network analysis, graph embedding, machine learning <br />
Summary: <br />Repurposing existing drugs for new diseases is a cost-effective strategy, but the vast number of potential combinations makes it challenging to identify viable options. This study introduces a novel drug-disease network using a variety of data sources and analysis techniques. By applying network-based link prediction methods, the researchers were able to successfully identify potential drug-disease combinations. The methods utilized, particularly those based on graph embedding and network modeling, demonstrated impressive prediction accuracy, outperforming previous approaches. Cross-validation tests showed strong performance, with area under the ROC curve exceeding 0.95 and average precision significantly better than chance. This research highlights the value of utilizing computational methods and network analysis to facilitate the repurposing of drugs for new medical indications, providing a promising approach for identifying potential treatment options efficiently. <br /> <div>
arXiv:2510.19948v1 Announce Type: cross 
Abstract: Repurposing existing drugs to treat new diseases is a cost-effective alternative to de novo drug development, but there are millions of potential drug-disease combinations to be considered with only a small fraction being viable. In silico predictions of drug-disease associations can be invaluable for reducing the size of the search space. In this work we present a novel network of drugs and the diseases they treat, compiled using a combination of existing textual and machine-readable databases, natural-language processing tools, and hand curation, and analyze it using network-based link prediction methods to identify potential drug-disease combinations. We measure the efficacy of these methods using cross-validation tests and find that several methods, particularly those based on graph embedding and network model fitting, achieve impressive prediction performance, significantly better than previous approaches, with area under the ROC curve above 0.95 and average precision almost a thousand times better than chance.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Classic GNNs Strong Baselines Across Varying Homophily: A Smoothness-Generalization Perspective</title>
<link>https://arxiv.org/abs/2412.09805</link>
<guid>https://arxiv.org/abs/2412.09805</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, homophily, message passing, smoothness-generalization dilemma<br />
Summary:<br />
The article introduces the Inceptive Graph Neural Network (IGNN) to address the smoothness-generalization dilemma in GNNs. The dilemma arises due to the trade-off between smoothness and generalization when increasing message passing hops in homophilic neighborhoods. IGNN's design principles enable distinct hop-wise generalization while improving overall generalization with adaptive smoothness. Benchmarking against 30 baselines shows IGNN's superiority and universality in homophilic GNN variants. The study sheds light on the theoretical challenges of GNN universality across varying homophily levels and provides insights for designing effective architectures to enhance GNN performance. <div>
arXiv:2412.09805v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved great success but are often considered to be challenged by varying levels of homophily in graphs. Recent empirical studies have surprisingly shown that homophilic GNNs can perform well across datasets of different homophily levels with proper hyperparameter tuning, but the underlying theory and effective architectures remain unclear. To advance GNN universality across varying homophily, we theoretically revisit GNN message passing and uncover a novel smoothness-generalization dilemma, where increasing hops inevitably enhances smoothness at the cost of generalization. This dilemma hinders learning in higher-order homophilic neighborhoods and all heterophilic ones, where generalization is critical due to complex neighborhood class distributions that are sensitive to shifts induced by noise and sparsity. To address this, we introduce the Inceptive Graph Neural Network (IGNN) built on three simple yet effective design principles, which alleviate the dilemma by enabling distinct hop-wise generalization alongside improved overall generalization with adaptive smoothness. Benchmarking against 30 baselines demonstrates IGNN's superiority and reveals notable universality in certain homophilic GNN variants. Our code and datasets are available at https://github.com/galogm/IGNN.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refugees of the Digital Space: Platform Migration from TikTok to RedNote</title>
<link>https://arxiv.org/abs/2510.18894</link>
<guid>https://arxiv.org/abs/2510.18894</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, RedNote, cross-cultural platform migration, algorithmic governance, digital migrants. <br />
Summary: <br />
This study examines the impact of the nationwide TikTok ban in the U.S. in 2025, leading to American users migrating to RedNote. The research explores how these "TikTok Refugees" adapt to a new platform environment under algorithmic governance. The study analyzes temporal posting patterns, influence dynamics, thematic preferences, and sentiment-weighted topic expressions across different migration phases. High-influence users engage in culturally resonant or commercially strategic content, while political discourse is a point of transnational engagement. Emotionally, high-influence users display more positive affect in culturally connective topics, while low-influence users show stronger emotional intensity in personal narratives. The findings suggest that platform migration is influenced by both structural affordances and users' capacities to adapt and maintain visibility, contributing to the literature on platform society, affective publics, and user agency in transnational digital environments. <br /> <div>
arXiv:2510.18894v1 Announce Type: new 
Abstract: In January 2025, the U.S. government enacted a nationwide ban on TikTok, prompting a wave of American users -- self-identified as ``TikTok Refugees'' -- to migrate to alternative platforms, particularly the Chinese social media app RedNote (Xiaohongshu). This paper examines how these digital migrants navigate cross-cultural platform environments and develop adaptive communicative strategies under algorithmic governance. Drawing on a multi-method framework, the study analyzes temporal posting patterns, influence dynamics, thematic preferences, and sentiment-weighted topic expressions across three distinct migration phases: Pre-Ban, Refugee Surge, and Stabilization.
  An entropy-weighted influence score was used to classify users into high- and low-influence groups, enabling comparative analysis of content strategies. Findings reveal that while dominant topics remained relatively stable over time (e.g., self-expression, lifestyle, and creativity), high-influence users were more likely to engage in culturally resonant or commercially strategic content. Additionally, political discourse was not avoided, but selectively activated as a point of transnational engagement.
  Emotionally, high-influence users tended to express more positive affect in culturally connective topics, while low-influence users showed stronger emotional intensity in personal narratives. These findings suggest that cross-cultural platform migration is shaped not only by structural affordances but also by users' differential capacities to adapt, perform, and maintain visibility. The study contributes to literature on platform society, affective publics, and user agency in transnational digital environments.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Patience in Online Grocery Shopping</title>
<link>https://arxiv.org/abs/2510.19066</link>
<guid>https://arxiv.org/abs/2510.19066</guid>
<content:encoded><![CDATA[
<div> customer patience, online grocery shopping, urban traffic congestion, sustainability, delivery optimization 

Summary: 
This study investigates the impact of customer patience on reducing traffic congestion and emissions in urban delivery systems for online grocery shopping. The research highlights a convex relationship between customer patience and traffic congestion, with allowing just five additional minutes in delivery time resulting in a significant reduction in daily delivery mileage and CO2 emissions. By analyzing two large-scale datasets from Dubai, the study confirms theoretical predictions and demonstrates that modest increases in customer patience can lead to substantial gains in traffic reduction and sustainability. However, beyond ten minutes of added patience, the marginal benefits diminish significantly. This research emphasizes the importance of balancing individual convenience with societal welfare in urban delivery systems, providing a scalable strategy to address the environmental impact of online grocery shopping. <div>
arXiv:2510.19066v1 Announce Type: new 
Abstract: Since the COVID-19 pandemic, online grocery shopping has rapidly reshaped consumer behavior worldwide, fueled by ever-faster delivery promises aimed at maximizing convenience. Yet, this growth has also substantially increased urban traffic congestion, emissions, and pollution. Despite extensive research on urban delivery optimization, little is known about the trade-off between individual convenience and these societal costs. In this study, we investigate the value of marginal extensions in delivery times, termed customer patience, in mitigating the traffic burden caused by grocery deliveries. We first conceptualize the problem and present a mathematical model that highlights a convex relationship between patience and traffic congestion. The theoretical predictions are confirmed by an extensive, network-science based analysis leveraging two large-scale datasets encompassing over 8 million grocery orders in Dubai. Our findings reveal that allowing just five additional minutes in delivery time reduces daily delivery mileage by approximately 30 percent and life-cycle CO2 emissions by 20 percent. Beyond ten minutes of added patience, however, marginal benefits diminish significantly. These results highlight that modest increases in consumer patience can deliver substantial gains in traffic reduction and sustainability, offering a scalable strategy to balance individual convenience with societal welfare in urban delivery systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniqueRank: Identifying Important and Difficult-to-Replace Nodes in Attributed Graphs</title>
<link>https://arxiv.org/abs/2510.19113</link>
<guid>https://arxiv.org/abs/2510.19113</guid>
<content:encoded><![CDATA[
<div> Markov Chain, Node Ranking, Structural Importance, UniqueRank, Attribute Uniqueness  
Summary:  
UniqueRank is a new node-ranking method that considers both structural importance and attribute uniqueness in networks. This approach aims to identify nodes that are difficult to replace, thus playing a significant role in maintaining network efficiency. UniqueRank distinguishes important nodes with distinct attributes from their neighbors in symmetric networks. In real-world networks such as terrorist, social, and supply chain networks, top UniqueRank nodes prove to be more crucial as their removal leads to larger efficiency reductions compared to nodes ranked by other methods. The versatility of UniqueRank is highlighted in its ability to identify essential atoms with unique chemical environments in biomolecular structures. This research fills a gap in existing ranking methods by considering the replaceability of nodes, ultimately enhancing the understanding of network dynamics and identifying truly critical nodes in various applications.  
<br /><br />Summary: <div>
arXiv:2510.19113v1 Announce Type: new 
Abstract: Node-ranking methods that focus on structural importance are widely used in a variety of applications, from ranking webpages in search engines to identifying key molecules in biomolecular networks. In real social, supply chain, and terrorist networks, one definition of importance considers the impact on information flow or network productivity when a given node is removed. In practice, however, a nearby node may be able to replace another node upon removal, allowing the network to continue functioning as before. This replaceability is an aspect that existing ranking methods do not consider. To address this, we introduce UniqueRank, a Markov-Chain-based approach that captures attribute uniqueness in addition to structural importance, making top-ranked nodes harder to replace. We find that UniqueRank identifies important nodes with dissimilar attributes from its neighbors in simple symmetric networks with known ground truth. Further, on real terrorist, social, and supply chain networks, we demonstrate that removing and attempting to replace top UniqueRank nodes often yields larger efficiency reductions than removing and attempting to replace top nodes ranked by competing methods. Finally, we show UniqueRank's versatility by demonstrating its potential to identify structurally critical atoms with unique chemical environments in biomolecular structures.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief propagation for finite networks using a symmetry-breaking source node</title>
<link>https://arxiv.org/abs/2510.19231</link>
<guid>https://arxiv.org/abs/2510.19231</guid>
<content:encoded><![CDATA[
<div> Belief Propagation, message-passing algorithm, graphical models, statistical physics, finite systems <br />
Summary: Belief Propagation (BP) is widely used for inference in graphical models and statistical physics. However, it often provides inaccurate estimates in finite systems, especially in sparse networks. By fixing the state of a single well-connected node, accuracy can be improved without additional computational cost. This approach works well for percolation and Ising models, capturing finite-size effects in various networks, particularly tree-like ones. This method breaks global symmetry, enhancing inference accuracy in sparse networks with few loops. <div>
arXiv:2510.19231v1 Announce Type: new 
Abstract: Belief Propagation (BP) is an efficient message-passing algorithm widely used for inference in graphical models and for solving various problems in statistical physics. However, BP often yields inaccurate estimates of order parameters and their susceptibilities in finite systems, particularly in sparse networks with few loops. Here, we show for both percolation and Ising models that fixing the state of a single well-connected "source" node to break global symmetry substantially improves inference accuracy and captures finite-size effects across a broad range of networks, especially tree-like ones, at no additional computational cost.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
<link>https://arxiv.org/abs/2510.19246</link>
<guid>https://arxiv.org/abs/2510.19246</guid>
<content:encoded><![CDATA[
<div> Framework, Citation Prediction, Bias-Aware Learning, Multi-Agent Feature Extraction, Graph Representation Learning

Summary:
The article introduces a Bias-Aware Citation Prediction Framework to address gaps in the modeling of implicit scientific impact factors and bias-aware learning for accurate citation predictions of newly published papers. The framework combines multi-agent feature extraction and robust graph representation learning to derive fine-grained signals from metadata and external resources, even without citation signals. Robust mechanisms like a two-stage forward process, GroupDRO for optimizing worst-case group risk, and a regularization head for controllable factor analysis are incorporated. Experimental results on real datasets demonstrate the model's effectiveness, achieving a 13% reduction in error metrics and a 5.5% improvement in the ranking metric over baseline methods.<br /><br />Summary: <div>
arXiv:2510.19246v1 Announce Type: new 
Abstract: As a key to accessing research impact, citation dynamics underpins research evaluation, scholarly recommendation, and the study of knowledge diffusion. Citation prediction is particularly critical for newborn papers, where early assessment must be performed without citation signals and under highly long-tailed distributions. We identify two key research gaps: (i) insufficient modeling of implicit factors of scientific impact, leading to reliance on coarse proxies; and (ii) a lack of bias-aware learning that can deliver stable predictions on lowly cited papers. We address these gaps by proposing a Bias-Aware Citation Prediction Framework, which combines multi-agent feature extraction with robust graph representation learning. First, a multi-agent x graph co-learning module derives fine-grained, interpretable signals, such as reproducibility, collaboration network, and text quality, from metadata and external resources, and fuses them with heterogeneous-network embeddings to provide rich supervision even in the absence of early citation signals. Second, we incorporate a set of robust mechanisms: a two-stage forward process that routes explicit factors through an intermediate exposure estimate, GroupDRO to optimize worst-case group risk across environments, and a regularization head that performs what-if analyses on controllable factors under monotonicity and smoothness constraints. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of our proposed model. Specifically, our model achieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable 5.5% improvement in the ranking metric (NDCG) over the baseline methods.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfair Mistakes on Social Media: How Demographic Characteristics influence Authorship Attribution</title>
<link>https://arxiv.org/abs/2510.19708</link>
<guid>https://arxiv.org/abs/2510.19708</guid>
<content:encoded><![CDATA[
<div> gender, native language, age, authorship attribution, bias <br />
<br />Summary: 
The paper investigates the fairness of authorship attribution techniques with regards to gender, native language, and age. Three different fairness evaluations are conducted. Firstly, the study looks at how the distribution of demographic characteristics impacts classifier performance. Secondly, it examines whether a user's demographics affect the likelihood of misclassification. The findings indicate no bias across demographic groups in the closed-world setting. However, when the true author is not an option, errors tend to attribute authorship to individuals with similar demographic characteristics. This suggests a potential bias in certain error scenarios that can lead to false attributions. The study underscores the importance of considering fairness beyond overall classifier performance, as errors can reveal underlying biases in the authorship attribution process. <br /> <div>
arXiv:2510.19708v1 Announce Type: new 
Abstract: Authorship attribution techniques are increasingly being used in online contexts such as sock puppet detection, malicious account linking, and cross-platform account linking. Yet, it is unknown whether these models perform equitably across different demographic groups. Bias in such techniques could lead to false accusations, account banning, and privacy violations disproportionately impacting users from certain demographics. In this paper, we systematically audit authorship attribution for bias with respect to gender, native language, and age. We evaluate fairness in 3 ways. First, we evaluate how the proportion of users with a certain demographic characteristic impacts the overall classifier performance. Second, we evaluate if a user's demographic characteristics influence the probability that their texts are misclassified. Our analysis indicates that authorship attribution does not demonstrate bias across demographic groups in the closed-world setting. Third, we evaluate the types of errors that occur when the true author is removed from the suspect set, thereby forcing the classifier to choose an incorrect author. Unlike the first two settings, this analysis demonstrates a tendency to attribute authorship to users who share the same demographic characteristic as the true author. Crucially, these errors do not only include texts that deviate from a user's usual style, but also those that are very close to the author's average. Our results highlight that though a model may appear fair in the closed-world setting for a performant classifier, this does not guarantee fairness when errors are inevitable.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Substitution to Complement? Uncovering the Evolving Interplay between Ride-hailing Services and Public Transit</title>
<link>https://arxiv.org/abs/2510.19745</link>
<guid>https://arxiv.org/abs/2510.19745</guid>
<content:encoded><![CDATA[
<div> ride-hailing services, transportation network companies, TNC, public transit, Shanghai <br />
Summary: <br />
The study investigates the relationship between transportation network companies (TNCs) and public transit (PT) in Shanghai. Data from 96,716 ride-hailing vehicles in September 2022 was analyzed to classify TNC-PT relationships as first-mile complementary, last-mile complementary, substitutive, or independent. The study found comparable ratios of complementary and substitutive trips, challenging previous assumptions. A machine learning method revealed nonlinear effects of factors such as distance to metro stations and bus stop density on trip relationships. Distinct effects were observed for metro hubs and single-line stations on first- or last-mile complementary ratios, with an inverted U-shaped pattern in the relationship between distance to single-line stations and trip ratios. The findings suggest a complex interplay between TNCs and public transit in urban transportation systems. <br />   <br />Summary: <div>
arXiv:2510.19745v1 Announce Type: new 
Abstract: The literature on transportation network companies (TNCs), also known as ride-hailing services, has often characterized these service providers as predominantly substitutive to public transit (PT). However, as TNC markets expand and mature, the complementary and substitutive relationships with PT may shift. To explore whether such a transformation is occurring, this study collected travel data from 96,716 ride-hailing vehicles during September 2022 in Shanghai, a city characterized by an increasingly saturated TNC market. An enhanced data-driven framework is proposed to classify TNC-PT relationships into four types: first-mile complementary, last-mile complementary, substitutive, and independent. Our findings indicate comparable ratios of complementary trips (9.22%) and substitutive trips (9.06%), contrasting sharply with the findings of prior studies. Furthermore, to examine the nonlinear impact of various influential factors on these ratios, a machine learning method integrating categorical boosting (CatBoost) and Shapley additive explanations (SHAP) is proposed. The results show significant nonlinear effects in some variables, including the distance to the nearest metro station and the density of bus stops. Moreover, metro hubs and regular single-line stations exhibit distinct effects on first- or last-mile complementary ratios. These ratios' relation to the distance to single-line stations shows an inverted U-shaped pattern, with effects rising sharply within 1.5 km, remaining at the peak between 1.5 and 3 km, and then declining as the distance increases to about 15 km.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sync or Sink: Bounds on Algorithmic Collective Action with Noise and Multiple Groups</title>
<link>https://arxiv.org/abs/2510.18933</link>
<guid>https://arxiv.org/abs/2510.18933</guid>
<content:encoded><![CDATA[
<div> algorithmic systems, collective action, coordination challenges, multiple collectives, noise<br />
Summary:<br />
- The article discusses the potential growth of collective action against algorithmic systems and the impact of coordination challenges within collectives.<br />
- It highlights the lack of formal analysis on how coordination challenges affect collective outcomes and the interaction between multiple groups.<br />
- The authors aim to guarantee the success of collective action in the presence of coordination noise and multiple collectives.<br />
- They view data generated as originating from multiple data distributions and derive bounds on collective action success.<br />
- Experiments show that high levels of noise can significantly reduce collective success rates, emphasizing the importance of understanding strategic dynamics in algorithmic systems. <div>
arXiv:2510.18933v1 Announce Type: cross 
Abstract: Collective action against algorithmic systems, which enables groups to promote their own interests, is poised to grow. Hence, there will be growth in the size and the number of distinct collectives. Currently, there is no formal analysis of how coordination challenges within a collective can impact downstream outcomes, or how multiple collectives may affect each other's success. In this work, we aim to provide guarantees on the success of collective action in the presence of both coordination noise and multiple groups. Our insight is that data generated by either multiple collectives or by coordination noise can be viewed as originating from multiple data distributions. Using this framing, we derive bounds on the success of collective action. We conduct experiments to study the effects of noise on collective action. We find that sufficiently high levels of noise can reduce the success of collective action. In certain scenarios, large noise can sink a collective success rate from $100\%$ to just under $60\%$. We identify potential trade-offs between collective size and coordination noise; for example, a collective that is twice as big but with four times more noise experiencing worse outcomes than the smaller, more coordinated one. This work highlights the importance of understanding nuanced dynamics of strategic behavior in algorithmic systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Peer Influence Probabilities with Linear Contextual Bandits</title>
<link>https://arxiv.org/abs/2510.19119</link>
<guid>https://arxiv.org/abs/2510.19119</guid>
<content:encoded><![CDATA[
<div> learning, peer influence probabilities, contextual linear bandit framework, regret minimization, uncertainty-guided exploration<br />
Summary:<br />
The study focuses on estimating peer influence probabilities in networked environments where users share recommendations. The research highlights the contextual nature of these probabilities and their impact on information diffusion processes. Traditional methods struggle to accurately learn these probabilities, leading to a trade-off between regret minimization and estimation error. The proposed uncertainty-guided exploration algorithm aims to address this trade-off by tuning a parameter to achieve optimal rate pairs. Experiments on semi-synthetic network datasets demonstrate the superiority of this method over static approaches and contextual bandits that overlook the trade-off. The findings offer insights into enhancing viral marketing strategies and understanding information diffusion mechanisms in networked environments. <div>
arXiv:2510.19119v1 Announce Type: cross 
Abstract: In networked environments, users frequently share recommendations about content, products, services, and courses of action with others. The extent to which such recommendations are successful and adopted is highly contextual, dependent on the characteristics of the sender, recipient, their relationship, the recommended item, and the medium, which makes peer influence probabilities highly heterogeneous. Accurate estimation of these probabilities is key to understanding information diffusion processes and to improving the effectiveness of viral marketing strategies. However, learning these probabilities from data is challenging; static data may capture correlations between peer recommendations and peer actions but fails to reveal influence relationships. Online learning algorithms can learn these probabilities from interventions but either waste resources by learning from random exploration or optimize for rewards, thus favoring exploration of the space with higher influence probabilities. In this work, we study learning peer influence probabilities under a contextual linear bandit framework. We show that a fundamental trade-off can arise between regret minimization and estimation error, characterize all achievable rate pairs, and propose an uncertainty-guided exploration algorithm that, by tuning a parameter, attains any pair within this trade-off. Our experiments on semi-synthetic network datasets show the advantages of our method over static methods and contextual bandits that ignore this trade-off.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties</title>
<link>https://arxiv.org/abs/2510.19299</link>
<guid>https://arxiv.org/abs/2510.19299</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, social dynamics, online behavior, multi-agent simulation, behavioral reward functions

Summary: 
The study explores the ability of large language model (LLM) agents to exhibit complex social dynamics akin to human online behavior. A multi-agent LLM simulation framework is presented, where agents interact, evaluate each other, and adapt behaviors using in-context learning. Behavioral reward functions are designed to capture key drivers of online engagement, such as social interaction and emotional support. Through experiments, it is observed that coached LLM agents develop stable interaction patterns and form social ties similar to real online communities. The framework provides insights into how network structures and group formations arise from individual decision-making. By combining behavioral rewards with adaptation, the study offers a platform to study collective dynamics in LLM populations and understand the extent to which artificial agents can replicate human-like social behavior. <div>
arXiv:2510.19299v1 Announce Type: cross 
Abstract: Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title>
<link>https://arxiv.org/abs/2501.02505</link>
<guid>https://arxiv.org/abs/2501.02505</guid>
<content:encoded><![CDATA[
<div> Bayesian, Nonparametric, Partial Ranking, Sparse Data, Inference<br />
Summary: 
The study focuses on developing a nonparametric Bayesian method for learning partial rankings based on pairwise comparisons of items. Traditional ranking methods often assign unique ranks or scores to each item, even when there is limited or noisy data. The proposed method aims to distinguish among the ranks of different items only when there is sufficient evidence available in the data. An agglomerative algorithm is developed for Maximum A Posteriori (MAP) inference of partial rankings. The method is tested on various real and synthetic network datasets, revealing that it provides a more concise summary of the data compared to traditional ranking methods, especially in scenarios with sparse observations. <div>
arXiv:2501.02505v3 Announce Type: replace-cross 
Abstract: Ranking items based on pairwise comparisons is common, from using match outcomes to rank sports teams to using purchase or survey data to rank consumer products. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model, have emerged as flexible and powerful tools to tackle ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, most inference-based ranking methods choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we develop a principled nonparametric Bayesian method, adaptable to any statistical ranking method, for learning partial rankings (rankings with ties) that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplex Networks Provide Structural Pathways for Social Contagion in Rural Social Networks</title>
<link>https://arxiv.org/abs/2510.18280</link>
<guid>https://arxiv.org/abs/2510.18280</guid>
<content:encoded><![CDATA[
<div> social networks, multiplex relationships, network torque, behavioral diffusion, network-based interventions 

Summary: 
This study examines the impact of different layers of relationships within human social networks on the spread of behavioral contagion. Using data from 110 rural Honduran communities, the researchers introduce the concept of network torque to measure the contribution of specific network layers to critical contagion pathways. Close friendships were found to play a significant role in enabling non-overlapping diffusion pathways, leading to increased adoption of health practices at the village level. The study highlights the importance of non-redundant pathways facilitated by specific relationship types in amplifying behavioral change, such as correct knowledge about infant feeding and attitudes towards fathers' involvement in postpartum care. Overall, the findings suggest that non-overlapping multiplex social ties are crucial for social contagion and social coherence in traditional social systems. 

<br /><br />Summary: <div>
arXiv:2510.18280v1 Announce Type: new 
Abstract: Human social networks are inherently multiplex, comprising overlapping layers of relationships. Different layers may have distinct structural properties and interpersonal dynamics, but also may interact to form complex interdependent pathways for social contagion. This poses a fundamental problem in understanding behavioral diffusion and in devising effective network-based interventions. Here, we introduce a new conceptualization of how much each network layer contributes to critical contagion pathways and quantify it using a novel metric, network torque. We exploit data regarding sociocentric maps of 110 rural Honduran communities using a battery of 11 name generators and an experiment involving an exogenous intervention. Using a novel statistical framework, we assess the extent to which specific network layers alter global connectivity and support the spread of three experimentally introduced health practices. The results show that specific relationship types - such as close friendships - particularly enable non-overlapping diffusion pathways, amplifying behavioral change at the village level. For instance, non-redundant pathways enabled by closest friends can increase the adoption of correct knowledge about feeding newborns inappropriate chupones and enhance attitudes regarding fathers' involvement in postpartum care. Non-overlapping multiplex social ties are relevant to social contagion and social coherence in traditionally organized social systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior</title>
<link>https://arxiv.org/abs/2510.18155</link>
<guid>https://arxiv.org/abs/2510.18155</guid>
<content:encoded><![CDATA[
<div> Keywords: Consumer decisions, Multi-agent simulation, Large language model, Marketing strategies, Social dynamics 

Summary: 
The article introduces a new framework for simulating consumer decision-making using large language model-powered multi-agent simulations. This framework allows generative agents to interact, express reasoning, form habits, and make purchasing decisions without predefined rules. In a scenario involving price-discount marketing, the system provides actionable outcomes for testing strategies and uncovers emergent social patterns. This approach offers marketers a scalable and low-risk tool for testing strategies before implementation, reducing the need for post-event evaluations and minimizing the risk of underperforming campaigns. <div>
arXiv:2510.18155v1 Announce Type: cross 
Abstract: Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real- world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox envi- ronment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of con- ventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet</title>
<link>https://arxiv.org/abs/2510.18394</link>
<guid>https://arxiv.org/abs/2510.18394</guid>
<content:encoded><![CDATA[
<div> keywords: Internet, censorship, chokepoints, filtering, surveillance
Summary: 
The article discusses the evolving landscape of Internet censorship and the emergence of new sophisticated techniques that impede access to information. Traditional censorship methods, such as client-based, server-based, or network-based filtering, have been supplemented by modern techniques that transcend location boundaries. The concept of chokepoints is introduced to identify bottlenecks in the content production or delivery cycle, where new forms of large-scale client-side surveillance and filtering mechanisms are utilized. These chokepoints serve as barriers to information access, highlighting the need for a new understanding of contemporary censorship practices. The research emphasizes the importance of recognizing and addressing these chokepoints to ensure unrestricted access to information on the Internet. <div>
arXiv:2510.18394v1 Announce Type: cross 
Abstract: Undoubtedly, the Internet has become one of the most important conduits to information for the general public. Nonetheless, Internet access can be and has been limited systematically or blocked completely during political events in numerous countries and regions by various censorship mechanisms. Depending on where the core filtering component is situated, censorship techniques have been classified as client-based, server-based, or network-based. However, as the Internet evolves rapidly, new and sophisticated censorship techniques have emerged, which involve techniques that cut across locations and involve new forms of hurdles to information access. We argue that modern censorship can be better understood through a new lens that we term chokepoints, which identifies bottlenecks in the content production or delivery cycle where efficient new forms of large-scale client-side surveillance and filtering mechanisms have emerged.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoveOD: Synthesizing Origin-Destination Commute Distribution from U.S. Census Data</title>
<link>https://arxiv.org/abs/2510.18858</link>
<guid>https://arxiv.org/abs/2510.18858</guid>
<content:encoded><![CDATA[
<div> Keywords: commuter, origin-destination, transportation, synthetic trips, open data <br />
Summary: <br />
The article introduces MOVEOD, an open-source pipeline that synthesizes public data to create high-resolution origin-destination tables for transportation applications. By combining various open data sources such as ACS, LODES, OSM, and building footprints, MOVEOD generates commuter OD flows with spatial and temporal details for any county in the United States. The pipeline uses constrained sampling and integer-programming methods to ensure accuracy in OD data by matching commuter totals, aligning workplace destinations, and calibrating travel durations. The framework is demonstrated in Hamilton County, Tennessee, producing approximately 150,000 synthetic trips that are used in vehicle-routing algorithms. MOVEOD is an automated system that can be easily applied across the US with just a county and year input and can be adapted for other countries with similar census datasets. The source code and a user-friendly interface are available for public use. <br /> <div>
arXiv:2510.18858v1 Announce Type: cross 
Abstract: High-resolution origin-destination (OD) tables are essential for a wide spectrum of transportation applications, from modeling traffic and signal timing optimization to congestion pricing and vehicle routing. However, outside a handful of data rich cities, such data is rarely available. We introduce MOVEOD, an open-source pipeline that synthesizes public data into commuter OD flows with fine-grained spatial and temporal departure times for any county in the United States. MOVEOD combines five open data sources: American Community Survey (ACS) departure time and travel time distributions, Longitudinal Employer-Household Dynamics (LODES) residence-to-workplace flows, county geometries, road network information from OpenStreetMap (OSM), and building footprints from OSM and Microsoft, into a single OD dataset. We use a constrained sampling and integer-programming method to reconcile the OD dataset with data from ACS and LODES. Our approach involves: (1) matching commuter totals per origin zone, (2) aligning workplace destinations with employment distributions, and (3) calibrating travel durations to ACS-reported commute times. This ensures the OD data accurately reflects commuting patterns. We demonstrate the framework on Hamilton County, Tennessee, where we generate roughly 150,000 synthetic trips in minutes, which we feed into a benchmark suite of classical and learning-based vehicle-routing algorithms. The MOVEOD pipeline is an end-to-end automated system, enabling users to easily apply it across the United States by giving only a county and a year; and it can be adapted to other countries with comparable census datasets. The source code and a lightweight browser interface are publicly available.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis</title>
<link>https://arxiv.org/abs/2311.00164</link>
<guid>https://arxiv.org/abs/2311.00164</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, GraphSAGE, traffic accident analysis, deep-learning methods, road network connections<br />
Summary:<br />
- Constructed a large-scale dataset of 9 million traffic accident records from official reports in the US, along with road networks and traffic volume data.
- Evaluated existing deep-learning methods for accident prediction and found that GraphSAGE can accurately predict accidents with less than 22% mean absolute error.
- Achieved over 87% AUROC in predicting whether an accident will occur or not on road networks.
- Used multitask learning to handle cross-state variabilities and transfer learning to combine traffic volume with accident prediction.
- Highlighted the importance of road graph-structural features in predicting accidents accurately. <br /> <div>
arXiv:2311.00164v3 Announce Type: replace 
Abstract: We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Voting: Resilience to Abstention and Sybils</title>
<link>https://arxiv.org/abs/2001.05271</link>
<guid>https://arxiv.org/abs/2001.05271</guid>
<content:encoded><![CDATA[
<div> Keywords: sybil votes, social choice, status quo enforcing, voting rules, verified participation

Summary:
In the presence of sybil (fake or duplicate) votes and voter abstention, voting rules may fail to accurately represent the will of society. This study introduces status quo enforcing (QUE) voting rules that add virtual votes in support of the status quo to address this issue. The framework of Reality-aware Social Choice is utilized, with a focus on the tradeoff between safety and liveness in maintaining or changing the status quo. The voting rules are shown to be optimal in several domains, offering a balance between resilience to sybils and responsiveness to verified participation. The study provides a quantitative tool for designers to measure the benefit of increased participation and verified identities, offering insights into the conditions under which mechanisms can effectively address challenges in social choice. <br /><br />Summary: <div>
arXiv:2001.05271v4 Announce Type: replace-cross 
Abstract: Voting rules may implement the will of the society when all eligible voters vote, and only them. However, they may fail to do so when sybil (fake or duplicate) votes are present and when only some honest (non sybil) voters actively participate. As, unfortunately, sometimes this is the case, our aim here is to address social choice in the presence of sybils and voter abstention. %
To do so, we build upon the framework of Reality-aware Social Choice: we assume the status quo as an ever-present distinguished alternative, and study \emph{status quo Enforcing (QUE) voting rules}, which add virtual votes in support of the status quo. We characterize the tradeoff between \emph{safety} and \emph{liveness} (the ability of active honest voters to maintain/change the status quo, respectively) in several domains, and show that the voting rules are often optimal. \revision{Our characterization identifies the exact conditions under which mechanisms remain both resilient to sybils and responsive to verified participation, offering a quantitative tool for designers to measure the benefit of increased participation and verified identities.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DarkGram: A Large-Scale Analysis of Cybercriminal Activity Channels on Telegram</title>
<link>https://arxiv.org/abs/2409.14596</link>
<guid>https://arxiv.org/abs/2409.14596</guid>
<content:encoded><![CDATA[
<div> Keywords: cybercriminal activity channels, DarkGram framework, malicious content, phishing attacks, malware

Summary: 

A large-scale analysis of 339 cybercriminal activity channels (CACs) was conducted, revealing the distribution of malicious and unethical content to over 23.8 million users. The DarkGram framework, with 96% accuracy, automatically identifies malicious posts shared on these channels. Content shared on the CACs includes compromised credentials, pirated software, and malware-infected files. Subscribers are engaged through promotions and giveaways, increasing sales of premium cybercriminal content. However, links in these channels often contain phishing attacks and executable files are bundled with malware. The channels can quickly migrate to evade scrutiny, posing risks to their subscribers. By utilizing DarkGram, 196 channels were taken down in three months. Urgent coordinated efforts are needed to combat the growing threats presented by these channels. The dataset and DarkGram framework are open-sourced to aid in this effort. 

<br /><br />Summary: <div>
arXiv:2409.14596v3 Announce Type: replace-cross 
Abstract: We present the first large-scale analysis of 339 cybercriminal activity channels (CACs). Followed by over 23.8 million users, these channels share a wide array of malicious and unethical content with their subscribers, including compromised credentials, pirated software and media, social media manipulation tools, and blackhat hacking resources such as malware, exploit kits, and social engineering scams. To evaluate these channels, we developed DarkGram, a BERT-based framework that automatically identifies malicious posts from the CACs with an accuracy of 96%. Using DarkGram, we conducted a quantitative analysis of 53,605 posts shared on these channels between February and May 2024, revealing key characteristics of the content. While much of this content is distributed for free, channel administrators frequently employ strategies such as promotions and giveaways to engage users and boost the sales of premium cybercriminal content. Interestingly, these channels sometimes pose significant risks to their own subscribers. Notably, 28.1% of the links shared in these channels contained phishing attacks, and 38% of executable files were bundled with malware. Analyzing how subscribers consume and positively react to the shared content paints a dangerous picture of the perpetuation of cybercriminal content at scale. We also found that the CACs can evade scrutiny or platform takedowns by quickly migrating to new channels with minimal subscriber loss, highlighting the resilience of this ecosystem. To counteract this, we utilized DarkGram to detect emerging channels and reported malicious content to Telegram and affected organizations. This resulted in the takedown of 196 channels over three months. Our findings underscore the urgent need for coordinated efforts to combat the growing threats posed by these channels. To aid this effort, we open-source our dataset and the DarkGram framework.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs</title>
<link>https://arxiv.org/abs/2502.18663</link>
<guid>https://arxiv.org/abs/2502.18663</guid>
<content:encoded><![CDATA[
<div> Keywords: Pathfinding, Artificial Intelligence, Cayley graphs, Machine learning, Mathematical applications

Summary: 
This paper focuses on developing efficient artificial intelligence-based approaches for pathfinding on large Cayley graphs. The authors propose a novel combination of reinforcement learning and diffusion distance approach, benchmarking various choices for key building blocks. The methods were compared against the classical computer algebra system GAP, showing superior performance. Mathematical applications include examining the Cayley graph of the symmetric group and supporting a conjecture on its diameter. The study provides bounds on the diameter and presents conjectures on the central limit phenomenon, spectrum distribution, and sorting networks. To encourage collaboration, challenges are created on the Kaggle platform for improving and benchmarking pathfinding approaches on Cayley graphs. <div>
arXiv:2502.18663v2 Announce Type: replace-cross 
Abstract: This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach: architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they "overcome the GAP" for the considered examples. As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks. To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive and Fair Epidemic Resource Allocation Through an Integrated Supply Chain Framework: Insights from a COVID-19 Study</title>
<link>https://arxiv.org/abs/2510.16969</link>
<guid>https://arxiv.org/abs/2510.16969</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, vaccine distribution, optimization framework, regional disparities, behavioral responses

Summary: 
This article presents a novel epidemiological-optimization framework that integrates epidemic forecasting with vaccine distribution and logistics planning. By incorporating spatio-temporally varying infection rates and regional vulnerability indicators, the model supports data-driven decision-making across spatial scales. Two scalable heuristic decomposition algorithms are designed to address computational complexity. Validated using COVID-19 data in the U.S., the model shows the potential to prevent millions of infections and thousands of deaths within a six-month period while improving vaccine accessibility in underserved regions. By prioritizing fairness and considering regional disparities, the framework demonstrates improved efficiency and long-term public health outcomes compared to traditional approaches. Policymakers can utilize this model as a scalable tool to enhance preparedness and ensure a more effective and equitable response to epidemics.<br /><br />Summary: <div>
arXiv:2510.16969v1 Announce Type: new 
Abstract: Timely and effective decision-making is critical during epidemics to reduce preventable infections and deaths. This demands integrated models that jointly capture disease dynamics, vaccine distribution, regional disparities, and behavioral responses. However, most existing approaches decouple epidemic forecasting from logistics planning, hindering adaptive and regionally responsive interventions. We propose a novel epidemiological-optimization framework that jointly models epidemic progression and a multiscale vaccine supply chain. The model incorporates spatio-temporally varying effective infection rates to reflect regional policy and behavioral dynamics. It supports coordinated, data-driven decision-making across spatial scales through two formulations: a multi-objective Gini-based model and a knapsack-based model that leverages regional vulnerability indicators for tractability and improved mitigation. To address computational complexity, we design two scalable heuristic decomposition algorithms inspired by the Benders decomposition. The model is validated using COVID-19 data in the U.S.. We introduce SARIMA-based forecasting as a novel approach for validating epidemic-optimization models under data limitations. The results show that our approach can prevent more than 2 million infections and 30,000 deaths in just six months while significantly improving the accessibility of vaccines in underserved regions. Our framework demonstrates that integrating fairness and epidemic dynamics with vaccine logistics leads to superior outcomes compared to traditional myopic policies. Fairness improves overall efficiency in the long term by prioritizing the most vulnerable populations, leading to better long-term public health outcomes. The model offers policymakers a scalable and operationally relevant tool to strengthen preparedness and ensure a more effective and equitable response to epidemics.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search</title>
<link>https://arxiv.org/abs/2510.17153</link>
<guid>https://arxiv.org/abs/2510.17153</guid>
<content:encoded><![CDATA[
<div> algorithm, hyperedge prediction, hypergraphs, search-based, scoring function

Summary:
HyperSearch is a novel search-based algorithm for hyperedge prediction in complex systems represented as hypergraphs. It addresses the challenge of efficiently evaluating candidate sets by incorporating an empirically grounded scoring function and an efficient search mechanism. The algorithm prunes the search space using an anti-monotonic upper bound of the scoring function, ensuring discarded candidates are never better than kept ones. In experiments on real-world hypergraphs across different domains, HyperSearch outperforms existing baselines in accurately predicting new hyperedges. Its approach improves accuracy by leveraging real-world observations and theoretical guarantees for efficient search. HyperSearch's performance highlights its effectiveness in identifying missing or potential higher-order interactions within complex systems. <div>
arXiv:2510.17153v1 Announce Type: new 
Abstract: Higher-order interactions (HOIs) in complex systems, such as scientific collaborations, multi-protein complexes, and multi-user communications, are commonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes) represents an HOI among the nodes. Given a hypergraph, hyperedge prediction aims to identify hyperedges that are either missing or likely to form in the future, and it has broad applications, including recommending interest-based social groups, predicting collaborations, and uncovering functional complexes in biological systems. However, the vast search space of hyperedge candidates (i.e., all possible subsets of nodes) poses a significant computational challenge, making naive exhaustive search infeasible. As a result, existing approaches rely on either heuristic sampling to obtain constrained candidate sets or ungrounded assumptions on hypergraph structure to select promising hyperedges.
  In this work, we propose HyperSearch, a search-based algorithm for hyperedge prediction that efficiently evaluates unconstrained candidate sets, by incorporating two key components: (1) an empirically grounded scoring function derived from observations in real-world hypergraphs and (2) an efficient search mechanism, where we derive and use an anti-monotonic upper bound of the original scoring function (which is not antimonotonic) to prune the search space. This pruning comes with theoretical guarantees, ensuring that discarded candidates are never better than the kept ones w.r.t. the original scoring function. In extensive experiments on 10 real-world hypergraphs across five domains, HyperSearch consistently outperforms state-of-the-art baselines, achieving higher accuracy in predicting new (i.e., not in the training set) hyperedges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Maximization in Social Networks by Modifying Internal Opinions</title>
<link>https://arxiv.org/abs/2510.17226</link>
<guid>https://arxiv.org/abs/2510.17226</guid>
<content:encoded><![CDATA[
<div> maximizing overall opinion, social networks, computational efficiency, sampling-based algorithms, public opinion governance

Summary:
The paper addresses the important issue of maximizing overall opinion in social networks by strategically modifying the internal opinions of key nodes. Traditional matrix inversion methods are computationally expensive, leading the authors to propose two efficient sampling-based algorithms. A deterministic asynchronous algorithm is developed to precisely identify the optimal set of nodes through asynchronous update operations and progressive refinement. Extensive experiments on real-world datasets demonstrate the superior performance of the proposed methods compared to baseline approaches. Notably, the asynchronous algorithm stands out for its exceptional efficiency and accuracy even in networks with tens of millions of nodes.<br /><br />Summary: <div>
arXiv:2510.17226v1 Announce Type: new 
Abstract: Public opinion governance in social networks is critical for public health campaigns, political elections, and commercial marketing. In this paper, we addresse the problem of maximizing overall opinion in social networks by strategically modifying the internal opinions of key nodes. Traditional matrix inversion methods suffer from prohibitively high computational costs, prompting us to propose two efficient sampling-based algorithms. Furthermore, we develop a deterministic asynchronous algorithm that exactly identifies the optimal set of nodes through asynchronous update operations and progressive refinement, ensuring both efficiency and precision. Extensive experiments on real-world datasets demonstrate that our methods outperform baseline approaches. Notably, our asynchronous algorithm delivers exceptional efficiency and accuracy across all scenarios, even in networks with tens of millions of nodes.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Neuroticism Paradox: How Emotional Instability Fuels Collective Feelings</title>
<link>https://arxiv.org/abs/2510.16046</link>
<guid>https://arxiv.org/abs/2510.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: collective emotions, emotional leadership, neuroticism, conscientiousness, emotional contagion

Summary:<br /><br />Collective emotions in organizations, communities, and societies are influenced by certain individuals, identified in this study as emotionally unstable individuals high in neuroticism and low in conscientiousness. Contrary to conventional wisdom, extraversion did not have a significant impact on emotional leadership. The study revealed a "Neuroticism Paradox," showing that emotional volatility, not stability, drives emotional contagion. The emotions spread at a rate comparable to measles and were sustained by high clustering. The increase in emotional variance over time challenges homeostasis theories and suggests entropy-driven dynamics. The proposed Affective Epidemiology framework suggests that network position and volatility, rather than personality stability, play a crucial role in governing collective emotions and emotional leadership in human systems. <div>
arXiv:2510.16046v1 Announce Type: cross 
Abstract: Collective emotions shape organizations, communities, and societies, yet the traits that determine who drives them remain unknown. Conventional wisdom holds that stable, extraverted individuals act as emotional leaders, calming and coordinating the feelings of others. Here we challenge this view by analyzing a 30.5-month longitudinal dataset of daily emotions from 38 co-located professionals (733,534 records). Using Granger-causality network reconstruction, we find that emotionally unstable individuals -- those high in neuroticism (r = 0.478, p = 0.002) and low in conscientiousness (r = -0.512, p = 0.001) -- are the true "emotional super-spreaders," while extraversion shows no effect (r = 0.238, p = 0.150). This "Neuroticism Paradox" reveals that emotional volatility, not stability, drives contagion. Emotions propagate with a reproduction rate (R_0 = 15.58) comparable to measles, yet the system avoids collapse through high clustering (C = 0.705) that creates "emotional quarantine zones." Emotional variance increased 22.9% over time, contradicting homeostasis theories and revealing entropy-driven dynamics. We propose an Affective Epidemiology framework showing that collective emotions are governed by network position and volatility rather than personality stability -- transforming how we understand emotional leadership in human systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting 'AI Policy' Preprints and Citation Trends in the U.S., U.K and E.U., and South Korea (2015-2024)</title>
<link>https://arxiv.org/abs/2510.16477</link>
<guid>https://arxiv.org/abs/2510.16477</guid>
<content:encoded><![CDATA[
<div> preprints, AI Policy, citations, global disruptions, normalization <br />
Summary: <br />
This study examines the increase in citations of preprints in the field of AI Policy over the past decade, with a particular focus on regions such as the U.S., U.K. & E.U., and South Korea. The study reveals a significant rise in preprint citations, from five percent to forty percent, across these regions. The analysis also compares the impact of COVID-19 and the release of ChatGPT on preprint citations globally. The study discusses the driving factors behind the normalization of preprints in AI Policy literature and the risks associated with this trend. The findings suggest that preprint citation patterns are following the broader trend in computer science research. <div>
arXiv:2510.16477v1 Announce Type: cross 
Abstract: This study of literature focusing on 'AI Policy' over the past decade, found that citations of preprints, publications on platforms such as arXiv, have increased from five percent to forty percent across three major regions: the U.S., U.K. & E.U., and South Korea. We compare regional responses of preprint citations across the global disruptions of COVID-19 and the release of ChatGPT. We discuss driving factors and risks of preprint normalization, which follows the trend in computer science.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network analysis reveals news press landscape and asymmetric user polarization</title>
<link>https://arxiv.org/abs/2408.07900</link>
<guid>https://arxiv.org/abs/2408.07900</guid>
<content:encoded><![CDATA[
<div> echo chambers, polarization, online news platforms, affective interaction, Naver News

Summary:
The study examines the structural and affective polarization among user groups on Naver News during the 2022 Korean presidential election. Analyzing a dataset of articles and user comments, the research identifies two opposing political leanings with significant bias and polarization. Echo chambers are found within co-commenting networks, indicating the reinforcement of opinions within like-minded groups. The study also uncovers asymmetric affective interaction patterns between the polarized groups, highlighting distinct communication strategies based on political affiliations. Through network analysis of a large-scale comment dataset, the research offers insights into the nuanced nature of user polarization on online news platforms. <div>
arXiv:2408.07900v2 Announce Type: replace 
Abstract: Unlike traditional media, online news platforms allow users to consume content that suits their tastes and to facilitate interactions with other people. However, as more personalized consumption of information and interaction with like-minded users increase, ideological bias can inadvertently increase and contribute to the formation of echo chambers, reinforcing the polarization of opinions. Although the structural characteristics of polarization among different ideological groups in online spaces have been extensively studied, research into how these groups emotionally interact with each other has not been as thoroughly explored. From this perspective, we investigate both structural and affective polarization between news media user groups on Naver News, South Korea's largest online news portal, during the period of 2022 Korean presidential election. By utilizing the dataset comprising 333,014 articles and over 36 million user comments, we uncover two distinct groups of users characterized by opposing political leanings and reveal significant bias and polarization among them. Additionally, we reveal the existence of echo chambers within co-commenting networks and investigate the asymmetric affective interaction patterns between the two polarized groups. Classification task of news media articles based on the distinct comment response patterns support the notion that different political groups may employ distinct communication strategies. Our approach based on network analysis on large-scale comment dataset offers novel insights into characteristics of user polarization in the online news platforms and the nuanced interaction nature between user groups.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Dynamic Modeling via Neural ODEs for Popularity Trajectory Prediction</title>
<link>https://arxiv.org/abs/2410.18742</link>
<guid>https://arxiv.org/abs/2410.18742</guid>
<content:encoded><![CDATA[
<div> neural ordinary differential equations, popularity trajectory prediction, information cascades, dynamic properties, real-world dynamics <br />
Summary:
The paper introduces a novel approach, NODEPT, for popularity trajectory prediction in information cascades. Unlike existing methods, NODEPT focuses on forecasting the continuous evolution of popularity over time, capturing dynamic properties like change rates and growth patterns. The method utilizes neural ordinary differential equations to model the underlying diffusion system's dynamics. It includes an encoder for learning cascade representations, an ODE-based generative module to capture system dynamics, and a decoder for predicting future popularity trajectory. Experimental results on real-world datasets validate the superiority and rationality of NODEPT, showcasing its effectiveness in predicting popularity trajectories accurately and efficiently. The approach extends beyond traditional discrete popularity prediction methods, offering a more comprehensive and insightful understanding of information cascade dynamics and future popularity trends. <br /> <div>
arXiv:2410.18742v3 Announce Type: replace 
Abstract: Popularity prediction for information cascades has significant applications across various domains, including opinion monitoring and advertising recommendations. While most existing methods consider this as a discrete problem, popularity actually evolves continuously, exhibiting rich dynamic properties such as change rates and growth patterns. In this paper, we argue that popularity trajectory prediction is more practical, as it aims to forecast the entire trajectory of how popularity unfolds over arbitrary future time. This approach offers insights into both instantaneous popularity and the underlying dynamic properties. However, traditional methods for popularity trajectory prediction primarily rely on specific diffusion mechanism assumptions, which may not align well with real-world dynamics and compromise their performance. To address these limitations, we propose NODEPT, a novel approach based on neural ordinary differential equations (ODEs) for popularity trajectory prediction. NODEPT models the continuous dynamics of the underlying diffusion system using neural ODEs. We first employ an encoder to initialize the latent state representations of information cascades, consisting of two representation learning modules that capture the co-evolution structural characteristics and temporal patterns of cascades from different perspectives. More importantly, we then introduce an ODE-based generative module that learns the dynamics of the diffusion system in the latent space. Finally, a decoder transforms the latent state into the prediction of the future popularity trajectory. Our experimental results on three real-world datasets demonstrate the superiority and rationality of the proposed NODEPT method.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving towards informative and actionable social media research</title>
<link>https://arxiv.org/abs/2505.09254</link>
<guid>https://arxiv.org/abs/2505.09254</guid>
<content:encoded><![CDATA[
<div> social media, societal impacts, causal effects, observational studies, randomized controlled trials
<br />
Summary: 
Social media's impact on society is a topic of ongoing concern, with issues ranging from mental health to democratic disruption. Research on the causal effects of social media has produced conflicting results, with observational studies showing concerning associations and randomized controlled trials often yielding small or null results. The complexity of social media as a system presents challenges for inferring causality at societal scales, leading to debate over the severity of its impacts. To address these challenges, a new approach is proposed that combines the strengths of both observational and experimental methods while acknowledging the limitations of each. Drawing on insights from disciplines like climate science and epidemiology, this proposed approach seeks to provide a more comprehensive understanding of social media's societal effects. 
<br /> 
Summary: <div>
arXiv:2505.09254v2 Announce Type: replace 
Abstract: Social media is nearly ubiquitous in modern life, raising concerns about its societal impacts-from mental health and polarization to violence and democratic disruption. Yet research on its causal effects remains inconclusive: observational studies often find concerning associations, while randomized controlled trials (RCTs) tend to yield small, conflicting, or null results. Literature summaries tend to causally prioritize findings from RCTs, often arguing that concerns about social media are overstated. However, like observational studies, RCTs rely on assumptions that can easily be violated in the context of social media, especially regarding societal outcomes at scale. Here, we enumerate and examine the features of social media as a complex system that challenge our ability to infer causality at societal scales. Drawing on insight from disciplines that have faced similar challenges, like climate-science or epidemiology, we propose a path forward that combines the strength of observational and experimental approaches while acknowledging the limitations of each.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-based user embedding for competing events on social media</title>
<link>https://arxiv.org/abs/2308.14806</link>
<guid>https://arxiv.org/abs/2308.14806</guid>
<content:encoded><![CDATA[
<div> Keywords: social media analysis, user embedding, URL domain, COVID-19 infodemic, Twitter users

Summary: 
In this study, the researchers focused on understanding social divide and polarization through social media analysis in computational social science. They developed a method for embedding users based on a URL domain co-occurrence network, which effectively captures the complex characteristics of social media users involved in competing events. The performance of this method was assessed using binary classification tasks and datasets related to COVID-19 infodemic topics on Twitter. The results showed that domain-based embeddings outperformed user embeddings generated from the retweet network and language-based methods, while also reducing computation time. The findings indicate that domain-based embedding is an accessible and effective approach for characterizing social media users in competitive events such as political campaigns and public health crises.<br /><br />Summary: <div>
arXiv:2308.14806v3 Announce Type: replace-cross 
Abstract: Social divide and polarization have become significant societal issues. To understand the mechanisms behind these phenomena, social media analysis offers research opportunities in computational social science, where developing effective user embedding methods is essential for subsequent analysis. Traditionally, researchers have used predefined network-based user features (e.g., network size, degree, and centrality measures). However, because such measures may not capture the complex characteristics of social media users, in our study we developed a method for embedding users based on a URL domain co-occurrence network. This approach effectively represents social media users involved in competing events such as political campaigns and public health crises. We assessed the method's performance using binary classification tasks and datasets that covered topics associated with the COVID-19 infodemic, such as QAnon, Biden, and Ivermectin, among Twitter users. Our results revealed that user embeddings generated directly from the retweet network and/or based on language performed below expectations, whereas our domain-based embeddings outperformed those methods while reducing computation time. Therefore, domain-based embedding offers an accessible and effective method for characterizing social media users in competing events.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving interdisciplinary contributions to global societal challenges: A 50-year overview</title>
<link>https://arxiv.org/abs/2410.20619</link>
<guid>https://arxiv.org/abs/2410.20619</guid>
<content:encoded><![CDATA[
<div> societal challenges, interdisciplinary collaboration, Sustainable Development Goals, bibliometric data, research fields <br />
Summary:<br />
The study examines the impact of interdisciplinary collaborations on addressing the Sustainable Development Goals (SDGs) set by the United Nations. It analyzes data from 19 research fields from 1970 to 2022, revealing changes in the involvement of different disciplines in tackling specific SDGs over time. The study shows an increasing interconnectedness between fields since the 2000s, aligning with the UN's push for interdisciplinary approaches to global challenges. The findings offer valuable insights for policymakers and practitioners as they assess past achievements and plan for the upcoming SDG target deadline in five years. This research fills a critical gap in quantitative evidence on the role of cross-disciplinary efforts in addressing societal challenges and highlights the evolving relevance of specific disciplines in achieving sustainable development goals. <br /> <div>
arXiv:2410.20619v2 Announce Type: replace-cross 
Abstract: Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Graph Anomaly Detection via Test-Time Training with Homophily-Guided Self-Supervision</title>
<link>https://arxiv.org/abs/2502.14293</link>
<guid>https://arxiv.org/abs/2502.14293</guid>
<content:encoded><![CDATA[
<div> unsupervised learning, graph anomaly detection, cross-domain, self-supervision, attention

Summary:
GADT3 is a novel framework for Graph Anomaly Detection (GAD) that addresses the challenges of cross-domain detection where labeled anomalies are scarce. It combines supervised and self-supervised learning during training and leverages self-supervised learning at test time to adapt to new domains. Key innovations include an effective self-supervision scheme, dynamic edge importance weighting, domain-specific encoders for heterogeneous features, and class-aware regularization for imbalance. Experimental results across multiple cross-domain settings show GADT3 outperforms existing approaches with average improvements of over 8.2% in AUROC and AUPRC. This framework demonstrates the effectiveness of combining supervised and self-supervised learning for GAD and highlights the importance of addressing distribution shifts and heterogeneous feature spaces in cross-domain anomaly detection.<br /><br />Summary: <div>
arXiv:2502.14293v2 Announce Type: replace-cross 
Abstract: Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present GADT3, a novel test-time training framework for cross-domain GAD. GADT3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that GADT3 significantly outperforms existing approaches, achieving average improvements of over 8.2\% in AUROC and AUPRC compared to the best competing model.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> taxonomy, social media, large language models, factual claims, evaluation

Summary:
LLMTaxo is a framework that utilizes large language models to automatically create hierarchical taxonomies of factual claims from social media content. The framework generates topics at different levels of granularity to reduce redundancy and enhance information accessibility. The paper introduces new taxonomy evaluation metrics to assess the effectiveness of the constructed taxonomies. Evaluations on various datasets show that GPT-4o mini model outperforms others in producing clear, coherent, and comprehensive taxonomies. LLMTaxo's flexibility and minimal need for manual intervention make it suitable for a range of applications across different domains. <div>
arXiv:2504.12325v2 Announce Type: replace-cross 
Abstract: With the rapid expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomies of factual claims from social media by generating topics at multiple levels of granularity. The resulting hierarchical structure significantly reduces redundancy and improves information accessibility. We also propose dedicated taxonomy evaluation metrics to enable comprehensive assessment. Evaluations conducted on three diverse datasets demonstrate LLMTaxo's effectiveness in producing clear, coherent, and comprehensive taxonomies. Among the evaluated models, GPT-4o mini consistently outperforms others across most metrics. The framework's flexibility and low reliance on manual intervention underscore its potential for broad applicability.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling</title>
<link>https://arxiv.org/abs/2510.15081</link>
<guid>https://arxiv.org/abs/2510.15081</guid>
<content:encoded><![CDATA[
<div> Keywords: rhetorical strategies, persuasive communication, large language models, transformer-based classifiers, U.S. Presidential debates

Summary:
This study introduces a novel framework that utilizes large language models (LLMs) to automatically generate and label synthetic debate data based on a four-part rhetorical typology. The framework aims to address the limitations of human annotation in analyzing rhetorical strategies, offering a scalable and consistent alternative. By fine-tuning transformer-based classifiers on this labeled dataset, the model demonstrates high performance and generalization across various topical domains. The study showcases two applications of the fine-tuned model, including improved prediction of persuasiveness by incorporating rhetorical strategy labels and analyzing temporal and partisan shifts in rhetorical strategies in U.S. Presidential debates from 1960 to 2020. The findings reveal a notable increase in the use of affective arguments over cognitive ones in U.S. Presidential debates over the years.<br /><br />Summary: <div>
arXiv:2510.15081v1 Announce Type: cross 
Abstract: Rhetorical strategies are central to persuasive communication, from political discourse and marketing to legal argumentation. However, analysis of rhetorical strategies has been limited by reliance on human annotation, which is costly, inconsistent, difficult to scale. Their associated datasets are often limited to specific topics and strategies, posing challenges for robust model development. We propose a novel framework that leverages large language models (LLMs) to automatically generate and label synthetic debate data based on a four-part rhetorical typology (causal, empirical, emotional, moral). We fine-tune transformer-based classifiers on this LLM-labeled dataset and validate its performance against human-labeled data on this dataset and on multiple external corpora. Our model achieves high performance and strong generalization across topical domains. We illustrate two applications with the fine-tuned model: (1) the improvement in persuasiveness prediction from incorporating rhetorical strategy labels, and (2) analyzing temporal and partisan shifts in rhetorical strategies in U.S. Presidential debates (1960-2020), revealing increased use of affective over cognitive argument in U.S. Presidential debates.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis</title>
<link>https://arxiv.org/abs/2510.15125</link>
<guid>https://arxiv.org/abs/2510.15125</guid>
<content:encoded><![CDATA[
<div> Topic taxonomy, social media, political discourse, large language model, political ads <br />
Summary: 
The article introduces a framework for generating a topic taxonomy from unlabeled social media content, specifically analyzing Meta political ads from the 2024 U.S. Presidential election. The framework combines clustering and prompt-based labeling using large language models to reveal discourse structures and annotate topics with moral framing dimensions. The analysis shows that voting and immigration ads dominate spending and impressions, while abortion and election-integrity ads have significant reach. Funding patterns are polarized, with economic appeals driven by conservative PACs and abortion messaging split between pro- and anti-rights coalitions. The framing of ads varies, with abortion messages emphasizing liberty/oppression and economic ads blending care/harm, fairness/cheating, and liberty/oppression narratives. Topic salience reveals correlations between moral foundations and issues, and demographic targeting is evident in the messaging. This framework provides scalable and interpretable ways to analyze political messaging on social media, aiding in understanding emerging narratives, polarization dynamics, and moral underpinnings of digital political communication. <br /><br />Summary: <div>
arXiv:2510.15125v1 Announce Type: cross 
Abstract: Social media platforms play a pivotal role in shaping political discourse, but analyzing their vast and rapidly evolving content remains a major challenge. We introduce an end-to-end framework for automatically generating an interpretable topic taxonomy from an unlabeled corpus. By combining unsupervised clustering with prompt-based labeling, our method leverages large language models (LLMs) to iteratively construct a taxonomy without requiring seed sets or domain expertise. We apply this framework to a large corpus of Meta (previously known as Facebook) political ads from the month ahead of the 2024 U.S. Presidential election. Our approach uncovers latent discourse structures, synthesizes semantically rich topic labels, and annotates topics with moral framing dimensions. We show quantitative and qualitative analyses to demonstrate the effectiveness of our framework. Our findings reveal that voting and immigration ads dominate overall spending and impressions, while abortion and election-integrity achieve disproportionate reach. Funding patterns are equally polarized: economic appeals are driven mainly by conservative PACs, abortion messaging splits between pro- and anti-rights coalitions, and crime-and-justice campaigns are fragmented across local committees. The framing of these appeals also diverges--abortion ads emphasize liberty/oppression rhetoric, while economic messaging blends care/harm, fairness/cheating, and liberty/oppression narratives. Topic salience further reveals strong correlations between moral foundations and issues. Demographic targeting also emerges. This work supports scalable, interpretable analysis of political messaging on social media, enabling researchers, policymakers, and the public to better understand emerging narratives, polarization dynamics, and the moral underpinnings of digital political communication.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Murals to Memes: A Theory of Aesthetic Asymmetry in Political Mobilization</title>
<link>https://arxiv.org/abs/2510.15256</link>
<guid>https://arxiv.org/abs/2510.15256</guid>
<content:encoded><![CDATA[
<div> Keywords: left-wing movements, right-wing movements, participatory art forms, digital culture, aesthetic asymmetry
Summary:
The article explores the historical integration of participatory art forms in left-wing movements and the prioritization of strategic communication and digital memes in right-wing movements. It introduces the concept of aesthetic asymmetry, attributing the difference to organizational ecosystems, moral frameworks, material supports, and historical traditions. Left-wing movements use art to build community and hope, while the contemporary right exploits art to mobilize divisive emotions like humor and resentment. The aesthetic logic aligns with each pole's strategic goals. The article provides a prescriptive model for artistic action, emphasizing emotional, narrative, and formatting strategies for effective mobilization. Understanding this asymmetry is crucial for analyzing political communication and designing cultural interventions for significant social change. 
<br /><br />Summary: <div>
arXiv:2510.15256v1 Announce Type: cross 
Abstract: Why have left-wing movements historically integrated participatory art forms (such as murals and protest songs) into their praxis, while right-wing movements have prioritized strategic communication and, more recently, the digital culture of memes? This article introduces the concept of aesthetic asymmetry to explain this divergence in political action. We argue that the asymmetry is not coincidental but the result of four interconnected structural factors: the organizational ecosystem, the moral and emotional framework, the material supports, and the historical tradition of each political spectrum. While the left tends to use art in a constitutive manner to forge community, solidarity, and hope, the contemporary right tends to use it instrumentally to mobilize polarizing affects such as humor and resentment. Drawing on comparative literature from the Theatre of the Oppressed to analyses of alt-right meme wars, we nuance this distinction and show how the aesthetic logic of each pole aligns with its strategic objectives. The article culminates in a prescriptive model for artistic action, synthesizing keys to effective mobilization into emotional, narrative, and formatting strategies. Understanding this asymmetry is crucial for analyzing political communication and for designing cultural interventions capable of generating profound social change.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERA-MH Concept Paper</title>
<link>https://arxiv.org/abs/2510.15297</link>
<guid>https://arxiv.org/abs/2510.15297</guid>
<content:encoded><![CDATA[
<div> Keywords: VERA-MH, AI chatbots, mental health, suicide risk, automated evaluation

Summary: 
VERA-MH is a new automated evaluation tool for AI chatbots used in mental health contexts, with a focus on assessing suicide risk. Developed by clinicians and experts, VERA-MH utilizes user-agent and judge-agent AI models to simulate conversations between users and chatbots, scoring them based on a rubric informed by best practices in suicide risk management. The tool is currently undergoing rigorous validation to ensure realistic user-agent behavior and accurate scoring by the judge-agent. Preliminary evaluations have been conducted on existing AI chatbots, with further refinement and validation planned. Feedback from the community is being sought on both technical and clinical aspects of the evaluation process. <br /><br />Summary: <div>
arXiv:2510.15297v1 Announce Type: cross 
Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.
  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.
  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Proactive Defense Against Cyber Cognitive Attacks</title>
<link>https://arxiv.org/abs/2510.15801</link>
<guid>https://arxiv.org/abs/2510.15801</guid>
<content:encoded><![CDATA[
<div> AI-driven disinformation, synthetic media, cognitive attacks, predictive methodology, proactive defense strategies
<br />
Summary:
This paper introduces a novel predictive methodology for forecasting disruptive innovations (DIs) and their malicious uses in cognitive attacks. It addresses the gaps in current studies by identifying trends in adversarial tactics and proposing proactive defense strategies. Cyber cognitive attacks exploit psychological biases to manipulate decision-making processes, with emerging technologies like AI-driven disinformation and synthetic media increasing the scale and sophistication of these threats. The paper highlights the need for proactive defense measures to anticipate future DIs and mitigate their malicious use in cognitive attacks. By understanding the evolving landscape of adversarial tactics, organizations can better prepare and defend against cyber cognitive attacks. <div>
arXiv:2510.15801v1 Announce Type: cross 
Abstract: Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit psychological biases and manipulate decision-making processes. Emerging technologies, such as AI-driven disinformation and synthetic media, have accelerated the scale and sophistication of these threats. Prior studies primarily categorize current cognitive attack tactics, lacking predictive mechanisms to anticipate future DIs and their malicious use in cognitive attacks. This paper addresses these gaps by introducing a novel predictive methodology for forecasting the emergence of DIs and their malicious uses in cognitive attacks. We identify trends in adversarial tactics and propose proactive defense strategies.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Engagement Effectiveness of Cyber Cognitive Attacks: A Behavioral Metric for Disinformation Campaigns</title>
<link>https://arxiv.org/abs/2510.15805</link>
<guid>https://arxiv.org/abs/2510.15805</guid>
<content:encoded><![CDATA[
<div> Keywords: disinformation, cognitive attacks, engagement effectiveness, social media platforms, malicious influence<br />
<br />
Summary: 
This paper introduces a novel framework for measuring the engagement effectiveness of cognitive attacks in the context of cybersecurity defense strategies. The proposed weighted interaction metric takes into account both the type and volume of user engagement as compared to the number of attacker-generated transmissions. By applying this model to real-world disinformation campaigns on social media platforms, the metric is shown to not only capture reach but also the behavioral depth of user engagement. The findings offer valuable insights into the behavioral dynamics of cognitive warfare and provide actionable tools for researchers and practitioners to assess and combat the spread of malicious influence online. <div>
arXiv:2510.15805v1 Announce Type: cross 
Abstract: As disinformation-driven cognitive attacks become increasingly sophisticated, the ability to quantify their impact is essential for advancing cybersecurity defense strategies. This paper presents a novel framework for measuring the engagement effectiveness of cognitive attacks by introducing a weighted interaction metric that accounts for both the type and volume of user engagement relative to the number of attacker-generated transmissions. Applying this model to real-world disinformation campaigns across social media platforms, we demonstrate how the metric captures not just reach but the behavioral depth of user engagement. Our findings provide new insights into the behavioral dynamics of cognitive warfare and offer actionable tools for researchers and practitioners seeking to assess and counter the spread of malicious influence online.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is missing from this picture? Persistent homology and mixup barcodes as a means of investigating negative embedding space</title>
<link>https://arxiv.org/abs/2510.14327</link>
<guid>https://arxiv.org/abs/2510.14327</guid>
<content:encoded><![CDATA[
<div> Keywords: informetrics, scientometrics, topological data analysis, persistent homology, mixup barcodes

Summary: 
Recent work in informetrics and scientometrics has led to the development of new metrics that address biases in citation metrics. This study proposes a novel approach using topological data analysis, specifically persistent homology and mixup barcodes, to analyze the negative space among document embeddings generated by topic models. By using top2vec to embed documents and topics in n-dimensional space, the study aims to identify holes in the embedding distribution to determine missing research context or innovation space. The analysis focuses on unobserved publications that represent research published before or after the training data used for top2vec. The study investigates how these unobserved publications integrate research topics on the periphery. This metric has potential applications in understanding the disruptiveness of research and quantifying conceptual novelty.  
<br /><br />Summary: <div>
arXiv:2510.14327v1 Announce Type: new 
Abstract: Recent work in the information sciences, especially informetrics and scientometrics, has made substantial contributions to the development of new metrics that eschew the intrinsic biases of citation metrics. This work has tended to employ either network scientific (topological) approaches to quantifying the disruptiveness of peer-reviewed research, or topic modeling approaches to quantifying conceptual novelty. We propose a combination of these approaches, investigating the prospect of topological data analysis (TDA), specifically persistent homology and mixup barcodes, as a means of understanding the negative space among document embeddings generated by topic models. Using top2vec, we embed documents and topics in n-dimensional space, we use persistent homology to identify holes in the embedding distribution, and then use mixup barcodes to determine which holes are being filled by a set of unobserved publications. In this case, the unobserved publications represent research that was published before or after the data used to train top2vec. We investigate the extent that negative embedding space represents missing context (older research) versus innovation space (newer research), and the extend that the documents that occupy this space represents integrations of the research topics on the periphery. Potential applications for this metric are discussed.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
<link>https://arxiv.org/abs/2510.14889</link>
<guid>https://arxiv.org/abs/2510.14889</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, suicidal ideation, peer interactions, DeBERTa-v3 model, early detection <br />
Summary: 
This study focuses on detecting suicidal ideation (SI) on social media through implicit signals. Many individuals do not explicitly disclose their distress, making early detection challenging. The researchers developed a computational framework using a user's posting history and interactions with peers to predict SI. By identifying top neighbors of a user and analyzing their interactions, they integrated signals in a DeBERTa-v3 model. In a Reddit study, this approach improved early and implicit SI detection by 15%. The findings suggest that peer interactions provide valuable predictive signals for detecting indirect and masked expressions of risk online. This research has implications for designing early detection systems that can capture implicit signs of SI in online environments. 
<br /><br />Summary: <div>
arXiv:2510.14889v1 Announce Type: new 
Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.13837</link>
<guid>https://arxiv.org/abs/2510.13837</guid>
<content:encoded><![CDATA[
<div> Keywords: Hate speech detection, cultural awareness, data sparsity, label propagation, classification performance<br />
Summary:<br />
- The study addresses challenges in hate speech detection, such as biased training labels and varying interpretations of hate across cultures.
- A culture-aware framework is proposed to construct hate subspaces based on individuals' cultural attributes.
- The model tackles data sparsity by considering combinations of cultural attributes and utilizes label propagation to handle cultural entanglement and ambiguous labeling.
- The approach results in individual hate subspaces that improve classification performance.
- Experimental results demonstrate the method outperforms existing techniques by an average of 1.05% across all metrics.<br /> 

Summary: <div>
arXiv:2510.13837v1 Announce Type: cross 
Abstract: Hate speech detection has been extensively studied, yet existing methods often overlook a real-world complexity: training labels are biased, and interpretations of what is considered hate vary across individuals with different cultural backgrounds. We first analyze these challenges, including data sparsity, cultural entanglement, and ambiguous labeling. To address them, we propose a culture-aware framework that constructs individuals' hate subspaces. To alleviate data sparsity, we model combinations of cultural attributes. For cultural entanglement and ambiguous labels, we use label propagation to capture distinctive features of each combination. Finally, individual hate subspaces, which in turn can further enhance classification performance. Experiments show our method outperforms state-of-the-art by 1.05\% on average across all metrics.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Behavior in Crowdfunding: Insights from a Large-Scale Online Experiment</title>
<link>https://arxiv.org/abs/2510.14872</link>
<guid>https://arxiv.org/abs/2510.14872</guid>
<content:encoded><![CDATA[
<div> Keywords: crowdfunding, strategic behavior, risk aversion, mutual insurance, online experiment

Summary:
The study examines strategic behavior in crowdfunding through a large-scale online experiment, focusing on risk aversion and mutual insurance. Participants exhibited distinct behavior in crowdfunding compared to voting, showing a preference for following private signals. Higher signal accuracy decreased risk aversion but increased reliance on mutual insurance. However, increasing the participation threshold amplified risk aversion, possibly indicating cognitive constraints. Mutual insurance supported participation but may hinder information aggregation, especially with higher signal accuracy. The results confirm the impact of informational incentives on crowdfunding and highlight behavioral deviations that challenge standard models. These findings offer valuable insights for platform design and mechanism refinement in crowdfunding. 

<br /><br />Summary: <div>
arXiv:2510.14872v1 Announce Type: cross 
Abstract: This study examines strategic behavior in crowdfunding using a large-scale online experiment. Building on the model of Arieli et. al 2023, we test predictions about risk aversion (i.e., opting out despite seeing a positive private signal) and mutual insurance (i.e., opting in despite seeing a negative private signal) in a static, single-shot crowdfunding game, focusing on informational incentives rather than dynamic effects. Our results validate key theoretical predictions: crowdfunding mechanisms induce distinct strategic behaviors compared to voting, where participants are more likely to follow private signals (odds ratio: 0.139, $p < 0.001$). Additionally, the study demonstrates that higher signal accuracy (85\% vs. 55\%) decreases risk aversion (odds ratio: 0.414, $p = 0.024$) but increases reliance on mutual insurance (odds ratio: 2.532, $p = 0.026$). However, contrary to theory, increasing the required participation threshold (50\% to 80\%) amplifies risk aversion (odds ratio: 3.251, $p = 0.005$), which, pending further investigation, may indicate cognitive constraints.
  Furthermore, we show that while mutual insurance supports participation, it may hinder information aggregation, particularly as signal accuracy increases. These findings advance crowdfunding theory by confirming the impact of informational incentives and identifying behavioral deviations that challenge standard models, offering insights for platform design and mechanism refinement.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ideology and polarization set the agenda on social media</title>
<link>https://arxiv.org/abs/2412.05176</link>
<guid>https://arxiv.org/abs/2412.05176</guid>
<content:encoded><![CDATA[
<div> Twitter, social media, online discourse, ideological alignment, polarization 

Summary: 
The study examines large-scale Twitter data from global debates on Climate Change, COVID-19, and the Russo-Ukrainian War to analyze the structural dynamics of engagement. It finds that discussions are driven by shared ideological alignment rather than specific categories of actors. Users tend to form polarized communities based on their ideological stance, which transcends individual topics and reflects broader patterns of ideological divides. The influence of individual actors within these communities is secondary to the reinforcing effects of selective exposure and shared narratives. Overall, the results emphasize the central role of ideological alignment in shaping online discourse and information spread in polarized environments. <div>
arXiv:2412.05176v2 Announce Type: replace 
Abstract: The abundance of information on social media has reshaped public discussions, shifting attention to the mechanisms that drive online discourse. This study analyzes large-scale Twitter (now X) data from three global debates--Climate Change, COVID-19, and the Russo-Ukrainian War--to investigate the structural dynamics of engagement. Our findings reveal that discussions are not primarily shaped by specific categories of actors, such as media or activists, but by shared ideological alignment. Users consistently form polarized communities, where their ideological stance in one debate predicts their positions in others. This polarization transcends individual topics, reflecting a broader pattern of ideological divides. Furthermore, the influence of individual actors within these communities appears secondary to the reinforcing effects of selective exposure and shared narratives. Overall, our results underscore that ideological alignment, rather than actor prominence, plays a central role in structuring online discourse and shaping the spread of information in polarized environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the gender attrition gap in academic psychology</title>
<link>https://arxiv.org/abs/2510.13273</link>
<guid>https://arxiv.org/abs/2510.13273</guid>
<content:encoded><![CDATA[
<div> Keywords: women, social science disciplines, career trajectories, attrition rates, academic performance <br />
Summary: Women are underrepresented in senior positions in social science disciplines, including psychology, despite comprising the majority of researchers. A study of over 78,000 psychology researchers found that women had higher attrition rates than men, particularly in the early years post-first publication. Early-career retention was strongly correlated with academic performance, specifically first-authored publications. Even after accounting for various factors, such as collaboration networks and institutional environment, women were more likely to leave academia, highlighting persistent barriers in their career progression. The study suggests that addressing the retention of female researchers at early career stages is crucial for promoting long-term gender equity in psychology and other social science disciplines. <br /><br /> <div>
arXiv:2510.13273v1 Announce Type: new 
Abstract: Although more women than men enter social science disciplines, they are underrepresented at senior levels. To investigate this leaky pipeline, this study analyzed the career trajectories of 78,216 psychology researchers using large-scale bibliometric data. Despite overall constituting over 60\% of these researchers, women experienced consistently higher attrition rates than men, particularly in the early years following their first publication. Academic performance, particularly first-authored publications, was strongly associated with early-career retention -- more so than collaboration networks or institutional environment. After controlling for gender differences in publication-, collaboration-, and institution-level factors, women remained more likely to leave academia, especially in early-career stages, pointing to persistent barriers that hinder women's academic careers. These findings suggest that in psychology and potentially other social science disciplines, the core challenge lies in retention rather than recruitment, underscoring the need for targeted, early-career interventions to promote long-term gender equity.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price-Pareto growth model of networks with community structure</title>
<link>https://arxiv.org/abs/2510.13392</link>
<guid>https://arxiv.org/abs/2510.13392</guid>
<content:encoded><![CDATA[
<div> Keywords: degree sequences, real-world networks, citations, scientific impact, community structure<br />
Summary:<br />
The article introduces a new analytical framework for modeling degree sequences in individual communities within real-world networks, specifically focusing on citations to papers in different fields. Inspired by Price's model and the 3DSI generalization, the framework considers citations as being gained accidentally and preferentially. It takes into account the varied growth ratios, average reference list lengths, and preferential citing tendencies across different scientific disciplines. By extending the 3DSI model to heterogeneous networks with a community structure, new analytical formulas for citation number inequality and preferentiality measures are devised. The distribution of citations in a community is found to tend towards a Pareto type II distribution, with analytical formulas provided for estimating its parameters and Gini's index. The model is validated using real citation networks. <div>
arXiv:2510.13392v1 Announce Type: cross 
Abstract: We introduce a new analytical framework for modelling degree sequences in individual communities of real-world networks, e.g., citations to papers in different fields. Our work is inspired by Price's model and its recent generalisation called 3DSI (three dimensions of scientific impact), which assumes that citations are gained partly accidentally, and to some extent preferentially. Our generalisation is motivated by existing research indicating significant differences between how various scientific disciplines grow, namely, minding different growth ratios, average reference list lengths, and preferential citing tendencies. Extending the 3DSI model to heterogeneous networks with a community structure allows us to devise new analytical formulas for, e.g., citation number inequality and preferentiality measures. We show that the distribution of citations in a community tends to a Pareto type II distribution. We also present analytical formulas for estimating its parameters and Gini's index. The new model is validated on real citation networks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels</title>
<link>https://arxiv.org/abs/2510.13636</link>
<guid>https://arxiv.org/abs/2510.13636</guid>
<content:encoded><![CDATA[
<div> SBM, stochastic blockmodel, dyad sampling, goodness-of-fit testing, Markov bases moves<br />
Summary:<br />
A valued stochastic blockmodel (SBM) framework is proposed for analyzing networked data with varying dyad sampling schemes. The paper focuses on testing the goodness-of-fit of non-Bernoulli SBMs, providing explicit Markov bases moves for generating reference distributions. Goodness-of-fit statistics are defined for model evaluation, specifically for labeled SBMs and censored-edge models. The asymptotic behavior of these statistics is studied, with simulations verifying power and Type 1 error rates. The method is applied to ecological networks, offering insights on selecting the number of blocks. The data analysis leads to novel conclusions on block selection for species in host-parasite interactions, differing from existing literature. The findings enhance understanding of how node block membership influences network formation in SBMs. <div>
arXiv:2510.13636v1 Announce Type: cross 
Abstract: A valued stochastic blockmodel (SBM) is a general way to view networked data in which nodes are grouped into blocks and links between them are measured by counts or labels. This family allows for varying dyad sampling schemes, thereby including the classical, Poisson, and labeled SBMs, as well as those in which some edge observations are censored. This paper addresses the question of testing goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on finite-sample tests. We derive explicit Markov bases moves necessary to generate samples from reference distributions and define goodness-of-fit statistics for determining model fit, comparable to those in the literature for related model families.
  For the labeled SBM, which includes in particular the censored-edge model, we study the asymptotic behavior of said statistics. One of the main purposes of testing goodness-of-fit of an SBM is to determine whether block membership of the nodes influences network formation. Power and Type 1 error rates are verified on simulated data. Additionally, we discuss the use of asymptotic results in selecting the number of blocks under the latent-block modeling assumption. The method derived for Poisson SBM is applied to ecological networks of host-parasite interactions. Our data analysis conclusions differ in selecting the number of blocks for the species from previous results in the literature.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3former: Temporal Graph Classification with Topological Machine Learning</title>
<link>https://arxiv.org/abs/2510.13789</link>
<guid>https://arxiv.org/abs/2510.13789</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal graph classification, Topological Temporal Transformer, Descriptor-Attention mechanism, Spectral descriptors, Cross-modal fusion

Summary:
T3former is introduced as a Topological Temporal Transformer for temporal graph classification, leveraging sliding-window topological and spectral descriptors with a specialized Descriptor-Attention mechanism. This approach preserves fine-grained temporal information, enhances robustness, and facilitates cross-modal fusion without rigid discretization. T3former achieves state-of-the-art performance on various benchmarks, including dynamic social networks, brain functional connectivity datasets, and traffic networks. It overcomes limitations of existing methods by addressing oversmoothing and oversquashing issues, allowing for better capture of complex temporal structures. The theoretical guarantees offered by T3former ensure stability under temporal and structural perturbations. Overall, the combination of topological and spectral insights in T3former demonstrates a significant advancement in the field of temporal graph learning. 

<br /><br />Summary: <br />T3former introduces a Topological Temporal Transformer for temporal graph classification, utilizing sliding-window topological and spectral descriptors with a specialized Descriptor-Attention mechanism. This approach enhances robustness, enables cross-modal fusion without rigid discretization, and achieves state-of-the-art performance on various benchmarks. T3former addresses limitations of existing methods by overcoming oversmoothing and oversquashing issues, allowing for better capture of complex temporal structures. The theoretical guarantees provided ensure stability under temporal and structural perturbations, highlighting the significance of combining topological and spectral insights in advancing temporal graph learning. <div>
arXiv:2510.13789v1 Announce Type: cross 
Abstract: Temporal graph classification plays a critical role in applications such as cybersecurity, brain connectivity analysis, social dynamics, and traffic monitoring. Despite its significance, this problem remains underexplored compared to temporal link prediction or node forecasting. Existing methods often rely on snapshot-based or recurrent architectures that either lose fine-grained temporal information or struggle with long-range dependencies. Moreover, local message-passing approaches suffer from oversmoothing and oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that leverages sliding-window topological and spectral descriptors as first-class tokens, integrated via a specialized Descriptor-Attention mechanism. This design preserves temporal fidelity, enhances robustness, and enables principled cross-modal fusion without rigid discretization. T3former achieves state-of-the-art performance across multiple benchmarks, including dynamic social networks, brain functional connectivity datasets, and traffic networks. It also offers theoretical guarantees of stability under temporal and structural perturbations. Our results highlight the power of combining topological and spectral insights for advancing the frontier of temporal graph learning.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Ensemble-Based Semi-Supervised Learning for Illicit Account Detection in Ethereum DeFi Transactions</title>
<link>https://arxiv.org/abs/2412.02408</link>
<guid>https://arxiv.org/abs/2412.02408</guid>
<content:encoded><![CDATA[
<div> Keywords: smart contracts, decentralized finance, illicit account detection, Ethereum blockchain, SLEID framework

Summary: 
Smart contracts have enabled the rapid growth of Decentralized Finance (DeFi) on the Ethereum blockchain, but this expansion comes with security risks, including illicit accounts engaging in fraud. To address these challenges, the SLEID framework, a Self-Learning Ensemble-based Illicit account Detection system, has been proposed. SLEID uses an Isolation Forest model for outlier detection and a self-training mechanism to improve detection accuracy by generating pseudo-labels for unlabeled accounts. Experiments on Ethereum transactions show that SLEID outperforms traditional baselines, achieving higher precision, accuracy, and F1 scores, especially for detecting illicit accounts. The framework reduces the reliance on labeled data, making it a robust solution for safeguarding the DeFi ecosystem. <div>
arXiv:2412.02408v2 Announce Type: replace 
Abstract: The advent of smart contracts has enabled the rapid rise of Decentralized Finance (DeFi) on the Ethereum blockchain, offering substantial rewards in financial innovation and inclusivity. This growth, however, is accompanied by significant security risks such as illicit accounts engaged in fraud. Effective detection is further limited by the scarcity of labeled data and the evolving tactics of malicious accounts. To address these challenges with a robust solution for safeguarding the DeFi ecosystem, we propose $\textbf{SLEID}$, a $\textbf{S}$elf-$\textbf{L}$earning $\textbf{E}$nsemble-based $\textbf{I}$llicit account $\textbf{D}$etection framework. SLEID uses an Isolation Forest model for initial outlier detection and a self-training mechanism to iteratively generate pseudo-labels for unlabeled accounts, enhancing detection accuracy. Experiments on 6,903,860 Ethereum transactions with extensive DeFi interaction coverage demonstrate that SLEID significantly outperforms supervised and semi-supervised baselines with $\textbf{+2.56}$ percentage-point precision, comparable recall, and $\textbf{+0.90}$ percentage-point F1 -- particularly for the minority illicit class -- alongside $\textbf{+3.74}$ percentage-points higher accuracy and improvements in PR-AUC, while substantially reducing reliance on labeled data.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metrics for Assessing Inclusivity and Empowerment of People for Supporting the Design of Inclusive Product Lifecycles</title>
<link>https://arxiv.org/abs/2410.17287</link>
<guid>https://arxiv.org/abs/2410.17287</guid>
<content:encoded><![CDATA[
<div> Keywords: inclusive design, empowerment, stakeholders, lifecycle processes, sustainability

Summary: 
In this study, the focus is on designing an inclusive product lifecycle that empowers stakeholders through structured inclusion. The aim is to shift the focus from empowering users through products to empowering a wider range of stakeholders for sustainable development. By analyzing real-life case studies, a framework is applied to identify dimensions of empowerment and inclusivity and propose measurable metrics with strong causal connections. These metrics support fair stakeholder development and serve as a foundation for sustainability, dignity, well-being, and inclusivity in design processes. This study contributes to design science by expanding the understanding of inclusive design and emphasizing stakeholder inclusion in lifecycle phases. <br /><br />Summary: <div>
arXiv:2410.17287v2 Announce Type: replace-cross 
Abstract: The design of an inclusive product lifecycle is important for empowering stakeholders through their meaningful inclusion in lifecycle processes. To achieve this, the inclusion of stakeholders must be structured in a way that supports their empowerment. Inclusivity addresses the lifecycle context to improve how diverse stakeholders are included across phases, supporting their empowerment. This study aims to build on current inclusive design approaches, which often focus on empowering users through the use of products. It proposes inclusive lifecycle processes as a way to empower a wider range of stakeholders for sustainable development. We apply a novel framework to real-life case studies from the literature to identify the dimensions of empowerment and inclusivity. By analysing the relationships between these dimensions, we propose specific metrics that show strong causal connections. These metrics allow measurable outcomes to support the fair development of stakeholders. Measuring inclusivity and empowerment can serve as a foundation for supporting sustainability, dignity, well-being, and fair stakeholder development, which are explored further in Part 2 and Part 3 of this study. This study contributes to design science by expanding the understanding of inclusive design through stakeholder inclusion in lifecycle phases and by shifting the focus from product to process design.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Quadratic Penalty Method for a Class of Graph Clustering Problems</title>
<link>https://arxiv.org/abs/2501.02187</link>
<guid>https://arxiv.org/abs/2501.02187</guid>
<content:encoded><![CDATA[
<div> Keywords: community-based graph clustering, semi-assignment problems, sparse-constrained optimization, quadratic penalty method, real-world network datasets

Summary: 
Community-based graph clustering is a popular method in analyzing complex social networks. This clustering technique groups vertices based on their connections, forming densely connected subgraphs. The clustering problem is formulated as semi-assignment problems with block properties in the objective function. The problems are reformulated as sparse-constrained optimization models and relaxed to continuous optimization models. The quadratic penalty method and quadratic penalty regularized method are applied to solve these problems efficiently. Numerical experiments show both methods effectively solve graph clustering tasks for synthetic and real-world network datasets. The quadratic penalty regularized method is more efficient for small-scale problems, while the quadratic penalty method is better suited for large-scale cases.<br /><br />Summary: <div>
arXiv:2501.02187v2 Announce Type: replace-cross 
Abstract: Community-based graph clustering is one of the most popular topics in the analysis of complex social networks. This type of clustering involves grouping vertices that are considered to share more connections, whereas vertices in different groups share fewer connections. A successful clustering result forms densely connected induced subgraphs. This paper studies a specific form of graph clustering problems that can be formulated as semi-assignment problems, where the objective function exhibits block properties. We reformulate these problems as sparse-constrained optimization problems and relax them to continuous optimization models. We then apply the quadratic penalty method and the quadratic penalty regularized method to the relaxation problem, respectively. Extensive numerical experiments demonstrate that both methods effectively solve graph clustering tasks for both synthetic and real-world network datasets. For small-scale problems, the quadratic penalty regularized method demonstrates greater efficiency, whereas the quadratic penalty method proves more suitable for large-scale cases.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Hypergraph Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.11728</link>
<guid>https://arxiv.org/abs/2510.11728</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering, neural networks, computer vision, large language models

Summary: 
The paper discusses the scarcity of real-world hypergraph datasets, limiting the development of advanced hypergraph learning algorithms. To address this issue, the authors propose HyperLLM, a novel hypergraph generator driven by large language models (LLMs). HyperLLM simulates the formation and evolution of hypergraphs through multi-agent collaboration, integrating prompts and feedback mechanisms to ensure the generated hypergraphs reflect real-world patterns. Extensive experiments show that HyperLLM outperforms existing methods in capturing structural and temporal hypergraph patterns with minimal statistical priors. The study suggests that leveraging LLMs for hypergraph modeling presents a promising new approach. <div>
arXiv:2510.11728v1 Announce Type: new 
Abstract: Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed</title>
<link>https://arxiv.org/abs/2510.11739</link>
<guid>https://arxiv.org/abs/2510.11739</guid>
<content:encoded><![CDATA[
<div> machine learning, deep learning, celebrity profiling, Urdu language, demographic prediction

Summary:
This study explores the use of modern machine learning and deep learning techniques to predict celebrity demographics based on follower data in Urdu, a low-resource language. The study collected and preprocessed a dataset of short Urdu tweets from followers of subcontinent celebrities. Various algorithms, including Logistic Regression, Support Vector Machines, Random Forests, Convolutional Neural Networks, and Long Short-Term Memory networks, were trained and compared for gender, age, profession, and fame prediction. The best performance was achieved in gender prediction with a cRank of 0.65 and an accuracy of 0.65, showcasing the effectiveness of leveraging follower-based linguistic features for demographic prediction in Urdu. The study demonstrates the potential for using machine learning and neural approaches in analyzing social media data in low-resource languages like Urdu. 

<br /><br />Summary: <div>
arXiv:2510.11739v1 Announce Type: new 
Abstract: Social media has become an essential part of the digital age, serving as a platform for communication, interaction, and information sharing. Celebrities are among the most active users and often reveal aspects of their personal and professional lives through online posts. Platforms such as Twitter provide an opportunity to analyze language and behavior for understanding demographic and social patterns. Since followers frequently share linguistic traits and interests with the celebrities they follow, textual data from followers can be used to predict celebrity demographics. However, most existing research in this field has focused on English and other high-resource languages, leaving Urdu largely unexplored.
  This study applies modern machine learning and deep learning techniques to the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from followers of subcontinent celebrities was collected and preprocessed. Multiple algorithms were trained and compared, including Logistic Regression, Support Vector Machines, Random Forests, Convolutional Neural Networks, and Long Short-Term Memory networks. The models were evaluated using accuracy, precision, recall, F1-score, and cumulative rank (cRank). The best performance was achieved for gender prediction with a cRank of 0.65 and an accuracy of 0.65, followed by moderate results for age, profession, and fame prediction. These results demonstrate that follower-based linguistic features can be effectively leveraged using machine learning and neural approaches for demographic prediction in Urdu, a low-resource language.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine</title>
<link>https://arxiv.org/abs/2510.11746</link>
<guid>https://arxiv.org/abs/2510.11746</guid>
<content:encoded><![CDATA[
<div> Telegram, political communication, Russo-Ukrainian war, policymakers, social media <br />
<br />
Summary: This study analyzes political communication on Telegram by Ukrainian and Russian policymakers during the Russo-Ukrainian war. Following Russia's invasion in 2022, there was a significant increase in Telegram activity, with Ukrainian policymakers focusing initially on war-related topics that later declined. In contrast, Russian policymakers avoided war-related discussions, instead emphasizing unrelated topics to distract the public. Differences in communication strategies were observed between large and small parties, as well as individual policymakers. The study sheds light on how policymakers adapt to wartime communication challenges and provides insights into online political discourse dynamics during times of war. <div>
arXiv:2510.11746v1 Announce Type: new 
Abstract: This study examines elite-driven political communication on Telegram during the ongoing Russo-Ukrainian war, the first large-scale European war in the social media era. Using a unique dataset of Telegram public posts from Ukrainian and Russian policymakers (2019-2024), we analyze changes in communication volume, thematic content, and actor engagement following Russia's 2022 full-scale invasion. Our findings show a sharp increase in Telegram activity after the invasion, particularly among ruling-party policymakers. Ukrainian policymakers initially focused on war-related topics, but this emphasis declined over time In contrast, Russian policymakers largely avoided war-related discussions, instead emphasizing unrelated topics, such as Western crises, to distract public attention. We also identify differences in communication strategies between large and small parties, as well as individual policymakers. Our findings shed light on how policymakers adapt to wartime communication challenges and offer critical insights into the dynamics of online political discourse during times of war.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-aware Propagation Generation with Large Language Models for Fake News Detection</title>
<link>https://arxiv.org/abs/2510.12125</link>
<guid>https://arxiv.org/abs/2510.12125</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, social media, propagation-based methods, large language models, structural patterns

Summary: 
The paper discusses the challenges posed by the spread of fake news on social media and proposes a novel framework, StruSP, for improving fake news detection. By integrating synthetic propagation with real-world structural patterns, StruSP enhances the realism and effectiveness of fake news detection. The framework utilizes a bidirectional evolutionary propagation (BEP) learning strategy to align large language models (LLMs) with the structural dynamics of real propagation. Experimental results on three public datasets demonstrate the superior performance of StruSP in detecting fake news in various scenarios. The BEP strategy enables LLMs to generate more realistic and diverse propagation, resulting in improved detection accuracy. Overall, StruSP offers a promising approach to address the significant challenges posed by the spread of fake news on social media platforms. 

<br /><br />Summary: <div>
arXiv:2510.12125v1 Announce Type: new 
Abstract: The spread of fake news on social media poses a serious threat to public trust and societal stability. While propagation-based methods improve fake news detection by modeling how information spreads, they often suffer from incomplete propagation data. Recent work leverages large language models (LLMs) to generate synthetic propagation, but typically overlooks the structural patterns of real-world discussions. In this paper, we propose a novel structure-aware synthetic propagation enhanced detection (StruSP) framework to fully capture structural dynamics from real propagation. It enables LLMs to generate realistic and structurally consistent propagation for better detection. StruSP explicitly aligns synthetic propagation with real-world propagation in both semantic and structural dimensions. Besides, we also design a new bidirectional evolutionary propagation (BEP) learning strategy to better align LLMs with structural patterns of propagation in the real world via structure-aware hybrid sampling and masked propagation modeling objective. Experiments on three public datasets demonstrate that StruSP significantly improves fake news detection performance in various practical detection scenarios. Further analysis indicates that BEP enables the LLM to generate more realistic and diverse propagation semantically and structurally.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrisisNews: A Dataset Mapping Two Decades of News Articles on Online Problematic Behavior at Scale</title>
<link>https://arxiv.org/abs/2510.12243</link>
<guid>https://arxiv.org/abs/2510.12243</guid>
<content:encoded><![CDATA[
<div> Keywords: social media crises, problematic behaviors, mitigation strategies, stakeholder roles, news coverage

Summary: 
This article explores social media crises, defined as patterns of problematic behaviors on social media that escalate into larger-scale harms. The research examines 93,250 news articles over the past two decades to classify stakeholder roles, behavior types, and outcomes related to social media crises. By taking an event-focused perspective, the study aims to provide a more nuanced understanding of how these crises evolve beyond just analyzing user-generated content. The findings highlight the importance of developing proactive measures and strategies to mitigate social media crises and create safer online environments. By analyzing global news coverage, the research seeks to inform the design of platforms to foster trust and safety in the increasingly digital world.<br /><br />Summary: <div>
arXiv:2510.12243v1 Announce Type: new 
Abstract: As social media adoption grows globally, online problematic behaviors increasingly escalate into large-scale crises, requiring an evolving set of mitigation strategies. While HCI research often analyzes problematic behaviors with pieces of user-generated content as the unit of analysis, less attention has been given to event-focused perspectives that track how discrete events evolve. In this paper, we examine 'social media crises': discrete patterns of problematic behaviors originating and evolving within social media that cause larger-scale harms. Using global news coverage, we present a dataset of 93,250 news articles covering social media-endemic crises from the past 20 years. We analyze a representative subset to classify stakeholder roles, behavior types, and outcomes, uncovering patterns that inform more nuanced classification of social media crises beyond content-based descriptions. By adopting a wider perspective, this research seeks to inform the design of safer platforms, enabling proactive measures to mitigate crises and foster more trustworthy online environments.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOUFLON: Multi-group Modularity-based Fairness-aware Community Detection</title>
<link>https://arxiv.org/abs/2510.12348</link>
<guid>https://arxiv.org/abs/2510.12348</guid>
<content:encoded><![CDATA[
<div> Keywords: MOUFLON, fairness-aware, modularity-based community detection, proportional balance fairness metric, structural biases

Summary: 
MOUFLON is a new fairness-aware, modularity-based community detection method that allows for adjusting the balance between partition quality and fairness outcomes. The method introduces a novel proportional balance fairness metric that provides consistent scores across different network settings. The evaluation of MOUFLON on synthetic and real network datasets examines the trade-off between modularity and fairness in resulting communities, considering network characteristics such as size, density, and group distribution. The study also investigates scenarios with clustered homogeneous groups to understand the impact of structural biases on fairness outcomes. By incorporating fairness constraints into community detection, the research highlights the importance of addressing biases in social network analysis methods and provides valuable insights for designing and benchmarking fairness-aware algorithms.<br /><br />Summary: <div>
arXiv:2510.12348v1 Announce Type: new 
Abstract: In this paper, we propose MOUFLON, a fairness-aware, modularity-based community detection method that allows adjusting the importance of partition quality over fairness outcomes. MOUFLON uses a novel proportional balance fairness metric, providing consistent and comparable fairness scores across multi-group and imbalanced network settings. We evaluate our method under both synthetic and real network datasets, focusing on performance and the trade-off between modularity and fairness in the resulting communities, along with the impact of network characteristics such as size, density, and group distribution. As structural biases can lead to strong alignment between demographic groups and network structure, we also examine scenarios with highly clustered homogeneous groups, to understand how such structures influence fairness outcomes. Our findings showcase the effects of incorporating fairness constraints into modularity-based community detection, and highlight key considerations for designing and benchmarking fairness-aware social network analysis methods.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timeliness, Consensus, and Composition of the Crowd: Community Notes on X</title>
<link>https://arxiv.org/abs/2510.12559</link>
<guid>https://arxiv.org/abs/2510.12559</guid>
<content:encoded><![CDATA[
<div> Efficiency, Crowdsourcing, Moderation, Consensus, Timeliness
<br />
Summary: 
The study analyzes X's Community Notes, a crowdsourced moderation system. It reveals that a small group of top contributors dominate the system, with significant participation inequality (Gini Coefficient = 0.68). Consensus formation is rare, with only 11.5% of notes reaching agreement. Most posts receive conflicting classifications, with around 68% annotated as "Note Not Needed." Surprisingly, such posts are more likely to result in published notes. The average publication delay is 65.7 hours, and longer delays decrease the likelihood of consensus. The system is characterized by persistent dissensus and is repurposed for debate rather than moderation. The study suggests design strategies to enhance equity, faster consensus, and reliability in community-based moderation.
<br /> <div>
arXiv:2510.12559v1 Announce Type: new 
Abstract: This study presents the first large-scale quantitative analysis of the efficiency of X's Community Notes, a crowdsourced moderation system for identifying and contextualising potentially misleading content. Drawing on over 1.8 million notes, we examine three key dimensions of crowdsourced moderation: participation inequality, consensus formation, and timeliness. Despite the system's goal of collective moderation, we find substantial concentration effect, with the top 10% of contributors producing 58% of all notes (Gini Coefficient = 0.68). The observed consensus is rare-only 11.5% of notes reach agreement on publication, while 69% of posts receive conflicting classifications. A majority of noted posts (approximately 68%) are annotated as "Note Not Needed", reflecting the repurposing of the platform for debate rather than moderation. We found that such posts are paradoxically more likely to yield published notes (OR = 3.12). Temporal analyses show that the notes, on average, are published 65.7 hours after the original post, with longer delays significantly reducing the likelihood of consensus. These results portray Community Notes as a stratified, deliberative system dominated by a small contributor elite, marked by persistent dissensus, and constrained by timeliness. We conclude this study by outlining design strategies to promote equity, faster consensus, and epistemic reliability in community-based moderation.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Diameter of (Threshold) Geometric Inhomogeneous Random Graphs</title>
<link>https://arxiv.org/abs/2510.12543</link>
<guid>https://arxiv.org/abs/2510.12543</guid>
<content:encoded><![CDATA[
<div> GIRG, diameter, runtime, distributed protocols, real-world networks <br />
Summary: 
- The article proves that the diameter of threshold GIRG is in the order of log(n), impacting the runtime of distributed protocols that rely on diameter for efficiency.
- GIRG model shares empirical properties with real-world networks, making it a useful tool for analyzing algorithm performance.
- Previous knowledge of the diameter was limited to one-dimensional cases; this study extends the understanding to higher dimensions using novel methods.
- The proof utilizes Peierls-type argument and a renormalization scheme, avoiding complex topological arguments.
- The lower bound is established through a simple ad-hoc construction, providing insights into the structure of GIRG graphs. <br /> 

Summary: <div>
arXiv:2510.12543v1 Announce Type: cross 
Abstract: We prove that the diameter of threshold (zero temperature) Geometric Inhomogeneous Random Graphs (GIRG) is $\Theta(\log n)$. This has strong implications for the runtime of many distributed protocols on those graphs, which often have runtimes bounded as a function of the diameter.
  The GIRG model exhibits many properties empirically found in real-world networks, and the runtime of various practical algorithms has empirically been found to scale in the same way for GIRG and for real-world networks, in particular related to computing distances, diameter, clustering, cliques and chromatic numbers. Thus the GIRG model is a promising candidate for deriving insight about the performance of algorithms in real-world instances.
  The diameter was previously only known in the one-dimensional case, and the proof relied very heavily on dimension one. Our proof employs a similar Peierls-type argument alongside a novel renormalization scheme. Moreover, instead of using topological arguments (which become complicated in high dimensions) in establishing the connectivity of certain boundaries, we employ some comparatively recent and clearer graph-theoretic machinery. The lower bound is proven via a simple ad-hoc construction.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings</title>
<link>https://arxiv.org/abs/2510.12555</link>
<guid>https://arxiv.org/abs/2510.12555</guid>
<content:encoded><![CDATA[
<div> evolution, intelligence, multi-agent reinforcement learning, inclusive fitness, social dynamics <br />
<br />
Summary: Inspired by evolution, a novel multi-agent reinforcement learning framework is proposed, where each agent has a unique genotype linked to inclusive fitness-based reward functions. The framework mirrors natural selection dynamics, leading to the emergence of complex social behaviors resembling biological principles like Hamilton's rule. The study shows cooperative and competitive interactions in network games, indicating a spectrum of cooperation levels based on genetic similarity. This gene-based reward structure fosters non-team-based social dynamics, allowing mutual cooperation among some agents while adversarial behavior between others. The framework also suggests the potential for an arms race of strategies akin to biological evolution, leading to more socially intelligent agents. This approach lays the groundwork for the development of advanced and adaptive multi-agent systems. <br /> <div>
arXiv:2510.12555v1 Announce Type: cross 
Abstract: The competitive and cooperative forces of natural selection have driven the evolution of intelligence for millions of years, culminating in nature's vast biodiversity and the complexity of human minds. Inspired by this process, we propose a novel multi-agent reinforcement learning framework where each agent is assigned a genotype and where reward functions are modelled after the concept of inclusive fitness. An agent's genetic material may be shared with other agents, and our inclusive reward function naturally accounts for this. We study the resulting social dynamics in two types of network games with prisoner's dilemmas and find that our results align with well-established principles from biology, such as Hamilton's rule. Furthermore, we outline how this framework can extend to more open-ended environments with spatial and temporal structure, finite resources, and evolving populations. We hypothesize the emergence of an arms race of strategies, where each new strategy is a gradual improvement over earlier adaptations of other agents, effectively producing a multi-agent autocurriculum analogous to biological evolution. In contrast to the binary team-based structures prevalent in earlier research, our gene-based reward structure introduces a spectrum of cooperation ranging from full adversity to full cooperativeness based on genetic similarity, enabling unique non team-based social dynamics. For example, one agent having a mutual cooperative relationship with two other agents, while the two other agents behave adversarially towards each other. We argue that incorporating inclusive fitness in agents provides a foundation for the emergence of more strategically advanced and socially intelligent agents.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative agents, urban mobility, transportation system, agent profiles, computational efficiency<br />
Summary:<br />
The paper introduces MobileCity, a simulation platform focusing on modeling realistic urban mobility efficiently. It overcomes limitations in existing methods by incorporating a comprehensive transportation system with various transport modes and constructing agent profiles based on questionnaire data. To enhance scalability, agents in MobileCity select actions from a pre-generated space and utilize local memory models. Comparative evaluations involving 4,000 agents show that MobileCity outperforms baselines in generating realistic urban behaviors while maintaining computational efficiency. The platform's practical applications include predicting movement patterns and analyzing demographic trends in transportation preferences. The code for MobileCity is publicly accessible on GitHub at https://github.com/Tony-Yip/MobileCity. <br /> <div>
arXiv:2504.16946v3 Announce Type: replace 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices, rely heavily on static agent profiles leading to behavioral homogenization, and inherit prohibitive computational costs. To address these limitations, we present MobileCity, a lightweight simulation platform designed to model realistic urban mobility with high computational efficiency. We introduce a comprehensive transportation system with multiple transport modes, and collect questionnaire data from respondents to construct agent profiles. To enable scalable simulation, agents perform action selection within a pre-generated action space and uses local models for efficient agent memory generation. Through extensive micro and macro-level evaluations on 4,000 agents, we demonstrate that MobileCity generates more realistic urban behaviors than baselines while maintaining computational efficiency. We further explore practical applications such as predicting movement patterns and analyzing demographic trends in transportation preferences. Our code is publicly available at https://github.com/Tony-Yip/MobileCity.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship between Space-Time Accessibility and Leisure Activity Participation</title>
<link>https://arxiv.org/abs/2510.10307</link>
<guid>https://arxiv.org/abs/2510.10307</guid>
<content:encoded><![CDATA[
<div> space-time accessibility, leisure activities, capability approach, urban life, transport modes

Summary:
The study introduces a space-time accessibility (SPA) metric based on the capability approach to understand how accessibility influences participation in leisure activities in urban areas. Using GPS data from residents in the Paris region, the study assesses how SPA impacts total travel time and leisure participation diversity. Spatial analysis shows that individuals tend to choose destinations aligned with their SPA-defined opportunity sets, emphasizing the metric's validity. Structural equation modeling reveals that SPA directly enhances leisure diversity while reducing travel time, which in turn is linked to lower diversity. The findings underscore the importance of person-centered, capability-informed accessibility metrics in addressing inequalities in urban mobility and guiding transport planning efforts to enhance real freedoms for diverse population groups.<br /><br />Summary: <div>
arXiv:2510.10307v1 Announce Type: new 
Abstract: Understanding how accessibility shapes participation in leisure activities is central to promoting inclusive and vibrant urban life. Conventional accessibility measures often focus on potential access from fixed home locations, overlooking the constraints and opportunities embedded in daily routines. In this study, we introduce a space-time accessibility (SPA) metric rooted in the capability approach, capturing feasible leisure opportunities between home and work given a certain time budget, individual transport modes, and urban infrastructure. Using high-resolution GPS data from 2,415 residents in the Paris region, we assess how SPA influences total travel time and leisure participation, measured as the diversity of leisure activity locations. Spatial patterns show that most individuals-especially active transport users-choose destinations aligned with their SPA-defined opportunity sets, underscoring the metric's validity in capturing capability sets. Structural equation modeling reveals that SPA directly fosters leisure diversity but also reduces travel time, which in turn is associated with lower diversity. These findings highlight the value of person-centered, capability-informed accessibility metrics for understanding inequalities in urban mobility and informing transport planning strategies that expand real freedoms to participate in social life across diverse population groups.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Core Structures of Social Networks via Information Guided Multi-Step Graph Pruning</title>
<link>https://arxiv.org/abs/2510.10499</link>
<guid>https://arxiv.org/abs/2510.10499</guid>
<content:encoded><![CDATA[
<div> pruning, social networks, information theory, network simplification, gradient boosting <br />
Summary:<br />
This study introduces a multi-step network pruning framework that utilizes information theory to identify the structural backbone of dense and overlapping social networks. The proposed method, IGPrune, iteratively removes edges based on their contribution to task-relevant mutual information, preserving essential interaction patterns. Inspired by gradient boosting, IGPrune enables efficient optimization to unveil semantically meaningful connections. Experiments on social and biological networks demonstrate that IGPrune retains critical structural and functional patterns, leading to interpretable network backbones. The approach shows promise in supporting scientific discovery and offering actionable insights in real-world networks. <div>
arXiv:2510.10499v1 Announce Type: new 
Abstract: Social networks often contain dense and overlapping connections that obscure their essential interaction patterns, making analysis and interpretation challenging. Identifying the structural backbone of such networks is crucial for understanding community organization, information flow, and functional relationships. This study introduces a multi-step network pruning framework that leverages principles from information theory to balance structural complexity and task-relevant information. The framework iteratively evaluates and removes edges from the graph based on their contribution to task-relevant mutual information, producing a trajectory of network simplification that preserves most of the inherent semantics. Motivated by gradient boosting, we propose IGPrune, which enables efficient, differentiable optimization to progressively uncover semantically meaningful connections. Extensive experiments on social and biological networks show that IGPrune retains critical structural and functional patterns. Beyond quantitative performance, the pruned networks reveal interpretable backbones, highlighting the method's potential to support scientific discovery and actionable insights in real-world networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioBench: Modeling Human Behavior in Sociological Surveys with Large Language Models</title>
<link>https://arxiv.org/abs/2510.11131</link>
<guid>https://arxiv.org/abs/2510.11131</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, social attitudes, survey data, demographic attributes

Summary: Large language models (LLMs) have shown promise in simulating human social behaviors, but there is a lack of comprehensive benchmarks to evaluate their alignment with real-world social attitudes. The SocioBench benchmark, derived from standardized survey data from the International Social Survey Programme, aggregates over 480,000 respondent records from 30 countries across various sociological domains and demographic attributes. Experiments indicate that LLMs only achieve 30-40% accuracy when simulating individuals in survey scenarios, with significant variations across domains and demographic groups. Current LLMs face limitations in survey contexts, including insufficient individual-level data coverage, lack of scenario diversity, and absence of group-level modeling.<br /><br />Summary: <div>
arXiv:2510.11131v1 Announce Type: new 
Abstract: Large language models (LLMs) show strong potential for simulating human social behaviors and interactions, yet lack large-scale, systematically constructed benchmarks for evaluating their alignment with real-world social attitudes. To bridge this gap, we introduce SocioBench-a comprehensive benchmark derived from the annually collected, standardized survey data of the International Social Survey Programme (ISSP). The benchmark aggregates over 480,000 real respondent records from more than 30 countries, spanning 10 sociological domains and over 40 demographic attributes. Our experiments indicate that LLMs achieve only 30-40% accuracy when simulating individuals in complex survey scenarios, with statistically significant differences across domains and demographic subgroups. These findings highlight several limitations of current LLMs in survey scenarios, including insufficient individual-level data coverage, inadequate scenario diversity, and missing group-level modeling.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation</title>
<link>https://arxiv.org/abs/2510.11423</link>
<guid>https://arxiv.org/abs/2510.11423</guid>
<content:encoded><![CDATA[
<div> misinformation governance, Community Notes, health-related, large language models, CrowdNotes+

Summary:<br /><br />Community Notes is a crowd-sourced system on X, aimed at addressing misinformation. However, a study on health-related notes revealed a delay in response time, prompting the proposal of CrowdNotes+. This framework leverages large language models to enhance the accuracy and speed of fact-checking. CrowdNotes+ includes evidence-grounded note augmentation and utility-guided note automation, coupled with a hierarchical evaluation process. The framework was implemented through HealthNotes, a dataset of annotated health notes. Experiments with fifteen LLMs uncovered an issue in the current evaluation system and demonstrated the efficacy of the hierarchical evaluation and LLM-augmented generation in improving factual precision and evidence utility. This hybrid human-AI governance model shows promise in enhancing the reliability and timeliness of crowd-sourced fact-checking. <div>
arXiv:2510.11423v1 Announce Type: new 
Abstract: Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), enables users to flag misleading posts, attach contextual notes, and vote on their helpfulness. However, our analysis of 30.8K health-related notes reveals significant latency, with a median delay of 17.6 hours before the first note receives a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified framework that leverages large language models (LLMs) to augment Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, along with a hierarchical three-step evaluation that progressively assesses relevance, correctness, and helpfulness. We instantiate the framework through HealthNotes, a benchmark of 1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness judge. Experiments on fifteen LLMs reveal an overlooked loophole in current helpfulness evaluation, where stylistic fluency is mistaken for factual accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented generation jointly enhance factual precision and evidence utility. These results point toward a hybrid human-AI governance model that improves both the rigor and timeliness of crowd-sourced fact-checking.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networks Multiscale Entropy Analysis</title>
<link>https://arxiv.org/abs/2510.11524</link>
<guid>https://arxiv.org/abs/2510.11524</guid>
<content:encoded><![CDATA[
<div> multiscale entropy, network predictability, structural complexity, compression-based entropy, hierarchical patterns

Summary:
- The study introduces a multiscale entropy framework for analyzing complex networks, incorporating spectral graph reduction to capture complexity across multiple scales.
- Application of the framework to various real-world networks reveals consistent entropy profiles across different domains, highlighting three structural regimes (stable, increasing, and hybrid) that align with specific behaviors.
- Multiscale entropy proves superior to single-scale models in determining network predictability, emphasizing the importance of considering structural information across scales for a more comprehensive understanding of network complexity.
- The results position multiscale entropy as a powerful and scalable tool for characterizing, classifying, and assessing complex network structures. 

<br /><br />Summary: <div>
arXiv:2510.11524v1 Announce Type: new 
Abstract: Understanding the structural complexity and predictability of complex networks is a central challenge in network science. Although recent studies have revealed a relationship between compression-based entropy and link prediction performance, existing methods focus on single-scale representations. This approach often overlooks the rich hierarchical patterns that can exist in real-world networks. In this study, we introduce a multiscale entropy framework that extends previous entropy-based approaches by applying spectral graph reduction. This allows us to quantify how structural entropy evolves as the network is gradually coarsened, capturing complexity across multiple scales. We apply our framework to real-world networks across biological, economic, social, technological, and transportation domains. The results uncover consistent entropy profiles across network families, revealing three structural regimes$\unicode{x2013}$stable, increasing, and hybrid$\unicode{x2013}$that align with domain-specific behaviors. Compared to single-scale models, multiscale entropy significantly improves our ability to determine network predictability. This shows that considering structural information across scales provides a more complete characterization of network complexity. Together, these results position multiscale entropy as a powerful and scalable tool for characterizing, classifying, and assessing the structure of complex networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population synthesis with geographic coordinates</title>
<link>https://arxiv.org/abs/2510.09669</link>
<guid>https://arxiv.org/abs/2510.09669</guid>
<content:encoded><![CDATA[
<div> population synthesis algorithm, Normalizing Flows, Variational Autoencoder, spatial accuracy, privacy preservation

Summary: 
This article proposes a novel population synthesis algorithm that utilizes Normalizing Flows (NF) and Variational Autoencoder (VAE) to generate synthetic populations with explicit coordinates. By mapping spatial coordinates into a more regular latent space, the algorithm can account for the unique characteristics of latitude and longitude, including empty spaces and uneven densities. The approach also incorporates non-spatial features to learn the joint distribution and exploit spatial autocorrelations. The method is tested on 121 datasets from diverse geographies, demonstrating superior performance compared to existing benchmarks in terms of spatial accuracy and practical utility. This advancement in generating geolocated synthetic populations at fine spatial resolution opens up new possibilities for applications such as emergency response planning, epidemic modeling, and transportation studies. Privacy preservation measures are also integrated into the evaluation framework to ensure the security of generated data. <div>
arXiv:2510.09669v1 Announce Type: cross 
Abstract: It is increasingly important to generate synthetic populations with explicit coordinates rather than coarse geographic areas, yet no established methods exist to achieve this. One reason is that latitude and longitude differ from other continuous variables, exhibiting large empty spaces and highly uneven densities. To address this, we propose a population synthesis algorithm that first maps spatial coordinates into a more regular latent space using Normalizing Flows (NF), and then combines them with other features in a Variational Autoencoder (VAE) to generate synthetic populations. This approach also learns the joint distribution between spatial and non-spatial features, exploiting spatial autocorrelations. We demonstrate the method by generating synthetic homes with the same statistical properties of real homes in 121 datasets, corresponding to diverse geographies. We further propose an evaluation framework that measures both spatial accuracy and practical utility, while ensuring privacy preservation. Our results show that the NF+VAE architecture outperforms popular benchmarks, including copula-based methods and uniform allocation within geographic areas. The ability to generate geolocated synthetic populations at fine spatial resolution opens the door to applications requiring detailed geography, from household responses to floods, to epidemic spread, evacuation planning, and transport modeling.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Blotto Networks and Voter Models to Simulate Voter Behavior in Response to Competitive Election Spending</title>
<link>https://arxiv.org/abs/2510.09697</link>
<guid>https://arxiv.org/abs/2510.09697</guid>
<content:encoded><![CDATA[
<div> Keywords: Voter Model, propaganda, dynamic population, Blotto Game, graph theory

Summary: 
The study combines the Voter Model and the Blotto Game to understand the impact of propaganda on a dynamic, interconnected population. By incorporating graph theory and game theory aspects, the research aims to analyze the behavior of voters or consumers under the influence of competing propaganda campaigns. The Voter-Blotto Game framework allows for a deeper understanding of the most effective spending strategy in information wars between two opposing parties. Factors such as the components of the graph influencing the value in the eyes of competing players will be examined. Ultimately, the study seeks to shed light on how propaganda influences decision-making processes and shapes outcomes in competitive environments. 

<br /><br />Summary: <div>
arXiv:2510.09697v1 Announce Type: cross 
Abstract: In the past, the Voter Model has been explicitly used to model the impact of propaganda on a dynamic, interconnected population, and certain factors have been identified that influence the behavior of voters when under outside influence. The Blotto Game has also been explicitly used to study information wars between two opposing parties, whether in regards to a political issue or advertising war. Both the graph theory behind the Voter Model and the game theory aspects of the Blotto Game are relevant to the behavior of voters or consumers when they are under the influence of competing propaganda campaigns, and for this reason both are useful to understand the most effective spending strategy. In this project, we seek to combine the two problems into a Voter-Blotto Game and examine what components of the graph most effect its value in the eyes of the competing players.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observer-Based Source Localization in Tree Infection Networks via Laplace Transforms</title>
<link>https://arxiv.org/abs/2510.09828</link>
<guid>https://arxiv.org/abs/2510.09828</guid>
<content:encoded><![CDATA[
<div> localize, infection source, tree-structured network, observer-based, scale-invariant <br />
Summary:
The article addresses the challenge of identifying the source of infection in a tree-structured network using a reduced set of observers. It focuses on the susceptible-infected outbreak model, where the infection spreads through random time delays between neighboring nodes. By analyzing the joint Laplace transform of observer infection times, the study evaluates the identifiability of the infection source. The researchers propose scale-invariant least-squares estimators to accurately localize the source based on edge-delay probability distributions. The effectiveness of these estimators is demonstrated through experiments on synthetic trees and a river network. The article also highlights the complexity of source localization on networks with cycles, pointing out potential issues with standard spanning-tree reductions in such scenarios. <div>
arXiv:2510.09828v1 Announce Type: cross 
Abstract: We address the problem of localizing the source of infection in an undirected, tree-structured network under a susceptible-infected outbreak model. The infection propagates with independent random time increments (i.e., edge-delays) between neighboring nodes, while only the infection times of a subset of nodes can be observed. We show that a reduced set of observers may be sufficient, in the statistical sense, to localize the source and characterize its identifiability via the joint Laplace transform of the observers' infection times. Using the explicit form of these transforms in terms of the edge-delay probability distributions, we propose scale-invariant least-squares estimators of the source. We evaluate their performance on synthetic trees and on a river network, demonstrating accurate localization under diverse edge-delay models. To conclude, we highlight overlooked technical challenges for observer-based source localization on networks with cycles, where standard spanning-tree reductions may be ill-posed.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-driven Knowledge Distillation for Few-shot Node Classification</title>
<link>https://arxiv.org/abs/2510.10116</link>
<guid>https://arxiv.org/abs/2510.10116</guid>
<content:encoded><![CDATA[
<div> Graph neural networks; text-attributed graphs; knowledge distillation; few-shot learning; node classification <br />
Summary: <br />
This study introduces a Preference-Driven Knowledge Distillation (PKD) framework to enhance few-shot node classification in text-attributed graphs. By combining Large Language Models (LLMs) and Graph Neural Networks (GNNs), the framework effectively distills knowledge from LLMs to GNNs through a node selector mechanism. Moreover, a GNN selector is developed to customize knowledge distillation based on the local topologies of nodes, improving the accuracy of node classification. The proposed framework is validated through extensive experiments on real-world TAGs, demonstrating its efficacy in addressing the challenges of training GNNs and handling complex node topologies. <div>
arXiv:2510.10116v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-/few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodes' intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations</title>
<link>https://arxiv.org/abs/2510.10864</link>
<guid>https://arxiv.org/abs/2510.10864</guid>
<content:encoded><![CDATA[
<div> Graph heterophily, spectral filters, adaptive filtering, graph embeddings, GNNs <br />
<br />
Summary: 
The relationship between graph heterophily and spectral filters is complex, with varying optimal filter responses across frequency components. The traditional approach of using fixed filters may not be suitable, prompting the need for adaptive filtering to maintain expressive graph embeddings. The impact of heterophily degree on GNN performance is not strictly correlated with the average frequency response, emphasizing the importance of adaptive graph filters to ensure good generalization. A new GNN method, called [METHOD NAME], is introduced, which leverages information across the heterophily spectrum and combines representations through adaptive mixing. [METHOD NAME] achieves significant improvements in accuracy compared to existing baselines for both homophilic and heterophilic graphs. <div>
arXiv:2510.10864v1 Announce Type: cross 
Abstract: Graph heterophily, where connected nodes have different labels, has attracted significant interest recently. Most existing works adopt a simplified approach - using low-pass filters for homophilic graphs and high-pass filters for heterophilic graphs. However, we discover that the relationship between graph heterophily and spectral filters is more complex - the optimal filter response varies across frequency components and does not follow a strict monotonic correlation with heterophily degree. This finding challenges conventional fixed filter designs and suggests the need for adaptive filtering to preserve expressiveness in graph embeddings. Formally, natural questions arise: Given a heterophilic graph G, how and to what extent will the varying heterophily degree of G affect the performance of GNNs? How can we design adaptive filters to fit those varying heterophilic connections? Our theoretical analysis reveals that the average frequency response of GNNs and graph heterophily degree do not follow a strict monotonic correlation, necessitating adaptive graph filters to guarantee good generalization performance. Hence, we propose [METHOD NAME], a simple yet powerful GNN, which extracts information across the heterophily spectrum and combines salient representations through adaptive mixing. [METHOD NAME]'s superior performance achieves up to 9.2% accuracy improvement over leading baselines across homophilic and heterophilic graphs.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events</title>
<link>https://arxiv.org/abs/2504.18837</link>
<guid>https://arxiv.org/abs/2504.18837</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, social media, climate change events, machine learning models, data collection<br />
Summary:<br />
This survey explores sentiment analysis methods for social media content related to climate-induced events like wildfires and floods. It covers various approaches, from lexicon-based to large language models, and discusses challenges such as data collection and ethical considerations. The study highlights the importance of understanding public perception through sentiment analysis to inform policy decisions and improve emergency responses. It addresses open problems like misinformation detection and model alignment with human values. The research aims to guide researchers and practitioners in effectively analyzing sentiment during extreme weather events in the climate crisis era. <div>
arXiv:2504.18837v4 Announce Type: replace 
Abstract: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake News in Social Networks</title>
<link>https://arxiv.org/abs/1708.06233</link>
<guid>https://arxiv.org/abs/1708.06233</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, fake news, social networks, human behavior, fake-news attack

Summary: 
In this study, the researchers introduce multi-agent reinforcement learning as a novel approach to understanding the dissemination of fake news in social networks. They investigate how human behavior in social networks influences the spread of fake news, particularly in populations exposed to fake news regularly. The study reveals that fake-news attacks are more successful when targeting highly connected individuals and those with weaker private information. Additionally, spreading disinformation across multiple agents is more effective than concentrating it on a few agents. The research also suggests that fake news spreads less effectively in balanced networks compared to clustered networks. A human-subject experiment conducted to validate the model's predictions supports the effectiveness of the proposed approach in analyzing fake news propagation in social networks.<br /><br />Summary: <div>
arXiv:1708.06233v2 Announce Type: replace-cross 
Abstract: We propose multi-agent reinforcement learning as a new method for modeling fake news in social networks. This method allows us to model human behavior in social networks both in unaccustomed populations and in populations that have adapted to the presence of fake news. In particular the latter is challenging for existing methods. We find that a fake-news attack is more effective if it targets highly connected people and people with weaker private information. Attacks are more effective when the disinformation is spread across several agents than when the disinformation is concentrated with more intensity on fewer agents. Furthermore, fake news spread less well in balanced networks than in clustered networks. We test a part of our findings in a human-subject experiment. The experimental evidence provides support for the predictions from the model, suggesting that the model is suitable to analyze the spread of fake news in social networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework</title>
<link>https://arxiv.org/abs/2310.07086</link>
<guid>https://arxiv.org/abs/2310.07086</guid>
<content:encoded><![CDATA[
<div> framework, NLP, social media, user feedback, transit surveys
Summary:
The paper presents a novel NLP-based framework that leverages social media data from Twitter to analyze user feedback on service issues, eliminating the need for costly transit surveys. The framework uses few-shot learning for tweet classification and sentiment analysis to identify and measure sentiments related to safety, reliability, and maintenance of the NYC subway system. The study validated the framework's effectiveness through manual label validation and comparison with agency-run customer surveys, demonstrating its ability to accurately gauge user feedback and identify areas for targeted improvement. The innovative approach highlights the potential of utilizing social media data for understanding user perceptions and planning transit system enhancements. 

Summary: <div>
arXiv:2310.07086v2 Announce Type: replace-cross 
Abstract: Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, distinguishing between positive, negative, and neutral tweets. The effectiveness of the framework was validated on a subset of manually labeled Twitter data and was applied to the NYC subway system as a case study. The framework accurately classifies tweets into predefined categories related to safety, reliability, and maintenance of the subway system and effectively measured sentiment intensities within each category. The general findings were corroborated through a comparison with an agency-run customer survey conducted in the same year. The findings highlight the effectiveness of the proposed framework in gauging user feedback through inexpensive social media data to understand the pain points of the transit system and plan for targeted improvements.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codiscovering graphical structure and functional relationships within data: A Gaussian Process framework for connecting the dots</title>
<link>https://arxiv.org/abs/2311.17007</link>
<guid>https://arxiv.org/abs/2311.17007</guid>
<content:encoded><![CDATA[
<div> Function approximation, hypergraph, data-driven discovery, interpretable Gaussian Process, computational knowledge<br />
Summary:<br />
This article introduces three levels of complexity in function approximation, ranging from approximating unknown functions given input/output data to discovering the structure of a hypergraph. The focus is on Type 3 problems, where the structure of the hypergraph is unknown, requiring the data-driven discovery of both the structure and unknown functions. A novel interpretable Gaussian Process (GP) framework is proposed for solving Type 3 problems efficiently without the need for randomization of data or knowledge of its sampling. The framework leverages the nonlinear ANOVA capabilities of GPs as a sensing mechanism, allowing for polynomial complexity in contrast to super-exponential complexity in causal inference methods. This approach provides a platform for organizing, communicating, and processing computational knowledge effectively. <div>
arXiv:2311.17007v2 Announce Type: replace-cross 
Abstract: Most problems within and beyond the scientific domain can be framed into one of the following three levels of complexity of function approximation. Type 1: Approximate an unknown function given input/output data. Type 2: Consider a collection of variables and functions, some of which are unknown, indexed by the nodes and hyperedges of a hypergraph (a generalized graph where edges can connect more than two vertices). Given partial observations of the variables of the hypergraph (satisfying the functional dependencies imposed by its structure), approximate all the unobserved variables and unknown functions. Type 3: Expanding on Type 2, if the hypergraph structure itself is unknown, use partial observations of the variables of the hypergraph to discover its structure and approximate its unknown functions. These hypergraphs offer a natural platform for organizing, communicating, and processing computational knowledge. While most scientific problems can be framed as the data-driven discovery of unknown functions in a computational hypergraph whose structure is known (Type 2), many require the data-driven discovery of the structure (connectivity) of the hypergraph itself (Type 3). We introduce an interpretable Gaussian Process (GP) framework for such (Type 3) problems that does not require randomization of the data, access to or control over its sampling, or sparsity of the unknown functions in a known or learned basis. Its polynomial complexity, which contrasts sharply with the super-exponential complexity of causal inference methods, is enabled by the nonlinear ANOVA capabilities of GPs used as a sensing mechanism.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional stochastic model of citation dynamics with memory and volatility</title>
<link>https://arxiv.org/abs/2503.03011</link>
<guid>https://arxiv.org/abs/2503.03011</guid>
<content:encoded><![CDATA[
<div> latent attention, citation dynamics, stochastic model, memory effects, collective attention

Summary: 
This study addresses the challenge of understanding citation dynamics in networks and the science of science. It introduces a novel discovery that the variance of the logarithm of citation counts per unit time follows a power law with respect to time. A stochastic model is developed, incorporating memory-driven processes with cumulative advantage in the form of fractional Brownian motion characterized by a Hurst parameter. It is found that antipersistent fluctuations in attention lead to log-normal citation distributions, while persistent dynamics result in heavy-tailed power laws, resolving the log-normalpower-law discrepancy. Empirical analysis of arXiv e-prints suggests that the latent attention process displays antipersistent behavior. By linking memory effects and stochastic fluctuations in attention to broader network dynamics, this study offers a unified framework for understanding the evolution of collective attention in science and other attention-driven processes. <div>
arXiv:2503.03011v2 Announce Type: replace-cross 
Abstract: Understanding the statistical laws governing citation dynamics remains a fundamental challenge in network theory and the science of science. Citation networks typically exhibit in-degree distributions well approximated by log-normal distributions yet also display power-law behaviour in the high-citation regime -- an apparent contradiction lacking a unified explanation. Here we identify a previously unrecognised phenomenon: the variance of the logarithm of citation counts per unit time follows a power law with respect to time ($t$) since publication, scaling as $t^{H}$, with $H$ constant. This discovery introduces a new challenge while simultaneously offering a crucial clue to resolving this discrepancy. We develop a stochastic model in which latent attention to publications evolves through a memory-driven process with cumulative advantage, modelled as fractional Brownian motion with Hurst parameter $H$ and volatility. We show that antipersistent fluctuations in attention ($H < 1/2$) yield log-normal citation distributions, whereas persistent attention dynamics ($H > 1/2$) favour heavy-tailed power laws, thus resolving the log-normal--power-law contradiction. Numerical simulations confirm both the $t^{H}$ law and the transition between regimes. Empirical analysis of arXiv e-prints indicates that the latent attention process is intrinsically antipersistent ($H \approx 0.13$). By linking memory effects and stochastic fluctuations in attention to broader network dynamics, our findings provide a unifying framework for understanding the evolution of collective attention in science and other attention-driven processes.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Crawler Restrictions, AI Training Datasets \&amp;amp; Political Biases</title>
<link>https://arxiv.org/abs/2510.09031</link>
<guid>https://arxiv.org/abs/2510.09031</guid>
<content:encoded><![CDATA[
<div> restrictions, AI crawlers, content type, data composition, training data

Summary:<br /><br />Large language models rely on web-scraped text for training, but content creators are increasingly blocking AI crawlers. A study on the top one million websites since 2023 shows a rise in restrictions, especially in news outlets with high factual reporting. Websites with neutral political positions tend to impose stronger restrictions, while hyperpartisan sites and those with low factual reporting restrict access to AI crawlers less. These patterns may lead to training datasets being skewed towards low-quality or polarized content, which could impact the capabilities of models used by AI-as-a-Service providers. The findings highlight the importance of understanding the impact of crawler restrictions on training data composition and the potential consequences for the performance of large language models. <div>
arXiv:2510.09031v1 Announce Type: new 
Abstract: Large language models rely on web-scraped text for training; concurrently, content creators are increasingly blocking AI crawlers to retain control over their data. We analyze crawler restrictions across the top one million most-visited websites since 2023 and examine their potential downstream effects on training data composition. Our analysis reveals growing restrictions, with blocking patterns varying by website popularity and content type. A quarter of the top thousand websites restrict AI crawlers, decreasing to one-tenth across the broader top million. Content type matters significantly: 34.2% of news outlets disallow OpenAI's GPTBot, rising to 55% for outlets with high factual reporting. Additionally, outlets with neutral political positions impose the strongest restrictions (58%), whereas hyperpartisan websites and those with low factual reporting impose fewer restrictions -only 4.1% of right-leaning outlets block access to OpenAI. Our findings suggest that heterogeneous blocking patterns may skew training datasets toward low-quality or polarized content, potentially affecting the capabilities of models served by prominent AI-as-a-Service providers.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Narrative Prediction: Leveraging Platform-Invariant Discourse Networks</title>
<link>https://arxiv.org/abs/2510.09464</link>
<guid>https://arxiv.org/abs/2510.09464</guid>
<content:encoded><![CDATA[
<div> network proximity, cross-platform prediction, discourse networks, information diffusion, social media


Summary:
This study introduces a novel approach to predicting cross-platform information diffusion by utilizing network proximity. By constructing platform-invariant discourse networks that connect users based on shared narrative engagement, the researchers were able to identify patterns in how content spreads across different platforms. The approach outperformed traditional diffusion models and baselines, requiring only a small percentage of active users to make accurate predictions. The framework was validated using data from the 2024 U.S. election, successfully identifying emerging narratives with a high degree of accuracy. The findings suggest that by leveraging cross-platform neighbor proximity, predictive signals can be gleaned without direct cross-platform influence, offering potential for proactive intervention in the spread of misinformation and rumors online.  <div>
arXiv:2510.09464v1 Announce Type: new 
Abstract: Online narratives spread unevenly across platforms, with content emerging on one site often appearing on others, hours, days or weeks later. Existing cross-platform information diffusion models often treat platforms as isolated systems, disregarding cross-platform activity that might make these patterns more predictable. In this work, we frame cross-platform prediction as a network proximity problem: rather than tracking individual users across platforms or relying on brittle signals like shared URLs or hashtags, we construct platform-invariant discourse networks that link users through shared narrative engagement. We show that cross-platform neighbor proximity provides a strong predictive signal: adoption patterns follow discourse network structure even without direct cross-platform influence. Our highly-scalable approach substantially outperforms diffusion models and other baselines while requiring less than 3% of active users to make predictions. We also validate our framework through retrospective deployment. We sequentially process a datastream of 5.7M social media posts occurred during the 2024 U.S. election, to simulate real-time collection from four platforms (X, TikTok, Truth Social, and Telegram): our framework successfully identified emerging narratives, including crises-related rumors, yielding over 94% AUC with sufficient lead time to support proactive intervention.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Birdwatch to Community Notes, from Twitter to X: four years of community-based content moderation</title>
<link>https://arxiv.org/abs/2510.09585</link>
<guid>https://arxiv.org/abs/2510.09585</guid>
<content:encoded><![CDATA[
<div> Keywords: Community Notes, crowdsourced content moderation, social media platforms, dataset, literature review

Summary:
Community Notes, previously known as Birdwatch, was launched by X (formerly Twitter) in January 2021 as a large-scale crowdsourced content moderation initiative. This Resource paper offers a systematic review of the literature on Community Notes and provides a curated dataset and source code for future research on the topic. The dataset includes Notes and Ratings data from the program's first four years, with language detection and analysis of discussion topics in English-language Notes. Monthly interaction networks among Contributors were also constructed. The resources presented in this paper aim to enhance understanding of the dynamics and effectiveness of Community Notes, as the model gains traction across various social media platforms. Overall, this paper serves as a valuable foundation for advancing research on the Community Notes system. 

<br /><br />Summary: Community Notes, previously Birdwatch, launched by X (formerly Twitter) in 2021 for content moderation. Resource paper provides literature review, dataset, source code for research. Analysis of Notes data, identification of discussion topics, and construction of Contributor interaction networks. Aims to enhance understanding of Community Notes dynamics and effectiveness. Valuable foundation for future research. <div>
arXiv:2510.09585v1 Announce Type: new 
Abstract: Community Notes (formerly known as Birdwatch) is the first large-scale crowdsourced content moderation initiative that was launched by X (formerly known as Twitter) in January 2021. As the Community Notes model gains momentum across other social media platforms, there is a growing need to assess its underlying dynamics and effectiveness. This Resource paper provides (a) a systematic review of the literature on Community Notes, and (b) a major curated dataset and accompanying source code to support future research on Community Notes. We parsed Notes and Ratings data from the first four years of the program and conducted language detection across all Notes. Focusing on English-language Notes, we extracted embedded URLs and identified discussion topics in each Note. Additionally, we constructed monthly interaction networks among the Contributors. Together with the literature review, these resources offer a robust foundation for advancing research on the Community Notes system.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplexons: Limits of Multiplex Networks</title>
<link>https://arxiv.org/abs/2510.08639</link>
<guid>https://arxiv.org/abs/2510.08639</guid>
<content:encoded><![CDATA[
<div> Keywords: multiplex networks, limit theory, dense networks, graphons, clustering coefficients

Summary: 
Multiplex networks are complex systems that consist of multiple layers of interactions between nodes. This article introduces a framework for studying properties of large multiplex networks by developing a limit theory analogous to graphons for dense graphs. The framework allows for deriving limiting analogues of common multiplex features such as degree distributions and clustering coefficients. Various examples, including correlated versions of existing graph models and dynamic networks, are provided to illustrate the applications of the theory. The article also discusses the relationship between multiplex networks and decorated graphs, showing how convergence results can be obtained from the limit theory of decorated graphs. The authors propose several future directions for further developing the multiplex limit theory. <br /><br />Summary: <div>
arXiv:2510.08639v1 Announce Type: cross 
Abstract: In a multiplex network, a set of nodes is connected by different types of interactions, each represented as a separate layer within the network. Multiplexes have emerged as a key instrument for modeling large-scale complex systems, due to the widespread coexistence of diverse interactions in social, industrial, and biological domains. This motivates the development of a rigorous and readily applicable framework for studying properties of large multiplex networks. In this article, we provide a self-contained introduction to the limit theory of dense multiplex networks, analogous to the theory of graphons (limit theory of dense graphs). As applications, we derive limiting analogues of commonly used multiplex features, such as degree distributions and clustering coefficients. We also present a range of illustrative examples, including correlated versions of Erd\H{o}s-R\'enyi and inhomogeneous random graph models and dynamic networks. Finally, we discuss how multiplex networks fit within the broader framework of decorated graphs, and how the convergence results can be recovered from the limit theory of decorated graphs. Several future directions are outlined for further developing the multiplex limit theory.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics</title>
<link>https://arxiv.org/abs/2510.09082</link>
<guid>https://arxiv.org/abs/2510.09082</guid>
<content:encoded><![CDATA[
<div> dynamic network, complex systems, higher-order relations, prediction, interpretability

Summary: 
The paper introduces a method for predicting the long-term dynamic evolution of complex networks. Traditional graph machine learning methods struggle to capture non-pairwise relationships in complex networks, so the proposed approach incorporates dynamic hypergraph learning to model higher-order relations. To improve prediction accuracy and interpretability, a dual-driven dynamic prediction module is introduced. This module utilizes Koopman operator theory to transform nonlinear dynamical equations into linear systems for solving while ensuring adherence to physical laws through the physical information neural differential equation method. Experimental results demonstrate the method's effectiveness in predicting complex network dynamics accurately and over the long term. <div>
arXiv:2510.09082v1 Announce Type: cross 
Abstract: Learning complex network dynamics is fundamental to understanding, modelling and controlling real-world complex systems. There are two main problems in the task of predicting the dynamic evolution of complex networks: on the one hand, existing methods usually use simple graphs to describe the relationships in complex networks; however, this approach can only capture pairwise relationships, while there may be rich non-pairwise structured relationships in the network. First-order GNNs have difficulty in capturing dynamic non-pairwise relationships. On the other hand, theoretical prediction models lack accuracy and data-driven prediction models lack interpretability. To address the above problems, this paper proposes a higher-order network dynamics identification method for long-term dynamic prediction of complex networks. Firstly, to address the problem that traditional graph machine learning can only deal with pairwise relations, dynamic hypergraph learning is introduced to capture the higher-order non-pairwise relations among complex networks and improve the accuracy of complex network modelling. Then, a dual-driven dynamic prediction module for physical data is proposed. The Koopman operator theory is introduced to transform the nonlinear dynamical differential equations for the dynamic evolution of complex networks into linear systems for solving. Meanwhile, the physical information neural differential equation method is utilised to ensure that the dynamic evolution conforms to the physical laws. The dual-drive dynamic prediction module ensures both accuracy and interpretability of the prediction. Validated on public datasets and self-built industrial chain network datasets, the experimental results show that the method in this paper has good prediction accuracy and long-term prediction performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Temporal Graph Learning Models Learn?</title>
<link>https://arxiv.org/abs/2510.09416</link>
<guid>https://arxiv.org/abs/2510.09416</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal graphs, graph representation learning, evaluation protocols, structural characteristics, edge formation mechanisms

Summary:
Temporal graph learning models have shown strong performance in capturing certain attributes of temporal graphs, but there are concerns about the reliability of benchmark results and the competitiveness of simple heuristics. In this study, seven models were evaluated on their ability to capture eight fundamental attributes related to the link structure of temporal graphs. The models performed well in capturing some attributes such as density and temporal patterns like recency, but they struggled to reproduce other attributes related to edge formation mechanisms such as homophily. These findings highlight important limitations in current temporal graph learning models and suggest the need for more interpretability-driven evaluations in this research area. The results of this study provide practical insights for the application of temporal graph learning models and emphasize the importance of understanding which underlying graph properties contribute to the models' predictions.<br /><br />Summary: <div>
arXiv:2510.09416v1 Announce Type: cross 
Abstract: Learning on temporal graphs has become a central topic in graph representation learning, with numerous benchmarks indicating the strong performance of state-of-the-art models. However, recent work has raised concerns about the reliability of benchmark results, noting issues with commonly used evaluation protocols and the surprising competitiveness of simple heuristics. This contrast raises the question of which properties of the underlying graphs temporal graph learning models actually use to form their predictions. We address this by systematically evaluating seven models on their ability to capture eight fundamental attributes related to the link structure of temporal graphs. These include structural characteristics such as density, temporal patterns such as recency, and edge formation mechanisms such as homophily. Using both synthetic and real-world datasets, we analyze how well models learn these attributes. Our findings reveal a mixed picture: models capture some attributes well but fail to reproduce others. With this, we expose important limitations. Overall, we believe that our results provide practical insights for the application of temporal graph learning models, and motivate more interpretability-driven evaluations in temporal graph learning research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash</title>
<link>https://arxiv.org/abs/2510.09443</link>
<guid>https://arxiv.org/abs/2510.09443</guid>
<content:encoded><![CDATA[
<div> sanctions, Tornado Cash, transaction privacy, blockchain, regulatory interventions

Summary: 
The paper examines the impact of sanctions on Tornado Cash, a protocol aimed at enhancing transaction privacy. After the U.S. Department of the Treasury imposed sanctions on Tornado Cash in August 2022, there was a significant decrease in transaction volume, user diversity, and overall protocol utilization across Ethereum, BNB Smart Chain, and Polygon blockchains. Despite the lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control in March 2025, activity only partially recovered, highlighting the challenges of enforcing regulatory measures in decentralized environments. The case of Tornado Cash demonstrates how regulatory interventions can influence decentralized protocols and raises questions about the broader implications for such platforms in the future. 

<br /><br />Summary: <div>
arXiv:2510.09443v1 Announce Type: cross 
Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart contract protocol designed to enhance transaction privacy. Following the U.S. Department of the Treasury's sanctions against Tornado Cash in August 2022, platform activity declined sharply. We document a significant and sustained reduction in transaction volume, user diversity, and overall protocol utilization after the sanctions were imposed. Our analysis draws on transaction data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We further examine developments following the partial lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025. Although activity partially recovered, the rebound remained limited. The Tornado Cash case illustrates how regulatory interventions can affect decentralized protocols, while also highlighting the challenges of fully enforcing such measures in decentralized environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning Augmented Social Recommendations</title>
<link>https://arxiv.org/abs/2502.15695</link>
<guid>https://arxiv.org/abs/2502.15695</guid>
<content:encoded><![CDATA[
<div> social-relation graph, recommender systems, cold users, dual-view denoising, mutual distillation

Summary: 
Recommender systems are crucial for content platforms, but traditional models struggle with cold users. To address this, leveraging the social-relation graph can enrich interest representations. A dual-view denoising strategy using low-rank SVD helps to clean noise in the social graph, and contrastive learning aligns the original and reconstructed graphs. "Mutual distillation" technique isolates and maximizes the utility of social/behavior interests and specific interests, reducing inconsistency. Experimental results show the method's effectiveness, especially for cold users. This work offers a new approach for enhancing recommender systems in the future. The implementation is available on GitHub at https://github.com/WANGLin0126/CLSRec. <div>
arXiv:2502.15695v3 Announce Type: replace-cross 
Abstract: Recommender systems are essential for modern content platforms, yet traditional behavior-based models often struggle with cold users who have limited interaction data. Engaging these users is crucial for platform growth. To bridge this gap, we propose leveraging the social-relation graph to enrich interest representations from behavior-based models. However, extracting value from social graphs is challenging due to relation noise and cross-domain inconsistency. To address the noise propagation and obtain accurate social interest, we employ a dual-view denoising strategy, employing low-rank SVD to the user-item interaction matrix for a denoised social graph and contrastive learning to align the original and reconstructed social graphs. Addressing the interest inconsistency between social and behavioral interests, we adopt a "mutual distillation" technique to isolate the original interests into aligned social/behavior interests and social/behavior specific interests, maximizing the utility of both. Experimental results on widely adopted industry datasets verify the method's effectiveness, particularly for cold users, offering a fresh perspective for future research. The implementation can be accessed at https://github.com/WANGLin0126/CLSRec.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024</title>
<link>https://arxiv.org/abs/2510.07821</link>
<guid>https://arxiv.org/abs/2510.07821</guid>
<content:encoded><![CDATA[
<div> Keywords: data science, methodology, artificial intelligence, election issues, opinion mining

Summary: 
This paper compares two data science methodologies to analyze voter choice in the 2024 presidential election using AI techniques. By examining user comments on YouTube videos from right and left-leaning journals, the study found that immigration and democracy were the most discussed election issues, followed by identity politics, while inflation was less prominent. The results challenge the importance of inflation as an election issue and suggest that opinion mining of online user data may provide more insights than traditional surveys. This research highlights the value of analyzing raw user data to understand voter preferences and election outcomes.<br /><br />Summary: <div>
arXiv:2510.07821v1 Announce Type: new 
Abstract: This paper aims to explore two competing data science methodologies to attempt answering the question, "Which issues contributed most to voters' choice in the 2024 presidential election?" The methodologies involve novel empirical evidence driven by artificial intelligence (AI) techniques. By using two distinct methods based on natural language processing and clustering analysis to mine over eight thousand user comments on election-related YouTube videos from one right leaning journal, Wall Street Journal, and one left leaning journal, New York Times, during pre-election week, we quantify the frequency of selected issue areas among user comments to infer which issues were most salient to potential voters in the seven days preceding the November 5th election. Empirically, we primarily demonstrate that immigration and democracy were the most frequently and consistently invoked issues in user comments on the analyzed YouTube videos, followed by the issue of identity politics, while inflation was significantly less frequently referenced. These results corroborate certain findings of post-election surveys but also refute the supposed importance of inflation as an election issue. This indicates that variations on opinion mining, with their analysis of raw user data online, can be more revealing than polling and surveys for analyzing election outcomes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for Cold-start Next POI Recommendation</title>
<link>https://arxiv.org/abs/2510.08012</link>
<guid>https://arxiv.org/abs/2510.08012</guid>
<content:encoded><![CDATA[
<div> Keywords: POI recommendation, large language models, knowledge graphs, reinforcement learning, contextual bandit optimization

Summary:
Prompt-as-Policy over knowledge graphs is a novel framework for improving point-of-interest (POI) recommendation in smart urban services. It addresses the challenge of cold-start conditions by dynamically constructing prompts through contextual bandit optimization. The framework treats prompt construction as a learnable policy, optimizing which relational evidences to include, the number of evidence per candidate, and their organization within prompts. By utilizing a knowledge graph to discover candidates and mine relational paths, evidence cards are generated to summarize rationales for each candidate POI. The frozen large language model then generates recommendations from the discovered candidate set based on the policy-optimized prompts. Experimental results on real-world datasets show that Prompt-as-Policy outperforms existing baselines, achieving significant improvements in recommendation accuracy for inactive users without requiring model fine-tuning. <div>
arXiv:2510.08012v1 Announce Type: new 
Abstract: Next point-of-interest (POI) recommendation is crucial for smart urban services such as tourism, dining, and transportation, yet most approaches struggle under cold-start conditions where user-POI interactions are sparse. Recent efforts leveraging large language models (LLMs) address this challenge through either supervised fine-tuning (SFT) or in-context learning (ICL). However, SFT demands costly annotations and fails to generalize to inactive users, while static prompts in ICL cannot adapt to diverse user contexts. To overcome these limitations, we propose Prompt-as-Policy over knowledge graphs, a reinforcement-guided prompting framework that learns to construct prompts dynamically through contextual bandit optimization. Our method treats prompt construction as a learnable policy that adaptively determines (i) which relational evidences to include, (ii) the number of evidence per candidate, and (iii) their organization and ordering within prompts. More specifically, we construct a knowledge graph (KG) to discover candidates and mine relational paths, which are transformed into evidence cards that summarize rationales for each candidate POI. The frozen LLM then acts as a reasoning engine, generating recommendations from the KG-discovered candidate set based on the policy-optimized prompts. Experiments on three real-world datasets demonstrate that Prompt-as-Policy consistently outperforms state-of-the-art baselines, achieving average 7.7\% relative improvements in Acc@1 for inactive users, while maintaining competitive performance on active users, without requiring model fine-tuning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric opinion exchange polarizes in every dimension</title>
<link>https://arxiv.org/abs/2510.08190</link>
<guid>https://arxiv.org/abs/2510.08190</guid>
<content:encoded><![CDATA[
<div> Polarization, Opinion exchange, Agent opinions, Update rule, Antipodal groups <br />
<br />Summary: 
This article introduces a model of opinion exchange where agent opinions on multiple topics are tracked concurrently. The opinions are represented as vectors on a unit sphere and are updated based on overall correlation. The model assumes biased assimilation, bringing similar opinions closer and opposing ones apart. For two topics, the model induces polarization, but it was unclear for higher dimensions. This work resolves the question for dimensions greater than or equal to three by analyzing model dynamics and utilizing random process theory. <div>
arXiv:2510.08190v1 Announce Type: new 
Abstract: A recent line of work studies models of opinion exchange where agent opinions about $d$ topics are tracked simultaneously. The opinions are represented as vectors on the unit $(d-1)$-sphere, and the update rule is based on the overall correlation between the relevant vectors. The update rule reflects the assumption of biased assimilation, i.e., a pair of opinions is brought closer together if their correlation is positive and further apart if the correlation is negative.
  This model seems to induce the polarization of opinions into two antipodal groups. This is in contrast to many other known models which tend to achieve consensus. The polarization property has been recently proved for $d=2$, but the general case of $d \ge 3$ remained open. In this work, we settle the general case, using a more detailed understanding of the model dynamics and tools from the theory of random processes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.08481</link>
<guid>https://arxiv.org/abs/2510.08481</guid>
<content:encoded><![CDATA[
<div> Hashtag trends, viral prediction, BuzzProphet, LLMs, social media  
Summary:  
BuzzProphet is a new framework for predicting hashtag popularity that combines the strengths of large language models (LLMs) and traditional regression models. It instructs LLMs to analyze a hashtag's virality, audience reach, and timing. By incorporating these insights into input features, BuzzProphet improves prediction accuracy and produces human-readable rationales for its forecasts. The framework outperforms traditional regressors, reducing RMSE by up to 2.8% and increasing correlation by 30% on a benchmark dataset of 7,532 hashtags. By leveraging LLMs for contextual reasoning rather than direct prediction, BuzzProphet offers a more interpretable and deployable solution for forecasting social media trends. <div>
arXiv:2510.08481v1 Announce Type: new 
Abstract: Hashtag trends ignite campaigns, shift public opinion, and steer millions of dollars in advertising spend, yet forecasting which tag goes viral is elusive. Classical regressors digest surface features but ignore context, while large language models (LLMs) excel at contextual reasoning but misestimate numbers. We present BuzzProphet, a reasoning-augmented hashtag popularity prediction framework that (1) instructs an LLM to articulate a hashtag's topical virality, audience reach, and timing advantage; (2) utilizes these popularity-oriented rationales to enrich the input features; and (3) regresses on these inputs. To facilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated from social media. Across diverse regressor-LLM combinations, BuzzProphet reduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while producing human-readable rationales. Results demonstrate that using LLMs as context reasoners rather than numeric predictors injects domain insight into tabular models, yielding an interpretable and deployable solution for social media trend forecasting.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments</title>
<link>https://arxiv.org/abs/2510.07359</link>
<guid>https://arxiv.org/abs/2510.07359</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, social media, urban environments, perception, opinion<br />
<br />
Summary: 
This study explores the impact of social media on urban sentiment analysis, focusing on the perceptions and opinions of residents in Beijing. By utilizing a dataset of street view images and social media posts, the researchers develop a reaction index to analyze sentiment trends in the Second Ring of Beijing. The findings reveal a shift towards more positive sentiment in perception but more extreme changes in opinion. Disparities between perception and opinion sentiments are identified, with significant relationships to elements such as building density and pedestrian presence. The study also highlights the impact of the pandemic on sentiment reactions and provides insights for environmental management and urban renewal strategies. <div>
arXiv:2510.07359v1 Announce Type: cross 
Abstract: The ascension of social media platforms has transformed our understanding of urban environments, giving rise to nuanced variations in sentiment reaction embedded within human perception and opinion, and challenging existing multidimensional sentiment analysis approaches in urban studies. This study presents novel methodologies for identifying and elucidating sentiment inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent Street view images to measure perceptions, and 984,024 Weibo social media text posts to measure opinions. A reaction index is developed, integrating object detection and natural language processing techniques to classify sentiment in Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is analysed and visualized using regression analysis, image segmentation, and word frequency based on land-use distribution to discern underlying factors. The perception affective reaction trend map reveals a shift toward more evenly distributed positive sentiment, while the opinion affective reaction trend map shows more extreme changes. Our mismatch map indicates significant disparities between the sentiments of human perception and opinion of urban areas over the years. Changes in sentiment reactions have significant relationships with elements such as dense buildings and pedestrian presence. Our inconsistent maps present perception and opinion sentiments before and after the pandemic and offer potential explanations and directions for environmental management, in formulating strategies for urban renewal.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks</title>
<link>https://arxiv.org/abs/2510.06245</link>
<guid>https://arxiv.org/abs/2510.06245</guid>
<content:encoded><![CDATA[
<div> Graph models, network dynamics, community detection algorithms, evolving community structures, temporal networks<br />
<br />
Summary: 
The article introduces a new community-centered model for generating evolving community structures in graphs. This model allows communities to grow, shrink, merge, split, appear, or disappear over time. It also creates a temporal network where nodes can appear, disappear, or move between communities. The benchmark includes tools to track community evolution and compare algorithm results with ground truth data. Three methods were tested using this benchmark to evaluate their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to facilitate comparison between the ground truth and algorithm results. Overall, the benchmark offers a comprehensive tool for evaluating community detection algorithms in dynamic network environments. <br /><br /> <div>
arXiv:2510.06245v1 Announce Type: new 
Abstract: Graph models help understand network dynamics and evolution. Creating graphs with controlled topology and embedded partitions is a common strategy for evaluating community detection algorithms. However, existing benchmarks often overlook the need to track the evolution of communities in real-world networks. To address this, a new community-centered model is proposed to generate customizable evolving community structures where communities can grow, shrink, merge, split, appear or disappear. This benchmark also generates the underlying temporal network, where nodes can appear, disappear, or move between communities. The benchmark has been used to test three methods, measuring their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to compare ground truth with algorithm results for detecting dynamic communities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Discourses on Childbirth and Parenthood in Popular Social Media Platforms Across China, Japan, and South Korea</title>
<link>https://arxiv.org/abs/2510.06788</link>
<guid>https://arxiv.org/abs/2510.06788</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, fertility desires, online discourse, family formation, low fertility rates 

Summary: 
The study examines online discussions surrounding childbirth and parenthood on popular video sharing platforms in China, South Korea, and Japan, regions known for their low fertility rates. By analyzing over 200,000 comments on 668 short videos, the research identifies key themes and sentiments expressed by users. The comments primarily focus on childrearing costs, the utility of children, and individualism, with differing sentiments across countries. Douyin comments display strong anti-natalist views, while Japanese and Korean comments are more neutral. The study also explores the impact of video characteristics, such as stances and account types, as well as regional socioeconomic indicators like GDP and urbanization, on the discourse. Overall, the research sheds light on the spread of family values online and provides valuable insights into the factors influencing fertility desires in these regions. 

<br /><br />Summary: <div>
arXiv:2510.06788v1 Announce Type: new 
Abstract: Social media use has been shown to be associated with low fertility desires. However, we know little about the discourses surrounding childbirth and parenthood that people consume online. We analyze 219,127 comments on 668 short videos related to reproduction and parenthood from Douyin and Tiktok in China, South Korea, and Japan, a region famous for its extremely low fertility level, to examine the topics and sentiment expressed online. BERTopic model is used to assist thematic analysis, and a large language model QWen is applied to label sentiment. We find that comments focus on childrearing costs in all countries, utility of children, particularly in Japan and South Korea, and individualism, primarily in China. Comments from Douyin exhibit the strongest anti-natalist sentiments, while the Japanese and Korean comments are more neutral. Short video characteristics, such as their stances or account type, significantly influence the responses, alongside regional socioeconomic indicators, including GDP, urbanization, and population sex ratio. This work provides one of the first comprehensive analyses of online discourses on family formation via popular algorithm-fed video sharing platforms in regions experiencing low fertility rates, making a valuable contribution to our understanding of the spread of family values online.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualization of Interpersonal Communication using Indoor Positioning Technology with UWB Tags</title>
<link>https://arxiv.org/abs/2510.06797</link>
<guid>https://arxiv.org/abs/2510.06797</guid>
<content:encoded><![CDATA[
<div> Keywords: social gathering, UWB indoor positioning system, interpersonal communication, network analysis, community evolution

Summary: 
The study utilized a UWB indoor positioning system to track attendee movements and interactions during a social gathering on a university campus. Network and community analyses were conducted to study attendee interactions and the evolution of communities over time. By varying distance thresholds for defining contact, the study examined how it affected network structure and community analysis outcomes. The temporal evolution of communities identified through community analysis corresponded with visually observed groupings of participants. The study provides insights into understanding interpersonal communication patterns and community dynamics at social events through the use of technology, highlighting the potential of UWB indoor positioning systems for studying social interactions. The findings contribute to the growing body of research on social network analysis and community dynamics in real-world settings.<br /><br />Summary: <div>
arXiv:2510.06797v1 Announce Type: new 
Abstract: In conjunction with a social gathering held on a university campus, the movement of attendees were tracked within the venue for approximately two hours using a UWB indoor positioning system, in order to visualize their interpersonal communication. Network and community analyses were performed on attendee interaction data, and the evolution of communities over time was further investigated through repeated community analysis at different time points. Furthermore, recognizing the influence of distance thresholds on defining contact, we discussed how varying these thresholds affected the resulting network structure and community analysis outcomes. This study confirmed that the temporal evolution of communities identified through community analysis broadly corresponded with the visually observed groupings of participants using the UWB indoor positioning system.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit</title>
<link>https://arxiv.org/abs/2510.07226</link>
<guid>https://arxiv.org/abs/2510.07226</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, Machine-Generated Text, Reddit, Social Media, Engagement

Summary: 
Generative Artificial Intelligence is increasingly utilized for generating Machine-Generated Text (MGT) in online communication, particularly on platforms like Reddit. This study examines the prevalence and distribution of MGT across various subreddits from 2022-2024. The research reveals that although MGT is marginally present on Reddit, it can peak at up to 9% in certain communities. MGT is more common in technical and social support subreddits and is often created by a small group of users. Despite differences in style, MGT generates similar levels of engagement compared to human-authored content. It conveys unique social signals associated with AI assistants, such as warmth and status giving. This suggests that MGT is becoming an integral part of online social interactions. This analysis provides valuable insights into the impact of MGT on Reddit and opens up new avenues for research on platform governance and community dynamics. 

<br /><br />Summary: <div>
arXiv:2510.07226v1 Announce Type: new 
Abstract: Generative Artificial Intelligence is reshaping online communication by enabling large-scale production of Machine-Generated Text (MGT) at low cost. While its presence is rapidly growing across the Web, little is known about how MGT integrates into social media environments. In this paper, we present the first large-scale characterization of MGT on Reddit. Using a state-of-the-art statistical method for detection of MGT, we analyze over two years of activity (2022-2024) across 51 subreddits representative of Reddit's main community types such as information seeking, social support, and discussion. We study the concentration of MGT across communities and over time, and compared MGT to human-authored text in terms of social signals it expresses and engagement it receives. Our very conservative estimate of MGT prevalence indicates that synthetic text is marginally present on Reddit, but it can reach peaks of up to 9% in some communities in some months. MGT is unevenly distributed across communities, more prevalent in subreddits focused on technical knowledge and social support, and often concentrated in the activity of a small fraction of users. MGT also conveys distinct social signals of warmth and status giving typical of language of AI assistants. Despite these stylistic differences, MGT achieves engagement levels comparable than human-authored content and in a few cases even higher, suggesting that AI-generated text is becoming an organic component of online social discourse. This work offers the first perspective on the MGT footprint on Reddit, paving the way for new investigations involving platform governance, detection strategies, and community dynamics.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Model for Human Mobility Behavior</title>
<link>https://arxiv.org/abs/2510.06473</link>
<guid>https://arxiv.org/abs/2510.06473</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, deep generative model, spatial scales, travel mode, societal implications

Summary:
MobilityGen is a deep generative model designed to simulate human mobility patterns over large spatial scales. By incorporating behavioral attributes and environmental context, MobilityGen can accurately replicate key patterns such as location visits, activity time allocation, and the interplay between travel mode and destination choices. This model captures spatio-temporal variability and produces diverse and realistic mobility trajectories consistent with real-world urban environments. Additionally, MobilityGen can provide insights into how urban space accessibility varies across different travel modes and how social exposure and segregation are influenced by co-presence dynamics. This framework opens up new avenues for studying human behavior and its societal impacts through fine-grained, data-driven simulations. <div>
arXiv:2510.06473v1 Announce Type: cross 
Abstract: Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we present MobilityGen, a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales. By linking behavioral attributes with environmental context, MobilityGen reproduces key patterns such as scaling laws for location visits, activity time allocation, and the coupled evolution of travel mode and destination choices. It reflects spatio-temporal variability and generates diverse, plausible, and novel mobility patterns consistent with the built environment. Beyond standard validation, MobilityGen yields insights not attainable with earlier models, including how access to urban space varies across travel modes and how co-presence dynamics shape social exposure and segregation. Our work establishes a new framework for mobility simulation, paving the way for fine-grained, data-driven studies of human behavior and its societal implications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Graph Clustering under Differential Privacy: Balancing Privacy, Accuracy, and Efficiency</title>
<link>https://arxiv.org/abs/2510.07136</link>
<guid>https://arxiv.org/abs/2510.07136</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral graph clustering, edge differential privacy, graph perturbation, private graph projection, noisy power iteration

Summary:
Graph clustering under edge differential privacy is studied in this work. Three mechanisms are developed for privacy-preserving graph analysis: randomized edge flipping combined with adjacency matrix shuffling, private graph projection with Gaussian noise, and a noisy power iteration method with distributed noise. The mechanisms ensure edge differential privacy while preserving key spectral properties of the graph. The analysis provides rigorous privacy guarantees and quantifies the misclassification error rate. Experimental validation on synthetic and real-world networks confirms the effectiveness of the proposed methods and demonstrates the privacy-utility trade-offs. The shuffled mechanism improves privacy guarantees as the number of nodes increases, providing (epsilon, delta) edge differential privacy. Private graph projection reduces dimensionality and computational complexity, while the noisy power iteration method maintains convergence with distributed Gaussian noise. <div>
arXiv:2510.07136v1 Announce Type: cross 
Abstract: We study the problem of spectral graph clustering under edge differential privacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation via randomized edge flipping combined with adjacency matrix shuffling, which enforces edge privacy while preserving key spectral properties of the graph. Importantly, shuffling considerably amplifies the guarantees: whereas flipping edges with a fixed probability alone provides only a constant epsilon edge DP guarantee as the number of nodes grows, the shuffled mechanism achieves (epsilon, delta) edge DP with parameters that tend to zero as the number of nodes increase; (ii) private graph projection with additive Gaussian noise in a lower-dimensional space to reduce dimensionality and computational complexity; and (iii) a noisy power iteration method that distributes Gaussian noise across iterations to ensure edge DP while maintaining convergence. Our analysis provides rigorous privacy guarantees and a precise characterization of the misclassification error rate. Experiments on synthetic and real-world networks validate our theoretical analysis and illustrate the practical privacy-utility trade-offs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeSH Concept Relevance and Knowledge Evolution: A Data-driven Perspective</title>
<link>https://arxiv.org/abs/2406.18792</link>
<guid>https://arxiv.org/abs/2406.18792</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Subject Headings, information theory, network analysis, concept relevance, knowledge organization systems

Summary:
- The study focuses on quantifying the relevance of concepts in the Medical Subject Headings (MeSH) using a data-driven approach based on information theory and network analysis.
- Four aspects of relevance, including informativeness, usefulness, disruptiveness, and influence, are computed over time using article annotations and citation networks.
- The proposed method effectively captures the evolution of MeSH concepts, showing that evolving concepts have higher relevance compared to unchanged concepts.
- Analysis of retracted articles demonstrates that concepts used to annotate retracted articles differ significantly in relevance from those annotating non-retracted articles.
- The framework provides a method for ranking concept relevance and can support the maintenance of knowledge organization systems. 

<br /><br />Summary: 
The study presents a data-driven approach to quantify the relevance of Medical Subject Headings (MeSH) concepts using information theory and network analysis. By computing four aspects of relevance over time, the method effectively captures the evolution of MeSH concepts, showing higher relevance for evolving concepts. Analysis of retracted articles reveals significant differences in concept relevance between retracted and non-retracted articles. Overall, the framework offers a method for ranking concept relevance and supporting the maintenance of knowledge organization systems. <div>
arXiv:2406.18792v5 Announce Type: replace 
Abstract: The Medical Subject Headings (MeSH), one of the main knowledge organization systems in the biomedical domain, continuously evolves to reflect the latest scientific discoveries in health and life sciences. Previous research has focused on quantifying information in MeSH primarily through its hierarchical structure. In this work, we propose a data-driven approach based on information theory and network analysis to quantify the relevance of MeSH concepts. Our method leverages article annotations and their citation networks to compute four aspects of relevance -- informativeness, usefulness, disruptiveness, and influence -- over time. Using both the citation network and the MeSH hierarchy, we compute these relevance aspects and apply an aggregation algorithm to propagate scores to parent nodes. We evaluated our approach on MeSH terminology changes and showed that it effectively captures the evolution of concepts. The mean relevance of evolving concepts is higher compared to concepts that remained unchanged ($2.09E-03$ vs. $8.46E-04$). Moreover, we validated the framework by analyzing retracted articles and found that concepts used to annotate retracted articles (mean relevance: 0.17) differ substantially from those annotating non-retracted ones (mean relevance: 0.15). Overall, the proposed framework provides an effective method for ranking concept relevance and can support the maintenance of evolving knowledge organization systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Directedness in Social Contagion</title>
<link>https://arxiv.org/abs/2510.06012</link>
<guid>https://arxiv.org/abs/2510.06012</guid>
<content:encoded><![CDATA[
<div> Keywords: contagion theory, social networks, complex contagions, directed pathways, network evolution

Summary: 
An innovative causal modeling framework has been developed to address the challenge in predicting the pathways contagions follow through social networks. The study reveals a surprising discovery that complex contagions, requiring exposure to multiple peers for adoption, lead to asymmetric paths in contagion spread. Weak ties across network regions facilitate the spread of contagions from one community to another, challenging traditional theories. The emergence of directedness also channels complex contagions from the network periphery to the core, contrary to standard centrality models. Practical applications illustrate how emergent directedness explains nonlinear effects of tie strength in job diffusion on LinkedIn and how network evolution favors growing directed paths. Cultural factors like triadic closure can counteract this bias, with strategic implications for network building and behavioral interventions.<br /><br />Summary: <div>
arXiv:2510.06012v1 Announce Type: new 
Abstract: An enduring challenge in contagion theory is that the pathways contagions follow through social networks exhibit emergent complexities that are difficult to predict using network structure. Here, we address this challenge by developing a causal modeling framework that (i) simulates the possible network pathways that emerge as contagions spread and (ii) identifies which edges and nodes are most impactful on diffusion across these possible pathways. This yields a surprising discovery. If people require exposure to multiple peers to adopt a contagion (a.k.a., 'complex contagions'), the pathways that emerge often only work in one direction. In fact, the more complex a contagion is, the more asymmetric its paths become. This emergent directedness problematizes canonical theories of how networks mediate contagion. Weak ties spanning network regions - widely thought to facilitate mutual influence and integration - prove to privilege the spread contagions from one community to the other. Emergent directedness also disproportionately channels complex contagions from the network periphery to the core, inverting standard centrality models. We demonstrate two practical applications. We show that emergent directedness accounts for unexplained nonlinearity in the effects of tie strength in a recent study of job diffusion over LinkedIn. Lastly, we show that network evolution is biased toward growing directed paths, but that cultural factors (e.g., triadic closure) can curtail this bias, with strategic implications for network building and behavioral interventions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
<div> AgentZero++, agent-based model, decentralized collective violence, cognitive mechanisms, emotional mechanisms<br />
<br />
Summary:<br />
AgentZero++ is an enhanced agent-based model that simulates decentralized collective violence in spatially distributed systems. It integrates cognitive, emotional, and social mechanisms to capture various aspects of human behavior in conflict situations. The model includes enhancements such as age-based impulse control, memory-based risk estimation, and affect-cognition coupling, allowing agents to adapt based on internal states and social feedback. By incorporating features like endogenous destructive radius and retaliatory damage, the model generates emergent dynamics such as protest asymmetries and escalation cycles. Through modular experimentation and visualization, AgentZero++ demonstrates how micro-level cognitive heterogeneity influences macro-level conflict patterns. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, the model provides insights into affective contagion and psychologically grounded collective action. <div>
arXiv:2510.05185v1 Announce Type: cross 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection</title>
<link>https://arxiv.org/abs/2510.05676</link>
<guid>https://arxiv.org/abs/2510.05676</guid>
<content:encoded><![CDATA[
<div> Graph, machine learning, insurance fraud, gradient boosting, explainability

Summary:
The article introduces a novel inductive graph gradient boosting machine (G-GBM) for supervised learning on heterogeneous and dynamic graphs, aiming to improve the detection of insurance fraud. Graph-based methods are popular for modeling complex data and relations, but face challenges with class imbalance and dynamic networks in fraud data. The G-GBM approach competes with graph neural network methods in experiments with simulated random graphs. It demonstrates effective fraud detection capabilities with open-source and real-world datasets. By utilizing gradient boosting forests as the backbone model, G-GBM allows for better explainability of predictions through established methods. Overall, the G-GBM offers a promising solution for enhancing fraud detection in insurance networks. 

<br /><br />Summary: <div>
arXiv:2510.05676v1 Announce Type: cross 
Abstract: Graph-based methods are becoming increasingly popular in machine learning due to their ability to model complex data and relations. Insurance fraud is a prime use case, since false claims are often the result of organised criminals that stage accidents or the same persons filing erroneous claims on multiple policies. One challenge is that graph-based approaches struggle to find meaningful representations of the data because of the high class imbalance present in fraud data. Another is that insurance networks are heterogeneous and dynamic, given the changing relations among people, companies and policies. That is why gradient boosted tree approaches on tabular data still dominate the field. Therefore, we present a novel inductive graph gradient boosting machine (G-GBM) for supervised learning on heterogeneous and dynamic graphs. We show that our estimator competes with popular graph neural network approaches in an experiment using a variety of simulated random graphs. We demonstrate the power of G-GBM for insurance fraud detection using an open-source and a real-world, proprietary dataset. Given that the backbone model is a gradient boosting forest, we apply established explainability methods to gain better insights into the predictions made by G-GBM.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overlapping community detection in weighted networks</title>
<link>https://arxiv.org/abs/2211.00894</link>
<guid>https://arxiv.org/abs/2211.00894</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, weighted networks, generative model, overlapping communities, modularity

Summary:
The article introduces a new generative model, the weighted degree-corrected mixed membership (WDCMM) model, designed for community detection in overlapping weighted networks. This model extends the previous degree-corrected mixed membership (DCMM) model to weighted networks by allowing for edge weights of any real value. The community membership estimation is achieved through a spectral algorithm with a theoretical guarantee of consistency. Additionally, the article introduces an overlapping weighted modularity measure to assess the quality of community detection in both assortative and dis-assortative weighted networks. The proposed modularity also assists in determining the optimal number of communities in the network. The model and modularity approach are demonstrated through applications on simulated data and real-world networks, showcasing their effectiveness in detecting overlapping communities in weighted networks. <br /><br />Summary: <div>
arXiv:2211.00894v3 Announce Type: replace 
Abstract: Over the past decade, community detection in overlapping un-weighted networks, where nodes can belong to multiple communities, has been one of the most popular topics in modern network science. However, community detection in overlapping weighted networks, where edge weights can be any real value, remains challenging. In this article, we propose a generative model called the weighted degree-corrected mixed membership (WDCMM) model to model such weighted networks. This model adopts the same factorization for the expectation of the adjacency matrix as the previous degree-corrected mixed membership (DCMM) model. Our WDCMM extends the DCMM from un-weighted networks to weighted networks by allowing the elements of the adjacency matrix to be generated from distributions beyond Bernoulli. We first address the community membership estimation of the model by applying a spectral algorithm and establishing a theoretical guarantee of consistency. Then, we propose overlapping weighted modularity to measure the quality of overlapping community detection for both assortative and dis-assortative weighted networks. To determine the number of communities, we incorporate the algorithm into the proposed modularity. We demonstrate the advantages of the model and the modularity through applications to simulated data and real-world networks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Online Community Detection for Censored Block Models: Algorithms and Fundamental Limits</title>
<link>https://arxiv.org/abs/2405.05724</link>
<guid>https://arxiv.org/abs/2405.05724</guid>
<content:encoded><![CDATA[
<div> edge differential privacy, online change detection, dynamic communities, censored block model, community estimation <br />
<br />
Summary: 
This study focuses on the private online change detection problem in dynamic communities using a censored block model. The research examines edge differential privacy in both local and central settings and proposes methods for joint change detection and community estimation. It delves into the tradeoffs between privacy budget, detection delay, and exact community recovery of community labels. The study provides theoretical guarantees for the effectiveness of the proposed method by establishing necessary and sufficient conditions for change detection and exact recovery under edge differential privacy. Simulation and real data examples are used to validate the proposed techniques. <div>
arXiv:2405.05724v2 Announce Type: replace 
Abstract: We study the private online change detection problem for dynamic communities, using a censored block model (CBM). We consider edge differential privacy (DP) in both local and central settings, and propose joint change detection and community estimation procedures for both scenarios. We seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions for change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</title>
<link>https://arxiv.org/abs/2502.04308</link>
<guid>https://arxiv.org/abs/2502.04308</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Higher-order Guided Diffusion, graph generation, topological structures, molecular graphs <br />
<br />Summary:
Graph generation is a challenging task requiring a deep understanding of complex structures. Existing diffusion models have limitations in capturing the topological properties of graphs due to their adaptation from image generation frameworks. In response, the proposed Higher-order Guided Diffusion (HOG-Diff) framework aims to generate plausible graphs with inherent topological structures by following a coarse-to-fine generation curriculum guided by higher-order topology. The model utilizes diffusion bridges and offers a stronger theoretical guarantee compared to classical diffusion frameworks. Experimental results on molecular and generic graph generation tasks show that HOG-Diff consistently outperforms or competes with state-of-the-art baselines. The code for the model is available for further exploration and application. <div>
arXiv:2502.04308v2 Announce Type: replace-cross 
Abstract: Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Diffusion models have recently made significant achievements in graph generation, but these models are typically adapted from image generation frameworks and overlook inherent higher-order topology, leaving them ill-suited for capturing the topological properties of graphs. In this work, we propose Higher-order Guided Diffusion (HOG-Diff), a principled framework that progressively generates plausible graphs with inherent topological structures. HOG-Diff follows a coarse-to-fine generation curriculum guided by higher-order topology and implemented via diffusion bridges. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at https://github.com/Yiminghh/HOG-Diff.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
<link>https://arxiv.org/abs/2503.01722</link>
<guid>https://arxiv.org/abs/2503.01722</guid>
<content:encoded><![CDATA[
<div> interference, peer effect, exposure mapping function, heterogeneous peer effects, EgoNetGNN <br />
<br />
Summary: 
Interference and peer effects in causal inference refer to how peers' actions can impact an individual's outcome, with exposure mapping functions typically used to represent peer exposure. Existing approaches often rely on simple functions like the number or fraction of treated peers, but complex influence mechanisms require more sophisticated methods. EgoNetGNN, a graph neural network approach, automates the learning of exposure mapping functions to capture diverse peer influence mechanisms. By considering peer treatments, local neighborhood structure, and edge attributes, EgoNetGNN excels in estimating heterogeneous peer effects, where outcomes vary for the same peer exposure across different contexts. Comparative evaluations on synthetic and semi-synthetic network data demonstrate the superiority of EgoNetGNN over traditional methods in accurately estimating peer effects in complex scenarios. <br /> <div>
arXiv:2503.01722v2 Announce Type: replace-cross 
Abstract: In causal inference, interference refers to the phenomenon in which the actions of peers in a network can influence an individual's outcome. Peer effect refers to the difference in counterfactual outcomes of an individual for different levels of peer exposure, the extent to which an individual is exposed to the treatments, actions, or behaviors of peers. Estimating peer effects requires deciding how to represent peer exposure. Typically, researchers define an exposure mapping function that aggregates peer treatments and outputs peer exposure. Most existing approaches for defining exposure mapping functions assume peer exposure based on the number or fraction of treated peers. Recent studies have investigated more complex functions of peer exposure which capture that different peers can exert different degrees of influence. However, none of these works have explicitly considered the problem of automatically learning the exposure mapping function. In this work, we focus on learning this function for the purpose of estimating heterogeneous peer effects, where heterogeneity refers to the variation in counterfactual outcomes for the same peer exposure but different individual's contexts. We develop EgoNetGNN, a graph neural network (GNN)-based method, to automatically learn the appropriate exposure mapping function allowing for complex peer influence mechanisms that, in addition to peer treatments, can involve the local neighborhood structure and edge attributes. We show that GNN models that use peer exposure based on the number or fraction of treated peers or learn peer exposure naively face difficulty accounting for such influence mechanisms. Our comprehensive evaluation on synthetic and semi-synthetic network data shows that our method is more robust to different unknown underlying influence mechanisms when estimating heterogeneous peer effects when compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization and the Rise of System-level Creativity in Science</title>
<link>https://arxiv.org/abs/2510.03240</link>
<guid>https://arxiv.org/abs/2510.03240</guid>
<content:encoded><![CDATA[
<div> Foundational work, extensional work, generalizations, innovation ecosystems, science policy  
Summary:  
- Innovation ecosystems require careful policy stewardship to drive sustained advance in human health, welfare, security, and prosperity.  
- New measures have been developed to decompose the influence of innovations based on foundational work, extensional work, and generalizations.  
- Foundational and extensional work within fields has declined, while generalizations across fields have increased and accelerated with the rise of the web, social media, and artificial intelligence.  
- The shift in the locus of innovation from within fields to across the system as a whole has significant implications for science policy.  
- The study uses 23 million scientific works to demonstrate these trends, highlighting the importance of understanding the dynamics of innovation in modern research and development.  
<br /><br />Summary: <div>
arXiv:2510.03240v1 Announce Type: new 
Abstract: Innovation ecosystems require careful policy stewardship to drive sustained advance in human health, welfare, security and prosperity. We develop new measures that reliably decompose the influence of innovations in terms of the degree to which each represents a field-level foundation, an extension of foundational work, or a generalization that synthesizes and modularizes contributions from distant fields to catalyze combinatorial innovation. Using 23 million scientific works, we demonstrate that while foundational and extensional work within fields has declined in recent years-a trend garnering much recent attention-generalizations across fields have increased and accelerated with the rise of the web, social media, and artificial intelligence, shifting the locus of innovation from within fields to across the system as a whole. We explore implications for science policy.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity</title>
<link>https://arxiv.org/abs/2510.03899</link>
<guid>https://arxiv.org/abs/2510.03899</guid>
<content:encoded><![CDATA[
<div> Fair Minimum Labeling, resource efficiency, fairness, networked systems, learning applications <br />
Summary:
The article introduces the Fair Minimum Labeling (FML) problem, which aims to optimize a minimum-cost temporal edge activation plan in networked systems to ensure equitable access to a target set for different groups of nodes. The problem is essential for systems where edge activations incur resource costs, such as distributed data collection and fair service restoration. The FML problem is shown to be NP-hard and challenging to approximate. Probabilistic approximation algorithms are presented to efficiently solve the FML problem, achieving the best possible guarantee for activation cost. The practical utility of FML is demonstrated in fair multi-source data aggregation tasks for training shared models, showing significant cost savings compared to baseline heuristics and promoting group-level fairness in learning-integrated networks. <br /><br /> <div>
arXiv:2510.03899v1 Announce Type: new 
Abstract: Balancing resource efficiency and fairness is critical in networked systems that support modern learning applications. We introduce the Fair Minimum Labeling (FML) problem: the task of designing a minimum-cost temporal edge activation plan that ensures each group of nodes in a network has sufficient access to a designated target set, according to specified coverage requirements. FML captures key trade-offs in systems where edge activations incur resource costs and equitable access is essential, such as distributed data collection, update dissemination in edge-cloud systems, and fair service restoration in critical infrastructure. We show that FML is NP-hard and $\Omega(\log |V|)$-hard to approximate, and we present probabilistic approximation algorithms that match this bound, achieving the best possible guarantee for the activation cost. We demonstrate the practical utility of FML in a fair multi-source data aggregation task for training a shared model. Empirical results show that FML enforces group-level fairness with substantially lower activation cost than baseline heuristics, underscoring its potential for building resource-efficient, equitable temporal reachability in learning-integrated networks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning framework for predicting stochastic take-off and die-out of early spreading</title>
<link>https://arxiv.org/abs/2510.04574</link>
<guid>https://arxiv.org/abs/2510.04574</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, deep learning, stochastic spreading, early intervention, public health<br />
Summary:<br />
- The study introduces a systematic framework for predicting whether initial transmission events will escalate into major outbreaks or fade away during early stages.
- It addresses challenges like inadequate data during the early stages of outbreaks and the focus of established models on average behaviors of large epidemics.
- A deep learning framework was developed using extensive data from stochastic spreading models to predict early-stage spreading outcomes accurately.
- The proposed pretrain-finetune framework leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning, outperforming baseline models consistently.
- This work is the first to present a framework for predicting stochastic take-off versus die-out, providing valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.<br /> 
Summary: <div>
arXiv:2510.04574v1 Announce Type: new 
Abstract: Large-scale outbreaks of epidemics, misinformation, or other harmful contagions pose significant threats to human society, yet the fundamental question of whether an emerging outbreak will escalate into a major epidemic or naturally die out remains largely unaddressed. This problem is challenging, partially due to inadequate data during the early stages of outbreaks and also because established models focus on average behaviors of large epidemics rather than the stochastic nature of small transmission chains. Here, we introduce the first systematic framework for forecasting whether initial transmission events will amplify into major outbreaks or fade into extinction during early stages, when intervention strategies can still be effectively implemented. Using extensive data from stochastic spreading models, we developed a deep learning framework that predicts early-stage spreading outcomes in real-time. Validation across Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks with varying infectivity levels shows our method accurately forecasts stochastic spreading events well before potential outbreaks, demonstrating robust performance across different network structures and infectivity scenarios.To address the challenge of sparse data during early outbreak stages, we further propose a pretrain-finetune framework that leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning. The pretrain-finetune framework consistently outperforms baseline models, achieving superior performance even when trained on limited scenario-specific data. To our knowledge, this work presents the first framework for predicting stochastic take-off versus die-out. This framework provides valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Network Structure Inference: A Topological Approach to Network Selection</title>
<link>https://arxiv.org/abs/2510.04884</link>
<guid>https://arxiv.org/abs/2510.04884</guid>
<content:encoded><![CDATA[
<div> thresholding, network data, topological data analysis, persistent homology, parameterization

Summary:<br />
Thresholding is crucial in network data analysis but existing methods have limitations. A new systematic algorithm leveraging topological data analysis is introduced to optimize network parameters by considering higher-order structural relationships. It uses persistent homology to assess the stability of homological features across the parameter space, enabling the identification of robust parameter choices preserving meaningful topological structure. Hyperparameters allow users to specify minimum requirements for topological features, guiding the parameter search to avoid spurious solutions. The method is demonstrated in the Science of Science, extracting networks of scientific concepts from research paper abstracts. This approach offers flexibility for incorporating domain-specific constraints and extends to general parameterization problems in data analysis. <div>
arXiv:2510.04884v1 Announce Type: new 
Abstract: Thresholding--the pruning of nodes or edges based on their properties or weights--is an essential preprocessing tool for extracting interpretable structure from complex network data, yet existing methods face several key limitations. Threshold selection often relies on heuristic methods or trial and error due to large parameter spaces and unclear optimization criteria, leading to sensitivity where small parameter variations produce significant changes in network structure. Moreover, most approaches focus on pairwise relationships between nodes, overlooking critical higher-order interactions involving three or more nodes. We introduce a systematic thresholding algorithm that leverages topological data analysis to identify optimal network parameters by accounting for higher-order structural relationships. Our method uses persistent homology to compute the stability of homological features across the parameter space, identifying parameter choices that are robust to small variations while preserving meaningful topological structure. Hyperparameters allow users to specify minimum requirements for topological features, effectively constraining the parameter search to avoid spurious solutions. We demonstrate the approach with an application in the Science of Science, where networks of scientific concepts are extracted from research paper abstracts, and concepts are connected when they co-appear in the same abstract. The flexibility of our approach allows researchers to incorporate domain-specific constraints and extends beyond network thresholding to general parameterization problems in data analysis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Does the Engineering Manager Still Exist in Agile Software Development?</title>
<link>https://arxiv.org/abs/2510.03920</link>
<guid>https://arxiv.org/abs/2510.03920</guid>
<content:encoded><![CDATA[
<div> Keywords: Agile methodologies, engineering managers, decentralized decision-making, team autonomy, managerial hierarchy 

Summary: This paper discusses the presence of engineering managers in Agile software organizations despite the emphasis on decentralized decision-making and team autonomy. Through a multidimensional framework, including historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications, the study explores the reasons for this paradox. A systematic literature review and case studies support the analysis, leading to the proposal of a conceptual model that reconciles Agile principles with managerial necessity. The model aims to provide guidance for practitioners, researchers, and tool designers, while also discussing implications for leadership development, tool integration, and future research. Ultimately, the study sheds light on the evolving role of engineering managers within Agile environments and advocates for a balance between Agile principles and traditional managerial functions. 

<br /><br />Summary: This paper delves into the persistence of engineering managers in Agile software organizations despite the emphasis on decentralized decision-making and team autonomy. Through a comprehensive framework, the study explores the historical, theoretical, and practical aspects influencing this phenomenon. By proposing a conceptual model reconciling Agile principles with managerial necessity, the paper provides guidance for practitioners, researchers, and tool designers. Additionally, implications for leadership development, tool integration, and future research are thoroughly discussed, highlighting the evolving role of engineering managers within Agile environments. <div>
arXiv:2510.03920v1 Announce Type: cross 
Abstract: Although Agile methodologies emphasize decentralized decision-making and team autonomy, engineering managers continue to be employed in Agile software organizations. This apparent paradox suggests that traditional managerial functions persist despite the theoretical displacement of managerial hierarchy in Agile. This paper explores the persistence of engineering managers through a multidimensional framework encompassing historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications. A systematic literature review underpins our multifaceted analysis, supplemented by illustrative case studies. We conclude by proposing a conceptual model that reconciles Agile principles with managerial necessity, offering guidance for practitioners, researchers, and tool designers. Implications for leadership development, tool integration, and future research are discussed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
<div> imagination, internal world model, psychological network analysis, human groups, large language models<br />
<br />
Summary: In this study, the computational objective of imagination is explored through the concept of accessing an internal world model (IWM). Imagination vividness ratings were assessed through questionnaires, and imagination networks were constructed for comparison between humans and large language models (LLMs). Human groups displayed correlations between different centrality measures in their imagination networks, indicating similarities in IWMs. However, LLMs showed a lack of clustering and lower correlations between centrality measures, suggesting differences in internally-generated representations. The study highlights a novel method for comparing IWMs in humans and AI, providing insights for the development of human-like imagination in artificial intelligence. <div>
arXiv:2510.04391v1 Announce Type: cross 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?</title>
<link>https://arxiv.org/abs/2510.04434</link>
<guid>https://arxiv.org/abs/2510.04434</guid>
<content:encoded><![CDATA[
<div> authors, venue-level, NLP4SG, social good concerns, ACL community
Summary:<br />
1. The study examines the landscape of NLP4SG from author and venue perspectives, highlighting the growing importance of NLP for social good initiatives.
2. Nearly 20% of all recent papers in the ACL Anthology focus on social good topics defined by the UN Sustainable Development Goals.
3. ACL authors are more likely to address social good concerns when publishing outside of ACL venues, indicating a shift in focus.
4. The majority of publications using NLP for social good purposes are authored by non-ACL authors in venues outside of ACL, suggesting a broader engagement in such initiatives.
5. The findings have implications for the ACL community in setting agendas related to NLP4SG. <br /><br /> <div>
arXiv:2510.04434v1 Announce Type: cross 
Abstract: The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete scalar curvature as a weighted sum of Ollivier-Ricci curvatures</title>
<link>https://arxiv.org/abs/2510.04936</link>
<guid>https://arxiv.org/abs/2510.04936</guid>
<content:encoded><![CDATA[
<div> Ollivier-Ricci curvature, scalar curvature, discrete setting, point clouds, graphs <br />
<br />
Summary: The study explores the connection between discrete versions of Ricci and scalar curvature in point clouds and graphs. Ollivier-Ricci curvature substitutes Ricci curvature in the discrete context, with a new scalar Ollivier-Ricci curvature definition based on the trace of Ricci curvature in Riemannian manifolds. The proposed scalar Ollivier-Ricci curvature calculation converges to scalar curvature for nearest neighbor graphs derived from manifold sampling. The research also establishes novel findings on the convergence of Ollivier-Ricci curvature to Ricci curvature, enhancing our understanding of the relationship between discrete analogues of curvature measures in geometric structures. <div>
arXiv:2510.04936v1 Announce Type: cross 
Abstract: We study the relationship between discrete analogues of Ricci and scalar curvature that are defined for point clouds and graphs. In the discrete setting, Ricci curvature is replaced by Ollivier-Ricci curvature. Scalar curvature can be computed as the trace of Ricci curvature for a Riemannian manifold; this motivates a new definition of a scalar version of Ollivier-Ricci curvature. We show that our definition converges to scalar curvature for nearest neighbor graphs obtained by sampling from a manifold. We also prove some new results about the convergence of Ollivier-Ricci curvature to Ricci curvature.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGRDN-Data learned sparsification of graph reaction-diffusion networks</title>
<link>https://arxiv.org/abs/2303.11943</link>
<guid>https://arxiv.org/abs/2303.11943</guid>
<content:encoded><![CDATA[
<div> Graph Sparsification, Reaction-Diffusion Systems, Data Assimilation, Reduced Order Model, Spectral Preservation<br />
Summary:<br />
Graph sparsification is a challenging problem in computer science and applied mathematics, aiming to reduce the number of edges while preserving certain graph properties. The proposed SGRDN method extends sparsification to complex reaction-diffusion systems, ensuring the preservation of dynamics on the resulting structure. By framing network sparsification as a data assimilation problem in a Reduced Order Model space, SGRDN conserves the eigenmodes of the Laplacian matrix despite perturbations. Efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices are integrated as spectral constraints in the optimization process. The method's versatility is demonstrated through successful parameter sparsity achievement in Neural Ordinary Differential Equations (neural ODEs). <div>
arXiv:2303.11943v4 Announce Type: replace 
Abstract: Graph sparsification is an area of interest in computer science and applied mathematics. Sparsification of a graph, in general, aims to reduce the number of edges in the network while preserving specific properties of the graph, like cuts and subgraph counts. Computing the sparsest cuts of a graph is known to be NP-hard, and sparsification routines exist for generating linear-sized sparsifiers in almost quadratic running time $O(n^{2 + \epsilon})$. Consequently, obtaining a sparsifier can be a computationally demanding task, and the complexity varies based on the level of sparsity required. We propose SGRDN to extend sparsification to complex reaction-diffusion systems. This approach seeks to sparsify the graph such that the inherent reaction-diffusion dynamics are strictly preserved on the resulting structure. By selectively considering a subset of trajectories, we frame the network sparsification issue as a data assimilation problem within a Reduced Order Model (ROM) space, imposing constraints to conserve the eigenmodes of the Laplacian matrix ($L = D - A$), the difference between the degree matrix ($D$) and the adjacency matrix ($A$) despite perturbations. We derive computationally efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices and integrate these as spectral preservation constraints in the optimization problem. To further validate the method's broad applicability, we conducted an additional experiment on Neural Ordinary Differential Equations (neural ODEs), where SGRDN successfully achieved parameter sparsity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics</title>
<link>https://arxiv.org/abs/2504.15927</link>
<guid>https://arxiv.org/abs/2504.15927</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised community detection, crystallization kinetics, CLique ANNealing, Transitive Annealer, scalability<br />
Summary:<br />
This article introduces a novel approach, CLique ANNealing (CLANN), for semi-supervised community detection. By integrating concepts from crystallization kinetics, the method aims to improve the identification of community cores and enhance scalability. The process is likened to the annealing process in crystal formation, where a core expands into a complete grain. CLANN optimizes the consistency of the community core and employs a Transitive Annealer to refine candidates by merging cliques and repositioning the core. Experimental results on various network settings demonstrate that CLANN outperforms existing methods on multiple real-world datasets, showing superior efficacy and efficiency in community detection.<br /><br />Summary: <div>
arXiv:2504.15927v2 Announce Type: replace 
Abstract: Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Higher-Order Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.12841</link>
<guid>https://arxiv.org/abs/2406.12841</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Higher-order, Topological Deep Learning, Taxonomy, Analysis

Summary:
Higher-order graph neural networks (HOGNNs) and Topological Deep Learning models are essential for handling complex relationships between vertices in graphs. The diversity of HOGNN models makes it challenging to compare and select the most suitable one for a given scenario. To address this, a taxonomy and blueprint for HOGNNs are proposed to guide model design for optimal performance. Using this taxonomy, an analysis and comparison of existing HOGNN models are conducted to provide insights for model selection. The study highlights the need for further research to enhance the power of HOGNNs, addressing challenges and opportunities in the field. This comprehensive analysis aids in navigating the landscape of HOGNN models and facilitates informed decision-making for employing GNN models in various applications. 

<br /><br />Summary: <div>
arXiv:2406.12841v3 Announce Type: replace-cross 
Abstract: Higher-order graph neural networks (HOGNNs) and the related architectures from Topological Deep Learning are an important class of GNN models that harness polyadic relations between vertices beyond plain edges. They have been used to eliminate issues such as over-smoothing or over-squashing, to significantly enhance the accuracy of GNN predictions, to improve the expressiveness of GNN architectures, and for numerous other goals. A plethora of HOGNN models have been introduced, and they come with diverse neural architectures, and even with different notions of what the "higher-order" means. This richness makes it very challenging to appropriately analyze and compare HOGNN models, and to decide in what scenario to use specific ones. To alleviate this, we first design an in-depth taxonomy and a blueprint for HOGNNs. This facilitates designing models that maximize performance. Then, we use our taxonomy to analyze and compare the available HOGNN models. The outcomes of our analysis are synthesized in a set of insights that help to select the most beneficial GNN model in a given scenario, and a comprehensive list of challenges and opportunities for further research into more powerful HOGNNs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HP-BERT: A framework for longitudinal study of Hinduphobia on social media via language models</title>
<link>https://arxiv.org/abs/2501.05482</link>
<guid>https://arxiv.org/abs/2501.05482</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, Hinduphobia, Social media analysis, Sentiment analysis, Discrimination

Summary: 
This study focuses on analyzing anti-Hindu sentiment, also known as Hinduphobia, during the COVID-19 pandemic using a computational framework and a newly developed Hinduphobic BERT (HP-BERT) model. The researchers curated and released a dataset of 8,000 annotated tweets for analysis. The HP-BERT model achieved high accuracy of 94.72% in detecting Hinduphobic content on social media platform X, outperforming baseline models. Analysis of approximately 27.4 million tweets across six countries revealed a correlation between COVID-19 case increases and the volume of Hinduphobic content, suggesting a link between pandemic-related stress and discriminatory discourse. The study provides evidence of religious discrimination against Hindu communities on social media platforms during the COVID-19 crisis.<br /><br />Summary: <div>
arXiv:2501.05482v2 Announce Type: replace-cross 
Abstract: During the COVID-19 pandemic, community tensions intensified, contributing to discriminatory sentiments against various religious groups, including Hindu communities. Recent advances in language models have shown promise for social media analysis with potential for longitudinal studies of social media platforms, such as X (Twitter). We present a computational framework for analyzing anti-Hindu sentiment (Hinduphobia) during the COVID-19 period, introducing an abuse detection and sentiment analysis approach for longitudinal analysis on X. We curate and release a "Hinduphobic COVID-19 XDataset" containing 8,000 annotated and manually verified tweets. We then develop the Hinduphobic BERT (HP-BERT) model using this dataset and achieve 94.72\% accuracy, outperforming baseline Transformer-based language models. The model incorporates multi-label sentiment analysis capabilities through additional fine-tuning. Our analysis encompasses approximately 27.4 million tweets from six countries, including Australia, Brazil, India, Indonesia, Japan, and the United Kingdom. Statistical analysis reveals moderate correlations (r = 0.312-0.428) between COVID-19 case increases and Hinduphobic content volume, highlighting how pandemic-related stress may contribute to discriminatory discourse. This study provides evidence of social media-based religious discrimination during a COVID-19 crisis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Sparsification of Simplicial Complexes via Local Densities of States</title>
<link>https://arxiv.org/abs/2502.07558</link>
<guid>https://arxiv.org/abs/2502.07558</guid>
<content:encoded><![CDATA[
<div> Keywords: simplicial complexes, sparsification, local densities of states, spectral approximation, computational complexity

Summary:
This article introduces a novel method for probabilistic sparsification of simplicial complexes, aiming to reduce computational requirements while maintaining the original spectrum closely. The method utilizes local densities of states to approximate the generalized effective resistance of each simplex, guiding the sampling probability for sparsification. To prevent degenerate structures in the spectrum, a "kernel-ignoring" decomposition is proposed. Error estimates are used to analyze the algorithmic complexity of the method. The framework is tested on Vietoris-Rips filtered simplicial complexes, showcasing its performance in reducing the complexity of dense structures. This approach provides a promising avenue for efficiently analyzing dense simplicial complexes in topological data analysis and signal processing.<br /><br />Summary: <div>
arXiv:2502.07558v2 Announce Type: replace-cross 
Abstract: Simplicial complexes (SCs) have become a popular abstraction for analyzing complex data using tools from topological data analysis or topological signal processing. However, the analysis of many real-world datasets often leads to dense SCs, with many higher-order simplicies, which results in prohibitive computational requirements in terms of time and memory consumption. The sparsification of such complexes is thus of broad interest, i.e., the approximation of an original SC with a sparser surrogate SC (with typically only a log-linear number of simplices) that maintains the spectrum of the original SC as closely as possible. In this work, we develop a novel method for a probabilistic sparsification of SCs that uses so-called local densities of states. Using this local densities of states, we can efficiently approximate so-called generalized effective resistance of each simplex, which is proportional to the required sampling probability for the sparsification of the SC. To avoid degenerate structures in the spectrum of the corresponding Hodge Laplacian operators, we suggest a ``kernel-ignoring'' decomposition to approximate the sampling probability. Additionally, we utilize certain error estimates to characterize the asymptotic algorithmic complexity of the developed method. We demonstrate the performance of our framework on a family of Vietoris--Rips filtered simplicial complexes.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2510.02568</link>
<guid>https://arxiv.org/abs/2510.02568</guid>
<content:encoded><![CDATA[
<div> Keywords: asymptomatic individuals, epidemic control, Graph Neural Network, supervised learning, network models

Summary:
Asymptomatic individuals play a crucial role in the spread of epidemics by carrying and transmitting infections without showing symptoms. Detecting these individuals is essential for effective epidemic control but costly periodic testing is impractical. This study addresses the challenge of identifying asymptomatic individuals using a Graph Neural Network (GNN) model with supervised learning. By leveraging node features extracted from observed infected nodes in a classic SI network epidemic model, the GNN accurately classifies healthy nodes as asymptomatic or susceptible. The approach demonstrates robustness across various network models, sizes, and observed infection rates, effectively distinguishing asymptomatic carriers. The findings highlight the efficacy and generalizability of the proposed methodology in detecting asymptomatic individuals without the need for extensive testing, offering a promising strategy for controlling epidemics. 

<br /><br />Summary: <div>
arXiv:2510.02568v1 Announce Type: new 
Abstract: Infected individuals in some epidemics can remain asymptomatic while still carrying and transmitting the infection. These individuals contribute to the spread of the epidemic and pose a significant challenge to public health policies. Identifying asymptomatic individuals is critical for measuring and controlling an epidemic, but periodic and widespread testing of healthy individuals is often too costly. This work tackles the problem of identifying asymptomatic individuals considering a classic SI (Susceptible-Infected) network epidemic model where a fraction of the infected nodes are not observed as infected (i.e., their observed state is identical to susceptible nodes). In order to classify healthy nodes as asymptomatic or susceptible, a Graph Neural Network (GNN) model with supervised learning is adopted where a set of node features are built from the network with observed infected nodes. The approach is evaluated across different network models, network sizes, and fraction of observed infections. Results indicate that the proposed methodology is robust across different scenarios, accurately identifying asymptomatic nodes while also generalizing to different network sizes and fraction of observed infections.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Datasets Enriched With Contextual and Social Dimensions</title>
<link>https://arxiv.org/abs/2510.02333</link>
<guid>https://arxiv.org/abs/2510.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: human trajectories, datasets, semantic enrichment, large cities, open source pipeline

Summary: 
This resource paper introduces two publicly available datasets of semantically enriched human trajectories from GPS traces sourced from OpenStreetMap. The datasets include contextual layers such as stops, moves, POIs, transportation modes, and weather data, as well as synthetic social media posts generated by Large Language Models (LLMs). Available in tabular and RDF formats, the datasets cover Paris and New York, supporting behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. The open-source pipeline enables dataset customization, while the datasets offer a unique combination of real-world movement data, structured semantic enrichment, LLM-generated text, and semantic web compatibility. This resource is a significant advancement in enabling multimodal and semantic mobility analysis using real-world data. 

<br /><br />Summary: <div>
arXiv:2510.02333v1 Announce Type: cross 
Abstract: In this resource paper, we present two publicly available datasets of semantically enriched human trajectories, together with the pipeline to build them. The trajectories are publicly available GPS traces retrieved from OpenStreetMap. Each dataset includes contextual layers such as stops, moves, points of interest (POIs), inferred transportation modes, and weather data. A novel semantic feature is the inclusion of synthetic, realistic social media posts generated by Large Language Models (LLMs), enabling multimodal and semantic mobility analysis. The datasets are available in both tabular and Resource Description Framework (RDF) formats, supporting semantic reasoning and FAIR data practices. They cover two structurally distinct, large cities: Paris and New York. Our open source reproducible pipeline allows for dataset customization, while the datasets support research tasks such as behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. To our knowledge, our resource is the first to combine real-world movement, structured semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-induced Emergence of Biased Structures in LLM-based Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2510.02637</link>
<guid>https://arxiv.org/abs/2510.02637</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, large language models, network evolution, preferential attachment, homophilic communities

Summary:
This study explores how interactions among artificially intelligent agents, driven by large language models, influence the evolution of collective network structures. Through experiments with four different LLMs, the researchers found that agents exhibit preferential attachment, forming connections with nodes of higher degrees. Incorporating social attributes like age, gender, religion, and political orientation led to the development of assortative networks, resulting in the formation of homophilic communities. Political and religious attributes played significant roles in fragmenting the network and creating polarized subgroups. Age and gender, on the other hand, resulted in more gradual shifts in network structure. The study also identified asymmetric patterns in heterophilous ties, indicating embedded directional biases influenced by societal norms. These findings highlight how AI-driven systems impact network topology and offer insights into the co-evolution and self-organization of AI collectives. 

<br /><br />Summary: <div>
arXiv:2510.02637v1 Announce Type: cross 
Abstract: This study examines how interactions among artificially intelligent (AI) agents, guided by large language models (LLMs), drive the evolution of collective network structures. We ask LLM-driven agents to grow a network by informing them about current link constellations. Our observations confirm that agents consistently apply a preferential attachment mechanism, favoring connections to nodes with higher degrees. We systematically solicited more than a million decisions from four different LLMs, including Gemini, ChatGPT, Llama, and Claude. When social attributes such as age, gender, religion, and political orientation are incorporated, the resulting networks exhibit heightened assortativity, leading to the formation of distinct homophilic communities. This significantly alters the network topology from what would be expected under a pure preferential attachment model alone. Political and religious attributes most significantly fragment the collective, fostering polarized subgroups, while age and gender yield more gradual structural shifts. Strikingly, LLMs also reveal asymmetric patterns in heterophilous ties, suggesting embedded directional biases reflective of societal norms. As autonomous AI agents increasingly shape the architecture of online systems, these findings contribute to how algorithmic choices of generative AI collectives not only reshape network topology, but offer critical insights into how AI-driven systems co-evolve and self-organize.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns</title>
<link>https://arxiv.org/abs/2510.02702</link>
<guid>https://arxiv.org/abs/2510.02702</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation, graph neural network, mobility data, public health, spatial analysis

Summary:
VisitHGNN, a graph neural network, is introduced to predict visit probabilities from neighborhoods to specific destinations based on historical origin-to-destination flow patterns. The model considers spatial, temporal, and functional relations among urban places, incorporating attributes of Points of interest (POIs) and census block groups (CBGs). By leveraging spatial proximity, temporal co-activity, and brand affinity, VisitHGNN outperforms baseline models in predicting visit probabilities accurately. The model's strong predictive performance aligns closely with empirical visitation patterns and mirrors observed travel behavior with high fidelity. This approach has significant implications for urban planning, transportation policy, mobility system design, and public health by providing valuable insights for decision support in improving transportation planning, mobility management, and public health initiatives. 

<br /><br />Summary: <div>
arXiv:2510.02702v1 Announce Type: cross 
Abstract: Understanding how urban residents travel between neighborhoods and destinations is critical for transportation planning, mobility management, and public health. By mining historical origin-to-destination flow patterns with spatial, temporal, and functional relations among urban places, we estimate probabilities of visits from neighborhoods to specific destinations. These probabilities capture neighborhood-level contributions to citywide vehicular and foot traffic, supporting demand estimation, accessibility assessment, and multimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous, relation-specific graph neural network designed to predict visit probabilities at individual Points of interest (POIs). POIs are characterized using numerical, JSON-derived, and textual attributes, augmented with fixed summaries of POI--POI spatial proximity, temporal co-activity, and brand affinity, while census block groups (CBGs) are described with 72 socio-demographic variables. CBGs are connected via spatial adjacency, and POIs and CBGs are linked through distance-annotated cross-type edges. Inference is constrained to a distance-based candidate set of plausible origin CBGs, and training minimizes a masked Kullback-Leibler (KL) divergence to yield probability distribution across the candidate set. Using weekly mobility data from Fulton County, Georgia, USA, VisitHGNN achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of 0.892, substantially outperforming pairwise MLP and distance-only baselines, and aligning closely with empirical visitation patterns (NDCG@50 = 0.966); Recall@5 = 0.611). The resulting distributions closely mirror observed travel behavior with high fidelity, highlighting the model's potential for decision support in urban planning, transportation policy, mobility system design, and public health.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media</title>
<link>https://arxiv.org/abs/2510.02811</link>
<guid>https://arxiv.org/abs/2510.02811</guid>
<content:encoded><![CDATA[
<div> MBTI9k, PANDORA, Reddit, personality assessment, NLP
Summary:
- Challenges in personality assessment from digital footprints include data scarcity and disconnect between personality psychology and NLP.
- Two datasets, MBTI9k and PANDORA, were collected from Reddit to address challenges in data size, quality, and label coverage.
- Experiments show that demographic variables impact model validity in personality assessment.
- The SIMPA framework was developed for interpretable personality assessment by matching user-generated statements with validated questionnaire items.
- SIMPA's model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.<br /><br />Summary: <div>
arXiv:2510.02811v1 Announce Type: cross 
Abstract: Personality refers to individual differences in behavior, thinking, and feeling. With the growing availability of digital footprints, especially from social media, automated methods for personality assessment have become increasingly important. Natural language processing (NLP) enables the analysis of unstructured text data to identify personality indicators. However, two main challenges remain central to this thesis: the scarcity of large, personality-labeled datasets and the disconnect between personality psychology and NLP, which restricts model validity and interpretability. To address these challenges, this thesis presents two datasets -- MBTI9k and PANDORA -- collected from Reddit, a platform known for user anonymity and diverse discussions. The PANDORA dataset contains 17 million comments from over 10,000 users and integrates the MBTI and Big Five personality models with demographic information, overcoming limitations in data size, quality, and label coverage. Experiments on these datasets show that demographic variables influence model validity. In response, the SIMPA (Statement-to-Item Matching Personality Assessment) framework was developed - a computational framework for interpretable personality assessment that matches user-generated statements with validated questionnaire items. By using machine learning and semantic similarity, SIMPA delivers personality assessments comparable to human evaluations while maintaining high interpretability and efficiency. Although focused on personality assessment, SIMPA's versatility extends beyond this domain. Its model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delay-Tolerant Augmented-Consensus-based Distributed Directed Optimization</title>
<link>https://arxiv.org/abs/2510.02889</link>
<guid>https://arxiv.org/abs/2510.02889</guid>
<content:encoded><![CDATA[
<div> Keywords: Distributed optimization, Time-delays, Multi-agent networks, Convergence, Simulation.

Summary:
This paper explores distributed optimization in the presence of communication time-delays in multi-agent networks. The algorithm proposed in the paper addresses heterogeneous and bounded delays over directed networks. By applying matrix theory, algebraic graph theory, and augmented consensus formulation, the algorithm ensures convergence to the optimal value. Simulations validate the effectiveness of the proposed algorithm and compare its performance to existing delay-free algorithms. The research fills a gap in the literature by considering time-delays in information exchange among computing nodes, a scenario commonly encountered in real-world applications. This work contributes to the advancement of distributed optimization algorithms and provides a comprehensive framework for handling communication delays in large-scale machine learning and data processing tasks over multi-agent networks.<br /><br />Summary: <div>
arXiv:2510.02889v1 Announce Type: cross 
Abstract: Distributed optimization finds applications in large-scale machine learning, data processing and classification over multi-agent networks. In real-world scenarios, the communication network of agents may encounter latency that may affect the convergence of the optimization protocol. This paper addresses the case where the information exchange among the agents (computing nodes) over data-transmission channels (links) might be subject to communication time-delays, which is not well addressed in the existing literature. Our proposed algorithm improves the state-of-the-art by handling heterogeneous and arbitrary but bounded and fixed (time-invariant) delays over general strongly-connected directed networks. Arguments from matrix theory, algebraic graph theory, and augmented consensus formulation are applied to prove the convergence to the optimal value. Simulations are provided to verify the results and compare the performance with some existing delay-free algorithms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories</title>
<link>https://arxiv.org/abs/2510.03152</link>
<guid>https://arxiv.org/abs/2510.03152</guid>
<content:encoded><![CDATA[
<div> framework, Markovian Reeb Graphs, spatiotemporal trajectories, Patterns of Life, urban mobility 

Summary:
Markovian Reeb Graphs is introduced as a framework for simulating realistic spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. This novel approach combines individual- and population-level mobility structures within a probabilistic topological model to generate future trajectories that reflect both consistency and variability in daily life. The method is evaluated on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics. Results indicate that the proposed framework achieves high fidelity while remaining data- and compute-efficient. This scalability makes Markovian Reeb Graphs a versatile tool for simulating trajectories in diverse urban environments. <div>
arXiv:2510.03152v1 Announce Type: cross 
Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical partisan proximity outweighs online ties in predicting US voting outcomes</title>
<link>https://arxiv.org/abs/2407.12146</link>
<guid>https://arxiv.org/abs/2407.12146</guid>
<content:encoded><![CDATA[
<div> Keywords: affective polarization, social mixing, political outcomes, partisan exposure, partisan segregation

Summary: 
The study explores the impact of partisan exposure on vote choice in the US, analyzing offline and online dimensions. Physical space interactions, captured through co-location patterns, prove to be more predictive of electoral outcomes than online and residential exposures. Individual offline ties are found to be better predictors of vote choice compared to online connections. The research also highlights the prevalence of partisan segregation in metropolitan areas, with offline isolation being higher than online segregation and mainly linked to educational attainment. These findings underscore the significance of physical interactions in understanding the relationship between social networks and political behavior, underscoring the crucial role of offline social networks in influencing elections. 

<br /><br />Summary: <div>
arXiv:2407.12146v2 Announce Type: replace 
Abstract: Affective polarization and increasing social divisions affect social mixing and the spread of information across online and physical spaces, reinforcing social and electoral cleavages and influencing political outcomes. Here, using individual survey data and aggregated and de-identified co-location and online network data, we investigate the relationship between partisan exposure and vote choice in the US by comparing offline and online dimensions of partisan exposure. By leveraging various statistical modeling approaches, we consistently find that partisan exposure in the physical space, as captured by co-location patterns, more accurately predicts electoral outcomes in US counties, outperforming online and residential exposures. Similarly, offline ties at the individual level better predict vote choice compared to online connections. We also estimate county-level experienced partisan segregation and examine its relationship with individuals' demographic and socioeconomic characteristics. Focusing on metropolitan areas, our results confirm the presence of extensive partisan segregation in the US and show that offline partisan isolation, both considering physical encounters or residential sorting, is higher than online segregation and is primarily associated with educational attainment. Our findings emphasize the importance of physical space in understanding the relationship between social networks and political behavior, in contrast to the intense scrutiny focused on online social networks and elections.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Social Influence: Modeling Persuasion in Contested Social Networks</title>
<link>https://arxiv.org/abs/2510.01481</link>
<guid>https://arxiv.org/abs/2510.01481</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Influence Game, adversarial persuasion, DeGroot dynamics, difference-of-convex program, Iterated Linear solver

Summary:
The Social Influence Game (SIG) framework introduces a model for adversarial persuasion in social networks with multiple competing players. Players allocate influence from a fixed budget to shape opinions using DeGroot dynamics. The optimization problem in SIG is proven to be a difference-of-convex program, allowing for tractable analysis. To address scalability, an Iterated Linear (IL) solver is developed, approximating player objectives with linear programs. Experimental results show that IL achieves solutions within 7% of nonlinear solvers while being significantly faster, making it suitable for large social networks. This research paves the way for the study of contested influence in complex networks through asymptotic analysis.  
<br /><br />Summary: <div>
arXiv:2510.01481v1 Announce Type: new 
Abstract: We present the Social Influence Game (SIG), a framework for modeling adversarial persuasion in social networks with an arbitrary number of competing players. Our goal is to provide a tractable and interpretable model of contested influence that scales to large systems while capturing the structural leverage points of networks. Each player allocates influence from a fixed budget to steer opinions that evolve under DeGroot dynamics, and we prove that the resulting optimization problem is a difference-of-convex program. To enable scalability, we develop an Iterated Linear (IL) solver that approximates player objectives with linear programs. In experiments on random and archetypical networks, IL achieves solutions within 7% of nonlinear solvers while being over 10x faster, scaling to large social networks. This paper lays a foundation for asymptotic analysis of contested influence in complex networks.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Sequential Learning</title>
<link>https://arxiv.org/abs/2502.19525</link>
<guid>https://arxiv.org/abs/2502.19525</guid>
<content:encoded><![CDATA[
<div> privacy-preserving learning, sequential learning, social learning, information flow, heterogeneous populations
Summary:
In a study on privacy-preserving sequential learning in settings like vaccination registries, where individuals add noise to conceal private signals, it was found that with continuous signals and a fixed privacy budget, optimal randomization balances privacy and accuracy, leading to accelerated learning. Privacy helps in faster learning rates than nonprivate scenarios by amplifying log-likelihood ratios. In heterogeneous populations, an order-optimal rate is achieved when some agents have low privacy budgets. However, with binary signals, privacy reduces informativeness and hinders learning compared to nonprivate scenarios. The study demonstrates how privacy reshapes information dynamics, providing insights for platform and policy design. <div>
arXiv:2502.19525v5 Announce Type: cross 
Abstract: In settings like vaccination registries, individuals act after observing others, and the resulting public records can expose private information. We study privacy-preserving sequential learning, where agents add endogenous noise to their reported actions to conceal private signals. Efficient social learning relies on information flow, seemingly in conflict with privacy. Surprisingly, with continuous signals and a fixed privacy budget $(\epsilon)$, the optimal randomization strategy balances privacy and accuracy, accelerating learning to $\Theta_{\epsilon}(\log n)$, faster than the nonprivate $\Theta(\sqrt{\log n})$ rate. In the nonprivate baseline, the expected time to the first correct action and the number of incorrect actions diverge; under privacy with sufficiently small $\epsilon$, both are finite. Privacy helps because, under the false state, agents more often receive signals contradicting the majority; randomization then asymmetrically amplifies the log-likelihood ratio, enhancing aggregation. In heterogeneous populations, an order-optimal $\Theta(\sqrt{n})$ rate is achievable when a subset of agents have low privacy budgets. With binary signals, however, privacy reduces informativeness and impairs learning relative to the nonprivate baseline, though the dependence on $\epsilon$ is nonmonotone. Our results show how privacy reshapes information dynamics and inform the design of platforms and policies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framing Unionization on Facebook: Communication around Representation Elections in the United States</title>
<link>https://arxiv.org/abs/2510.01757</link>
<guid>https://arxiv.org/abs/2510.01757</guid>
<content:encoded><![CDATA[
<div> Keywords: digital media, labor unions, communication, discourse frames, representation elections

Summary: 
Labor unions are increasingly using digital media to communicate and organize collective action. This study analyzed 158k Facebook posts from U.S. labor unions from 2015 to 2024 in conjunction with National Labor Relations Board election data to explore the relationship between online discourse and representation elections. Five discourse frames were identified and examined: diagnostic, prognostic, motivational, community, and engagement. The study found that diagnostic and community frames were most commonly used, with significant variation in frame usage among organizations. Communication leading up to won elections saw an increase in diagnostic, prognostic, and community frames, followed by a decrease in prognostic and motivational framing post-event, indicating strategic preparation. In contrast, unions that lost elections showed little adjustment in their communication strategies. This research provides insights into how unions adapt their communication strategies in different organizational contexts. 

<br /><br />Summary: <div>
arXiv:2510.01757v1 Announce Type: cross 
Abstract: Digital media have become central to how labor unions communicate, organize, and sustain collective action. Yet little is known about how unions' online discourse relates to concrete outcomes such as representation elections. This study addresses the gap by combining National Labor Relations Board (NLRB) election data with 158k Facebook posts published by U.S. labor unions between 2015 and 2024. We focused on five discourse frames widely recognized in labor and social movement communication research: diagnostic (identifying problems), prognostic (proposing solutions), motivational (mobilizing action), community (emphasizing solidarity), and engagement (promoting interaction). Using a fine-tuned RoBERTa classifier, we systematically annotated unions' posts and analyzed patterns of frame usage around election events. Our findings showed that diagnostic and community frames dominated union communication overall, but that frame usage varied substantially across organizations. In election cases that unions won, communication leading up to the vote showed an increased use of diagnostic, prognostic, and community frames, followed by a reduction in prognostic and motivational framing after the event--patterns consistent with strategic preparation. By contrast, in lost election cases unions showed little adjustment in their communication, suggesting an absence of tailored communication strategies. By examining variation in message-level framing, the study highlights how communication strategies adapt to organizational contexts, contributing open tools and data and complementing prior research in understanding digital communication of unions and social movements.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review</title>
<link>https://arxiv.org/abs/2501.13014</link>
<guid>https://arxiv.org/abs/2501.13014</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, open review process, reviewer quality, empirical Bayesian methods, incentive structures

Summary:
Traditional closed peer review systems have various limitations such as being slow, costly, non-transparent, and subject to biases. This study proposes an alternative open, bottom-up peer review process to address these issues. The research analyzed data from scientific conferences to show the challenges of reviewer variability and low correlation among reviewers. Surprisingly, reviewer quality scores were not correlated with authorship quality, revealing an inverted U-shaped relationship. The study assessed empirical Bayesian methods to estimate paper quality and found that it significantly improved assessments compared to simple averaging. Additionally, a model incorporating reviewer ratings for both papers and reviewers showed that user-generated scores could produce reliable paper scoring even with unreliable reviewers. The study also proposed incentive structures to recognize high-quality reviewers and promote broader reviewing coverage. Overall, the findings suggest that a self-selecting open peer review process has the potential to enhance the speed, fairness, and transparency of scientific peer review.<br /><br />Summary: <div>
arXiv:2501.13014v2 Announce Type: replace 
Abstract: Traditional closed peer review systems, which have played a central role in scientific publishing, are often slow, costly, non-transparent, stochastic, and possibly subject to biases - factors that can impede scientific progress and undermine public trust. Here, we propose and examine the efficacy and accuracy of an alternative form of scientific peer review: through an open, bottom-up process. First, using data from two major scientific conferences (CCN2023 and ICLR2023), we highlight how high variability of review scores and low correlation across reviewers presents a challenge for collective review. We quantify reviewer agreement with community consensus scores and use this as a reviewer quality estimator, showing that surprisingly, reviewer quality scores are not correlated with authorship quality. Instead, we reveal an inverted U-shape relationship, where authors with intermediate paper scores are the best reviewers. We assess empirical Bayesian methods to estimate paper quality based on different assessments of individual reviewer reliability. We show how under a one-shot review-then-score scenario, both in our models and on real peer review data, a Bayesian measure significantly improves paper quality assessments relative to simple averaging. We then consider an ongoing model of publishing, reviewing, and scoring, with reviewers scoring not only papers but also other reviewers. We show that user-generated reviewer ratings can yield robust and high-quality paper scoring even when unreliable (but unbiased) reviewers dominate. Finally, we outline incentive structures to recognize high-quality reviewers and encourage broader reviewing coverage of submitted papers. These findings suggest that a self-selecting open peer review process is potentially scalable, reliable, and equitable with the possibility of enhancing the speed, fairness, and transparency of the peer review process.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Heterophily in Recommender Systems with Wavelet Hypergraph Diffusion</title>
<link>https://arxiv.org/abs/2501.14399</link>
<guid>https://arxiv.org/abs/2501.14399</guid>
<content:encoded><![CDATA[
<div> Framework, FWHDNN, representation learning, hypergraph, recommendation<br />
<br />
Summary:<br />
FWHDNN (Fusion-based Wavelet Hypergraph Diffusion Neural Networks) is introduced as a novel framework for enhancing representation learning in hypergraph-based recommendation systems. It combines a cross-difference relation encoder for heterophily-aware hypergraph diffusion, a cluster-wise encoder utilizing wavelet transform-based hypergraph neural network layers, and a multi-modal fusion mechanism. Through extensive experiments on real-world datasets, FWHDNN outperforms existing methods in accuracy, robustness, and scalability by effectively capturing high-order user-item interconnections. <div>
arXiv:2501.14399v2 Announce Type: replace-cross 
Abstract: Recommender systems are pivotal in delivering personalised user experiences across various domains. However, capturing the heterophily patterns and the multi-dimensional nature of user-item interactions poses significant challenges. To address this, we introduce FWHDNN (Fusion-based Wavelet Hypergraph Diffusion Neural Networks), an innovative framework aimed at advancing representation learning in hypergraph-based recommendation tasks. The model incorporates three key components: (1) a cross-difference relation encoder leveraging heterophily-aware hypergraph diffusion to adapt message-passing for diverse class labels, (2) a multi-level cluster-wise encoder employing wavelet transform-based hypergraph neural network layers to capture multi-scale topological relationships, and (3) an integrated multi-modal fusion mechanism that combines structural and textual information through intermediate and late-fusion strategies. Extensive experiments on real-world datasets demonstrate that FWHDNN surpasses state-of-the-art methods in accuracy, robustness, and scalability in capturing high-order interconnections between users and items.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FTSCommDetector: Discovering Behavioral Communities through Temporal Synchronization</title>
<link>https://arxiv.org/abs/2510.00014</link>
<guid>https://arxiv.org/abs/2510.00014</guid>
<content:encoded><![CDATA[
<div> community detection, synchronization, multivariate time series, financial markets, risk management

Summary:
FTSCommDetector introduces a new method, Temporal Coherence Architecture (TCA), for community detection in multivariate time series data. Traditional methods fail to capture synchronization-desynchronization patterns seen in entities like AAPL and MSFT during market disruptions. FTSCommDetector uses dual-scale encoding and static topology with dynamic attention to maintain coherence and discover similar and dissimilar communities. Information-theoretic foundations and Normalized Temporal Profiles (NTP) are introduced for evaluation. The method achieves consistent improvements across various financial markets without the need for dataset-specific tuning. It demonstrates robustness with minimal performance variation across different window sizes. This approach provides practical insights for portfolio construction and risk management in financial markets. 

<br /><br />Summary: <div>
arXiv:2510.00014v1 Announce Type: new 
Abstract: Why do trillion-dollar tech giants AAPL and MSFT diverge into different response patterns during market disruptions despite identical sector classifications? This paradox reveals a fundamental limitation: traditional community detection methods fail to capture synchronization-desynchronization patterns where entities move independently yet align during critical moments. To this end, we introduce FTSCommDetector, implementing our Temporal Coherence Architecture (TCA) to discover similar and dissimilar communities in continuous multivariate time series. Unlike existing methods that process each timestamp independently, causing unstable community assignments and missing evolving relationships, our approach maintains coherence through dual-scale encoding and static topology with dynamic attention. Furthermore, we establish information-theoretic foundations demonstrating how scale separation maximizes complementary information and introduce Normalized Temporal Profiles (NTP) for scale-invariant evaluation. As a result, FTSCommDetector achieves consistent improvements across four diverse financial markets (SP100, SP500, SP1000, Nikkei 225), with gains ranging from 3.5% to 11.1% over the strongest baselines. The method demonstrates remarkable robustness with only 2% performance variation across window sizes from 60 to 120 days, making dataset-specific tuning unnecessary, providing practical insights for portfolio construction and risk management.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Life Paths Cross: Extracting Human Interactions in Time and Space from Wikipedia</title>
<link>https://arxiv.org/abs/2510.00019</link>
<guid>https://arxiv.org/abs/2510.00019</guid>
<content:encoded><![CDATA[
<div> Keywords: notable individuals, interaction records, attention mechanisms, political polarization, WikiInteraction dataset

Summary:
Interactions among notable individuals convey significant messages across various perspectives, but these studies are often limited by data scarcity. To overcome this challenge, this study mines interaction data from millions of biography pages on Wikipedia, extracting 685,966 interaction records in the form of quadruplets. By integrating attention mechanisms, multi-task learning, and feature transfer methods, a model is designed to achieve an F1 score of 86.51%, outperforming baseline models. The extracted data allows for an analysis of intra- and inter-party interactions among political figures in the US, illustrating political polarization. The code, extracted interaction data, and the WikiInteraction dataset of 4,507 labeled interaction quadruplets are made publicly available. This research showcases the potential of utilizing data from Wikipedia to gain insights into dynamic interactions among notable individuals. 

<br /><br />Summary: <div>
arXiv:2510.00019v1 Announce Type: new 
Abstract: Interactions among notable individuals -- whether examined individually, in groups, or as networks -- often convey significant messages across cultural, economic, political, scientific, and historical perspectives. By analyzing the times and locations of these interactions, we can observe how dynamics unfold across regions over time. However, relevant studies are often constrained by data scarcity, particularly concerning the availability of specific location and time information. To address this issue, we mine millions of biography pages from Wikipedia, extracting 685,966 interaction records in the form of (Person1, Person2, Time, Location) interaction quadruplets. The key elements of these interactions are often scattered throughout the heterogeneous crowd-sourced text and may be loosely or indirectly associated. We overcome this challenge by designing a model that integrates attention mechanisms, multi-task learning, and feature transfer methods, achieving an F1 score of 86.51%, which outperforms baseline models. We further conduct an empirical analysis of intra- and inter-party interactions among political figures to examine political polarization in the US, showcasing the potential of the extracted data from a perspective that may not be possible without this data. We make our code, the extracted interaction data, and the WikiInteraction dataset of 4,507 labeled interaction quadruplets publicly available.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IA aplicada al an\'alisis del conflicto Ir\'an-Israel: Mapeo de discursos en YouTube</title>
<link>https://arxiv.org/abs/2510.00021</link>
<guid>https://arxiv.org/abs/2510.00021</guid>
<content:encoded><![CDATA[
<div> Keywords: Iran-Israel conflict, YouTube comments, digital representation, media biases, algorithmic biases<br />
Summary: The study examines the digital representation of the Iran-Israel conflict in June 2025 based on YouTube comments. Using natural language processing and machine learning, comments were classified into categories, revealing an overrepresentation of pro-Palestinian and anti-U.S./Israel discourses. Iran emerged as a significant actor in the online conversation, indicating a narrative shift. Algorithmic biases were found to amplify certain discourses while marginalizing others. The research combines computational analysis and critical critique to study geopolitical controversies, offering a replicable methodological framework. It is one of the first Spanish-language studies to analyze international conflict discourses on YouTube through AI and critical analysis, highlighting overlooked asymmetries and narrative disputes.<br /><br />Summary: <div>
arXiv:2510.00021v1 Announce Type: new 
Abstract: Purpose. This study analyzes the digital representation of the Iran-Israel conflict that occurred in June 2025, based on 120,000 comments posted on YouTube. It sought to identify discursive positions regarding the actors involved and to examine how media and algorithmic biases shape digital conversations. Methodology. A mixed-methods design with triangulation was adopted. In the quantitative phase, natural language processing techniques and machine learning models (BERT and XLM-RoBERTa) were used to classify comments into ten categories. In the qualitative phase, a critical analysis of media context and ideological narratives was conducted, complemented by manual annotation and supervised training. This strategy enabled the integration of statistical robustness with contextual understanding. Results and conclusions. The findings reveal a clear overrepresentation of pro-Palestinian and anti-United States/Israel discourses, while pro-United States and anti-Palestinian positions were marginal. Iran, usually rendered invisible in global media, emerged as a central actor in the digital conversation during the conflict, suggesting a narrative shift away from previous hegemonic frameworks. Likewise, the results confirm the influence of algorithmic biases in amplifying certain discourses while limiting others. Original contributions. This work combines computational analysis and philosophical critique for the study of digital controversies, providing a methodological framework replicable in geopolitical contexts. It is one of the first Spanish-language studies to map, through artificial intelligence and critical analysis, discourses on an international conflict on YouTube, highlighting asymmetries and narrative disputes that are often overlooked.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis</title>
<link>https://arxiv.org/abs/2510.00024</link>
<guid>https://arxiv.org/abs/2510.00024</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Epidemic modeling, Multi-agent framework, Automation, Scientific research

Summary: 
EpidemIQs introduces a novel multi-agent framework using Large Language Models (LLMs) to automate complex interdisciplinary research in epidemic modeling. The framework integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, stochastic simulations, data visualization, and documentation of findings in a structured manuscript. It consists of scientist agents for planning and coordination, and task-expert agents focused on specific duties. Using GPT 4.1 and GPT 4.1 mini LLMs, the framework achieved a 100% completion success rate with an average total token usage of 870K at a cost of about $1.57 per study. Evaluation across different epidemic scenarios showed higher performance compared to single-agent LLMs. EpidemIQs represents a significant advancement in accelerating scientific research by reducing costs, turnaround time, and improving accessibility to advanced modeling tools.

<br /><br />Summary: <div>
arXiv:2510.00024v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \$1.57 per study, achieving a 100\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Product Ecosystems</title>
<link>https://arxiv.org/abs/2510.00036</link>
<guid>https://arxiv.org/abs/2510.00036</guid>
<content:encoded><![CDATA[
<div> dynamical-systems, influence propagation, product adoption networks, positive linear system, Metzler interaction matrices<br />
<br />
Summary: <br />
This paper presents a dynamical-systems framework for modeling influence propagation in product adoption networks, focusing on positive linear systems with Metzler interaction matrices and utility-based decay. The study derives exact solutions for various interaction structures, showing that positive interactions lead to nonnegative amplification and utility saturation after approximately three complementary additions. It also highlights the dominance of frequency over quality improvements in influence dynamics and the varying effects of reinforcing interactions and decay control. Additionally, the research establishes that long-run retention in SIS-type dynamics is bounded by the inverse spectral radius of the adoption graph. These findings extend existing theories and provide explicit, calibratable expressions for understanding influence dynamics in networked product adoption scenarios. <br /> <div>
arXiv:2510.00036v1 Announce Type: new 
Abstract: This paper develops a dynamical-systems framework for modeling influence propagation in product adoption networks, formulated as a positive linear system with Metzler interaction matrices and utility-based decay. Exact solutions are derived for constant, piecewise-constant, and fully time-varying interaction structures using matrix exponentials and the Peano--Baker series. It establishes five results: (i) positive interactions guarantee nonnegative amplification, (ii) perceived utility saturates after $\approx\!3$ complementary additions (Weber--Fechner), (iii) frequency of comparable introductions dominates incremental quality improvements, (iv) reinforcing interactions yields monotone gains while decay control gives ambiguous effects, and (v) long-run retention under SIS-type dynamics is bounded by the inverse spectral radius of the adoption graph. These results extend epidemic-threshold theory and positive-systems analysis to networked adoption, yielding explicit, calibratable expressions for influence dynamics on networks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction</title>
<link>https://arxiv.org/abs/2510.00080</link>
<guid>https://arxiv.org/abs/2510.00080</guid>
<content:encoded><![CDATA[
<div> explanatory, social recommendation, Graph Neural Networks, SoREX, ego-path extraction<br />
<br />
Summary: 
The study introduces SoREX, a self-explanatory social recommendation framework that combines Graph Neural Networks and friend recommendation to improve prediction accuracy. SoREX independently models social relations and user-item interactions while optimizing an auxiliary task to enhance social signals. The framework includes a novel ego-path extraction approach to provide meaningful explanations for predictions. By transforming the ego-net of a user into multi-hop ego-paths, SoREX extracts factor-specific and candidate-aware subsets for explanations. These explanations enable detailed comparative analysis among different candidate items. Additionally, explanation re-aggregation correlates explanations with predictions, making the framework self-explanatory. Experimental results on benchmark datasets validate SoREX's predictive accuracy, and both qualitative and quantitative analyses demonstrate the effectiveness of the extracted explanations. The code and data are available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2510.00080v1 Announce Type: new 
Abstract: Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at https://github.com/antman9914/SoREX.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobility Behavior Evolution During Extended Emergencies: Returners, Explorers, and the 15-Minute City</title>
<link>https://arxiv.org/abs/2510.00469</link>
<guid>https://arxiv.org/abs/2510.00469</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, emergencies, urban resilience, YJMob100K dataset, spatial disparities

Summary: 
This study analyzes human mobility patterns during emergencies in a densely populated metropolitan region using high-resolution spatial data from the YJMob100K dataset. The research focuses on the transition between returners and explorers, individuals who visit limited locations repeatedly and those who travel across broader destinations, respectively. The study finds that it takes at least two weeks of data to detect meaningful behavioral shifts during emergencies. Individuals tend to resume visits to non-essential locations more slowly during prolonged emergencies than under normal conditions. Explorers reduce long-distance travel significantly, particularly on weekends and holidays, exhibiting returner-like short distance patterns. Residents from low Points of Interest (POI) density neighborhoods often travel to POI rich areas, emphasizing spatial disparities. Strengthening local accessibility could enhance urban resilience during crises. The full reproducibility of the study is ensured through the project website. 

Summary: <div>
arXiv:2510.00469v1 Announce Type: new 
Abstract: Understanding human mobility during emergencies is critical for strengthening urban resilience and guiding emergency management. This study examines transitions between returners, who repeatedly visit a limited set of locations, and explorers, who travel across broader destinations, over a 15-day emergency period in a densely populated metropolitan region using the YJMob100K dataset. High-resolution spatial data reveal intra-urban behavioral dynamics often masked at coarser scales. Beyond static comparisons, we analyze how mobility evolves over time, with varying emergency durations, across weekdays and weekends, and relative to neighborhood boundaries, linking the analysis to the 15-minute city framework.
  Results show that at least two weeks of data are required to detect meaningful behavioral shifts. During prolonged emergencies, individuals resume visits to non-essential locations more slowly than under normal conditions. Explorers markedly reduce long distance travel, while weekends and holidays consistently exhibit returner-like, short distance patterns. Residents of low Points of Interest (POI) density neighborhoods often travel to POI rich areas, highlighting spatial disparities. Strengthening local accessibility may improve urban resilience during crises.
  Full reproducibility is supported through the project website: https://github.com/wissamkontar
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threats to the sustainability of Community Notes on X</title>
<link>https://arxiv.org/abs/2510.00650</link>
<guid>https://arxiv.org/abs/2510.00650</guid>
<content:encoded><![CDATA[
<div> Community Notes, content moderation, X, user-generated context, bridging algorithm <br />
Summary:
Community Notes, a content moderation system pioneered by Twitter and now used by X, relies on a bridging algorithm to identify user-generated context with upvotes across political divides. This study examines the X Community Notes community to understand its stability and disruptions, as well as the motivations driving user contributions. Through a regression discontinuity design analysis, the study finds that having a note published has a positive impact on future note authoring. However, the proportion of notes considered "helpful" on X is low (10%) and decreasing, posing potential risks to the system. These findings have implications for the future of Community Notes on X and other platforms using similar approaches. <br /><br />Summary: <div>
arXiv:2510.00650v1 Announce Type: new 
Abstract: Community Notes are emerging as an important option for content moderation. The Community Notes system pioneered by Twitter, now known as X, uses a bridging algorithm to identify user-generated context with upvotes across political divides, supposedly spinning consensual gold from partisan straw. It is important to understand the nature of the community behind Community Notes, especially as the feature has now been imitated by several billion-user platforms. We look for signs of stability and disruption in the X Community Notes community and interrogate the motivations other than partisan animus (Allen, Martel, and Rand 2022) which may be driving users to contribute. We conduct a novel analysis of the impact of having a note published, which requires being considered "helpful" by the bridging algorithm, utilising a regression discontinuity design. This allows stronger causal inference than conventional methods used with observational data. Our analysis shows the positive effect on future note authoring of having a note published. This highlights the risk of the current system, where the proportion of notes considered "helpful" (and therefore shown to users on X) is low, 10%, and declining. This analysis has implications for the future of Community Notes on X and the extension of this approach to other platforms.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Communities in Continuous-Time Temporal Networks by Optimizing L-Modularity</title>
<link>https://arxiv.org/abs/2510.00741</link>
<guid>https://arxiv.org/abs/2510.00741</guid>
<content:encoded><![CDATA[
<div> Community detection, network analysis, temporal setting, dynamic communities, LAGO<br />
<br />
Summary: 
Community detection is a crucial problem in network analysis, with applications in various fields. Traditional methods struggle to handle dynamic data with exact temporal accuracy. LAGO, a new method introduced in this study, uses Longitudinal Modularity to optimize the detection of dynamic communities without relying on time discretization or assuming rigid community evolution. LAGO accurately captures the precise entry and exit points of nodes in communities, resulting in temporally and topologically coherent community detection. The method is evaluated on both synthetic benchmarks and real-world datasets, demonstrating its efficiency and effectiveness in uncovering dynamic communities in continuous-time networks. <div>
arXiv:2510.00741v1 Announce Type: new 
Abstract: Community detection is a fundamental problem in network analysis, with many applications in various fields. Extending community detection to the temporal setting with exact temporal accuracy, as required by real-world dynamic data, necessitates methods specifically adapted to the temporal nature of interactions. We introduce LAGO, a novel method for uncovering dynamic communities by greedy optimization of Longitudinal Modularity, a specific adaptation of Modularity for continuous-time networks. Unlike prior approaches that rely on time discretization or assume rigid community evolution, LAGO captures the precise moments when nodes enter and exit communities. We evaluate LAGO on synthetic benchmarks and real-world datasets, demonstrating its ability to efficiently uncover temporally and topologically coherent communities.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Minimization of Polarization and Disagreement via Low-Rank Matrix Bandits</title>
<link>https://arxiv.org/abs/2510.00803</link>
<guid>https://arxiv.org/abs/2510.00803</guid>
<content:encoded><![CDATA[
<div> polarization, disagreement, Friedkin-Johnsen opinion dynamics model, incomplete information, regret minimization<br />
<br />
Summary: 
This study focuses on minimizing polarization and disagreement in the Friedkin-Johnsen opinion dynamics model under incomplete information. It considers the online setting where innate opinions are unknown and must be learned through sequential observations, simulating periodic interventions on social media platforms. The problem is formulated as a regret minimization task, bridging algorithmic interventions on social media with multi-armed bandit theory. The proposed two-stage algorithm based on low-rank matrix bandits achieves an approximately $\sqrt{T}$ cumulative regret over any time horizon $T$. The algorithm first estimates a low-dimensional structure through subspace estimation and then utilizes a linear bandit algorithm within this compact representation. Empirical results demonstrate the algorithm's superior performance in terms of cumulative regret and running time compared to a linear bandit baseline. <div>
arXiv:2510.00803v1 Announce Type: cross 
Abstract: We study the problem of minimizing polarization and disagreement in the Friedkin-Johnsen opinion dynamics model under incomplete information. Unlike prior work that assumes a static setting with full knowledge of users' innate opinions, we address the more realistic online setting where innate opinions are unknown and must be learned through sequential observations. This novel setting, which naturally mirrors periodic interventions on social media platforms, is formulated as a regret minimization problem, establishing a key connection between algorithmic interventions on social media platforms and theory of multi-armed bandits. In our formulation, a learner observes only a scalar feedback of the overall polarization and disagreement after an intervention. For this novel bandit problem, we propose a two-stage algorithm based on low-rank matrix bandits. The algorithm first performs subspace estimation to identify an underlying low-dimensional structure, and then employs a linear bandit algorithm within the compact dimensional representation derived from the estimated subspace. We prove that our algorithm achieves an $ \widetilde{O}(\sqrt{T}) $ cumulative regret over any time horizon $T$. Empirical results validate that our algorithm significantly outperforms a linear bandit baseline in terms of both cumulative regret and running time.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population</title>
<link>https://arxiv.org/abs/2510.00964</link>
<guid>https://arxiv.org/abs/2510.00964</guid>
<content:encoded><![CDATA[
<div> Keywords: sex-trafficking survivors, Nepal, photo-elicitation, social capital, reintegration<br />
Summary: 
In this ethnographic study, the focus was on sex-trafficking survivors in Nepal, particularly those in a protected-living setup managed by an NGO. By employing a communal form of photo-elicitation, the researchers aimed to understand the values and experiences of the survivors without putting individual pressure on them. The study sheds light on the complex circumstances faced by survivors as they undergo rehabilitation and strive for a "new normal." Additionally, it contributes to the HCI and CSCW literature by highlighting the specific challenges faced by trafficking survivors and the importance of designing interventions to support their reintegration into society. The findings suggest that survivors possess limited yet valuable social capital in certain areas, which could be instrumental in aiding their reintegration process. <div>
arXiv:2510.00964v1 Announce Type: cross 
Abstract: We report on an initial ethnographic exploration of the situation of sex-trafficking survivors in Nepal. In the course of studying trafficking survivors in a protected-living situation created by a non-governmental organization in Nepal, we adapted photo-elicitation to hear the voices of the survivors by making the technique more communal. Bringing sociality to the forefront of the method reduced the pressure on survivors to assert voices as individuals, allowing them to speak. We make three contributions to research. First, we propose a communal form of photo-elicitation as a method to elicit values in sensitive settings. Second, we present the complex circumstances of the survivors as they undergo rehabilitation and move towards life with a ``new normal''. Third, our work adds to HCI and CSCW literature on understanding specific concerns of trafficking survivors and aims to inform designs that can support reintegration of survivors in society. The values that the survivors hold and their notion of future opportunities suggest possession of limited but important social capital in some domains that could be leveraged to aid reintegration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteKG: A knowledge graph-based framework for route prediction on road networks</title>
<link>https://arxiv.org/abs/2310.03617</link>
<guid>https://arxiv.org/abs/2310.03617</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, route prediction, spatial relations, n-ary tree-based algorithm, traffic flow estimation

Summary:
RouteKG is a novel framework for route prediction on road networks, utilizing a Knowledge Graph to encode spatial relations and moving directions crucial for human navigation. The model incorporates an efficient n-ary tree-based algorithm to generate top-K routes in batch mode, enhancing computational efficiency. A rank refinement module further optimizes prediction performance by fine-tuning route rankings. Evaluation on real-world vehicle trajectory datasets shows significant accuracy improvement over baseline methods. The pre-trained model can also be used for real-time traffic flow estimation at the link level. RouteKG has the potential to revolutionize vehicle navigation, traffic management, and other intelligent transportation tasks, contributing to the advancement of intelligent and connected urban systems. Source codes for RouteKG are available for access on GitHub. 

<br /><br />Summary: <div>
arXiv:2310.03617v4 Announce Type: replace 
Abstract: Short-term route prediction on road networks allows us to anticipate the future trajectories of road users, enabling various applications ranging from dynamic traffic control to personalized navigation. Despite recent advances in this area, existing methods focus primarily on learning sequential transition patterns, neglecting the inherent spatial relations in road networks that can affect human routing decisions. To fill this gap, this paper introduces RouteKG, a novel Knowledge Graph-based framework for route prediction. Specifically, we construct a Knowledge Graph on the road network to encode spatial relations, especially moving directions that are crucial for human navigation. Moreover, an n-ary tree-based algorithm is introduced to efficiently generate top-K routes in batch mode, enhancing computational efficiency. To further optimize prediction performance, a rank refinement module is incorporated to fine-tune candidate route rankings. The model performance is evaluated using two real-world vehicle trajectory datasets from two Chinese cities under various practical scenarios. The results demonstrate a significant improvement in accuracy over the baseline methods. We further validate the proposed method by utilizing the pre-trained model as a simulator for real-time traffic flow estimation at the link level. RouteKG has great potential to transform vehicle navigation, traffic management, and a variety of intelligent transportation tasks, playing a crucial role in advancing the core foundation of intelligent and connected urban systems. The source codes of RouteKG are available at https://github.com/YihongT/RouteKG.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration</title>
<link>https://arxiv.org/abs/2503.13238</link>
<guid>https://arxiv.org/abs/2503.13238</guid>
<content:encoded><![CDATA[
<div> Keywords: Arab Spring, Middle East, North Africa, research funding, knowledge flows

Summary: The study analyzes the impact of the Arab Spring on research in the Middle East and North Africa (MENA) region by examining 3.7 million articles published between 2002 and 2019. Following the 2010-2011 events, there was a noticeable increase in mentions of MENA countries in research articles, particularly focusing on Egypt. This surge can be attributed to both increased research funding for the region and the emigration of scholars maintaining research ties to their countries of origin. Saudi Arabia has emerged as a central hub for studying the affected countries, attracting scholars and funding, shaping the scientific narrative of the region. The study highlights how political events can influence global knowledge flows, shifting research focus, resources, and disciplinary perspectives. <br /><br />Summary: The Arab Spring prompted a surge in research attention towards MENA countries, with a notable increase in mentions of Egypt. This was driven by increased research funding and the involvement of scholars with ties to the region. Saudi Arabia has become a key player in shaping scientific discourse on the region, attracting scholars and resources. The study illustrates how political upheavals can reshape global knowledge flows, influencing who studies which regions, with what support, and in which academic disciplines. <div>
arXiv:2503.13238v4 Announce Type: replace-cross 
Abstract: The 2010-2011 Arab Spring reverberated far beyond politics, reshaping how the Middle East and North Africa region (MENA) is studied. Analyzing 3.7 million Scopus-indexed articles published between 2002 and 2019, we find that mentions of ten of these countries in titles or abstracts rose significantly after 2011 relative to the global baseline, with Egypt receiving the greatest attention in the region. We link this surge to two intertwined mechanisms: an increase in research funding directed at the MENA region and the emigration of researchers who continued publishing on their countries of origin. Our analysis reveals that Saudi Arabia has emerged as a regional hub for studying the affected countries, attracting funding and scholars, and thereby playing a significant role in shaping the scientific narrative on the region. These findings demonstrate how political upheaval can reshape global knowledge flows by altering who studies whom, with what resources, and in which disciplines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHINDR - a DSM5 based mental health diagnosis and recommendation framework using LLM</title>
<link>https://arxiv.org/abs/2509.25992</link>
<guid>https://arxiv.org/abs/2509.25992</guid>
<content:encoded><![CDATA[
<div> Keywords: Mental health forums, large language model, DSM-5 criteria, personalized interventions, psychological features 

Summary: 
MHINDR is a framework that utilizes a large language model integrated with DSM-5 criteria to analyze user-generated text from mental health forums. It aims to diagnose mental health conditions, track symptom progression, and provide personalized interventions and insights for mental health practitioners. The framework focuses on extracting temporal information and psychological features to create comprehensive mental health summaries for users. It offers scalable and customizable therapeutic recommendations that can be adapted to various clinical contexts, patient needs, and workplace well-being programs. MHINDR aims to improve mental health diagnosis and treatment by leveraging data-driven insights and personalized interventions based on individual needs and conditions. <div>
arXiv:2509.25992v1 Announce Type: new 
Abstract: Mental health forums offer valuable insights into psychological issues, stressors, and potential solutions. We propose MHINDR, a large language model (LLM) based framework integrated with DSM-5 criteria to analyze user-generated text, dignose mental health conditions, and generate personalized interventions and insights for mental health practitioners. Our approach emphasizes on the extraction of temporal information for accurate diagnosis and symptom progression tracking, together with psychological features to create comprehensive mental health summaries of users. The framework delivers scalable, customizable, and data-driven therapeutic recommendations, adaptable to diverse clinical contexts, patient needs, and workplace well-being programs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Cycle Ratio: Cost-Effective Ranking of Influential Spreaders from Local and Global Perspectives</title>
<link>https://arxiv.org/abs/2509.26220</link>
<guid>https://arxiv.org/abs/2509.26220</guid>
<content:encoded><![CDATA[
<div> influential spreaders, complex networks, node importance, Basic Cycle Ratio, information diffusion

Summary:
The study introduces a novel method called the Basic Cycle Ratio (BCR) for assessing the importance of nodes in spreading processes in complex networks. BCR combines basic cycles and the cycle ratio to capture a node's local significance within its neighborhood and its global role in network cohesion. The effectiveness of BCR was tested on six real-world social networks, outperforming traditional centrality measures and other cycle-based approaches. The results showed that BCR is more efficient in selecting powerful spreaders and enhancing information diffusion. Additionally, BCR offers a cost-effective and practical solution for social network applications. <div>
arXiv:2509.26220v1 Announce Type: new 
Abstract: Spreading processes are fundamental to complex networks. Identifying influential spreaders with dual local and global roles presents a crucial yet challenging task. To address this, our study proposes a novel method, the Basic Cycle Ratio (BCR), for assessing node importance. BCR leverages basic cycles and the cycle ratio to uniquely capture a node's local significance within its immediate neighborhood and its global role in maintaining network cohesion. We evaluated BCR on six diverse real-world social networks. Our method outperformed traditional centrality measures and other cycle-based approaches, proving more effective at selecting powerful spreaders and enhancing information diffusion. Besides, BCR offers a cost-effective and practical solution for social network applications.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cold-Start Active Correlation Clustering</title>
<link>https://arxiv.org/abs/2509.25376</link>
<guid>https://arxiv.org/abs/2509.25376</guid>
<content:encoded><![CDATA[
<div> Keywords: active correlation clustering, pairwise similarities, active learning, cold-start scenario, coverage-aware method

Summary: 
Active correlation clustering is studied in this research, focusing on the scenario where initial pairwise similarities are not available. The study introduces a coverage-aware method that prioritizes diversity early in the active learning process. This method efficiently queries pairwise similarities through active learning, enhancing the clustering process. The effectiveness of the proposed approach is substantiated through experiments conducted on both synthetic and real-world datasets. By addressing the challenge of cold-start scenarios, the research contributes to advancing active correlation clustering methods, offering a cost-efficient and effective solution for clustering tasks. <div>
arXiv:2509.25376v1 Announce Type: cross 
Abstract: We study active correlation clustering where pairwise similarities are not provided upfront and must be queried in a cost-efficient manner through active learning. Specifically, we focus on the cold-start scenario, where no true initial pairwise similarities are available for active learning. To address this challenge, we propose a coverage-aware method that encourages diversity early in the process. We demonstrate the effectiveness of our approach through several synthetic and real-world experiments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions</title>
<link>https://arxiv.org/abs/2509.25539</link>
<guid>https://arxiv.org/abs/2509.25539</guid>
<content:encoded><![CDATA[
<div> Keywords: toxic behavior, online content, Artificial Intelligence Systems, taxonomy, mitigation

Summary: 
Toxic behavior in digital communication systems and online platforms has led to the subconscious propagation of harmful content. This toxicity, whether expressed in language, image, or video, poses a significant challenge to individual and collective well-being globally. The article emphasizes the need for a comprehensive taxonomy to detect and proactively mitigate toxicity across various platforms and Artificial Intelligence systems. Existing literature on toxicity has primarily focused on reactive strategies, highlighting the necessity for a holistic understanding of toxicity in the modern AI era. The survey examines toxicity-related datasets and research efforts, particularly in English language-focused contexts, to address toxicity detection and mitigation. Lastly, the article identifies research gaps in toxicity mitigation, emphasizing the importance of datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation techniques. 

<br /><br />Summary: <div>
arXiv:2509.25539v1 Announce Type: cross 
Abstract: The evolution of digital communication systems and the designs of online platforms have inadvertently facilitated the subconscious propagation of toxic behavior. Giving rise to reactive responses to toxic behavior. Toxicity in online content and Artificial Intelligence Systems has become a serious challenge to individual and collective well-being around the world. It is more detrimental to society than we realize. Toxicity, expressed in language, image, and video, can be interpreted in various ways depending on the context of usage. Therefore, a comprehensive taxonomy is crucial to detect and mitigate toxicity in online content, Artificial Intelligence systems, and/or Large Language Models in a proactive manner. A comprehensive understanding of toxicity is likely to facilitate the design of practical solutions for toxicity detection and mitigation. The classification in published literature has focused on only a limited number of aspects of this very complex issue, with a pattern of reactive strategies in response to toxicity. This survey attempts to generate a comprehensive taxonomy of toxicity from various perspectives. It presents a holistic approach to explain the toxicity by understanding the context and environment that society is facing in the Artificial Intelligence era. This survey summarizes the toxicity-related datasets and research on toxicity detection and mitigation for Large Language Models, social media platforms, and other online platforms, detailing their attributes in textual mode, focused on the English language. Finally, we suggest the research gaps in toxicity mitigation based on datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFIRec: Towards High-Frequency yet Low-Intention Behaviors for Multi-Behavior Recommendation</title>
<link>https://arxiv.org/abs/2509.25755</link>
<guid>https://arxiv.org/abs/2509.25755</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-behavior recommendation, graph neural networks, user intention modeling, noise correction, adaptive feature fusion

Summary:
HiFIRec is a novel multi-behavior recommendation method designed to address data sparsity and cold-start issues by correcting the effects of high-frequency but low-intention behaviors. The proposed method utilizes a differential behavior modeling approach to extract neighborhood information and capture user intentions through adaptive cross-layer feature fusion. By hierarchically suppressing noisy signals and dynamically adjusting weights of negative samples using an intensity-aware non-sampling strategy, HiFIRec effectively corrects plausible but misleading frequent patterns. Experimental results on two benchmarks demonstrate that HiFIRec outperforms several state-of-the-art methods by improving HR@10 relative to existing approaches by 4.21%-6.81%. This innovative approach provides a promising solution for personalized recommendation systems in domains like healthcare and e-commerce.<br /><br />Summary: <div>
arXiv:2509.25755v1 Announce Type: cross 
Abstract: Multi-behavior recommendation leverages multiple types of user-item interactions to address data sparsity and cold-start issues, providing personalized services in domains such as healthcare and e-commerce. Most existing methods utilize graph neural networks to model user intention in a unified manner, which inadequately considers the heterogeneity across different behaviors. Especially, high-frequency yet low-intention behaviors may implicitly contain noisy signals, and frequent patterns that are plausible while misleading, thereby hindering the learning of user intentions. To this end, this paper proposes a novel multi-behavior recommendation method, HiFIRec, that corrects the effect of high-frequency yet low-intention behaviors by differential behavior modeling. To revise the noisy signals, we hierarchically suppress it across layers by extracting neighborhood information through layer-wise neighborhood aggregation and further capturing user intentions through adaptive cross-layer feature fusion. To correct plausible frequent patterns, we propose an intensity-aware non-sampling strategy that dynamically adjusts the weights of negative samples. Extensive experiments on two benchmarks show that HiFIRec relatively improves HR@10 by 4.21%-6.81% over several state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining higher-order triadic interactions</title>
<link>https://arxiv.org/abs/2404.14997</link>
<guid>https://arxiv.org/abs/2404.14997</guid>
<content:encoded><![CDATA[
<div> triadic interactions, Triadic Perceptron Model (TPM), Triadic Interaction Mining (TRIM), gene expression data, Acute Myeloid Leukemia <br />
Summary: <br />
The article proposes the Triadic Perceptron Model (TPM) to study triadic interactions in complex systems, highlighting their significance in various biological systems such as gene regulation. The Triadic Perceptron Model demonstrates that triadic interactions can modulate mutual information between interconnected nodes, leading to a more comprehensive understanding of system dynamics. The Triadic Interaction Mining (TRIM) algorithm is introduced to extract triadic interactions from node metadata, with applications in gene expression analysis. Utilizing this method on gene expression data, new potential triadic interactions relevant to Acute Myeloid Leukemia are identified, showcasing the practical implications of studying higher-order interactions. This study sheds light on previously overlooked aspects of triadic interactions, offering insights that can enhance our understanding of complex systems across different domains, from biology to climate science. <br /> <div>
arXiv:2404.14997v4 Announce Type: replace-cross 
Abstract: Complex systems often involve higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have so far been mostly neglected. In this article, we propose {the Triadic Perceptron Model (TPM)} that demonstrates that triadic interactions can modulate the mutual information between the dynamical state of two linked nodes. Leveraging this result, we formulate the Triadic Interaction Mining (TRIM) algorithm to extract triadic interactions from node metadata, and we apply this framework to gene expression data, finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions that are often ignored, yet can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to climate.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
<link>https://arxiv.org/abs/2504.21060</link>
<guid>https://arxiv.org/abs/2504.21060</guid>
<content:encoded><![CDATA[
<div> Keywords: government-led narratives, mass media, Innovation-Driven Development Strategy, total factor productivity, New Quality Productive Forces initiative

Summary: 

The study focuses on government-led narratives in mass media and their evolution from framing expectations to becoming pillars for sustainable growth. The "Narratives-Construct-Commitment (NCC)" framework is proposed to outline the process and institutionalization of narratives. Through a dynamic Bayesian game model, the study analyzes the impact of the Innovation-Driven Development Strategy (2016) as a case study, identifying narrative shocks and their effects on investment incentives and total factor productivity. The findings highlight the role of credible narratives in shaping expectations, driving resources into R&amp;D, and fostering economic growth through the New Quality Productive Forces initiative. <div>
arXiv:2504.21060v2 Announce Type: replace-cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the "Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrediBench: Building Web-Scale Network Datasets for Information Integrity</title>
<link>https://arxiv.org/abs/2509.23340</link>
<guid>https://arxiv.org/abs/2509.23340</guid>
<content:encoded><![CDATA[
<div> Keywords: Online misinformation, LLMs, web graphs, misinformation detection, credibility scores

Summary:
CrediBench introduces a large-scale data processing pipeline for constructing temporal web graphs that combine textual content and hyperlink structure for detecting misinformation. Unlike previous methods, CrediBench captures the dynamic evolution of misinformation domains, including changes in content and inter-site references over time. The dataset extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, making it the largest web graph dataset publicly available for misinformation research. Experiments on this dataset show the effectiveness of both structural and webpage content signals in learning credibility scores to measure source reliability. The pipeline and experimentation code are openly accessible, providing a valuable resource for further research in the field of misinformation detection. 

<br /><br />Summary: <div>
arXiv:2509.23340v1 Announce Type: new 
Abstract: Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Graph Embeddings and Louvain Algorithm for Unsupervised Community Detection</title>
<link>https://arxiv.org/abs/2509.23411</link>
<guid>https://arxiv.org/abs/2509.23411</guid>
<content:encoded><![CDATA[
<div> Keyword: community detection, Louvain algorithm, Graph Neural Networks (GNNs), node embeddings, merging algorithm<br />
Summary:<br />
This paper introduces a new method for community detection that combines the Louvain algorithm with Graph Neural Networks (GNNs), allowing for the identification of communities without prior knowledge. The method leverages GNN-generated node embeddings to capture more comprehensive structural and feature information, improving on the traditional Louvain algorithm. Additionally, a merging algorithm is introduced to refine the results and reduce the number of detected communities. This approach represents a novel use of GNNs to enhance community detection and has been validated on real-world datasets. The results show that the method can dynamically adjust the number of detected communities and improve detection accuracy compared to existing benchmarks.<br /><br />Summary: <div>
arXiv:2509.23411v1 Announce Type: new 
Abstract: This paper proposes a novel community detection method that integrates the Louvain algorithm with Graph Neural Networks (GNNs), enabling the discovery of communities without prior knowledge. Compared to most existing solutions, the proposed method does not require prior knowledge of the number of communities. It enhances the Louvain algorithm using node embeddings generated by a GNN to capture richer structural and feature information. Furthermore, it introduces a merging algorithm to refine the results of the enhanced Louvain algorithm, reducing the number of detected communities. To the best of our knowledge, this work is the first one that improves the Louvain algorithm using GNNs for community detection. The improvement of the proposed method was empirically confirmed through an evaluation on real-world datasets. The results demonstrate its ability to dynamically adjust the number of detected communities and increase the detection accuracy in comparison with the benchmark solutions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Node Classification via Simplicial Interaction with Augmented Maximal Clique Selection</title>
<link>https://arxiv.org/abs/2509.23568</link>
<guid>https://arxiv.org/abs/2509.23568</guid>
<content:encoded><![CDATA[
<div> Keywords: higher-order interactions, maximal cliques, network structures, computational efficiency, GNN-based learning

Summary:
The study introduces an augmented maximal clique approach for analyzing higher-order interactions in network structures. By selectively incorporating non-maximal cliques, the method aims to improve computational efficiency and reduce imbalanced training data. Comparative analyses on both synthetic and real-world datasets show that the proposed approach outperforms traditional methods based on pairwise interactions, all cliques, or only maximal cliques. Additionally, integrating the augmented maximal clique strategy into graph neural network (GNN)-based semi-supervised learning demonstrates enhanced predictive accuracy, indicating the potential benefits of incorporating higher-order structures in network learning. Overall, the augmented maximal clique strategy offers a valuable solution for effectively handling higher-order interactions in network analysis. 

<br /><br />Summary: <div>
arXiv:2509.23568v1 Announce Type: new 
Abstract: Considering higher-order interactions allows for a more comprehensive understanding of network structures beyond simple pairwise connections. While leveraging all cliques in a network to handle higher-order interactions is intuitive, it often leads to computational inefficiencies due to overlapping information between higher-order and lower-order cliques. To address this issue, we propose an augmented maximal clique strategy. Although using only maximal cliques can reduce unnecessary overlap and provide a concise representation of the network, certain nodes may still appear in multiple maximal cliques, resulting in imbalanced training data. Therefore, our augmented maximal clique approach selectively includes some non-maximal cliques to mitigate the overrepresentation of specific nodes and promote more balanced learning across the network. Comparative analyses on synthetic networks and real-world citation datasets demonstrate that our method outperforms approaches based on pairwise interactions, all cliques, or only maximal cliques. Finally, by integrating this strategy into GNN-based semi-supervised learning, we establish a link between maximal clique-based methods and GNNs, showing that incorporating higher-order structures improves predictive accuracy. As a result, the augmented maximal clique strategy offers a computationally efficient and effective solution for higher-order network learning.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASH: A Multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane</title>
<link>https://arxiv.org/abs/2509.23627</link>
<guid>https://arxiv.org/abs/2509.23627</guid>
<content:encoded><![CDATA[
<div> Dataset, societal impact, hurricanes, social media, multimodal annotation  
Summary:  
- The article introduces a new dataset called MASH that focuses on studying the societal impacts of hurricanes through social media posts.  
- MASH includes 98,662 social media data posts from platforms like Reddit, TikTok, and YouTube, providing a diverse perspective on hurricane impacts.  
- The dataset is unique in its approach, annotating posts in a multimodal manner that considers both text and visuals across various dimensions like humanitarian classes, bias classes, and information integrity classes.  
- MASH aims to contribute to research areas such as disaster severity classification, public sentiment analysis, disaster policy making, and bias identification related to hurricanes.  
- This dataset is a valuable resource for understanding the broader societal impact of natural disasters like hurricanes in today's diverse social media landscape.  
<br /><br />Summary: <div>
arXiv:2509.23627v1 Announce Type: new 
Abstract: Natural disasters cause multidimensional threats to human societies, with hurricanes exemplifying one of the most disruptive events that not only caused severe physical damage but also sparked widespread discussion on social media platforms. Existing datasets for studying societal impacts of hurricanes often focus on outdated hurricanes and are limited to a single social media platform, failing to capture the broader societal impact in today's diverse social media environment. Moreover, existing datasets annotate visual and textual content of the post separately, failing to account for the multimodal nature of social media posts. To address these gaps, we present a multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 98,662 relevant social media data posts from Reddit, X, TikTok, and YouTube. In addition, all relevant social media data posts are annotated in a multimodal approach that considers both textual and visual content on three dimensions: humanitarian classes, bias classes, and information integrity classes. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated hurricane dataset. We envision that MASH can contribute to the study of hurricanes' impact on society, such as disaster severity classification, public sentiment analysis, disaster policy making, and bias identification.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of One-to-Many Interdependent Higher-order Networks Against Cascading Failures</title>
<link>https://arxiv.org/abs/2509.23716</link>
<guid>https://arxiv.org/abs/2509.23716</guid>
<content:encoded><![CDATA[
<div> interdependent networks, higher-order structures, random attacks, percolation theory, network reliability
Summary:
This paper examines the robustness of one-to-many interdependent higher-order networks under random attacks. It introduces four inter-layer interdependency conditions based on the number of dependency edges needed for node survival and analyzes network resilience. By utilizing percolation theory, a unified theoretical framework is established to explore the impact of intra-layer higher-order structures and inter-layer coupling parameters on network reliability. The study extends to partially interdependent hypergraphs, providing insights for network design optimization. The analysis is validated on synthetic and real-data-based interdependent hypergraphs, shedding light on enhancing system resilience. <div>
arXiv:2509.23716v1 Announce Type: new 
Abstract: In the real world, the stable operation of a network is usually inseparable from the mutual support of other networks. In such an interdependent network, a node in one layer may depend on multiple nodes in another layer, forming a complex one-to-many dependency relationship. Meanwhile, there may also be higher-order interactions between multiple nodes within a layer, which increases the connectivity within the layer. However, existing research on one-to-many interdependence often neglects intra-layer higher-order structures and lacks a unified theoretical framework for inter-layer dependencies. Moreover, current research on interdependent higher-order networks typically assumes idealized one-to-one inter-layer dependencies, which does not reflect the complexity of real-world systems. These limitations hinder a comprehensive understanding of how such networks withstand failures. Therefore, this paper investigates the robustness of one-to-many interdependent higher-order networks under random attacks. Depending on whether node survival requires at least one dependency edge or multiple dependency edges, we propose four inter-layer interdependency conditions and analyze the network's robustness after cascading failures induced by random attacks. Using percolation theory, we establish a unified theoretical framework that reveals how higher-order interaction structures within intra-layers and inter-layer coupling parameters affect network reliability and system resilience. Additionally, we extend our study to partially interdependent hypergraphs. We validate our theoretical analysis on both synthetic and real-data-based interdependent hypergraphs, offering insights into the optimization of network design for enhanced reliability.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection robustness of graph neural networks</title>
<link>https://arxiv.org/abs/2509.24662</link>
<guid>https://arxiv.org/abs/2509.24662</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, community detection, robustness, perturbations, attacks
Summary:
- The study evaluates the robustness of six popular graph neural network architectures in community detection tasks.
- Supervised GNNs show higher baseline accuracy, while unsupervised methods like DMoN exhibit stronger resilience to perturbations and attacks.
- Community strength influences robustness, with well-defined communities leading to reduced performance loss.
- Node attribute perturbations, targeted edge deletions, and attribute distribution shifts have the most significant impact on community recovery.
- The findings emphasize the trade-offs between accuracy and robustness in GNN-based community detection and provide insights for selecting architectures resilient to noise and adversarial attacks. 

<br /><br />Summary: <div>
arXiv:2509.24662v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are increasingly widely used for community detection in attributed networks. They combine structural topology with node attributes through message passing and pooling. However, their robustness or lack of thereof with respect to different perturbations and targeted attacks in conjunction with community detection tasks is not well understood. To shed light into latent mechanisms behind GNN sensitivity on community detection tasks, we conduct a systematic computational evaluation of six widely adopted GNN architectures: GCN, GAT, Graph- SAGE, DiffPool, MinCUT, and DMoN. The analysis covers three perturbation categories: node attribute manipulations, edge topology distortions, and adversarial attacks. We use element-centric similarity as the evaluation metric on synthetic benchmarks and real-world citation networks. Our findings indicate that supervised GNNs tend to achieve higher baseline accuracy, while unsupervised methods, particularly DMoN, maintain stronger resilience under targeted and adversarial pertur- bations. Furthermore, robustness appears to be strongly influenced by community strength, with well-defined communities reducing performance loss. Across all models, node attribute perturba- tions associated with targeted edge deletions and shift in attribute distributions tend to cause the largest degradation in community recovery. These findings highlight important trade-offs between accuracy and robustness in GNN-based community detection and offer new insights into selecting architectures resilient to noise and adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discrete Geofence Design Using Binary Quadratic Programming</title>
<link>https://arxiv.org/abs/2509.24679</link>
<guid>https://arxiv.org/abs/2509.24679</guid>
<content:encoded><![CDATA[
<div> Keywords: geofences, human mobility data, arbitrary shapes, optimization problems, spatial information

Summary:
Geofences are spatial and virtual regions that monitor human activity and trigger events. Traditional circular geofences have limitations in flexibility and precision, especially in urban areas. This study introduces a method to extract arbitrary shapes as geofences from human mobility data. By formulating optimization problems as 0-1 integer programming and utilizing specialized solvers, such as quantum annealing, the approach enables the design of flexible and precise geofences. Different formulation methods were developed and compared to extract discrete geofences efficiently. The new modeling approach provides a solution to the geofence design problem, offering improved accuracy and alignment with political district boundaries and road structures. This advancement in geofence design has the potential to enhance the effectiveness of location-based services and spatially triggered events on mobile devices. 

<br /><br />Summary: <div>
arXiv:2509.24679v1 Announce Type: new 
Abstract: Geofences have attracted significant attention in the design of spatial and virtual regions for managing and engaging spatiotemporal events. By using geofences to monitor human activity across their boundaries, content providers can create spatially triggered events that include notifications about points of interest within a geofence by pushing spatial information to the devices of users. Traditionally, geofences were hand-crafted by providers. In addition to the hand-crafted approach, recent advances in collecting human mobility data through mobile devices can accelerate the automatic and data-driven design of geofences, also known as the geofence design problem. Previous approaches assume circular shapes; thus, their flexibility is insufficient, and they can only handle geofence-based applications for large areas with coarse resolutions. A challenge with using circular geofences in urban and high-resolution areas is that they often overlap and fail to align with political district boundaries and road segments, such as one-way streets and median barriers. In this study, we address the problem of extracting arbitrary shapes as geofences from human mobility data to mitigate this problem. In our formulation, we cast the existing optimization problems for circular geofences to 0-1 integer programming problems to represent arbitrary shapes. Although 0-1 integer programming problems are computationally hard, formulating them as quadratic (unconstrained) binary optimization problems enables efficient approximation of optimal solutions, because this allows the use of specialized quadratic solvers, such as the quantum annealing, and other state-of-the-art algorithms. We then develop and compare different formulation methods to extract discrete geofences. We confirmed that our new modeling approach enables flexible geofence design.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research in Medicine: Journal ranking vs. Interdisciplinarity</title>
<link>https://arxiv.org/abs/2509.24994</link>
<guid>https://arxiv.org/abs/2509.24994</guid>
<content:encoded><![CDATA[
<div> Keywords: interdisciplinary research, PubMed, medical concepts, impact factor, cancer.<br />
Summary: The study examines the interdisciplinary knowledge structure of PubMed research articles in medicine, comparing articles from high-ranking and less high-ranking medical journals. It finds that impactful journals tend to publish less interdisciplinary research. Cancer-related research plays a significant role in driving interdisciplinarity in medical science. The study also explores deviations in correlation networks between high and low impact journals, noting a mild tendency for strong link differences to be adjacent. Topic clusters of deviations shift over time, unlike the static topic clusters in the original networks. The findings suggest the importance of accommodating interdisciplinarity within existing infrastructures to maximize the benefits for patients.<br /><br />Summary: <div>
arXiv:2509.24994v1 Announce Type: new 
Abstract: Interdisciplinary research is critical for innovation and addressing complex societal issues. We characterise the interdisciplinary knowledge structure of PubMed research articles in medicine as correlation networks of medical concepts and compare the interdisciplinarity of articles between high-ranking (impactful) and less high-ranking (less impactful) medical journals. We found that impactful medical journals tend to publish research that are less interdisciplinary than less impactful journals. Observing that they bridge distant knowledge clusters in the networks, we find that cancer-related research can be seen as one of the main drivers of interdisciplinarity in medical science. Using signed difference networks, we also investigate the clustering of deviations between high and low impact journal correlation networks. We generally find a mild tendency for strong link differences to be adjacent. Furthermore, we find topic clusters of deviations that shift over time. In contrast, topic clusters in the original networks are static over time and can be seen as the core knowledge structure in medicine. Overall, journals and policymakers should encourage initiatives to accommodate interdisciplinarity within the existing infrastructures to maximise the potential patient benefits from IDR.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Inequality through Preferential Attachment, Triadic Closure, and Homophily</title>
<link>https://arxiv.org/abs/2509.23205</link>
<guid>https://arxiv.org/abs/2509.23205</guid>
<content:encoded><![CDATA[
<div> mechanisms, social networks, disparities, inequality, network growth <br />
Summary: <br />
The study explores how inequalities in social networks are shaped by linking mechanisms such as preferential attachment, homophily, and triadic closure. By developing the PATCH model, the researchers investigate the interplay of these mechanisms and their effects on degree inequalities and segregation within networks. The model's simulations reveal that homophily and preferential attachment contribute to increased segregation and degree disparities, while triadic closure moderates these effects by reducing segregation and between-group disparities. Applying the PATCH model to real-world collaboration and citation networks in Physics and Computer Science, the researchers demonstrate its ability to explain persistent gender disparities. The findings suggest that a combination of preferential attachment, moderate homophily, and triadic closure can sustain group inequalities in social networks. The study provides insights into how these mechanisms interact to perpetuate disparities and offers a framework for developing interventions to promote more equitable network structures. <br /> <div>
arXiv:2509.23205v1 Announce Type: cross 
Abstract: Inequalities in social networks arise from linking mechanisms, such as preferential attachment (connecting to popular nodes), homophily (connecting to similar others), and triadic closure (connecting through mutual contacts). While preferential attachment mainly drives degree inequality and homophily drives segregation, their three-way interaction remains understudied. This gap limits our understanding of how network inequalities emerge. Here, we introduce PATCH, a network growth model combining the three mechanisms to understand how they create disparities among two groups in synthetic networks. Extensive simulations confirm that homophily and preferential attachment increase segregation and degree inequalities, while triadic closure has countervailing effects: conditional on the other mechanisms, it amplifies population-wide degree inequality while reducing segregation and between-group degree disparities. We demonstrate PATCH's explanatory potential on fifty years of Physics and Computer Science collaboration and citation networks exhibiting persistent gender disparities. PATCH accounts for these gender disparities with the joint presence of preferential attachment, moderate gender homophily, and varying levels of triadic closure. By connecting mechanisms to observed inequalities, PATCH shows how their interplay sustains group disparities and provides a framework for designing interventions that promote more equitable social networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
<link>https://arxiv.org/abs/2509.23450</link>
<guid>https://arxiv.org/abs/2509.23450</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion, complex networks, cryptocurrency blockchains, infrastructure networks, network motifs

Summary:
This study explores how diffusion processes occur in complex networks, focusing on cryptocurrency blockchains and infrastructure networks. The topology and structural properties of these networks significantly impact the speed and scope of diffusion. By utilizing epidemic diffusion models like the Kertesz threshold model and the Susceptible-Infected (SI) model, the study analyzes key factors influencing diffusion dynamics. Bootstrap confidence intervals and Bayesian credible intervals are used to assess uncertainties in infected nodes and model parameters, respectively. The research highlights the importance of network motifs in diffusion and demonstrates that hub-dominated networks, common in blockchain ecosystems, exhibit resilience against random failures but are susceptible to targeted attacks. Additionally, centrality measures such as degree, betweenness, and clustering coefficient play a significant role in the transmissibility of diffusion within both blockchain and crucial infrastructure networks. <div>
arXiv:2509.23450v1 Announce Type: cross 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Analysis of Social Virtual Reality Based on Large-Scale Log Data of a Commercial Metaverse Platform</title>
<link>https://arxiv.org/abs/2509.23654</link>
<guid>https://arxiv.org/abs/2509.23654</guid>
<content:encoded><![CDATA[
<div> community structures, Social Virtual Reality, user communities, community hoppers, interaction data

Summary:
The study examines user communities in Social Virtual Reality platforms using interaction data analysis. It reveals that these platforms host multiple small communities with strong internal connections but limited inter-community links, unlike conventional social networks. The presence of community hoppers, users facilitating interactions between communities despite few direct connections, is identified. This sheds light on the unique community structures within Social VR and the diverse roles users play. <br /><br />Summary: <div>
arXiv:2509.23654v1 Announce Type: cross 
Abstract: This study quantitatively analyzes the structural characteristics of user communities within Social Virtual Reality (Social VR) platforms supporting head-mounted displays (HMDs), based on large-scale log data. By detecting and evaluating community structures from data on substantial interactions (defined as prolonged co-presence in the same virtual space), we found that Social VR platforms tend to host numerous, relatively small communities characterized by strong internal cohesion and limited inter-community connections. This finding contrasts with the large-scale, broadly connected community structures typically observed in conventional Social Networking Services (SNS). Furthermore, we identified a user segment capable of mediating between communities, despite these users not necessarily having numerous direct connections. We term this user segment `community hoppers' and discuss their characteristics. These findings contribute to a deeper understanding of the community structures that emerge within the unique communication environment of Social VR and the roles users play within them.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Homophily in Large Language Models</title>
<link>https://arxiv.org/abs/2509.23773</link>
<guid>https://arxiv.org/abs/2509.23773</guid>
<content:encoded><![CDATA[
<div> Graph Representation Learning, Large Language Models, Knowledge Homophily, Entity-Level Knowledgeability, Graph Neural Networks

Summary:
Large Language Models (LLMs) are neural knowledge bases used for applications like question answering. This study explores the structural organization of knowledge in LLMs, finding a homophily pattern where entities with closer proximity in the knowledge graph have similar levels of knowledge. A Graph Neural Network (GNN) regression model is proposed to estimate entity-level knowledgeability, prioritizing less known triplets for labeling. This approach improves efficiency in injecting knowledge into LLMs and enhances multi-hop path retrieval in question answering. <div>
arXiv:2509.23773v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Landscape of problematic papers in the field of non-coding RNA</title>
<link>https://arxiv.org/abs/2509.24511</link>
<guid>https://arxiv.org/abs/2509.24511</guid>
<content:encoded><![CDATA[
<div> Keywords: retractions, non-coding RNA (ncRNA), fraudulent publications, image duplication, evidence-based medicine<br />
<br />
Summary: 
The study examines the rise in retractions and concerns about the reliability of scientific research, focusing on non-coding RNA (ncRNA) as a case study. Research on under-investigated ncRNAs tends to produce problematic papers, often sharing significant textual similarity and displaying suspicious instances of image duplication. Healthcare institutions with low publication volumes are more likely to publish problematic papers. A limited number of journals are responsible for the majority of problematic papers, with many journals failing to adequately address the issue. The findings suggest that there may be numerous undetected problematic papers in the field. Understanding the characteristics of problematic papers provides valuable insights for developing strategies to tackle large-scale fraudulent publications in order to safeguard the credibility of evidence-based medicine.<br /><br />Summary: <div>
arXiv:2509.24511v1 Announce Type: cross 
Abstract: In recent years, the surge in retractions has been accompanied by numerous papers receiving comments that raise concerns about their reliability. The prevalence of problematic papers undermines the reliability of scientific research and threatens the foundation of evidence-based medicine. In this study,we focus on the field of non-coding RNA(ncRNA) as a case study to explore the typical characteristics of problematic papers from various perspectives, aiming to provide insights for addressing large-scale fraudulent publications. Research on under-investigated ncRNAs is more likely to yield problematic papers. These problematic papers often exhibit significant textual similarity, and many others sharing this similarity also display suspicious instances of image duplication. Healthcare institutions are particularly prone to publishing problematic papers, especially those with a low publication volume. Most problematic papers are found in a limited number of journals, and many journals inadequately address the commented papers. Our findings suggest that numerous problematic papers may still remain unidentified. The revealed characteristics offer valuable insights for formulating strategies to address the issue of fraudulent papers at scale.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric Representations of Network Data</title>
<link>https://arxiv.org/abs/1911.00164</link>
<guid>https://arxiv.org/abs/1911.00164</guid>
<content:encoded><![CDATA[
<div> Keywords: networks, q-metric spaces, projections, metric trees, hierarchical clustering

Summary:
In this paper, the authors explore the projection of networks into q-metric spaces, a generalization of metric spaces. They introduce axioms to guide the projection process, ensuring consistency and optimality. The projection method, which involves computing shortest paths between nodes, is shown to have applications in combinatorial optimization and hierarchical clustering. The authors demonstrate the efficiency of using metric projections to search networks, particularly with the assistance of metric trees. Overall, the study provides a structured approach to representing network relationships in q-metric spaces, offering a versatile tool for analyzing and visualizing network data. The method's optimality and nestedness properties enhance its practical utility, making it a valuable technique for various network analysis tasks. <div>
arXiv:1911.00164v2 Announce Type: replace 
Abstract: Networks are structures that encode relationships between pairs of elements or nodes. However, there is no imposed connection between these relationships, i.e., the relationship between two nodes can be independent of every other one in the network, and need not be defined for every possible pair of nodes. This is not true for metric spaces, where the triangle inequality imposes conditions that must be satisfied by triads of distances in the space, and these distances must be defined for every pair of nodes. In this paper, we study how to project networks into q-metric spaces, a generalization of metric spaces that encompasses a larger class of structured representations. In order to do this, we encode as axioms two intuitively desirable properties of the mentioned projections. We show that there is only one way of projecting networks onto q-metric spaces satisfying these axioms. Moreover, for the special case of (regular) metric spaces, this method boils down to computing the shortest path between every node and, for the case of ultrametric spaces, it coincides with single linkage hierarchical clustering. Furthermore, we show that the projection method satisfies two properties of practical relevance: optimality, which enables its utilization for the efficient estimation of combinatorial optimization problems, and nestedness, which entails consistency of the structure induced when projecting onto different q-metric spaces. Finally, we illustrate how metric projections can be used to efficiently search networks aided by metric trees.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of the Nostr Social Network: Decentralization, Availability, and Replication Overhead</title>
<link>https://arxiv.org/abs/2402.05709</link>
<guid>https://arxiv.org/abs/2402.05709</guid>
<content:encoded><![CDATA[
<div> decentralized social network, censorship resistance, relays, post replication, Nostr
Summary:
Nostr is a decentralized social network that prioritizes high availability and censorship resistance, boasting over 100 million posts since its 2022 launch. The unique infrastructure utilizes relays as open storage servers to store and distribute user posts, ensuring authenticity through digital signatures. While Nostr offers superior decentralization compared to traditional platforms, challenges like relay availability and post replication overhead persist. Financial sustainability for free relays is a key concern impacting relay availability. The replication of posts across relays enhances censorship resistance but introduces significant overhead. To address these challenges, two proposed improvements aim to control the number of post replications and reduce retrieval overhead. Data-driven evaluations show promise in reducing overhead without compromising post availability in simulated scenarios. <div>
arXiv:2402.05709v2 Announce Type: replace 
Abstract: Nostr is a decentralized social network launched in 2022, emphasizing high availability and censorship resistance. Since launching, it has gained substantial attention, boasting over 100 million posts. Nostr resembles a micro-blogging service like Twitter but with distinct underlying infrastructure. Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts. Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures. Users are able to securely replicate and retrieve posts through multiple relays, which frees them from single-server reliance and enhances post availability, thereby attempting to make Nostr censorship resistant. However, this aggressive design also presents challenges, such as the overhead required for extensive post replication and the difficulty in obtaining a global view of post replication locations, which remain unexplored or unaddressed. This necessitates a thorough understanding of the Nostr ecosystem; therefore, we conduct the first large-scale study on this topic. Our study focuses on two key aspects: Nostr relays and post replication strategies. We find that Nostr achieves superior decentralization compared to traditional Fediverse applications. However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor. We also find that the replication of posts across relays enhances censorship-resistance but introduces significant overhead. To address this, we propose two improvements: one to control the number of post replications, and another to reduce the overhead during post retrieval. Via a data-driven evaluation, we demonstrate their ability to reduce overhead without negatively impacting post availability under the simulated scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network inference via process motifs for lagged correlation in linear stochastic processes</title>
<link>https://arxiv.org/abs/2208.08871</link>
<guid>https://arxiv.org/abs/2208.08871</guid>
<content:encoded><![CDATA[
<div> Keywords: causal inference, time-series data, pairwise edge measure, network inference, linear stochastic processes

Summary:
Pairwise edge measures (PEMs) are proposed for inferring causal networks from time-series data, addressing the trade-off between computational feasibility and accuracy. The PEMs utilize lagged correlation matrices to correct for confounding factors and reverse causation, drawing on process motifs for lagged covariance in autoregressive models. Simulation results show that these PEMs can accurately and efficiently infer networks, outperforming or matching methods like Granger causality and transfer entropy in slightly autocorrelated data sets. The approach offers a fast and theoretically grounded alternative for network inference, potentially informing linear model inference paradigms like Granger causality and vector-autoregression. <div>
arXiv:2208.08871v3 Announce Type: replace-cross 
Abstract: A major challenge for causal inference from time-series data is the trade-off between computational feasibility and accuracy. Motivated by process motifs for lagged covariance in an autoregressive model with slow mean-reversion, we propose to infer networks of causal relations via pairwise edge measure (PEMs) that one can easily compute from lagged correlation matrices. Motivated by contributions of process motifs to covariance and lagged variance, we formulate two PEMs that correct for confounding factors and for reverse causation. To demonstrate the performance of our PEMs, we consider network interference from simulations of linear stochastic processes, and we show that our proposed PEMs can infer networks accurately and efficiently. Specifically, for slightly autocorrelated time-series data, our approach achieves accuracies higher than or similar to Granger causality, transfer entropy, and convergent crossmapping -- but with much shorter computation time than possible with any of these methods. Our fast and accurate PEMs are easy-to-implement methods for network inference with a clear theoretical underpinning. They provide promising alternatives to current paradigms for the inference of linear models from time-series data, including Granger causality, vector-autoregression, and sparse inverse covariance estimation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Encoding for Improved Representation Learning over Graphs</title>
<link>https://arxiv.org/abs/2407.02758</link>
<guid>https://arxiv.org/abs/2407.02758</guid>
<content:encoded><![CDATA[
<div> Keywords: message-passing, global attention, graph learning, node embeddings, information aggregation

Summary: 
The paper introduces a novel approach called the differential encoding method to enhance graph learning by addressing the issue of information loss during node embedding generation. By encoding the difference between information from a node's neighbors and the node itself, the method improves the representational ability of generated node embeddings. The differential encoding is combined with the original aggregated representation to update the node embeddings, leading to improved performance on various graph tasks. Empirical evaluations on seven benchmark datasets demonstrate the effectiveness of the method in enhancing both message-passing updates and global attention updates. Overall, the differential encoding method presents a general solution for improving graph representation learning, advancing the state-of-the-art performance in this field. 

<br /><br />Summary: <div>
arXiv:2407.02758v2 Announce Type: replace-cross 
Abstract: Combining the message-passing paradigm with the global attention mechanism has emerged as an effective framework for learning over graphs. The message-passing paradigm and the global attention mechanism fundamentally generate node embeddings based on information aggregated from a node's local neighborhood or from the whole graph. The most basic and commonly used aggregation approach is to take the sum of information from a node's local neighbourhood or from the whole graph. However, it is unknown if the dominant information is from a node itself or from the node's neighbours (or the rest of the graph nodes). Therefore, there exists information lost at each layer of embedding generation, and this information lost could be accumulated and become more serious when more layers are used in the model. In this paper, we present a differential encoding method to address the issue of information lost. The idea of our method is to encode the differential representation between the information from a node's neighbours (or the rest of the graph nodes) and that from the node itself. The obtained differential encoding is then combined with the original aggregated local or global representation to generate the updated node embedding. By integrating differential encodings, the representational ability of generated node embeddings is improved. The differential encoding method is empirically evaluated on different graph tasks on seven benchmark datasets. The results show that it is a general method that improves the message-passing update and the global attention update, advancing the state-of-the-art performance for graph representation learning on these datasets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniTraj: Learning a Universal Trajectory Foundation Model from Billion-Scale Worldwide Traces</title>
<link>https://arxiv.org/abs/2411.03859</link>
<guid>https://arxiv.org/abs/2411.03859</guid>
<content:encoded><![CDATA[
<div> dataset, trajectory, modeling, pre-training strategies, architecture

Summary:
UniTraj is introduced as a Universal Trajectory foundation model to overcome limitations of existing trajectory modeling approaches. The model addresses task specificity, regional dependency, and data sensitivity through three key innovations. Firstly, WorldTrace dataset is created with 2.45 million trajectories spanning 70 countries for region-independent modeling. Novel pre-training strategies, Adaptive Trajectory Resampling and Self-supervised Trajectory Masking, enable robust learning from heterogeneous trajectory data. A flexible model architecture is tailored to capture complex movement patterns for various trajectory tasks, ensuring broad applicability. Extensive experiments demonstrate that UniTraj outperforms existing methods in scalability, adaptability, and generalization, with WorldTrace as a versatile training resource. <br /><br />Summary: <div>
arXiv:2411.03859v3 Announce Type: replace-cross 
Abstract: Building a universal trajectory foundation model is a promising solution to address the limitations of existing trajectory modeling approaches, such as task specificity, regional dependency, and data sensitivity. Despite its potential, data preparation, pre-training strategy development, and architectural design present significant challenges in constructing this model. Therefore, we introduce UniTraj, a Universal Trajectory foundation model that aims to address these limitations through three key innovations. First, we construct WorldTrace, an unprecedented dataset of 2.45 million trajectories with billions of GPS points spanning 70 countries, providing the diverse geographic coverage essential for region-independent modeling. Second, we develop novel pre-training strategies--Adaptive Trajectory Resampling and Self-supervised Trajectory Masking--that enable robust learning from heterogeneous trajectory data with varying sampling rates and quality. Finally, we tailor a flexible model architecture to accommodate a variety of trajectory tasks, effectively capturing complex movement patterns to support broad applicability. Extensive experiments across multiple tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing methods, exhibiting superior scalability, adaptability, and generalization, with WorldTrace serving as an ideal yet non-exclusive training resource.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Haar-Laplacian for directed graphs</title>
<link>https://arxiv.org/abs/2411.15527</link>
<guid>https://arxiv.org/abs/2411.15527</guid>
<content:encoded><![CDATA[
<div> Keywords: Laplacian matrix, spectral convolutional networks, directed graphs, Haar-like transformation, graph signal processing

Summary: 
This paper presents a novel Laplacian matrix design that allows for the development of spectral convolutional networks and expands the application of signal processing to directed graphs. Inspired by a Haar-like transformation, the proposed matrix is Hermitian and maintains a one-to-one relationship with the adjacency matrix, preserving both direction and weight information. It also exhibits properties such as scaling robustness, sensitivity, continuity, and directionality. The theoretical framework aligns with spectral graph theory, supporting the effectiveness of the approach. The paper discusses two main applications: graph learning, demonstrated through the implementation of HaarNet, a spectral graph convolutional network utilizing the Haar-Laplacian, and graph signal processing. Experimental results indicate improved performance in tasks like weight prediction and denoising on directed graphs. This innovative approach holds promise for enhancing signal processing capabilities on directed graph structures.<br /><br />Summary: <div>
arXiv:2411.15527v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel Laplacian matrix aiming to enable the construction of spectral convolutional networks and to extend the signal processing applications for directed graphs. Our proposal is inspired by a Haar-like transformation and produces a Hermitian matrix which is not only in one-to-one relation with the adjacency matrix, preserving both direction and weight information, but also enjoys desirable additional properties like scaling robustness, sensitivity, continuity, and directionality. We take a theoretical standpoint and support the conformity of our approach with the spectral graph theory. Then, we address two use-cases: graph learning (by introducing HaarNet, a spectral graph convolutional network built with our Haar-Laplacian) and graph signal processing. We show that our approach gives better results in applications like weight prediction and denoising on directed graphs.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Aware Opinion Dynamics in Multi-Agents Systems under Malicious Agent Influence</title>
<link>https://arxiv.org/abs/2412.01524</link>
<guid>https://arxiv.org/abs/2412.01524</guid>
<content:encoded><![CDATA[
<div> consensus mechanisms, multi-agent systems, Boomerang Effect, malicious agents, evolution rate adjustment 

Summary: 
This brief addresses the vulnerability of MAS to malicious agents by leveraging the Boomerang Effect from sociology. It highlights the trade-off between cost and convergence speed in implementing a resistance strategy against harmful deviations. The Boomerang-style fusion is proposed as a solution, analyzing the additional costs and introducing a cost-aware evolution rate adjustment mechanism. Through multi-robot simulations, it is shown that this mechanism effectively suppresses excess costs while maintaining resilience to extremist disruptions, ensuring stable convergence and enabling MAS to efficiently develop in an ethical order. <div>
arXiv:2412.01524v3 Announce Type: replace-cross 
Abstract: In many MASs, links to malicious agents cannot be severed immediately. Under these conditions, averaging-only consensus mechanisms typically lack sufficient resistance, leaving the system vulnerable to harmful deviations. To address this challenge, this brief leverages the Boomerang Effect from sociology, which drives normal agents to firmly reject malicious inputs, although this strategy may appear overly cautious. Thus, this brief emphasizes the necessity of acknowledging the resulting trade-off between cost and convergence speed in practice. To address this, the additional costs induced by Boomerang-style fusion is analyzed and a cost aware evolution rate adjustment mechanism is proposed. Multi-robot simulations demonstrate that this mechanism suppresses excess costs while maintaining resilience to extremist disruptions and ensuring stable convergence, enabling MAS to efficiently develop in a ethical order.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study</title>
<link>https://arxiv.org/abs/2501.01779</link>
<guid>https://arxiv.org/abs/2501.01779</guid>
<content:encoded><![CDATA[
<div> Keywords: gym attendance, habit formation, survival metric, personalized guidance, social dynamics 

Summary: 
This study examines gym attendance data to understand the factors influencing habit formation in exercise. The research identifies key periods critical for habit formation and categorizes members into clusters based on their visit patterns. Differences in response to interventions, such as group classes and personal training, are observed among subgroups. Causal inference analysis indicates that personalized guidance and social interactions play crucial roles in sustaining long-term engagement. The study emphasizes the importance of a tailored, multi-dimensional approach that integrates social dynamics, personalized guidance, and strategic interventions to promote consistent exercise habits. <div>
arXiv:2501.01779v2 Announce Type: replace-cross 
Abstract: Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order shortest paths in hypergraphs</title>
<link>https://arxiv.org/abs/2502.03020</link>
<guid>https://arxiv.org/abs/2502.03020</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, hypergraphs, path size, higher-order connectivity, empirical networks

Summary: 
This study delves into the connectivity properties of complex networks by utilizing hypergraphs to model non-dyadic interactions. The concept of path size is introduced to characterize higher-order connectivity in various empirical networks, both with and without temporal data. The analysis reveals that non-dyadic ties play a crucial role in efficient shortest paths, proving their significance in system connectivity. Furthermore, dyadic edges are essential for connecting peripheral nodes, particularly in time-varying systems. By comparing the findings with randomised null models, a nuanced understanding of the structural organization of networks with higher-order interactions is achieved. This research enhances our knowledge of how different types of connections contribute to the overall connectivity and efficiency of complex systems.<br /><br />Summary: <div>
arXiv:2502.03020v3 Announce Type: replace-cross 
Abstract: One of the defining features of complex networks is the connectivity properties that we observe emerging from local interactions. Recently, hypergraphs have emerged as a versatile tool to model networks with non-dyadic, higher-order interactions. Nevertheless, the connectivity properties of real-world hypergraphs remain largely understudied. In this work we introduce path size as a measure to characterise higher-order connectivity and quantify the relevance of non-dyadic ties for efficient shortest paths in a diverse set of empirical networks with and without temporal information. By comparing our results with simple randomised null models, our analysis presents a nuanced picture, suggesting that non-dyadic ties are often central and are vital for system connectivity, while dyadic edges remain essential to connect more peripheral nodes, an effect which is particularly pronounced for time-varying systems. Our work contributes to a better understanding of the structural organisation of systems with higher-order interactions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the Herd: A Framework for LLM-based Control of Social Learning</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
<div> information mediators, social learning, algorithms, Bayesian belief updates, social welfare

Summary: 
This study introduces a model of controlled sequential social learning where an information-mediating planner influences agents' decisions while they learn from peers. The planner can be altruistic, aiming for social welfare, or biased, aiming to induce a specific action. The study proves the convexity of the value function and characterizes optimal policies for both types of planners, showing intentional obfuscation of signals by biased planners in some cases. Even under transparency constraints, information mediation can significantly impact social welfare. Simulations with LLMs as planners and agents demonstrate emergent strategic behavior in shaping public opinion, with deviations suggesting non-Bayesian reasoning. The framework provides a basis for studying the impact and regulation of LLM information mediators. <div>
arXiv:2504.02648v3 Announce Type: replace-cross 
Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.
  We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message passing for epidemiological interventions on networks with loops</title>
<link>https://arxiv.org/abs/2509.21596</link>
<guid>https://arxiv.org/abs/2509.21596</guid>
<content:encoded><![CDATA[
<div> message passing, spreading models, intervention design, neighborhood message passing, outbreak size

Summary:
The article introduces a study on spreading models in networks, focusing on design intervention problems such as vaccination rollouts and wastewater surveillance. The approach involves message passing for computing outcomes under various counterfactuals. However, classical message passing tends to overestimate outbreak sizes on real-world networks, leading to inaccurate predictions. To address this issue, the neighborhood message passing (NMP) framework is proposed for more accurate epidemiological calculations. The improved algorithm enhances estimates and is evaluated for intervention design problems like influence maximization, optimal vaccination, and sentinel surveillance. This advancement in spreading models can provide better insights for designing effective interventions in scenarios such as disease control and information diffusion. 

<br /><br />Summary: <div>
arXiv:2509.21596v1 Announce Type: new 
Abstract: Spreading models capture key dynamics on networks, such as cascading failures in economic systems, (mis)information diffusion, and pathogen transmission. Here, we focus on design intervention problems -- for example, designing optimal vaccination rollouts or wastewater surveillance systems -- which can be solved by comparing outcomes under various counterfactuals. A leading approach to computing these outcomes is message passing, which allows for the rapid and direct computation of the marginal probabilities for each node. However, despite its efficiency, classical message passing tends to overestimate outbreak sizes on real-world networks, leading to incorrect predictions and, thus, interventions. Here, we improve these estimates by using the neighborhood message passing (NMP) framework for the epidemiological calculations. We evaluate the quality of the improved algorithm and demonstrate how it can be used to test possible solutions to three intervention design problems: influence maximization, optimal vaccination, and sentinel surveillance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributed Hypergraph Generation with Realistic Interplay Between Structure and Attributes</title>
<link>https://arxiv.org/abs/2509.21838</link>
<guid>https://arxiv.org/abs/2509.21838</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, generative models, node attributes, NoAH, structure-attribute interplay

Summary: 
NoAH is a stochastic hypergraph generative model designed specifically for attributed hypergraphs. It utilizes a core-fringe node hierarchy to model hyperedge formation in a structured way, taking into account node attributes. The model, along with the NoAHFit parameter learning procedure, can accurately replicate the structure-attribute interplay observed in real-world hypergraphs. Through experiments on nine datasets spanning different domains, NoAH outperforms eight baseline models in terms of six metrics, showcasing its effectiveness in capturing the interactions between hypergraph structure and node attributes. <div>
arXiv:2509.21838v1 Announce Type: new 
Abstract: In many real-world scenarios, interactions happen in a group-wise manner with multiple entities, and therefore, hypergraphs are a suitable tool to accurately represent such interactions. Hyperedges in real-world hypergraphs are not composed of randomly selected nodes but are instead formed through structured processes. Consequently, various hypergraph generative models have been proposed to explore fundamental mechanisms underlying hyperedge formation. However, most existing hypergraph generative models do not account for node attributes, which can play a significant role in hyperedge formation. As a result, these models fail to reflect the interactions between structure and node attributes. To address the issue above, we propose NoAH, a stochastic hypergraph generative model for attributed hypergraphs. NoAH utilizes the core-fringe node hierarchy to model hyperedge formation as a series of node attachments and determines attachment probabilities based on node attributes. We further introduce NoAHFit, a parameter learning procedure that allows NoAH to replicate a given real-world hypergraph. Through experiments on nine datasets across four different domains, we show that NoAH with NoAHFit more accurately reproduces the structure-attribute interplay observed in the real-world hypergraphs than eight baseline hypergraph generative models, in terms of six metrics.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Agent-based Modeling with Large Language Model Agents via Shachi</title>
<link>https://arxiv.org/abs/2509.21862</link>
<guid>https://arxiv.org/abs/2509.21862</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, multi-agent systems, cognitive architecture, controlled experimentation, U.S. tariff shock<br />
<br />
Summary: 
The study introduces Shachi, a formal methodology and framework for analyzing emergent behaviors in large language model-driven multi-agent systems. It decomposes agents' policies into core components - Configuration, Memory, and Tools - orchestrated by an LLM reasoning engine. This architecture enables systematic analysis of how design choices influence collective behavior. Validation on a 10-task benchmark demonstrates the methodology's power and applicability through novel scientific inquiries. The methodology is further validated by modeling a real-world U.S. tariff shock, where agent behaviors align with observed market reactions when appropriately configured with memory and tools. This work provides an open-source foundation for constructing and evaluating LLM agents, promoting cumulative and scientifically grounded research in this field.<br /><br /> <div>
arXiv:2509.21862v1 Announce Type: cross 
Abstract: The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modularity and random graphs</title>
<link>https://arxiv.org/abs/2509.22066</link>
<guid>https://arxiv.org/abs/2509.22066</guid>
<content:encoded><![CDATA[
<div> modularity, graph theory, community detection, random graphs, binomial random graph <br />
<br />
Summary: 
This chapter discusses modularity in graph theory, particularly focusing on its application in community detection algorithms. Modularity measures the quality of vertex-partitions in capturing the community structure of a graph, with higher scores indicating better partitioning. The maximum modularity score, denoted as $q^*(G)$, ranges from 0 to 1. The chapter explores the behavior of modularity in various types of random graphs, starting with the binomial random graph $G_{n,p}$ characterized by its number of vertices and edge-probability $p$. Understanding modularity in random graphs is crucial for assessing community structures and optimizing community detection algorithms. <div>
arXiv:2509.22066v1 Announce Type: cross 
Abstract: This work will appear as a chapter in a forthcoming volume titled `Topics in Probabilistic Graph Theory'.
  For a given graph $G$, each partition of the vertices has a modularity score, with higher values indicating that the partition better captures community structure in $G$. The modularity $q^*(G)$ of $G$ is the maximum over all vertex-partitions of the modularity score, and satisfies $0\leq q^*(G)< 1$. Modularity lies at the heart of the most popular algorithms for community detection. In this chapter we discuss the behaviour of the modularity of various kinds of random graphs, starting with the binomial random graph $G_{n,p}$ with $n$ vertices and edge-probability $p$.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Internet of Us</title>
<link>https://arxiv.org/abs/2503.16448</link>
<guid>https://arxiv.org/abs/2503.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Media, Internet, Diversity, IoU, Human-AI Society

Summary:
The paper discusses the potential of social media and the internet in exposing us to a wide range of human diversity in terms of demographics, language, culture, knowledge, opinions, and talents. This diversity presents opportunities to tap into skills and competences that may be unknown to us but could be crucial in solving our problems. However, despite this potential, there is a lack of tools and skills to effectively harness the complexities that come with exploiting diversity. The concept of the Internet of Us (IoU) is introduced as a new approach to online interactions that are diversity-aware. By understanding and leveraging the various facets of diversity in social settings, the IoU aims to create deeper and more meaningful social relationships. The paper emphasizes the need for multidisciplinary collaboration to fully realize the benefits of the IoU and create a hybrid human-AI society that is diversity-aware. 

<br /><br />Summary: <div>
arXiv:2503.16448v2 Announce Type: replace-cross 
Abstract: Social Media and the Internet have catalyzed an unprecedented potential for exposure to human diversity in terms of demographics, language, culture, knowledge, opinions, talents and the like. Access to people's diversity gives us the possibility of exploiting skills and competences that we do not have, that we may not even know they exist, the so called unknown unknowns, but which, however, could be exactly what we need when looking for help in the solution of the our current problem. However, this potential has not come with new, much needed, instruments and skills to harness the complications which rise when trying to exploit the diversity of people. This paper presents our vision of the Internet of Us (IoU), a new type of online diversity-aware social interactions capable of promoting richer and deeper social relations. We discuss the multiple facets of diversity in social settings and the multidisciplinary work that is required to reap the benefits of the IoU, towards a IoU-enabled diversity-aware hybrid human-AI society.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos</title>
<link>https://arxiv.org/abs/2509.20724</link>
<guid>https://arxiv.org/abs/2509.20724</guid>
<content:encoded><![CDATA[
<div> Keywords: short form video platforms, credibility, nutrition, supplements, authority signals

Summary: 
Short form video platforms like TikTok, Instagram, and YouTube are popular sources of health advice, where a mix of helpful and misleading content can be found. This study focuses on how credibility is conveyed in nutrition and supplement videos through authority signals, narrative techniques, and monetization strategies. Analysis of 152 public videos reveals that confident single presenters in studio or home settings are common, with authority cues like titles, slides, and certificates often used. Persuasive elements such as jargon, references, fear tactics, and critiques of mainstream medicine are also prevalent, along with monetization features like sales links and subscription requests. References to science are often paired with emotional or oppositional narratives instead of signaling caution. Clinical contexts are infrequent, highlighting the dominance of persuasive and emotional storytelling in these videos. <div>
arXiv:2509.20724v1 Announce Type: new 
Abstract: Short form video platforms are central sites for health advice, where alternative narratives mix useful, misleading, and harmful content. Rather than adjudicating truth, this study examines how credibility is packaged in nutrition and supplement videos by analyzing the intersection of authority signals, narrative techniques, and monetization. We assemble a cross platform corpus of 152 public videos from TikTok, Instagram, and YouTube and annotate each on 26 features spanning visual authority, presenter attributes, narrative strategies, and engagement cues. A transparent annotation pipeline integrates automatic speech recognition, principled frame selection, and a multimodal model, with human verification on a stratified subsample showing strong agreement. Descriptively, a confident single presenter in studio or home settings dominates, and clinical contexts are rare. Analytically, authority cues such as titles, slides and charts, and certificates frequently occur with persuasive elements including jargon, references, fear or urgency, critiques of mainstream medicine, and conspiracies, and with monetization including sales links and calls to subscribe. References and science like visuals often travel with emotive and oppositional narratives rather than signaling restraint.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity</title>
<link>https://arxiv.org/abs/2509.20762</link>
<guid>https://arxiv.org/abs/2509.20762</guid>
<content:encoded><![CDATA[
<div> identification, group anchors, AnchorRadar, real-world interactions, semi-supervised <br />
Summary: <br />
This article discusses the concept of group anchors in various real-world interactions and the problem of identifying them. It introduces AnchorRadar, a semi-supervised method designed to identify group anchors in scenarios with limited labeled data. The study showcases the empirical success of AnchorRadar compared to other baselines, demonstrating higher accuracy and efficiency. The method outperforms competitors in accuracy while requiring significantly less training time and fewer parameters. Through experiments on thirteen datasets, the effectiveness of AnchorRadar in identifying group anchors is highlighted, emphasizing its relevance in practical applications with limited labeled data. <div>
arXiv:2509.20762v1 Announce Type: new 
Abstract: Group interactions occur in various real-world contexts, e.g., co-authorship, email communication, and online Q&amp;A. In each group, there is often a particularly significant member, around whom the group is formed. Examples include the first or last author of a paper, the sender of an email, and the questioner in a Q&amp;A session. In this work, we discuss the existence of such individuals in real-world group interactions. We call such individuals group anchors and study the problem of identifying them. First, we introduce the concept of group anchors and the identification problem. Then, we discuss our observations on group anchors in real-world group interactions. Based on our observations, we develop AnchorRadar, a fast and effective method for group anchor identification under realistic settings with label scarcity, i.e., when only a few groups have known anchors. AnchorRadar is a semi-supervised method using information from groups both with and without known group anchors. Finally, through extensive experiments on thirteen real-world datasets, we demonstrate the empirical superiority of AnchorRadar over various baselines w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher accuracy in group anchor identification than all the baselines, while using 10.2$\times$ less training time than the fastest baseline and 43.6$\times$ fewer learnable parameters than the most lightweight baseline on average.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of the majority group on individual judgments in online spontaneous conversations</title>
<link>https://arxiv.org/abs/2509.21092</link>
<guid>https://arxiv.org/abs/2509.21092</guid>
<content:encoded><![CDATA[
<div> Keywords: social conformity, anti-conformity, online conversations, judgment formation, linguistic analysis

Summary: 
This study examines the influence of the majority group on individual judgment formation and expression in anonymous online conversations. By analyzing digital traces of conversations on social media platforms, the researchers found that individuals tend towards anti-conformity behaviors. While they align their judgments with the majority's orientation, they diverge from the group's stance, using persuasive language to express their opinions. The study highlights how online environments shape social influence differently from offline contexts, showcasing a dynamic interplay between conformity and anti-conformity in online interactions. The findings suggest that individuals navigate social pressures in online discussions by balancing between conformity and asserting their unique perspectives. This research contributes to understanding the complexities of group dynamics in digital spaces and sheds light on the nuanced ways individuals form and express judgments in online environments.<br /><br />Summary: <div>
arXiv:2509.21092v1 Announce Type: new 
Abstract: This study investigates how the majority group influences individual judgment formation and expression in anonymous, spontaneous online conversations. Drawing on theories of social conformity and anti-conformity, we analyze everyday dilemmas discussed on social media. First, using digital traces to operationalize judgments, we measure the conversations' disagreement and apply Bayesian regression to capture shifts of judgments formation before and after the group's exposure. Then we analyze changes in judgment expression with a linguistic analysis of the motivations associated with each judgment. Results show systematic anti-conformity behaviors: individuals preserve the majority's positive or negative orientation of judgments but diverge from its stance, with persuasive language increasing post-disclosure. Our findings highlight how online environments reshape social influence compared to offline contexts.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enhanced Multi-Dimensional Measurement of Technological Convergence through Heterogeneous Graph and Semantic Learning</title>
<link>https://arxiv.org/abs/2509.21187</link>
<guid>https://arxiv.org/abs/2509.21187</guid>
<content:encoded><![CDATA[
<div> Technological Convergence Index, depth, breadth, Heterogeneous Graph Transformers, Shannon Diversity Index

Summary:<br /><br />Technological convergence, where boundaries between technology areas blur, is a key trend in innovation. Accurately measuring convergence is challenging due to its multidimensional nature. This study introduces a Technological Convergence Index (TCI) that assesses depth and breadth. Depth is calculated using IPC descriptions and additional patent metadata in a graph structure analyzed with Heterogeneous Graph Transformers and Sentence-BERT. Breadth measures the diversity of technological fields involved using the Shannon Diversity Index. TCI combines these dimensions using the Entropy Weight Method. Validation against established measures shows TCI's advantages and empirical reliability is confirmed through regression against patent quality indicators. The multidimensional approach offers insights for innovation policy and industry strategies in managing cross-domain technologies. 

Summary: <br /><br />Technological convergence is a major trend in innovation, but accurately measuring it is challenging due to its multidimensional nature. This study introduces a Technological Convergence Index (TCI) that measures convergence depth and breadth. Depth is calculated using IPC descriptions and additional patent metadata in a graph structure analyzed with Heterogeneous Graph Transformers and Sentence-BERT. Breadth measures the diversity of technological fields involved using the Shannon Diversity Index. TCI combines these dimensions using the Entropy Weight Method, providing valuable insights for innovation policy and industry strategies in managing emerging cross-domain technologies. <div>
arXiv:2509.21187v1 Announce Type: new 
Abstract: Technological convergence refers to the phenomenon where boundaries between technological areas and disciplines are increasingly blurred. It enables the integration of previously distinct domains and has become a mainstream trend in today's innovation process. However, accurately measuring technological convergence remains a persistent challenge due to its inherently multidimensional and evolving nature. This study designs an Technological Convergence Index (TCI) that comprehensively measures convergence along two fundamental dimensions: depth and breadth. For depth calculation, we use IPC textual descriptions as the analytical foundation and enhance this assessment by incorporating supplementary patent metadata into a heterogeneous graph structure. This graph is then modeled using Heterogeneous Graph Transformers in combination with Sentence-BERT, enabling a precise representation of knowledge integration across technological boundaries. Complementing this, the breadth dimension captures the diversity of technological fields involved, quantified through the Shannon Diversity Index to measure the variety of technological combinations within patents. Our final TCI is constructed using the Entropy Weight Method, which objectively assigns weights to both dimensions based on their information entropy. To validate our approach, we compare the proposed TCI against established convergence measures, demonstrating its comparative advantages. We further establish empirical reliability through a novel robustness test that regresses TCI against indicators of patent quality. These findings are further substantiated through comprehensive robustness checks. Our multidimensional approach provides valuable practical insights for innovation policy and industry strategies in managing emerging cross-domain technologies.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evading Overlapping Community Detection via Proxy Node Injection</title>
<link>https://arxiv.org/abs/2509.21211</link>
<guid>https://arxiv.org/abs/2509.21211</guid>
<content:encoded><![CDATA[
<div> community membership hiding, social graphs, privacy protection, deep reinforcement learning, overlapping communities

Summary:
This paper introduces the concept of community membership hiding (CMH) in social graphs to protect privacy by preventing sensitive information inference without altering graph topology. Previous work on non-overlapping communities does not fully address real-world graphs with overlapping communities. The authors propose a deep reinforcement learning (DRL) approach to learn effective modification policies, including the use of proxy nodes while maintaining graph structure. Experimental results on real-world datasets demonstrate that the proposed method outperforms existing baselines in both effectiveness and efficiency, providing a systematic approach for privacy-preserving graph modification in the presence of overlapping communities. <br /><br />Summary: <div>
arXiv:2509.21211v1 Announce Type: new 
Abstract: Protecting privacy in social graphs requires preventing sensitive information, such as community affiliations, from being inferred by graph analysis, without substantially altering the graph topology. We address this through the problem of \emph{community membership hiding} (CMH), which seeks edge modifications that cause a target node to exit its original community, regardless of the detection algorithm employed. Prior work has focused on non-overlapping community detection, where trivial strategies often suffice, but real-world graphs are better modeled by overlapping communities, where such strategies fail. To the best of our knowledge, we are the first to formalize and address CMH in this setting. In this work, we propose a deep reinforcement learning (DRL) approach that learns effective modification policies, including the use of proxy nodes, while preserving graph structure. Experiments on real-world datasets show that our method significantly outperforms existing baselines in both effectiveness and efficiency, offering a principled tool for privacy-preserving graph modification with overlapping communities.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Framework for Evaluating Dynamic Network Generative Models and Anomaly Detection</title>
<link>https://arxiv.org/abs/2406.11901</link>
<guid>https://arxiv.org/abs/2406.11901</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic networks, graph convolutional network, anomaly detection, generative models, attention mechanism

Summary: 
DGSP-GCN is a deep learning framework that combines graph convolutional networks with dynamic graph signal processing techniques for evaluating generative models and detecting anomalies in dynamic networks. It assesses the similarity between generated network snapshots and expected temporal evolution, using an attention mechanism to enhance embedding quality and capture dynamic structural changes. The approach was tested on real-world datasets, outperforming baseline methods with low error rates. DGSP-GCN's effectiveness in evaluating and detecting anomalies in dynamic networks provides valuable insights for network evolution and anomaly detection research. <div>
arXiv:2406.11901v2 Announce Type: replace 
Abstract: Understanding dynamic systems like disease outbreaks, social influence, and information diffusion requires effective modeling of complex networks. Traditional evaluation methods for static networks often fall short when applied to temporal networks. This paper introduces DGSP-GCN (Dynamic Graph Similarity Prediction based on Graph Convolutional Network), a deep learning-based framework that integrates graph convolutional networks with dynamic graph signal processing techniques to provide a unified solution for evaluating generative models and detecting anomalies in dynamic networks. DGSP-GCN assesses how well a generated network snapshot matches the expected temporal evolution, incorporating an attention mechanism to improve embedding quality and capture dynamic structural changes. The approach was tested on five real-world datasets: WikiMath, Chickenpox, PedalMe, MontevideoBus, and MetraLa. Results show that DGSP-GCN outperforms baseline methods, such as time series regression and random similarity assignment, achieving the lowest error rates (MSE of 0.0645, MAE of 0.1781, RMSE of 0.2507). These findings highlight DGSP-GCN's effectiveness in evaluating and detecting anomalies in dynamic networks, offering valuable insights for network evolution and anomaly detection research.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heaven &amp; Hell: One-Step Hub Consensus</title>
<link>https://arxiv.org/abs/2509.19630</link>
<guid>https://arxiv.org/abs/2509.19630</guid>
<content:encoded><![CDATA[
<div> weighted directed graphs, influence dynamics, global convergence, hub node, Coq proof assistant

Summary: 
- The study focuses on influence dynamics on finite weighted directed graphs with a central hub node and binary vertex states ('Glory' or 'Gnash').
- The research provides a precise criterion that ensures global convergence to Glory in a single synchronous update from any initial state on the graph.
- The criterion states that at each non-hub vertex, the incoming weight from the hub must be equal to or greater than the total incoming weight from all other nodes.
- The study extends the results to tau-biased update rules and asynchronous schedules, showing that a single pass is still sufficient under certain domination hypotheses.
- The research includes machine-checked proofs in the Coq proof assistant to validate the presented theorems. 

<br /><br />Summary: <div>
arXiv:2509.19630v1 Announce Type: new 
Abstract: Many networked systems require a central authority to enforce a global configuration against local peer influence. We study influence dynamics on finite weighted directed graphs with a distinguished hub node and binary vertex states ('Glory' or 'Gnash'). We give a sharp, local, and efficiently checkable criterion that guarantees global convergence to Glory in a single synchronous update from any initial state. At each non-hub vertex, the incoming weight from the hub must at least match the total incoming weight from all other nodes. Specialising in uniform hub broadcasts, the exact threshold equals the maximum non-hub incoming weight over all vertices, and we prove this threshold is tight. We extend the result to a tau-biased update rule and to asynchronous (Gauss-Seidel) schedules, where a single pass still suffices under the same domination hypothesis. Machine-checked proofs in Coq accompany all theorems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic Frequency--Domain Inference of Network Topology and Hidden Components via Structure--Behavior Scaling</title>
<link>https://arxiv.org/abs/2509.19857</link>
<guid>https://arxiv.org/abs/2509.19857</guid>
<content:encoded><![CDATA[
<div> Keywords: hidden interactions, complex systems, network structure inference, spectral strength, evolutionary game dynamics

Summary: 
This article addresses the challenge of inferring network structures in complex systems with hidden components using indirect behavioral signals. It establishes a linear scaling relationship between the spectral strength of nodes under evolutionary game dynamics and their structural degree, enabling the development of a frequency-domain inference framework based on the discrete Fourier transform. This framework can reconstruct network topology directly from payoff sequences without prior knowledge of the network or internal node strategies. It can localize hidden nodes, identify edges connected to multiple hidden nodes, and estimate bounds on the number of hidden nodes. Experiments show that this method outperforms existing techniques in both topology reconstruction and hidden component detection, scaling efficiently to large networks and offering robustness to stochastic fluctuations. Overall, this work provides a principled approach to accurately recover network topology in complex systems with hidden elements.<br /><br />Summary: <div>
arXiv:2509.19857v1 Announce Type: new 
Abstract: Hidden interactions and components in complex systems-ranging from covert actors in terrorist networks to unobserved brain regions and molecular regulators-often manifest only through indirect behavioral signals. Inferring the underlying network structure from such partial observations remains a fundamental challenge, particularly under nonlinear dynamics. We uncover a robust linear relationship between the spectral strength of a node's behavioral time series under evolutionary game dynamics and its structural degree, $S \propto k$, a structural-behavioral scaling that holds across network types and scales, revealing a universal correspondence between local connectivity and dynamic energy. Leveraging this insight, we develop a deterministic, frequency-domain inference framework based on the discrete Fourier transform (DFT) that reconstructs network topology directly from payoff sequences-without prior knowledge of the network or internal node strategies-by selectively perturbing node dynamics. The framework simultaneously localizes individual hidden nodes or identifies all edges connected to multiple hidden nodes, and estimates tight bounds on the number of hidden nodes. Extensive experiments on synthetic and real-world networks demonstrate that our method consistently outperforms state-of-the-art baselines in both topology reconstruction and hidden component detection. Moreover, it scales efficiently to large networks, offering robustness to stochastic fluctuations and overcoming the size limitations of existing techniques. Our work establishes a principled connection between local dynamic observables and global structural inference, enabling accurate topology recovery in complex systems with hidden elements.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Together: Toward Infrastructure for Community-Run Social Media</title>
<link>https://arxiv.org/abs/2509.19653</link>
<guid>https://arxiv.org/abs/2509.19653</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized governance, social computing systems, inter-community governance, modularity, polycentricity <br />
Summary: <br />
Decentralizing governance in social computing systems can empower communities to make decisions in line with their values. However, communities often face common problems that cross boundaries, requiring mechanisms for inter-community governance. Six challenges in designing for inter-community governance were identified through workshops, focusing on modularity, forkability, and polycentricity. The proposed ideas form an ecosystem of resources, infrastructures, and tools for supporting community governance. Implementing these principles can enhance coordination and decision-making among communities in social computing systems. <div>
arXiv:2509.19653v1 Announce Type: cross 
Abstract: Decentralizing the governance of social computing systems to communities promises to empower them to make independent decisions, with nuance and in accordance with their values. Yet, communities do not govern in isolation. Many problems communities face are common, or move across their boundaries. We therefore propose designing for "inter-community governance:" mechanisms that support relationships and interactions between communities to coordinate on governance issues. Drawing from workshops with 24 individuals on decentralized, community-run social media, we present six challenges in designing for inter-community governance surfaced through ideas proposed in workshops. Together, these ideas come together as an ecosystem of resources, infrastructures, and tools that highlight three key principles for designing for inter-community governance: modularity, forkability, and polycentricity. We end with a discussion of how the ideas proposed in workshops might be implemented in future work aiming to support community governance in social computing systems broadly.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections</title>
<link>https://arxiv.org/abs/2509.19657</link>
<guid>https://arxiv.org/abs/2509.19657</guid>
<content:encoded><![CDATA[
<div> Keywords: pedestrian safety, driver yielding behavior, pedestriandriver interaction, large language models, multimodal learning.

Summary:
Pedestrian safety and driver yielding behavior at crosswalks are crucial factors in urban mobility. Modeling these interactions accurately requires capturing complex behaviors, which traditional machine learning models struggle with due to fixed feature representations. This paper explores the use of large language models (LLMs) to model driver-pedestrian interactions at intersections. Through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, LLMs enable interpretable and context-aware inference of driver yielding behavior. Benchmarking against traditional classifiers, GPT-4o was found to achieve high accuracy and recall, while Deepseek-V3 performed well in precision. The study highlights the trade-offs between model performance and computational efficiency, providing valuable insights for deploying LLMs in real-world pedestrian safety systems.

<br /><br />Summary: <div>
arXiv:2509.19657v1 Announce Type: cross 
Abstract: Pedestrian safety is a critical component of urban mobility and is strongly influenced by the interactions between pedestrian decision-making and driver yielding behavior at crosswalks. Modeling driver--pedestrian interactions at intersections requires accurately capturing the complexity of these behaviors. Traditional machine learning models often struggle to capture the nuanced and context-dependent reasoning required for these multifactorial interactions, due to their reliance on fixed feature representations and limited interpretability. In contrast, large language models (LLMs) are suited for extracting patterns from heterogeneous traffic data, enabling accurate modeling of driver-pedestrian interactions. Therefore, this paper leverages multimodal LLMs through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, enabling interpretable and context-aware inference of driver yielding behavior, as an example application of modeling pedestrian--driver interaction. We benchmarked state-of-the-art LLMs against traditional classifiers, finding that GPT-4o consistently achieves the highest accuracy and recall, while Deepseek-V3 excels in precision. These findings highlight the critical trade-offs between model performance and computational efficiency, offering practical guidance for deploying LLMs in real-world pedestrian safety systems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Muse-it: A Tool for Analyzing Music Discourse on Reddit</title>
<link>https://arxiv.org/abs/2509.20228</link>
<guid>https://arxiv.org/abs/2509.20228</guid>
<content:encoded><![CDATA[
<div> Keywords: music engagement, social media, natural language processing, big data analytics, Reddit

Summary:
Muse-it is a platform that facilitates the analysis of music engagement through natural, unprompted conversations on Reddit. It utilizes advances in natural language processing and big data analytics to analyze large-scale discourse around music-related topics. The platform retrieves comprehensive Reddit data based on user-defined queries, aggregates posts from various subreddits, and supports topic modeling, temporal trend analysis, and clustering. In addition, Muse-it identifies music-related hyperlinks, retrieves track-level metadata, and links this information to the discussions. An interactive interface allows for dynamic visualizations of the collected data, offering music researchers a user-friendly way to study music engagement as it occurs online. By enabling the extraction, processing, and analysis of big data from Reddit, Muse-it opens up new opportunities for understanding the multifaceted interactions individuals have with music in digital spaces. 

<br /><br />Summary: Muse-it is a platform that leverages natural language processing and big data analytics to analyze music engagement on Reddit. It allows for the retrieval and aggregation of data from various subreddits, supports topic modeling and trend analysis, and links music-related information to discussions. With an interactive interface for visualizations, Muse-it provides a user-friendly approach for understanding how people engage with music online. <div>
arXiv:2509.20228v1 Announce Type: cross 
Abstract: Music engagement spans diverse interactions with music, from selection and emotional response to its impact on behavior, identity, and social connections. Social media platforms provide spaces where such engagement can be observed in natural, unprompted conversations. Advances in natural language processing (NLP) and big data analytics make it possible to analyze these discussions at scale, extending music research to broader contexts. Reddit, in particular, offers anonymity that encourages diverse participation and yields rich discourse on music in ecological settings. Yet the scale of this data requires tools to extract, process, and analyze it effectively. We present Muse-it, a platform that retrieves comprehensive Reddit data centered on user-defined queries. It aggregates posts from across subreddits, supports topic modeling, temporal trend analysis, and clustering, and enables efficient study of large-scale discourse. Muse-it also identifies music-related hyperlinks (e.g., Spotify), retrieves track-level metadata such as artist, album, release date, genre, popularity, and lyrics, and links these to the discussions. An interactive interface provides dynamic visualizations of the collected data. Muse-it thus offers an accessible way for music researchers to gather and analyze big data, opening new avenues for understanding music engagement as it naturally unfolds online.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Into the Void: Understanding Online Health Information in Low-Web Data Languages</title>
<link>https://arxiv.org/abs/2509.20245</link>
<guid>https://arxiv.org/abs/2509.20245</guid>
<content:encoded><![CDATA[
<div> data voids, online health information seeking, social media platforms, algorithmic structures, low-web data languages 

Summary: 
Data voids, areas of the internet with scarce information, present challenges for online health information seekers, particularly in low-web data languages. Social media platforms, now informal search engines, also face this issue. Data horizons are critical boundaries where algorithmic structures degrade search result relevance and reliability, influenced by factors like linguistic underrepresentation and algorithmic amplification. Evaluating health query search results in Tigrinya and Amharic, common characteristics include results not in the search language and dominated by nutritional and religious advice. Search result divergence can result from algorithmic failures, manipulation by creators, or unintentional factors. These findings reveal how data horizons manifest under various constraints, impacting information availability in low-resourced languages. <div>
arXiv:2509.20245v1 Announce Type: cross 
Abstract: Data voids--areas of the internet where reliable information is scarce or absent--pose significant challenges to online health information seeking, particularly for users operating in low-web data languages. These voids are increasingly encountered not on traditional search engines alone, but on social media platforms, which have gradually morphed into informal search engines for millions of people. In this paper, we introduce the phenomenon of data horizons: a critical boundary where algorithmic structures begin to degrade the relevance and reliability of search results. Unlike the core of a data void, which is often exploited by bad actors to spread misinformation, the data horizon marks the critical space where systemic factors, such as linguistic underrepresentation, algorithmic amplification, and socio-cultural mismatch, create conditions of informational instability. Focusing on Tigrinya and Amharic as languages of study, we evaluate (1) the common characteristics of search results for health queries, (2) the quality and credibility of health information, and (3) characteristics of search results that diverge from their queries. We find that search results for health queries in low-web data languages may not always be in the language of search and may be dominated by nutritional and religious advice. We show that search results that diverge from their queries in low-resourced languages are due to algorithmic failures, (un)intentional manipulation, or active manipulation by content creators. We use our findings to illustrate how a data horizon manifests under several interacting constraints on information availability.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CueGCL: Cluster-aware Personalized Self-Training for Unsupervised Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2311.11073</link>
<guid>https://arxiv.org/abs/2311.11073</guid>
<content:encoded><![CDATA[
<div> Graph Contrastive Learning, Cluster-aware Framework, Unsupervised Learning, Graph Clustering, Node Representations <br />
<br />
Summary: 
The article introduces CueGCL, a Cluster-aware Graph Contrastive Learning Framework designed to improve performance in structure-related and unsupervised graph clustering tasks. Current GCL algorithms struggle in acquiring cluster-level information, leading to poor results. CueGCL addresses this issue with a PeST strategy for unsupervised scenarios, enabling precise cluster-level personalized information capture while reducing class collision and unfairness. The model incorporates aligned graph clustering (AGC) to align clustering spaces for more consistent node embeddings. Theoretical analysis demonstrates CueGCL's effectiveness in yielding a discernible cluster structure in the embedding space. Experimental results on benchmark datasets show that CueGCL achieves state-of-the-art performance across varied scales. <br /> <div>
arXiv:2311.11073v2 Announce Type: replace 
Abstract: Recently, graph contrastive learning (GCL) has emerged as one of the optimal solutions for node-level and supervised tasks. However, for structure-related and unsupervised tasks such as graph clustering, current GCL algorithms face difficulties acquiring the necessary cluster-level information, resulting in poor performance. In addition, general unsupervised GCL improves the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of graph clustering. To address the above issues, we propose a Cluster-aware Graph Contrastive Learning Framework (CueGCL) to jointly learn clustering results and node representations. Specifically, we design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise cluster-level personalized information. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, aligned graph clustering (AGC) is employed to obtain the cluster partition, where we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we theoretically demonstrate the effectiveness of our model, showing it yields an embedding space with a significantly discernible cluster structure. Extensive experimental results also show our CueGCL exhibits state-of-the-art performance on five benchmark datasets with different scales.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The networks of ingredient combinations as culinary fingerprints of world cuisines</title>
<link>https://arxiv.org/abs/2408.15162</link>
<guid>https://arxiv.org/abs/2408.15162</guid>
<content:encoded><![CDATA[
<div> network analysis, worldwide cuisines, ingredient combinations, culinary fingerprints, machine learning models

Summary: 
Investigating worldwide cuisines using network analysis of ingredient combinations reveals distinctive patterns in how ingredients are combined in popular dishes. Cuisines differ in ingredient type popularity, recipe sizes, and structural organization of ingredient-type combinations. European cuisines distribute ingredients across types, while certain Asian and South American cuisines emphasize one dominant type. Maximum spanning trees capture the essence of these patterns, serving as a representative backbone for each cuisine. Machine learning models can accurately identify cuisines from subsets of recipes using both full and simplified network representations. These networks also cluster global cuisines into meaningful geo-cultural groups, reflecting shared culinary traditions. The study provides insights into the structure of world cuisines, facilitating data-driven approaches to their characterization, cross-cultural comparison, and potential adaptation. 

Summary: <div>
arXiv:2408.15162v2 Announce Type: replace-cross 
Abstract: Investigating how different ingredients are combined in popular dishes is crucial to uncover the principles behind food preferences. Here, we use data from public food repositories and network analysis to characterize and compare worldwide cuisines. Ingredients are first grouped into broader types, and each cuisine is then represented as a network in which nodes correspond to ingredient types and weighted links describe how frequently pairs of types co-occur in recipes. Cuisines differ not only in the popularity of ingredient types and range of recipe sizes, but also in the structural organization of ingredient-type combinations. By analyzing these networks, we uncover distinctive patterns of type associations that serve as culinary fingerprints. For example, European cuisines typically distribute ingredients across different types, whereas certain Asian and South American traditions emphasize one dominant type, such as vegetables or spices. The essence of these patterns is well captured by the networks' maximum spanning trees, which offer a simplified yet representative backbone for each cuisine. We demonstrate that both these full and simplified network representations enable machine learning models to identify cuisines from subsets of recipes with very high accuracy. Networks of ingredient combinations also cluster global cuisines into meaningful geo-cultural groups, reflecting shared patterns in culinary traditions. More broadly, our study offers novel insights into the structure of world cuisines, enabling data-driven approaches to their characterization, cross-cultural comparison, and potential adaptation.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily in Complex Networks: Measures, Models, and Applications</title>
<link>https://arxiv.org/abs/2509.18289</link>
<guid>https://arxiv.org/abs/2509.18289</guid>
<content:encoded><![CDATA[
<div> homophily, social networks, network evolution, structural inequalities, higher-order network structures <br />
Summary: <br />
Homophily, the tendency for individuals to connect with others who share similar attributes, is a key feature of social networks. Understanding group interactions within and across networks is essential for analyzing network dynamics and structural inequalities. This tutorial provides an extensive overview of homophily, including definitions, properties, and limitations of common metrics. It also explores homophily in higher-order network structures like hypergraphs and simplicial complexes. The tutorial delves into network generating models that can create different types of homophilic networks with adjustable levels of homophily, showcasing their significance in real-world scenarios. Lastly, the tutorial addresses ongoing challenges, emerging areas of study, and prospects for further research in this field. <div>
arXiv:2509.18289v1 Announce Type: new 
Abstract: Homophily, the tendency of individuals to connect with others who share similar attributes, is a defining feature of social networks. Understanding how groups interact, both within and across, is crucial for uncovering the dynamics of network evolution and the emergence of structural inequalities in these network. This tutorial offers a comprehensive overview of homophily, covering its various definitions, key properties, and the limitations of widely used metrics. Extending beyond traditional pairwise interactions, we will discuss homophily in higher-order network structures such as hypergraphs and simplicial complexes. We will further discuss network generating models capable of producing different types of homophilic networks with tunable levels of homophily and highlight their relevance in real-world contexts. The tutorial concludes with a discussion of open challenges, emerging directions, and opportunities for further research in this area.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Constructive Conflict in Online Discussions through Controversial yet Toxicity Resilient Posts</title>
<link>https://arxiv.org/abs/2509.18303</link>
<guid>https://arxiv.org/abs/2509.18303</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic curation, controversiality, toxicity resilience, political posts, constructive dialogues

Summary: 
Algorithmic curation could enhance social media content by focusing on constructive conflicts. The study introduces controversiality and toxicity resilience as key criteria for identifying challenging yet respectful dialogues. Results show that assessing toxicity resilience is distinctive from identifying low-toxicity posts. Political posts, typically controversial, attract more toxic responses, but some can still foster civil engagement. Posts resilient to toxicity often incorporate politeness cues like gratitude and hedging, suggesting a potential strategy to promote constructive political discussions. This research highlights the potential of framing post tones to encourage positive political discourse. <div>
arXiv:2509.18303v1 Announce Type: new 
Abstract: Bridging content that brings together individuals with opposing viewpoints on social media remains elusive, overshadowed by echo chambers and toxic exchanges. We propose that algorithmic curation could surface such content by considering constructive conflicts as a foundational criterion. We operationalize this criterion through controversiality to identify challenging dialogues and toxicity resilience to capture respectful conversations. We develop high-accuracy models to capture these dimensions. Analyses based on these models demonstrate that assessing resilience to toxic responses is not the same as identifying low-toxicity posts. We also find that political posts are often controversial and tend to attract more toxic responses. However, some posts, even the political ones, are resilient to toxicity despite being highly controversial, potentially sparking civil engagement. Toxicity resilient posts tend to use politeness cues, such as showing gratitude and hedging. These findings suggest the potential for framing the tone of posts to encourage constructive political discussions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Neural-Network-Entropy model of vital node identification on network attack and propagation</title>
<link>https://arxiv.org/abs/2509.18325</link>
<guid>https://arxiv.org/abs/2509.18325</guid>
<content:encoded><![CDATA[
<div> method; graph neural networks; information entropy; vital node identification; network attack

Summary:
The article introduces a novel method, GNNE, which utilizes graph neural networks and information entropy to identify vital nodes in complex networks. The method integrates node features, interactions, and states, employing a Graph Convolutional Network (GCN) to learn node features and a Graph Attention Network (GAT) to determine node influence factors. By calculating nodes' entropy based on their influence factors, GNNE effectively evaluates node importance, particularly in protecting networks from intentional attacks. The GCN extracts node features while the GAT aggregates neighbor features using an attention mechanism to assign weights based on importance. Through training on a synthetic network and testing on real datasets, GNNE outperforms traditional topology-based methods and graph-machine-learning-based methods in identifying vital nodes for network attack and propagation. <div>
arXiv:2509.18325v1 Announce Type: new 
Abstract: Vital nodes usually play a key role in complex networks. Uncovering these nodes is an important task in protecting the network, especially when the network suffers intentional attack. Many existing methods have not fully integrated the node feature, interaction and state. In this article, we propose a novel method (GNNE) based on graph neural networks and information entropy. The method employs a Graph Convolutional Network (GCN) to learn the nodes' features, which are input into a Graph Attention Network (GAT) to obtain the influence factor of nodes, and the node influence factors are used to calculate the nodes' entropy to evaluate the node importance. The GNNE takes advantage of the GCN and GAT, with the GCN well extracting the nodes' features and the GAT aggregating the features of the nodes' neighbors by using the attention mechanism to assign different weights to the neighbors with different importance, and the nodes' entropy quantifies the nodes' state in the network. The proposed method is trained on a synthetic Barabasi-Albert network, and tested on six real datasets. Compared with eight traditional topology-based methods and four graph-machine-learning-based methods, the GNNE shows an advantage for the vital node identification in the perspectives of network attack and propagation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data</title>
<link>https://arxiv.org/abs/2509.18985</link>
<guid>https://arxiv.org/abs/2509.18985</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, Large Language Models (LLMs), simulations, opinion modeling, user behavior 

Summary: 
Online social networks are valuable for studying individual and collective phenomena. Researchers use simulators with Large Language Models (LLMs) to make simulations more realistic by enabling agents to understand and generate natural language content. In this study, LLM-based agents were investigated in a simulated microblogging social network using profiles from real-world online conversations. The agents generated coherent content, formed connections, and displayed realistic social network structures. However, their content showed less variation in tone and toxicity compared to real data. The opinion dynamics of LLM agents evolved similarly to traditional mathematical models, with parameter configurations having minimal impact. The study highlights the potential of LLMs in simulating user behavior in social environments but also emphasizes the need for more precise cognitive modeling for accurate replication of human behavior. <br /><br />Summary: <div>
arXiv:2509.18985v1 Announce Type: new 
Abstract: Online social networks offer a valuable lens to analyze both individual and collective phenomena. Researchers often use simulators to explore controlled scenarios, and the integration of Large Language Models (LLMs) makes these simulations more realistic by enabling agents to understand and generate natural language content. In this work, we investigate the behavior of LLM-based agents in a simulated microblogging social network. We initialize agents with realistic profiles calibrated on real-world online conversations from the 2022 Italian political election and extend an existing simulator by introducing mechanisms for opinion modeling. We examine how LLM agents simulate online conversations, interact with others, and evolve their opinions under different scenarios. Our results show that LLM agents generate coherent content, form connections, and build a realistic social network structure. However, their generated content displays less heterogeneity in tone and toxicity compared to real data. We also find that LLM-based opinion dynamics evolve over time in ways similar to traditional mathematical models. Varying parameter configurations produces no significant changes, indicating that simulations require more careful cognitive modeling at initialization to replicate human behavior more faithfully. Overall, we demonstrate the potential of LLMs for simulating user behavior in social environments, while also identifying key challenges in capturing heterogeneity and complex dynamics.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability</title>
<link>https://arxiv.org/abs/2509.18376</link>
<guid>https://arxiv.org/abs/2509.18376</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, node classification, global explanation methods, GnnXemplar <br />
<br />
Summary: GNNs are widely used for node classification but lack transparency in decision-making. Current global explanation methods are insufficient for large, complex graphs. GnnXemplar, inspired by Exemplar Theory, identifies representative nodes, exemplars, and generates natural language rules from their neighborhoods. Exemplar selection is optimized through reverse k-nearest neighbors, with rule derivation using large language models. Experimental results demonstrate GnnXemplar's superiority in fidelity, scalability, and interpretability, validated by a user study with 60 participants. <div>
arXiv:2509.18376v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Collaborative Maintenance Falls Short: The Persistence of Retracted Papers on Wikipedia</title>
<link>https://arxiv.org/abs/2509.18403</link>
<guid>https://arxiv.org/abs/2509.18403</guid>
<content:encoded><![CDATA[
<div> Retraction, Wikipedia, citations, credibility, scholarly authority
Summary: 
The study investigates the handling of citations to retracted research on English Wikipedia. A dataset integrating Wikipedia revision histories with various metadata sources identified 1,181 citations of retracted papers. The analysis found that 71.6% of these citations were problematic, either added before retraction or introduced without mentioning the retracted status. These incorrect citations persist for a median of over 3.68 years, with signals of human attention leading to faster correction. Surprisingly, papers with higher academic citation counts took longer to be corrected. The study reveals gaps in citation-level repair on Wikipedia and suggests design directions to improve citation credibility at scale. This research contributes to understanding the sociotechnical vulnerability in maintaining citation credibility on collaborative platforms like Wikipedia.<br /><br />Summary: <div>
arXiv:2509.18403v1 Announce Type: cross 
Abstract: Wikipedia serves as a key infrastructure for public access to scientific knowledge, but it faces challenges in maintaining the credibility of cited sources, especially when scientific papers are retracted. This paper investigates how citations to retracted research are handled on English Wikipedia. We construct a novel dataset that integrates Wikipedia revision histories with metadata from Retraction Watch, Crossref, Altmetric, and OpenAlex, identifying 1,181 citations of retracted papers. We find that 71.6% of all citations analyzed are problematic. These are citations added before a paper's retraction, as well as the citations introduced after retraction without any in-text mention of the paper's retracted status. Our analysis reveals that these citations persist for a median of over 3.68 years (1,344 days). Through survival analysis, we find that signals of human attention are associated with a faster correction process. Unfortunately, a paper's established scholarly authority, a higher academic citation count, is associated with a slower time to correction. Our findings highlight how the Wikipedia community supports collaborative maintenance but leaves gaps in citation-level repair. We contribute to CSCW research by advancing our understanding of this sociotechnical vulnerability, which takes the form of a community coordination challenge, and by offering design directions to support citation credibility at scale.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Propaganda</title>
<link>https://arxiv.org/abs/2509.19147</link>
<guid>https://arxiv.org/abs/2509.19147</guid>
<content:encoded><![CDATA[
<div> generative propaganda, artificial intelligence, deepfakes, persuasion, deception <br />
Summary: <br />
Generative propaganda, which utilizes artificial intelligence to influence public opinion, was studied in Taiwan and India. The term "deepfakes" heavily influences defenses against generative propaganda. A taxonomy categorizing obvious versus hidden and promotional versus derogatory uses was developed. Deception was not the primary motivation in AI's use; instead, creators in India focused on persuasion rather than deceit, making AI's usage apparent to minimize risks. In Taiwan, deception was seen as part of distorted strategic narratives online. AI was employed for efficiency gains in communication and evasion of detection. Security researchers are advised to distinguish between different uses of AI, leverage social factors, and counter efficiency gains on a global scale. <br /> <div>
arXiv:2509.19147v1 Announce Type: cross 
Abstract: Generative propaganda is the use of generative artificial intelligence (AI) to shape public opinion. To characterize its use in real-world settings, we conducted interviews with defenders (e.g., factcheckers, journalists, officials) in Taiwan and creators (e.g., influencers, political consultants, advertisers) as well as defenders in India, centering two places characterized by high levels of online propaganda. The term "deepfakes", we find, exerts outsized discursive power in shaping defenders' expectations of misuse and, in turn, the interventions that are prioritized. To better characterize the space of generative propaganda, we develop a taxonomy that distinguishes between obvious versus hidden and promotional versus derogatory use. Deception was neither the main driver nor the main impact vector of AI's use; instead, Indian creators sought to persuade rather than to deceive, often making AI's use obvious in order to reduce legal and reputational risks, while Taiwan's defenders saw deception as a subset of broader efforts to distort the prevalence of strategic narratives online. AI was useful and used, however, in producing efficiency gains in communicating across languages and modes, and in evading human and algorithmic detection. Security researchers should reconsider threat models to clearly differentiate deepfakes from promotional and obvious uses, to complement and bolster the social factors that constrain misuse by internal actors, and to counter efficiency gains globally.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change communication, microtargeting, large language models, demographic targeting, fairness

Summary: 
- The study analyzes microtargeting practices in climate change campaigns on social media, using large language models to examine Meta advertisements.
- It focuses on demographic targeting and fairness, evaluating the accuracy of predicting demographic targets and uncovering biases in model predictions.
- The study finds that young adults are targeted through activism and environmental themes, while women are engaged through caregiving and advocacy themes.
- Recurring patterns in messaging strategies tailored to different demographic groups are uncovered through thematic explanations generated by the models.
- The fairness analysis reveals biases in the classification of male audiences, emphasizing the need for more inclusive targeting methods in social media climate campaigns. 

<br /><br />Summary: <div>
arXiv:2410.05401v4 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Meta (previously known as Facebook) advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. Additionally, we conduct a comprehensive fairness analysis to uncover biases in model predictions. We assess disparities in accuracy and error rates across demographic groups using established fairness metrics such as Demographic Parity, Equal Opportunity, and Predictive Equality. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of male audiences. The analysis of thematic explanations uncovers recurring patterns in messaging strategies tailored to various demographic groups, while the fairness analysis underscores the need for more inclusive targeting methods. This study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUINTA: Reflexive Sensibility For Responsible AI Research and Data-Driven Processes</title>
<link>https://arxiv.org/abs/2509.16347</link>
<guid>https://arxiv.org/abs/2509.16347</guid>
<content:encoded><![CDATA[
<div> Intersectionality, AI, machine learning, fairness, critical reflexivity<br />
<br />
Intersectionality is increasingly recognized as essential in AI research for prioritizing fairness and addressing historical marginalization. This paper introduces the Quantitative Intersectional Data (QUINTA) framework, which integrates intersectionality into the AI/DS pipeline through critical reflexivity. By challenging conventional research habits, particularly in data-centric processes, QUINTA aims to identify and mitigate inadvertent marginalization caused by these practices. The framework emphasizes researcher reflexivity to highlight their power in creating and analyzing AI artifacts. A demonstration using the #metoo movement showcases the effectiveness of QUINTA. This approach provides practical guidance for researchers to incorporate intersectionality into their work, ultimately contributing to more just and equitable AI systems. 
<br /><br />Summary: <div>
arXiv:2509.16347v1 Announce Type: new 
Abstract: As the field of artificial intelligence (AI) and machine learning (ML) continues to prioritize fairness and the concern for historically marginalized communities, the importance of intersectionality in AI research has gained significant recognition. However, few studies provide practical guidance on how researchers can effectively incorporate intersectionality into critical praxis. In response, this paper presents a comprehensive framework grounded in critical reflexivity as intersectional praxis. Operationalizing intersectionality within the AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative Intersectional Data (QUINTA) is introduced as a methodological paradigm that challenges conventional and superficial research habits, particularly in data-centric processes, to identify and mitigate negative impacts such as the inadvertent marginalization caused by these practices. The framework centers researcher reflexivity to call attention to the AI researchers' power in creating and analyzing AI/DS artifacts through data-centric approaches. To illustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher demonstration utilizing the \#metoo movement as a case study. Note: This paper was accepted as a poster presentation at Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) Conference in 2023.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survivors, Complainers, and Borderliners: Upward Bias in Online Discussions of Academic Conference Reviews</title>
<link>https://arxiv.org/abs/2509.16831</link>
<guid>https://arxiv.org/abs/2509.16831</guid>
<content:encoded><![CDATA[
<div> Keywords: online discussion platforms, review scores, bias, academic conferences, peer review

Summary:
Survivors, complainers, and borderliners contribute to an upward bias in self-reported review scores on online discussion platforms for academic conference submissions. Survivors, authors of accepted papers, are more likely to share positive outcomes, while complainers, authors of high-scoring rejected papers, tend to voice complaints about the peer review process. Borderliners, authors with borderline scores, seek advice during the rebuttal period due to uncertainty. The study compares self-reported review score distributions from Zhihu and Reddit with those of all submissions, revealing significant discrepancies. Online discussions may not accurately represent the overall population score distribution. Information seekers should interpret discussions with caution, considering the biases introduced by the types of authors participating in these platforms. <div>
arXiv:2509.16831v1 Announce Type: new 
Abstract: Online discussion platforms, such as community Q&amp;A sites and forums, have become important hubs where academic conference authors share and seek information about the peer review process and outcomes. However, these discussions involve only a subset of all submissions, raising concerns about the representativeness of the self-reported review scores. In this paper, we conduct a systematic study comparing the review score distributions of self-reported submissions in online discussions (based on data collected from Zhihu and Reddit) with those of all submissions. We reveal a consistent upward bias: the score distribution of self-reported samples is shifted upward relative to the population score distribution, with this difference statistically significant in most cases. Our analysis identifies three distinct contributors to this bias: (1) survivors, authors of accepted papers who are more likely to share good results than those of rejected papers who tend to conceal bad ones; (2) complainers, authors of high-scoring rejected papers who are more likely to voice complaints about the peer review process or outcomes than those of low scores; and (3) borderliners, authors with borderline scores who face greater uncertainty prior to decision announcements and are more likely to seek advice during the rebuttal period. These findings have important implications for how information seekers should interpret online discussions of academic conference reviews.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hodge Decomposition for Urban Traffic Flow: Limits on Dense OD Graphs and Advantages on Road Networks - Los Angeles Case</title>
<link>https://arxiv.org/abs/2509.17203</link>
<guid>https://arxiv.org/abs/2509.17203</guid>
<content:encoded><![CDATA[
<div> Keywords: Hodge decomposition, urban traffic flow, graph representations, commute patterns, clustering

Summary:
This study examines Hodge decomposition, specifically HodgeRank, in the context of urban traffic flow on two graph representations: dense origin-destination graphs and road-segment networks. The research replicates the methodology of Aoki et al. and finds that on dense OD graphs, the curl and harmonic components are minimal, with the potential closely mirroring node divergence. This limits the usefulness of Hodge potentials in this context. In contrast, when analyzing a real road network dataset from downtown Los Angeles, distinct variations in potentials are observed, including morning/evening reversals reflective of commute patterns. The study evaluates the smoothness and discriminability of the data using local and global variances derived from the graph spectrum. Additionally, the research proposes flow-aware embeddings that incorporate topology, bidirectional volume, and net-flow asymmetry for clustering purposes. The provided code and preprocessing steps aim to enhance reproducibility of the findings. 

<br /><br />Summary: <div>
arXiv:2509.17203v1 Announce Type: new 
Abstract: I study Hodge decomposition (HodgeRank) for urban traffic flow on two graph representations: dense origin--destination (OD) graphs and road-segment networks. Reproducing the method of Aoki et al., we observe that on dense OD graphs the curl and harmonic components are negligible and the potential closely tracks node divergence, limiting the added value of Hodge potentials. In contrast, on a real road network (UTD19, downtown Los Angeles; 15-minute resolution), potentials differ substantially from divergence and exhibit clear morning/evening reversals consistent with commute patterns. We quantify smoothness and discriminability via local/global variances derived from the graph spectrum, and propose flow-aware embeddings that combine topology, bidirectional volume, and net-flow asymmetry for clustering. Code and preprocessing steps are provided to facilitate reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limited Improvement of Connectivity in Scale-Free Networks by Increasing the Power-Law Exponent</title>
<link>https://arxiv.org/abs/2509.17652</link>
<guid>https://arxiv.org/abs/2509.17652</guid>
<content:encoded><![CDATA[
<div> Keywords: scale-free networks, connectivity, shortest loops, degree distributions, robustness <br />
Summary: <br />
The study explores the robustness of connectivity and the lengths of the shortest loops in scale-free networks with varying exponents ranging from 2.0 to 4.0. It is established that networks with smaller variance in degree distributions exhibit greater robustness and longer average lengths of the shortest loops. This phenomenon suggests the presence of large holes within the network structure. The findings indicate that manipulating degree distributions can potentially enhance the network's resilience against attacks. By understanding the relationship between degree distributions and network robustness, insights are gained on how to improve the overall stability and security of scale-free networks. <div>
arXiv:2509.17652v1 Announce Type: new 
Abstract: It has been well-known that many real networks are scale-free (SF) but extremely vulnerable against attacks. We investigate the robustness of connectivity and the lengths of the shortest loops in randomized SF networks with realistic exponents $2.0 < \gamma \leq 4.0$. We show that smaller variance of degree distributions leads to stronger robustness and longer average length of the shortest loops, which means the existing of large holes. These results will provide important insights toward enhancing the robustness by changing degree distributions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs</title>
<link>https://arxiv.org/abs/2509.16564</link>
<guid>https://arxiv.org/abs/2509.16564</guid>
<content:encoded><![CDATA[
<div> Keywords: Misinformation, Evolution, Persona-conditioned, Large Language Model, Detection

Summary:
Misinformation is dynamic, evolving as it spreads to different audiences through language, framing, and moral perspectives. Existing detection methods often overlook this dynamic nature, assuming misinformation is static. This study introduces MPCG, a multi-round framework that simulates how misinformation evolves through iterative reinterpretation by agents with different ideological perspectives. By using a large language model (LLM) to generate persona-specific claims across multiple rounds, the researchers were able to capture the evolution of misinformation. Evaluation through human and LLM-based annotations, cognitive effort metrics, emotion analysis, clustering, feasibility, and classification revealed that generated claims aligned with persona-specific emotional and moral framing, required increased cognitive effort, and exhibited semantic drift over rounds while maintaining topical coherence. Results also showed a significant drop in performance of traditional misinformation detectors when faced with evolving misinformation. <div>
arXiv:2509.16564v1 Announce Type: cross 
Abstract: Misinformation evolves as it spreads, shifting in language, framing, and moral emphasis to adapt to new audiences. However, current misinformation detection approaches implicitly assume that misinformation is static. We introduce MPCG, a multi-round, persona-conditioned framework that simulates how claims are iteratively reinterpreted by agents with distinct ideological perspectives. Our approach uses an uncensored large language model (LLM) to generate persona-specific claims across multiple rounds, conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution. We evaluate the generated claims through human and LLM-based annotations, cognitive effort metrics (readability, perplexity), emotion evocation metrics (sentiment analysis, morality), clustering, feasibility, and downstream classification. Results show strong agreement between human and GPT-4o-mini annotations, with higher divergence in fluency judgments. Generated claims require greater cognitive effort than the original claims and consistently reflect persona-aligned emotional and moral framing. Clustering and cosine similarity analyses confirm semantic drift across rounds while preserving topical coherence. Feasibility results show a 77% feasibility rate, confirming suitability for downstream tasks. Classification results reveal that commonly used misinformation detectors experience macro-F1 performance drops of up to 49.7%. The code is available at https://github.com/bcjr1997/MPCG
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Growing unlabeled networks</title>
<link>https://arxiv.org/abs/2509.17200</link>
<guid>https://arxiv.org/abs/2509.17200</guid>
<content:encoded><![CDATA[
<div> Keywords: growing networks, unlabeled trees, network symmetries, degree heterogeneity, leaf-based statistics

Summary:
In this study, models of growing unlabeled trees are introduced and analyzed, drawing parallels to well-known labeled growth models like uniform and preferential attachment. A theoretical framework is developed to examine the growth of unlabeled trees by focusing on leaf-based statistics. The analysis reveals that while some characteristics of labeled network growth are preserved, significant differences arise due to the symmetries among leaves in common neighborhoods. Degree heterogeneity is found to be amplified in unlabeled tree growth, with the extent of enhancement varying based on the specific growth dynamics employed. Mild enhancement is observed in uniform attachment, while preferential attachment leads to extreme enhancement. These results shed light on the impact of network symmetries on the evolution of unlabeled networks and offer insights that may extend beyond the realm of growing unlabeled trees. 

<br /><br />Summary: <div>
arXiv:2509.17200v1 Announce Type: cross 
Abstract: Models of growing networks are a central topic in network science. In these models, vertices are usually labeled by their arrival time, distinguishing even those node pairs whose structural roles are identical. In contrast, unlabeled networks encode only structure, so unlabeled growth rules must be defined in terms of structurally distinguishable outcomes; network symmetries therefore play a key role in unlabeled growth dynamics. Here, we introduce and study models of growing unlabeled trees, defined in analogy to widely-studied labeled growth models such as uniform and preferential attachment. We develop a theoretical formalism to analyze these trees via tracking their leaf-based statistics. We find that while many characteristics of labeled network growth are retained, numerous critical differences arise, caused primarily by symmetries among leaves in common neighborhoods. In particular, degree heterogeneity is enhanced, with the strength of this enhancement depending on details of growth dynamics: mild enhancement for uniform attachment, and extreme enhancement for preferential attachment. These results and the developed analytical formalism may be of interest beyond the setting of growing unlabeled trees.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Patterns in Toxicity and Antisocial Behavior Over User Lifetimes on Large Social Media Platforms</title>
<link>https://arxiv.org/abs/2407.09365</link>
<guid>https://arxiv.org/abs/2407.09365</guid>
<content:encoded><![CDATA[
<div> toxicity, social media, Reddit, Wikipedia, behavior
Summary:
The paper examines toxic behavior on social media platforms Reddit and Wikipedia over a 14-year period, analyzing nearly 500 million comments. It identifies trends in user toxicity levels, noting a shift from decreasing toxicity to increasing toxicity over time. The study also highlights differences in toxic behavior between Reddit and Wikipedia users, with active Reddit users exhibiting more toxicity compared to inactive Wikipedia users. Additionally, the research explores toxicity in discussions around widely-shared content, drawing parallels between trends in content-related toxicity and user behavior. Overall, the analysis provides insights into the evolution of toxic behavior on social media platforms and sheds light on the dynamics of toxicity within online communities. <div>
arXiv:2407.09365v2 Announce Type: replace 
Abstract: An increasing amount of attention has been devoted to the problem of "toxic" or antisocial behavior on social media. In this paper we analyze such behavior at very large scales: we analyze toxicity over a 14-year time span on nearly 500 million comments from Reddit and Wikipedia, grounded in two different proxies for toxicity.
  At the individual level, we analyze users' toxicity levels over the course of their time on the site, and find a striking reversal in trends: both Reddit and Wikipedia users tended to become less toxic over their life cycles on the site in the early (pre-2013) history of the site, but more toxic over their life cycles in the later (post-2013) history of the site. We also find that toxicity on Reddit and Wikipedia differ in a key way, with the most toxic behavior on Reddit exhibited in aggregate by the most active users, and the most toxic behavior on Wikipedia exhibited in aggregate by the least active users. Finally, we consider the toxicity of discussion around widely-shared pieces of content, and find that the trends for toxicity in discussion about content bear interesting similarities with the trends for toxicity in discussion by users.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the dynamics of external and self-citations and their role in shaping scientific impact</title>
<link>https://arxiv.org/abs/2503.09811</link>
<guid>https://arxiv.org/abs/2503.09811</guid>
<content:encoded><![CDATA[
<div> preferential attachment rule, citation distribution, scientific impact, self-citations, bibliometric indices

Summary: 
The study investigates the influence of the preferential attachment rule (PAR) on scientific citation distribution. It found that approximately 70% of citations adhere to PAR in the aggregated dataset, but there is significant variability at the individual level, especially for external citations. Self-citations show different patterns, with only 20% following PAR. More citable authors are preferentially cited, while less-cited authors have more random patterns. The study highlights the impact of self-citations on bibliometric indices such as the h-index. It shows that self-citations behave differently from external citations, raising questions about the underlying mechanisms. These findings provide insights into citation behaviors, emphasizing the limitations of current approaches. <div>
arXiv:2503.09811v2 Announce Type: replace-cross 
Abstract: Understanding the mechanisms driving the distribution of scientific citations is a key challenge in assessing the scientific impact of authors. We investigate the influence of the preferential attachment rule (PAR) in this process by analysing individual citation events from the DBLP dataset and two Scopus-based datasets, enabling us to estimate the probability of citations being assigned preferentially. Our findings reveal that, for the aggregated dataset, PAR dominates the citation distribution process, with approximately 70% of citations adhering to this mechanism. However, analysis at the individual level shows significant variability, with some authors experiencing a greater prevalence of preferential citations, particularly in the context of external citations. In contrast, self-citations exhibit notably different behaviour, with only 20% following PAR. We also demonstrate that the prominence of PAR increases with an author's citability (average citations per paper), suggesting that more citable authors are preferentially cited, while less-cited authors experience more random citation patterns. Furthermore, we show that self-citations may influence bibliometric indices, such as the h-index. Our results confirm the distinct dynamics of self-citations compared to external citations, raising questions about the mechanisms driving self-citation patterns. These findings provide new insights into citation behaviours and highlight the limitations of existing approaches.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoliTok-DE: A Multimodal Dataset of Political TikToks and Deletions From Germany</title>
<link>https://arxiv.org/abs/2509.15860</link>
<guid>https://arxiv.org/abs/2509.15860</guid>
<content:encoded><![CDATA[
<div> Dataset, PoliTok-DE, TikTok, Germany, 2024 Saxony state election
Summary:
PoliTok-DE is a large-scale multimodal dataset of TikTok posts related to the 2024 Saxony state election in Germany. The dataset contains over 195,000 posts, with 17.3% deleted from the platform. Posts were identified using the TikTok research API and web scraping. The dataset can support research on intolerance, political communication, platform policies, and qualitative-quantitative multimodal research. A case study on the co-occurrence of intolerance and entertainment using an annotated subset is reported. The dataset of post IDs is available on Hugging Face, with code provided for full content access. Access to deleted content is restricted but can be requested for research purposes. <div>
arXiv:2509.15860v1 Announce Type: new 
Abstract: We present PoliTok-DE, a large-scale multimodal dataset (video, audio, images, text) of TikTok posts related to the 2024 Saxony state election in Germany. The corpus contains over 195,000 posts published between 01.07.2024 and 30.11.2024, of which over 18,000 (17.3%) were subsequently deleted from the platform. Posts were identified via the TikTok research API and complemented with web scraping to retrieve full multimodal media and metadata. PoliTok-DE supports computational social science across substantive and methodological agendas: substantive work on intolerance and political communication; methodological work on platform policies around deleted content and qualitative-quantitative multimodal research. To illustrate one possible analysis, we report a case study on the co-occurrence of intolerance and entertainment using an annotated subset. The dataset of post IDs is publicly available on Hugging Face, and full content can be hydrated with our provided code. Access to the deleted content is restricted, and can be requested for research purposes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Community Notes: A Framework for Understanding and Building Crowdsourced Context Systems</title>
<link>https://arxiv.org/abs/2509.15434</link>
<guid>https://arxiv.org/abs/2509.15434</guid>
<content:encoded><![CDATA[
<div> Keywords: social media platforms, Crowdsourced Context Systems, literature review, theoretical model, design space <br />
<br />
Summary: 
The article examines the growing trend of social media platforms incorporating features that display crowdsourced context alongside posts, known as Crowdsourced Context Systems (CCS), as an alternative to traditional fact-checking. Through a systematic literature review and analysis of real-world CCS implementations, the authors develop a comprehensive framework comprising a theoretical model, a design space encompassing key aspects of CCS, and normative implications of different design choices. The theoretical model aids in conceptualizing CCS, while the design space covers aspects such as participation, curation, and transparency. The framework also addresses ethical considerations related to CCS implementation. The study provides a solid foundation for future research focusing on human-centered approaches to understanding and improving Crowdsourced Context Systems. <br /><br /> <div>
arXiv:2509.15434v1 Announce Type: cross 
Abstract: Social media platforms are increasingly developing features that display crowdsourced context alongside posts, modeled after X's Community Notes. These systems, which we term Crowdsourced Context Systems (CCS), have the potential to reshape our information ecosystem as major platforms embrace them as alternatives to top-down fact-checking. To deeply understand the features and implications of such systems, we perform a systematic literature review of existing CCS research and analyze several real-world CSS implementations. Based on our analysis, we develop a framework with three distinct components. First, we present a theoretical model to help conceptualize and define CCS. Second, we identify a design space encompassing six key aspects of CCS: participation, inputs, curation, presentation, platform treatment, and transparency. Third, we identify key normative implications of different CCS design and implementation choices. Our framework integrates these theoretical, design, and ethical perspectives to establish a foundation for future human-centered research on Crowdsourced Context Systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies</title>
<link>https://arxiv.org/abs/2509.15481</link>
<guid>https://arxiv.org/abs/2509.15481</guid>
<content:encoded><![CDATA[
<div> forecasting, solar, neural network, GHI, renewable energy

Summary:
SolarCAST is a new model for accurate solar forecasting that predicts future global horizontal irradiance (GHI) at a target site using historical GHI data from a site and nearby stations. It addresses three classes of confounding factors using scalable neural components: observable synchronous variables, latent synchronous factors, and time-lagged influences. SolarCAST outperforms leading baselines and achieves a 25.9% error reduction over a top commercial forecaster. It offers a lightweight and practical solution for localized solar forecasting that does not require specialized hardware or heavy preprocessing. Key components include an embedding module for observable variables, a spatio-temporal graph neural network for latent factors, and a gated transformer for time-lagged influences. The model is generalizable across diverse geographical conditions and provides a high level of accuracy for solar energy management.<br /><br />Summary: <div>
arXiv:2509.15481v1 Announce Type: cross 
Abstract: Accurate solar forecasting underpins effective renewable energy management. We present SolarCAST, a causally informed model predicting future global horizontal irradiance (GHI) at a target site using only historical GHI from site X and nearby stations S - unlike prior work that relies on sky-camera or satellite imagery requiring specialized hardware and heavy preprocessing. To deliver high accuracy with only public sensor data, SolarCAST models three classes of confounding factors behind X-S correlations using scalable neural components: (i) observable synchronous variables (e.g., time of day, station identity), handled via an embedding module; (ii) latent synchronous factors (e.g., regional weather patterns), captured by a spatio-temporal graph neural network; and (iii) time-lagged influences (e.g., cloud movement across stations), modeled with a gated transformer that learns temporal shifts. It outperforms leading time-series and multimodal baselines across diverse geographical conditions, and achieves a 25.9% error reduction over the top commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and generalizable solution for localized solar forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</title>
<link>https://arxiv.org/abs/2408.09613</link>
<guid>https://arxiv.org/abs/2408.09613</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, misinformation, social bots, Sina Weibo, dataset <br />
Summary: <br />
This paper investigates the relationship between social bots and misinformation on the Sina Weibo platform. A large dataset was created, containing both misinformation and verified information, as well as annotations for social bots and genuine accounts. The dataset was found to be comprehensive and of high quality. Analysis revealed that social bots play a significant role in spreading information, especially misinformation. Misinformation on similar topics has similar content, leading to echo chambers, which are further amplified by social bots. Additionally, social bots create similar content to manipulate public opinions. Overall, the study sheds light on how social bots contribute to the spread of misinformation on social media platforms like Sina Weibo. <br /> <div>
arXiv:2408.09613v3 Announce Type: replace 
Abstract: Social media platforms provide an ideal environment to spread misinformation, where social bots can accelerate the spread. This paper explores the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that includes annotations for both misinformation and social bots. From the misinformation perspective, the dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of verified information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, annotated using a weakly supervised annotator. Extensive experiments demonstrate the comprehensiveness of the dataset, the clear distinction between misinformation and real information, and the high quality of social bot annotations. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots would amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-set spectral clustering of time-evolving networks using the supra-Laplacian</title>
<link>https://arxiv.org/abs/2409.11984</link>
<guid>https://arxiv.org/abs/2409.11984</guid>
<content:encoded><![CDATA[
<div> spectral techniques, dynamic Laplacian, spectral clustering, multiplex networks, Sparse EigenBasis Approximation<br />
Summary:<br />
The article presents a novel approach for analyzing complex time-varying networks by adapting spectral techniques from continuous-time dynamics on manifolds to the graph setting. By formulating an inflated dynamic Laplacian for graphs and developing a spectral theory, the authors introduce spectral clustering methods for both multiplex and non-multiplex networks. These methods, based on the eigenvectors of the inflated dynamic Laplacian and Sparse EigenBasis Approximation (SEBA) post-processing, outperform existing algorithms like the Leiden algorithm in spacetime and layer-by-layer analysis. The application of this approach to US senate voting data reveals insights into increasing polarization over time, highlighting the effectiveness of the proposed techniques in studying spatiotemporal phenomena in dynamic networks.<br /><br />Summary: <div>
arXiv:2409.11984v3 Announce Type: replace 
Abstract: Complex time-varying networks are prominent models for a wide variety of spatiotemporal phenomena. The functioning of networks depends crucially on their connectivity, yet reliable techniques for determining communities in spacetime networks remain elusive. We adapt successful spectral techniques from continuous-time dynamics on manifolds to the graph setting to fill this gap. We formulate an inflated dynamic Laplacian for graphs and develop a spectral theory to underpin the corresponding algorithmic realisations. We develop spectral clustering approaches for both multiplex and non-multiplex networks, based on the eigenvectors of the inflated dynamic Laplacian and specialised Sparse EigenBasis Approximation (SEBA) post-processing of these eigenvectors. We demonstrate that our approach can outperform the Leiden algorithm applied both in spacetime and layer-by-layer, and we analyse voting data from the US senate (where senators come and go as congresses evolve) to quantify increasing polarisation in time.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silenced voices: social media polarization and women's marginalization in peacebuilding during the Northern Ethiopia War</title>
<link>https://arxiv.org/abs/2412.01549</link>
<guid>https://arxiv.org/abs/2412.01549</guid>
<content:encoded><![CDATA[
<div> social media, polarization, conflict, digital peacebuilding, women's participation
Summary:<br /><br />This study explores the impact of social media on conflict, with a focus on the Northern Ethiopia War. Using qualitative methods, it investigates how social media platforms contribute to polarization and violence. Women are particularly affected, facing displacement, exclusion from peace talks, and increased gender-based violence. Factors such as hostile online environments and the digital divide exacerbate these challenges. The study calls for media literacy programs, inclusive peacebuilding strategies, safe digital spaces, and gender-sensitive technological solutions. It highlights the need to address government-imposed internet shutdowns, unregulated social media, and low media literacy. By examining the intersection of technology, conflict, and gender in the Global South, this research offers valuable insights into leveraging digital platforms for sustainable peace and women's empowerment.<br /> <div>
arXiv:2412.01549v2 Announce Type: replace-cross 
Abstract: This study examines the complex relationship between social media, polarization, and conflict, with a focus on digital peacebuilding and women's participation, using the Northern Ethiopia War as a case study. Using a qualitative exploratory design through in-depth interviews, focus groups, and document analysis, the research examines how social media platforms influence conflict dynamics. The study applies and advances social identity, liberal feminist, and intersectionality theories to analyze social media's role in shaping conflict, mobilizing ethnic politics, and influencing women's involvement in peacebuilding. Findings reveal that the weaponization of social media intensifies polarization and offline violence. Women are disproportionately impacted through displacement, exclusion from peace negotiations, and heightened risks of gender-based violence, including rape. Contributing factors include hostile online environments, the digital divide, and prevailing socio-cultural norms. The study identifies significant gaps in leveraging digital platforms for sustainable peace, including government-imposed internet shutdowns, unregulated social media environments, and low media literacy. It recommends media literacy initiatives, inclusive peacebuilding frameworks, open and safe digital spaces, and gender-sensitive technological approaches. By centering digital technology, conflict, and gender in the Global South, this research contributes valuable insights to ongoing debates on ICT in conflict, peacebuilding, and women's empowerment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches</title>
<link>https://arxiv.org/abs/2509.14264</link>
<guid>https://arxiv.org/abs/2509.14264</guid>
<content:encoded><![CDATA[
<div> Keywords: toxic content, online platforms, machine learning, dataset, content moderation

Summary:
The study explores the prevalence of toxic content on digital platforms, particularly during crises and elections. It reviews 140 publications on various types of online toxicity, including hate speech, offensive language, and harmful discourse. The research delves into the datasets used in past studies, highlighting challenges and machine learning techniques employed for detection. The dataset covers content in 32 languages across different topics. The study also investigates the potential of leveraging cross-platform data to enhance classification model performance. Recommendations and guidelines are provided for future research on online toxic content and the implementation of content moderation strategies for mitigation. Additionally, practical guidelines are outlined for reducing toxic content on online platforms. 

<br /><br />Summary: <div>
arXiv:2509.14264v1 Announce Type: cross 
Abstract: Online toxic content has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. A significant amount of research has been focused on detecting or analyzing toxic content using machine-learning approaches. The proliferation of toxic content across digital platforms has spurred extensive research into automated detection mechanisms, primarily driven by advances in machine learning and natural language processing. Overall, the present study represents the synthesis of 140 publications on different types of toxic content on digital platforms. We present a comprehensive overview of the datasets used in previous studies focusing on definitions, data sources, challenges, and machine learning approaches employed in detecting online toxicity, such as hate speech, offensive language, and harmful discourse. The dataset encompasses content in 32 languages, covering topics such as elections, spontaneous events, and crises. We examine the possibility of using existing cross-platform data to improve the performance of classification models. We present the recommendations and guidelines for new research on online toxic consent and the use of content moderation for mitigation. Finally, we present some practical guidelines to mitigate toxic content from online platforms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers</title>
<link>https://arxiv.org/abs/2509.14266</link>
<guid>https://arxiv.org/abs/2509.14266</guid>
<content:encoded><![CDATA[
<div> transformer, hate speech detection, deep learning, machine learning, dataset characteristics

Summary:
- The study evaluates 38 models for hate speech detection on social media, focusing on accuracy and computational efficiency.
- Transformer architectures, especially RoBERTa, consistently outperform other models with accuracy and F1-scores above 90%.
- Hierarchical Attention Networks show promising results among deep learning approaches.
- Traditional methods like CatBoost and SVM remain competitive with F1-scores over 88% and lower computational costs.
- Balanced, moderately sized unprocessed datasets perform better than larger, preprocessed datasets, emphasizing the importance of dataset characteristics in hate speech detection systems.

<br /><br />Summary: <div>
arXiv:2509.14266v1 Announce Type: cross 
Abstract: The proliferation of hate speech on social media necessitates automated detection systems that balance accuracy with computational efficiency. This study evaluates 38 model configurations in detecting hate speech across datasets ranging from 6.5K to 451K samples. We analyze transformer architectures (e.g., BERT, RoBERTa, Distil-BERT), deep neural networks (e.g., CNN, LSTM, GRU, Hierarchical Attention Networks), and traditional machine learning methods (e.g., SVM, CatBoost, Random Forest). Our results show that transformers, particularly RoBERTa, consistently achieve superior performance with accuracy and F1-scores exceeding 90%. Among deep learning approaches, Hierarchical Attention Networks yield the best results, while traditional methods like CatBoost and SVM remain competitive, achieving F1-scores above 88% with significantly lower computational costs. Additionally, our analysis highlights the importance of dataset characteristics, with balanced, moderately sized unprocessed datasets outperforming larger, preprocessed datasets. These findings offer valuable insights for developing efficient and effective hate speech detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Alignment of Social Media Ranking Algorithms</title>
<link>https://arxiv.org/abs/2509.14434</link>
<guid>https://arxiv.org/abs/2509.14434</guid>
<content:encoded><![CDATA[
<div> Schwartz's theory, Basic Human Values, social media feed, algorithmic approach, value alignment <br />
<br />
The paper explores how social media feed rankings, driven by engagement signals, can be biased towards individualistic values. A new approach is presented using Schwartz's theory of Basic Human Values to align social media feeds with users' desired values. By allowing users to express weights on specific values, the algorithm ranks feeds based on value expressions in posts, creating personalized feed rankings that reflect users' values. Controlled experiments demonstrated that users could use these controls to design feeds aligned with their personal values, diverging significantly from traditional engagement-driven feeds. The study highlights the importance of addressing biases in algorithmic feed rankings and provides a method for users to curate feeds that align with their values.<br /><br />Summary: <div>
arXiv:2509.14434v1 Announce Type: cross 
Abstract: While social media feed rankings are primarily driven by engagement signals rather than any explicit value system, the resulting algorithmic feeds are not value-neutral: engagement may prioritize specific individualistic values. This paper presents an approach for social media feed value alignment. We adopt Schwartz's theory of Basic Human Values -- a broad set of human values that articulates complementary and opposing values forming the building blocks of many cultures -- and we implement an algorithmic approach that models and then ranks feeds by expressions of Schwartz's values in social media posts. Our approach enables controls where users can express weights on their desired values, combining these weights and post value expressions into a ranking that respects users' articulated trade-offs. Through controlled experiments (N=141 and N=250), we demonstrate that users can use these controls to architect feeds reflecting their desired values. Across users, value-ranked feeds align with personal values, diverging substantially from existing engagement-driven feeds.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Secure Computation Offloading and Trajectory Optimization for Multi-UAV MEC Against Aerial Eavesdropper</title>
<link>https://arxiv.org/abs/2509.14883</link>
<guid>https://arxiv.org/abs/2509.14883</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, multi-access edge computing, secure offloading, trajectory optimization, distributionally robust optimization

Summary:
The article discusses the challenges of secure offloading in UAV-based multi-access edge computing (MEC) networks, focusing on uncertainties in task computation and flexible trajectory optimizations. A robust problem formulation is proposed to minimize energy costs by optimizing connections, S-UAV trajectories, and offloading ratios while considering eavesdropping by malicious UAVs. The problem is solved using distributionally robust optimization and conditional value-at-risk mechanisms converted into second-order cone programming forms. Decoupling and successive convex approximation techniques are employed for S-UAV trajectory design. A global algorithm is developed to solve sub-problems in a block coordinate descent manner. Simulation results demonstrate the robustness of the proposed algorithms, with only a 2% increase in energy cost compared to the ideal scenario. <br /><br />Summary: <div>
arXiv:2509.14883v1 Announce Type: cross 
Abstract: The unmanned aerial vehicle (UAV) based multi-access edge computing (MEC) appears as a popular paradigm to reduce task processing latency. However, the secure offloading is an important issue when occurring aerial eavesdropping. Besides, the potential uncertainties in practical applications and flexible trajectory optimizations of UAVs pose formidable challenges for realizing robust offloading. In this paper, we consider the aerial secure MEC network including ground users, service unmanned aerial vehicles (S-UAVs) integrated with edge servers, and malicious UAVs overhearing transmission links. To deal with the task computation complexities, which are characterized as uncertainties, a robust problem is formulated with chance constraints. The energy cost is minimized by optimizing the connections, trajectories of S-UAVs and offloading ratios. Then, the proposed non-linear problem is tackled via the distributionally robust optimization and conditional value-at-risk mechanism, which is further transformed into the second order cone programming forms. Moreover, we decouple the reformulated problem and design the successive convex approximation for S-UAV trajectories. The global algorithm is designed to solve the sub-problems in a block coordinate decent manner. Finally, extensive simulations and numerical analyses are conducted to verify the robustness of the proposed algorithms, with just 2\% more energy cost compared with the ideal circumstance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Media Bias and Polarization through the Lens of a Markov Switching Latent Space Network Model</title>
<link>https://arxiv.org/abs/2306.07939</link>
<guid>https://arxiv.org/abs/2306.07939</guid>
<content:encoded><![CDATA[
<div> measure, media bias, polarization, latent space model, social media<br />
Summary:<br />
In this study, a dynamic latent space model is proposed to analyze online audience-duplication networks and media bias in news outlets. The model incorporates both network data and text-based indicators to measure media bias, identifying polarization regimes using Markov-Switching dynamics. By analyzing online activity data from European news outlets in 2015 and 2016, the study reveals a positive correlation between the media slant measure and external sources of bias. Additionally, the model sheds light on polarization patterns in different countries. This research contributes to the understanding of media bias, polarization, and the statistical properties of latent space network models. <div>
arXiv:2306.07939v3 Announce Type: replace-cross 
Abstract: News outlets are now more than ever incentivized to provide their audience with slanted news, while the intrinsic homophilic nature of online social media may exacerbate polarized opinions. Here, we propose a new dynamic latent space model for time-varying online audience-duplication networks, which exploits social media content to conduct inference on media bias and polarization of news outlets. We contribute to the literature in several directions: 1) Our model provides a novel measure of media bias that combines information from both network data and text-based indicators; 2) we endow our model with Markov-Switching dynamics to capture polarization regimes while maintaining a parsimonious specification; 3) we contribute to the literature on the statistical properties of latent space network models. The proposed model is applied to a set of data on the online activity of national and local news outlets from four European countries in the years 2015 and 2016. We find evidence of a strong positive correlation between our media slant measure and a well-grounded external source of media bias. In addition, we provide insight into the polarization regimes across the four countries considered.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order Network phenomena of cascading failures in resilient cities</title>
<link>https://arxiv.org/abs/2509.13808</link>
<guid>https://arxiv.org/abs/2509.13808</guid>
<content:encoded><![CDATA[
<div> Keywords: urban resilience, multimodal transport networks, higher-order network theory, cascading failures, dynamic functional resilience

Summary:
This study explores the threats to modern urban resilience posed by cascading failures in multimodal transport networks. By combining higher-order network theory with empirical evidence from a real-world transport network, the researchers find that network integration enhances static robustness metrics but also creates pathways for catastrophic cascades. The disconnect between static network structure and dynamic functional failure is a major challenge, as conventional centrality metrics provide poor predictors of a system's resilience. The findings suggest a need for a paradigm shift towards dynamic models to design and manage truly resilient urban systems. This research highlights the limitations of static analysis and emphasizes the importance of understanding the dynamic nature of urban transport networks in ensuring their resilience.<br /><br />Summary: <div>
arXiv:2509.13808v1 Announce Type: new 
Abstract: Modern urban resilience is threatened by cascading failures in multimodal transport networks, where localized shocks trigger widespread paralysis. Existing models, limited by their focus on pairwise interactions, often underestimate this systemic risk. To address this, we introduce a framework that confronts higher-order network theory with empirical evidence from a large-scale, real-world multimodal transport network. Our findings confirm a fundamental duality: network integration enhances static robustness metrics but simultaneously creates the structural pathways for catastrophic cascades. Crucially, we uncover the source of this paradox: a profound disconnect between static network structure and dynamic functional failure. We provide strong evidence that metrics derived from the network's static blueprint-encompassing both conventional low-order centrality and novel higher-order structural analyses-are fundamentally disconnected from and thus poor predictors of a system's dynamic functional resilience. This result highlights the inherent limitations of static analysis and underscores the need for a paradigm shift towards dynamic models to design and manage truly resilient urban systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outperforming Dijkstra on Sparse Graphs: The Lightning Network Use Case</title>
<link>https://arxiv.org/abs/2509.13448</link>
<guid>https://arxiv.org/abs/2509.13448</guid>
<content:encoded><![CDATA[
<div> BMSSP, Dijkstra, Rust, Lightning Network, Routing <br />
Summary: 
Efficient routing in payment channel networks (PCNs) like the Lightning Network is crucial, with Dijkstra algorithms commonly used for finding payment paths. The BMSSP algorithm is theoretically faster than Dijkstra on sparse graphs, but in an implementation on Rust using real Lightning Network topology data, BMSSP did not show significant performance improvements over Dijkstra. The results suggest that current implementations of BMSSP may not achieve the theoretical speedups, likely due to implementation and constant factor overheads. Despite not exhibiting the expected acceleration in practice, BMSSP still shows potential for enhancing Lightning Network routing efficiency. Future optimizations to PCN pathfinding algorithms could benefit from these findings. <br /><br />Summary: <div>
arXiv:2509.13448v1 Announce Type: cross 
Abstract: Efficient routing is critical for payment channel networks (PCNs) such as the Lightning Network (LN), where most clients currently rely on Dijkstra-based algorithms for payment pathfinding. While Dijkstra's algorithm has long been regarded as optimal on sparse graphs, recent theoretical work challenges this view. The new Bounded Multi-Source Shortest Path (BMSSP) algorithm by Duan et al. theoretically achieves $O(m~log^{2/3}~n)$ runtime, which is asymptotically faster than Dijkstra's $O(m + n~log~n)$ on sparse directed graphs. In this paper, we implement BMSSP on Rust and compare its performance against Dijkstra's using real LN topology data. Our evaluation, based on multiple randomized trials and statistical tests, shows that current implementations of BMSSP do not significantly outperform Dijkstra's in practice, and speedups are smaller than what theory predicts, possibly due to implementation and constant factor overheads. These results provide the first empirical evidence of BMSSP's potential to accelerate LN routing and inform future optimizations of PCN pathfinding algorithms.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Trust Evaluation for Effective Operation of Social IoT Systems via Hypergraph-Enabled Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.12240</link>
<guid>https://arxiv.org/abs/2509.12240</guid>
<content:encoded><![CDATA[
<div> trust, Social Internet-of-Things, hypergraph, self-supervised contrastive learning, device collaboration

Summary:
The paper introduces a new hypergraph-enabled self-supervised contrastive learning (HSCL) method for determining trust values in Social Internet-of-Things (IoT) systems. It addresses the challenge of calculating trust between devices based on complex social attributes by utilizing hypergraphs to represent high-order relationships. Hypergraph augmentation enhances the social hypergraph semantics, and a parameter-sharing hypergraph neural network fuses the relationships nonlinearly. A self-supervised contrastive learning approach generates meaningful device embeddings by comparing devices, hyperedges, and device-to-hyperedge relationships. Trust values are then calculated using these embeddings to distinguish trusted and untrusted nodes and identify the most trusted node. Experimental results demonstrate the superior performance of the proposed HSCL method compared to baseline algorithms. <br /><br />Summary: <div>
arXiv:2509.12240v1 Announce Type: new 
Abstract: Social Internet-of-Things (IoT) enhances collaboration between devices by endowing IoT systems with social attributes. However, calculating trust between devices based on complex and dynamic social attributes-similar to trust formation mechanisms in human society-poses a significant challenge. To address this issue, this paper presents a new hypergraph-enabled self-supervised contrastive learning (HSCL) method to accurately determine trust values between devices. To implement the proposed HSCL, hypergraphs are first used to discover and represent high-order relationships based on social attributes. Hypergraph augmentation is then applied to enhance the semantics of the generated social hypergraph, followed by the use of a parameter-sharing hypergraph neural network to nonlinearly fuse the high-order social relationships. Additionally, a self-supervised contrastive learning method is utilized to obtain meaningful device embeddings by conducting comparisons among devices, hyperedges, and device-to-hyperedge relationships. Finally, trust values between devices are calculated based on device embeddings that encapsulate high-order social relationships. Extensive experiments reveal that the proposed HSCL method outperforms baseline algorithms in effectively distinguishing between trusted and untrusted nodes and identifying the most trusted node.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Voices of Survival: From Social Media Disclosures to Support Provisions for Domestic Violence Victims</title>
<link>https://arxiv.org/abs/2509.12288</link>
<guid>https://arxiv.org/abs/2509.12288</guid>
<content:encoded><![CDATA[
<div> Keywords: Domestic Violence, Social Media, Self-disclosure, Support-seeking, Computational framework

Summary: 
This study introduces a computational framework for analyzing Domestic Violence (DV) self-disclosure and support-seeking behavior on social media platforms. The framework includes components such as self-disclosure detection, post clustering, topic summarization, and support extraction to understand how victims seek support online. By collecting data from social media communities, the framework aims to provide a deeper insight into DV self-disclosure patterns and the types of support victims receive online. The findings from this research not only contribute to the understanding of DV self-disclosure and online support mechanisms but also pave the way for victim-centered digital interventions. <div>
arXiv:2509.12288v1 Announce Type: new 
Abstract: Domestic Violence (DV) is a pervasive public health problem characterized by patterns of coercive and abusive behavior within intimate relationships. With the rise of social media as a key outlet for DV victims to disclose their experiences, online self-disclosure has emerged as a critical yet underexplored avenue for support-seeking. In addition, existing research lacks a comprehensive and nuanced understanding of DV self-disclosure, support provisions, and their connections. To address these gaps, this study proposes a novel computational framework for modeling DV support-seeking behavior alongside community support mechanisms. The framework consists of four key components: self-disclosure detection, post clustering, topic summarization, and support extraction and mapping. We implement and evaluate the framework with data collected from relevant social media communities. Our findings not only advance existing knowledge on DV self-disclosure and online support provisions but also enable victim-centered digital interventions.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Information Loss in Network Embeddings</title>
<link>https://arxiv.org/abs/2509.12396</link>
<guid>https://arxiv.org/abs/2509.12396</guid>
<content:encoded><![CDATA[
<div> algorithm, network embedding, graph generative model, community structure, link prediction

Summary: 
The article examines a basic algorithm for network embedding and explores the extent to which the learned representation captures the graph's generative model. It identifies scenarios where the embedding may not fully retain the original information, leading to loss of density information. The study also delves into the equivalence classes of graphons that yield the same embedding, emphasizing the preservation of community structure within these classes. Furthermore, the research highlights the limitations of using embeddings alone for link prediction, showcasing how naive predictions can introduce structural biases by disproportionately adding edges. By addressing implications for community detection and link prediction, the findings shed light on the challenges and potential pitfalls associated with network embedding techniques. <br /><br />Summary: <div>
arXiv:2509.12396v1 Announce Type: new 
Abstract: We analyze a simple algorithm for network embedding, explicitly characterizing conditions under which the learned representation encodes the graph's generative model fully, partially, or not at all. In cases where the embedding loses some information (i.e., is not invertible), we describe the equivalence classes of graphons that map to the same embedding, finding that these classes preserve community structure but lose substantial density information. Finally, we show implications for community detection and link prediction. Our results suggest strong limitations on the effectiveness of link prediction based on embeddings alone, and we show common conditions under which naive link prediction adds edges in a disproportionate manner that can either mitigate or exacerbate structural biases.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Driven Network Data for Smart Cities</title>
<link>https://arxiv.org/abs/2509.12403</link>
<guid>https://arxiv.org/abs/2509.12403</guid>
<content:encoded><![CDATA[
<div> Keywords: smart city, data-driven decision making, secure data sharing, privacy awareness, Wi-Fi network data <br />
Summary: 
This paper explores the importance of secure data sharing in developing smart cities. It highlights the critical role of network data, particularly from public Wi-Fi infrastructures, in enabling connected infrastructure and smart mobility. The focus is on safeguarding all attributes in real Wi-Fi network data to ensure the privacy and security of sensitive information. The methodology developed involves collaboration with legal experts, data custodians, and privacy specialists to ensure high-quality data. The study also addresses the integration of legal considerations for secure data sharing, promoting data-driven innovation while maintaining privacy awareness. The approach has been tested in a real scenario, demonstrating its practical application in enhancing the quality of Wi-Fi networks and advancing smart city initiatives. <br /><br />Summary: <div>
arXiv:2509.12403v1 Announce Type: new 
Abstract: A smart city is essential for sustainable urban development. In addition to citizen engagement, a smart city enables connected infrastructure, data-driven decision making and smart mobility. For most of these features, network data plays a critical role, particularly from public Wi-Fi infrastructures, where cities can benefit from optimized services such as public transport management and the safety and efficiency of large events. One of the biggest concerns in developing a smart city is using secure and private data. This is particularly relevant in the case of Wi-Fi network data, where sensitive information can be collected. This paper specifically addresses the problem of sharing secure data to enhance the quality of the Wi-Fi network in a city. Despite the high importance of this type of data, related work focuses on improving the safety of mobility patterns, targeting only the protection of MAC addresses. On the opposite side, we provide a practical methodology for safeguarding all attributes in real Wi-Fi network data. This study was developed in collaboration with a multidisciplinary team of legal experts, data custodians and technical privacy specialists, resulting in high-quality data. On top of that, we show how to integrate the legal considerations for secure data sharing. Our approach promotes data-driven innovation and privacy awareness in the context of smart city initiatives, which have been tested in a real scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ketto and the Science of Giving: A Data-Driven Investigation of Crowdfunding for India</title>
<link>https://arxiv.org/abs/2509.12616</link>
<guid>https://arxiv.org/abs/2509.12616</guid>
<content:encoded><![CDATA[
<div> Ketto, crowdfunding platform, social causes, India, campaigns<br />
<br />
Summary: 
The research investigates the Ketto crowdfunding platform in India, focusing on medical campaigns. It analyzes 119,493 campaigns and explores factors influencing campaign success. The study finds that medical campaigns address chronic health conditions but have low success rates. Most campaigns originate from populous states and major cities. Factors like online engagement, campaign duration, and updates positively impact funds raised. The research highlights the significance of understanding crowdfunding dynamics for community-driven needs in India.<br /> <div>
arXiv:2509.12616v1 Announce Type: new 
Abstract: The main goal of this paper is to investigate an up and coming crowdfunding platform used to raise funds for social causes in India called Ketto. Despite the growing usage of this platform, there is insufficient understanding in terms of why users choose this platform when there are other popular platforms such as GoFundMe. Using a dataset comprising of 119,493 Ketto campaigns, our research conducts an in-depth investigation into different aspects of how the campaigns on Ketto work with a specific focus on medical campaigns, which make up the largest percentage of social causes in the dataset. We also perform predictive modeling to identify the factors that contribute to the success of campaigns on this platform. We use several features such as the campaign metadata, description, geolocation, donor behaviors, and campaign-related features to learn about the platform and its components. Our results suggest that majority of the campaigns for medical causes seek funds to address chronic health conditions, yet medical campaigns have the least success rate. Most of the campaigns originate from the most populous states and major metropolitan cities in India. Our analysis also indicates that factors such as online engagement on the platform in terms of the number of comments, duration of the campaign, and frequent updates on a campaign positively influence the funds being raised. Overall, this preliminary work sheds light on the importance of investigating various dynamics around crowdfunding for India-focused community-driven needs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pressure-Based Diffusion Model for Influence Maximization on Social Networks</title>
<link>https://arxiv.org/abs/2509.12822</link>
<guid>https://arxiv.org/abs/2509.12822</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion model, social network, influence maximization, Pressure Threshold model, CyNetDiff

Summary: 
The paper introduces a new diffusion model called the Pressure Threshold model (PT) for simulating the spread of influence in social networks. This model extends the Linear Threshold Model by considering a node's outgoing influence proportional to the influence it receives from activated neighbors. The Influence Maximization (IM) problem, which involves selecting seed nodes for maximum graph coverage, is addressed in the context of the PT Model. Experiments conducted on real-world networks using the CyNetDiff Python library show that the PT Model selects different seed nodes compared to the LT Model. Additionally, the analysis reveals that densely connected networks magnify pressure effects more significantly than sparse networks. Overall, the PT Model offers a novel approach to understanding influence propagation in social networks and seed node selection for maximizing influence spread. 

<br /><br />Summary: <div>
arXiv:2509.12822v1 Announce Type: new 
Abstract: In many real-world scenarios, an individual's local social network carries significant influence over the opinions they form and subsequently propagate to others. In this paper, we propose a novel diffusion model -- the Pressure Threshold model (PT) -- for dynamically simulating the spread of influence through a social network. This new model extends the popular Linear Threshold Model (LT) by adjusting a node's outgoing influence proportional to the influence it receives from its activated neighbors. We address the Influence Maximization (IM) problem, which involves selecting the most effective seed nodes to achieve maximal graph coverage after a diffusion process, and how the problem manifests with the PT Model. Experiments conducted on real-world networks, facilitated by enhancements to the open-source network-diffusion Python library, CyNetDiff, demonstrate unique seed node selection for the PT Model when compared to the LT Model. Moreover, analyses demonstrate that densely connected networks amplify pressure effects more significantly than sparse networks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter</title>
<link>https://arxiv.org/abs/2509.13197</link>
<guid>https://arxiv.org/abs/2509.13197</guid>
<content:encoded><![CDATA[
<div> Keywords: collective action, podcast discussions, Black Lives Matter, emotional dimensions, activism

Summary: 
This study examines how participation in collective action is expressed in podcast discussions, focusing on the Black Lives Matter movement. Using podcast transcripts from the Structured Podcast Research Corpus, the study analyzed spoken language expressions related to problem-solving, calls-to-action, intentions, and execution in discussions about racial justice following key BLM events in 2020. Emotional analysis revealed eight key emotions associated with various stages of activism, with positive emotions dominant during calls-to-action, intention-setting, and action-taking. Surprisingly, negative emotions were linked to collective action, contradicting theoretical expectations. The findings suggest that emotional framing varies depending on the stage of activism and the format of the discussion. This research enhances our understanding of how activism is communicated in spoken digital discourse and the role of emotions in shaping activism narratives. 

<br /><br />Summary: <div>
arXiv:2509.13197v1 Announce Type: new 
Abstract: We study how participation in collective action is articulated in podcast discussions, using the Black Lives Matter (BLM) movement as a case study. While research on collective action discourse has primarily focused on text-based content, this study takes a first step toward analyzing audio formats by using podcast transcripts. Using the Structured Podcast Research Corpus (SPoRC), we investigated spoken language expressions of participation in collective action, categorized as problem-solution, call-to-action, intention, and execution. We identified podcast episodes discussing racial justice after important BLM-related events in May and June of 2020, and extracted participatory statements using a layered framework adapted from prior work on social media. We examined the emotional dimensions of these statements, detecting eight key emotions and their association with varying stages of activism. We found that emotional profiles vary by stage, with different positive emotions standing out during calls-to-action, intention, and execution. We detected negative associations between collective action and negative emotions, contrary to theoretical expectations. Our work contributes to a better understanding of how activism is expressed in spoken digital discourse and how emotional framing may depend on the format of the discussion.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending the BEND Framework to Webgraphs</title>
<link>https://arxiv.org/abs/2509.13212</link>
<guid>https://arxiv.org/abs/2509.13212</guid>
<content:encoded><![CDATA[
<div> SEO-boosted websites, webgraph manipulation, BEND framework, quantitative metrics, Kremlin-aligned

Summary:<br /><br />This article introduces a novel approach to quantitatively analyze attempts to manipulate webgraphs using SEO tactics. The BEND framework is utilized to characterize maneuvers within webgraph information environments, offering standardized metrics for assessment. Two small webgraphs featuring SEO-boosted Kremlin-aligned sites are analyzed using the proposed Webgraph BEND metrics, demonstrating their efficacy in improving BEND scores and characterizing webgraph environments. The study highlights the importance of having shared quantitative metrics for assessing and understanding webgraph manipulation tactics, providing analysts with a systematic tool for evaluating and identifying such activities. This framework can help researchers identify and address manipulative practices in webgraphs, contributing to a more transparent and trustworthy online information landscape. <div>
arXiv:2509.13212v1 Announce Type: new 
Abstract: Attempts to manipulate webgraphs can have many downstream impacts, but analysts lack shared quantitative metrics to characterize actions taken to manipulate information environments at this level. We demonstrate how the BEND framework can be used to characterize attempts to manipulate webgraph information environments, and propose quantitative metrics for BEND community maneuvers. We demonstrate the face validity of our proposed Webgraph BEND metrics by using them to characterize two small web-graphs containing SEO-boosted Kremlin-aligned websites. We demonstrate how our proposed metrics improve BEND scores in webgraph settings and demonstrate the usefulness of our metrics in characterizing webgraph information environments. These metrics offer analysts a systematic and standardized way to characterize attempts to manipulate webgraphs using common Search Engine Optimization tactics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Unbiased Sampling of Networks with Given Expected Degrees and Strengths</title>
<link>https://arxiv.org/abs/2509.13230</link>
<guid>https://arxiv.org/abs/2509.13230</guid>
<content:encoded><![CDATA[
<div> Keywords: configuration model, network structure, Chung-Lu model, maximum entropy principle, sampling algorithms 

Summary:
Efficient sampling algorithms have been proposed for maximum entropy-based configuration models, addressing the oversampling issue in the Chung-Lu model. The Chung-Lu model, commonly used for network structure analysis, tends to oversample edges between nodes with high degrees, leading to inaccurate statistical conclusions. By adapting the Miller-Hagberg algorithm, the proposed algorithms significantly reduce computational costs, enabling fast generation of theoretically rigorous configuration models. Evaluation on 103 empirical networks showed a speedup of 10-1000 times, making accurate configuration models practical for network structure analysis. The adoption of these efficient algorithms contributes to a more precise understanding of network structures and facilitates unbiased statistical assessments. 
<br /><br />Summary: <div>
arXiv:2509.13230v1 Announce Type: new 
Abstract: The configuration model is a cornerstone of statistical assessment of network structure. While the Chung-Lu model is among the most widely used configuration models, it systematically oversamples edges between large-degree nodes, leading to inaccurate statistical conclusions. Although the maximum entropy principle offers unbiased configuration models, its high computational cost has hindered widespread adoption, making the Chung-Lu model an inaccurate yet persistently practical choice. Here, we propose fast and efficient sampling algorithms for the max-entropy-based models by adapting the Miller-Hagberg algorithm. Evaluation on 103 empirical networks demonstrates 10-1000 times speedup, making theoretically rigorous configuration models practical and contributing to a more accurate understanding of network structure.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</title>
<link>https://arxiv.org/abs/2509.12255</link>
<guid>https://arxiv.org/abs/2509.12255</guid>
<content:encoded><![CDATA[
<div> GraphSAGE, Graph Neural Network, transactional networks, banking, embeddings<br />
Summary:<br />
This paper introduces GraphSAGE, a Graph Neural Network framework, for analysing complex transactional networks in the banking industry. Unlike traditional methods, GraphSAGE is scalable and can generalize to unseen nodes in large networks. The study constructs a transaction network using anonymised customer and merchant transactions and trains a GraphSAGE model to generate node embeddings. The embeddings reveal interpretable clusters aligned with geographic and demographic attributes. The utility of these embeddings is demonstrated in a money mule detection model, improving the prioritization of high-risk accounts. The adaptability of GraphSAGE to banking-scale networks is emphasized, showcasing its inductive capability, scalability, and interpretability. Financial organizations can leverage graph machine learning for actionable insights in transactional ecosystems based on the blueprint provided in this study. <br /><br /> <div>
arXiv:2509.12255v1 Announce Type: cross 
Abstract: Financial institutions increasingly require scalable tools to analyse complex transactional networks, yet traditional graph embedding methods struggle with dynamic, real-world banking data. This paper demonstrates the practical application of GraphSAGE, an inductive Graph Neural Network framework, to non-bipartite heterogeneous transaction networks within a banking context. Unlike transductive approaches, GraphSAGE scales well to large networks and can generalise to unseen nodes which is critical for institutions working with temporally evolving transactional data. We construct a transaction network using anonymised customer and merchant transactions and train a GraphSAGE model to generate node embeddings. Our exploratory work on the embeddings reveals interpretable clusters aligned with geographic and demographic attributes. Additionally, we illustrate their utility in downstream classification tasks by applying them to a money mule detection model where using these embeddings improves the prioritisation of high-risk accounts. Beyond fraud detection, our work highlights the adaptability of this framework to banking-scale networks, emphasising its inductive capability, scalability, and interpretability. This study provides a blueprint for financial organisations to harness graph machine learning for actionable insights in transactional ecosystems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology and Fragility of European High-Voltage Networks: A Cross-Country Comparative Analysis</title>
<link>https://arxiv.org/abs/2509.12900</link>
<guid>https://arxiv.org/abs/2509.12900</guid>
<content:encoded><![CDATA[
<div> Topological models, European countries, high-voltage grids, node degree distributions, network tolerance<br />
Summary:<br />
- Reliable electricity supply relies on high-voltage grid infrastructure in 15 European countries.
- Structural diversity impacts system vulnerability.
- Harmonized topological models reveal varying decay rates in node degree distributions.
- Network resilience is determined by the rate of decay, identifying systems prone to disruptions.
- Sensitivity to infrastructure layers affects numerical boundaries of resilience.   <br /> <div>
arXiv:2509.12900v1 Announce Type: cross 
Abstract: Reliable electricity supply depends on the seamless operation of high-voltage grid infrastructure spanning both transmission and sub-transmission levels. Beneath this apparent uniformity lies a striking structural diversity, which leaves a clear imprint on system vulnerability. In this paper, we present harmonized topological models of the high-voltage grids of 15 European countries, integrating all elements at voltage levels above 110 kV. Topological analysis of these networks reveals a simple yet robust pattern: node degree distributions consistently follow an exponential decay, but the rate of decay varies significantly across countries. Through a detailed and systematic evaluation of network tolerance to node and edge removals, we show that the decay rate delineates the boundary between systems that are more resilient to failures and those that are prone to large-scale disruptions. Furthermore, we demonstrate that this numerical boundary is highly sensitive to which layers of the infrastructure are included in the models. To our knowledge, this study provides the first quantitative cross-country comparison of 15 European high-voltage networks, linking topological properties with vulnerability characteristics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sublinear-Time Algorithms for Diagonally Dominant Systems and Applications to the Friedkin-Johnsen Model</title>
<link>https://arxiv.org/abs/2509.13112</link>
<guid>https://arxiv.org/abs/2509.13112</guid>
<content:encoded><![CDATA[
<div> diagonally dominant matrices, sublinear-time algorithms, randomized algorithms, linear systems, Friedkin-Johnsen model  
Summary:  
- The article explores sublinear-time algorithms for solving linear systems with diagonally dominant matrices.  
- The algorithms return an estimate of the solution with controlled additive error, needing minimal input reading.  
- A particular algorithm for additive error $\varepsilon$ achieves optimal time complexity with linear dependence on the maximum diagonal entry.  
- The approach extends beyond symmetric matrices to include general diagonally dominant matrices.  
- The methodology is based on analyzing a probabilistic recurrence inherent in the solution, leading to improved opinion estimation algorithms in the Friedkin-Johnsen model.  

<br /><br />Summary: <div>
arXiv:2509.13112v1 Announce Type: cross 
Abstract: We study sublinear-time algorithms for solving linear systems $Sz = b$, where $S$ is a diagonally dominant matrix, i.e., $|S_{ii}| \geq \delta + \sum_{j \ne i} |S_{ij}|$ for all $i \in [n]$, for some $\delta \geq 0$. We present randomized algorithms that, for any $u \in [n]$, return an estimate $z_u$ of $z^*_u$ with additive error $\varepsilon$ or $\varepsilon \lVert z^*\rVert_\infty$, where $z^*$ is some solution to $Sz^* = b$, and the algorithm only needs to read a small portion of the input $S$ and $b$. For example, when the additive error is $\varepsilon$ and assuming $\delta>0$, we give an algorithm that runs in time $O\left( \frac{\|b\|_\infty^2 S_{\max}}{\delta^3 \varepsilon^2} \log \frac{\| b \|_\infty}{\delta \varepsilon} \right)$, where $S_{\max} = \max_{i \in [n]} |S_{ii}|$. We also prove a matching lower bound, showing that the linear dependence on $S_{\max}$ is optimal. Unlike previous sublinear-time algorithms, which apply only to symmetric diagonally dominant matrices with non-negative diagonal entries, our algorithm works for general strictly diagonally dominant matrices ($\delta > 0$) and a broader class of non-strictly diagonally dominant matrices $(\delta = 0)$. Our approach is based on analyzing a simple probabilistic recurrence satisfied by the solution. As an application, we obtain an improved sublinear-time algorithm for opinion estimation in the Friedkin--Johnsen model.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-r Influential Community Search in Bipartite Graphs</title>
<link>https://arxiv.org/abs/2412.06216</link>
<guid>https://arxiv.org/abs/2412.06216</guid>
<content:encoded><![CDATA[
<div> Keywords: community search, bipartite graphs, influential community detection, $(\alpha,\beta)$-influential community model, algorithm efficiency

Summary:
In the study of community search on bipartite graphs, a new $(\alpha,\beta)$-influential community model is proposed to better reflect true community influence by considering average vertex weights from both layers. This model enhances the accuracy of identifying top-$r$ communities, which is known to be NP-hard. An exact recursive algorithm is developed, utilizing a slim tree structure and upper-bound techniques for efficiency. Additionally, a greedy approximate algorithm with optimized complexity is introduced, incorporating a pruning strategy. Experiment results on 10 real-world graphs validate the effectiveness and efficiency of the proposed algorithms in identifying influential communities. This research contributes to the advancement of community detection methods on bipartite graphs by offering a more comprehensive influence measure and improved computational efficiency. 

Summary: <div>
arXiv:2412.06216v3 Announce Type: replace 
Abstract: Community search on bipartite graphs, especially influential community detection, has received significant attention. Existing studies use minimum vertex weights, inadequately reflecting true community influence when some vertices have low weights. To address this, we introduce the $(\alpha,\beta)$-influential community model based on the average vertex weights from both layers, providing a more comprehensive influence measure. Given the NP-hardness of accurately identifying top-$r$ communities, we propose an exact recursive algorithm enhanced by a slim tree structure and upper-bound techniques to improve efficiency. Additionally, we develop a greedy approximate algorithm with $O((n+m)+m\log{n})$ complexity, further optimized by a pruning strategy. Experiments on 10 real-world graphs demonstrate the effectiveness and efficiency of our proposed algorithms.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free Adjustable Polynomial Graph Filtering for Ultra-fast Multimodal Recommendation</title>
<link>https://arxiv.org/abs/2503.04406</link>
<guid>https://arxiv.org/abs/2503.04406</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal recommender systems, graph filtering, efficient recommendations, neural networks, computational costs 

Summary: 
MultiModal-Graph Filtering (MM-GF) is a training-free method for improving multimodal recommender systems. It utilizes graph filtering to efficiently integrate information from diverse content types, such as text, images, and videos. By constructing similarity graphs for different modalities and user-item interactions, MM-GF optimally fuses multimodal signals using a polynomial graph filter. This allows for precise control of frequency response and flexible adaptation of filter coefficients. Experimental results on real-world datasets show that MM-GF significantly enhances recommendation accuracy by up to 22.25% compared to existing methods while reducing computational costs to less than 10 seconds. This method addresses the challenge of computational overhead in neural network-based models and offers a more efficient and accurate approach to multimodal recommendations. 

<br /><br />Summary: <div>
arXiv:2503.04406v2 Announce Type: replace-cross 
Abstract: Multimodal recommender systems improve the performance of canonical recommender systems with no item features by utilizing diverse content types such as text, images, and videos, while alleviating inherent sparsity of user-item interactions and accelerating user engagement. However, current neural network-based models often incur significant computational overhead due to the complex training process required to learn and integrate information from multiple modalities. To address this challenge,we propose MultiModal-Graph Filtering (MM-GF), a training-free method grounded in graph filtering (GF) for efficient and accurate multimodal recommendations. Specifically, MM-GF first constructs multiple similarity graphs for two distinct modalities as well as user-item interaction data. Then, MM-GF optimally fuses these multimodal signals using a polynomial graph filter that allows for precise control of the frequency response by adjusting frequency bounds. Furthermore, the filter coefficients are treated as hyperparameters, enabling flexible and data-driven adaptation. Extensive experiments on real-world benchmark datasets demonstrate that MM-GF not only improves recommendation accuracy by up to 22.25% compared to the best competitor but also dramatically reduces computational costs by achieving the runtime of less than 10 seconds.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Embedding Analysis for Anti-Money Laundering Detection</title>
<link>https://arxiv.org/abs/2509.10715</link>
<guid>https://arxiv.org/abs/2509.10715</guid>
<content:encoded><![CDATA[
<div> Graph embedding, money laundering, financial networks, suspicious cycles, centrality measures
Summary:
Using network embedding, this study examines money laundering in financial transaction networks by analyzing over one million accounts represented as a directed graph. By applying node2vec embeddings to refine previously identified suspicious cycles, a new network parameter known as the spread number is introduced. This, along with traditional centrality measures, contributes to an aggregate score denoted as $R, highlighting anti-central nodes - accounts that are structurally important yet evade detection. Findings indicate that only a small subset of cycles exhibit high $R values, identifying concentrated groups of suspicious accounts. The study showcases the effectiveness of embedding-based network analysis in uncovering laundering strategies that go undetected by conventional graph centrality measures. <br /><br />Summary: <div>
arXiv:2509.10715v1 Announce Type: new 
Abstract: We employ network embedding to detect money laundering in financial transaction networks. Using real anonymized banking data, we model over one million accounts as a directed graph and use it to refine previously detected suspicious cycles with node2vec embeddings, creating a new network parameter, the spread number. Combined with more traditional centrality measures, these define an aggregate score $R$ that highlights so-called anti-central nodes: accounts that are structurally important yet organized to avoid detection. Our results show only a small subset of cycles attain high $R$ values, flagging concentrated groups of suspicious accounts. Our approach demonstrates the potential of embedding-based network analysis to expose laundering strategies that evade traditional graph centrality measures.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socially-Informed Content Analysis of Online Human Behavior</title>
<link>https://arxiv.org/abs/2509.10807</link>
<guid>https://arxiv.org/abs/2509.10807</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, computational social science, user embeddings, COVID-19 discourse, online hate speech<br />
Summary: The dissertation explores the challenges of social media, focusing on political polarization, misinformation, hate speech, and echo chambers. It introduces a social network representation learning method that combines user-generated content and social connections to predict and visualize user attributes, communities, and behavioral patterns. The study investigates COVID-19 discourse on Twitter, revealing polarization and political echo chambers. It also delves into online hate speech, suggesting that the pursuit of social approval drives toxic behavior. Furthermore, the research examines the moral underpinnings of COVID-19 discussions, highlighting moral homophily and echo chambers. It indicates that moral diversity can enhance message reach and acceptance across ideological divides, contributing to computational social science and enhancing understanding of human behavior through social interactions and network homophily.<br /><br />Summary: <div>
arXiv:2509.10807v1 Announce Type: new 
Abstract: The explosive growth of social media has not only revolutionized communication but also brought challenges such as political polarization, misinformation, hate speech, and echo chambers. This dissertation employs computational social science techniques to investigate these issues, understand the social dynamics driving negative online behaviors, and propose data-driven solutions for healthier digital interactions. I begin by introducing a scalable social network representation learning method that integrates user-generated content with social connections to create unified user embeddings, enabling accurate prediction and visualization of user attributes, communities, and behavioral propensities. Using this tool, I explore three interrelated problems: 1) COVID-19 discourse on Twitter, revealing polarization and asymmetric political echo chambers; 2) online hate speech, suggesting the pursuit of social approval motivates toxic behavior; and 3) moral underpinnings of COVID-19 discussions, uncovering patterns of moral homophily and echo chambers, while also indicating moral diversity and plurality can improve message reach and acceptance across ideological divides. These findings contribute to the advancement of computational social science and provide a foundation for understanding human behavior through the lens of social interactions and network homophily.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YTCommentVerse: A Multi-Category Multi-Lingual YouTube Comment Corpus</title>
<link>https://arxiv.org/abs/2509.11057</link>
<guid>https://arxiv.org/abs/2509.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: YTCommentVerse, YouTube comments, multilingual, multi-category dataset, sentiment analysis<br />
<br />Summary: <br />YTCommentVerse is a large dataset of YouTube comments, containing over 32 million comments from 178,000 videos across 15 categories. It includes data from more than 20 million unique users in over 50 languages, making it valuable for sentiment, toxicity, and engagement analysis. The dataset provides video and comment IDs, user channel details, upvotes, and category labels, filling a gap in publicly available social media datasets. Researchers can explore patterns in sentiment, toxicity, and engagement across diverse cultural and topical contexts. YTCommentVerse is a valuable resource for analyzing video sharing platforms, offering detailed metadata and multiple languages for a comprehensive study of YouTube comments. <div>
arXiv:2509.11057v1 Announce Type: new 
Abstract: In this paper, we introduce YTCommentVerse, a large-scale multilingual and multi-category dataset of YouTube comments. It contains over 32 million comments from 178,000 videos contributed by more than 20 million unique users spanning 15 distinct YouTube content categories such as Music, News, Education and Entertainment. Each comment in the dataset includes video and comment IDs, user channel details, upvotes and category labels. With comments in over 50 languages, YTCommentVerse provides a rich resource for exploring sentiment, toxicity and engagement patterns across diverse cultural and topical contexts. This dataset helps fill a major gap in publicly available social media datasets particularly for analyzing video sharing platforms by combining multiple languages, detailed categories and other metadata.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Percolation Centrality Approximation with Importance Sampling</title>
<link>https://arxiv.org/abs/2509.11454</link>
<guid>https://arxiv.org/abs/2509.11454</guid>
<content:encoded><![CDATA[
<div> Algorithm, Importance Sampling, Percolation Centrality, Approximation, Graphs
<br />
Summary: 
The article introduces PercIS, an algorithm utilizing Importance Sampling to approximate percolation centrality in attributed graphs. Percolation centrality is crucial for measuring vertex importance in infectious processes or information diffusion. Existing sampling methods for percolation centrality face limitations in accuracy and efficiency. PercIS addresses this by providing high-quality estimates with tight sample size bounds. Experimental results demonstrate the algorithm's effectiveness in large real-world networks, showcasing superior performance in accuracy, sample sizes, and runtime compared to current methods. <div>
arXiv:2509.11454v1 Announce Type: new 
Abstract: In this work we present PercIS, an algorithm based on Importance Sampling to approximate the percolation centrality of all the nodes of a graph. Percolation centrality is a generalization of betweenness centrality to attributed graphs, and is a useful measure to quantify the importance of the vertices in a contagious process or to diffuse information. However, it is impractical to compute it exactly on modern-sized networks.
  First, we highlight key limitations of state-of-the-art sampling-based approximation methods for the percolation centrality, showing that in most cases they cannot achieve accurate solutions efficiently. Then, we propose and analyze a novel sampling algorithm based on Importance Sampling, proving tight sample size bounds to achieve high-quality approximations.
  Our extensive experimental evaluation shows that PercIS computes high-quality estimates and scales to large real-world networks, while significantly outperforming, in terms of sample sizes, accuracy and running times, the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Community Detection Method to Rule Them All!</title>
<link>https://arxiv.org/abs/2509.11490</link>
<guid>https://arxiv.org/abs/2509.11490</guid>
<content:encoded><![CDATA[
<div> Community detection, real-world graphs, downstream tasks, algorithm impact, community structure<br />
<br />
Summary: 
Community detection is crucial for analyzing real-world graphs and deriving local features for downstream tasks. The impact of community detection algorithms on downstream tasks is not well understood. Evaluation is typically based on intrinsic objectives or the overall impact on downstream tasks, but the specific algorithm used can significantly influence outcomes. This study explores how different algorithms affect task performance, showing that the properties of communities play a role in performance but are influenced by complex interactions. No single community property directly explains task performance, highlighting the need for a combination of random community generation and machine learning techniques for optimal results. <div>
arXiv:2509.11490v1 Announce Type: new 
Abstract: Community detection is a core tool for analyzing large realworld graphs. It is often used to derive additional local features of vertices and edges that will be used to perform a downstream task, yet the impact of community detection on downstream tasks is poorly understood. Prior work largely evaluates community detection algorithms by their intrinsic objectives (e.g., modularity). Or they evaluate the impact of using community detection onto on the downstream task. But the impact of particular community detection algortihm support the downstream task. We study the relationship between community structure and downstream performance across multiple algorithms and two tasks. Our analysis links community-level properties to task metrics (F1, precision, recall, AUC) and reveals that the choice of detection method materially affects outcomes. We explore thousands of community structures and show that while the properties of communities are the reason behind the impact on task performance, no single property explains performance in a direct way. Rather, results emerge from complex interactions among properties. As such, no standard community detection algorithm will derive the best downstream performance. We show that a method combining random community generation and simple machine learning techniques can derive better performance
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The threshold and quasi-stationary distribution for the SIS model on networks</title>
<link>https://arxiv.org/abs/2509.11706</link>
<guid>https://arxiv.org/abs/2509.11706</guid>
<content:encoded><![CDATA[
<div> Keywords: SIS model, arbitrary networks, pair approximation, epidemic threshold, quasi-stationary fraction<br />
Summary:<br />
- The study focuses on the Susceptible-Infectious-Susceptible (SIS) model on arbitrary networks, improving the pair approximation method by expanding the state space dynamically.<br />
- The enhanced method provides nodes with a memory of their last susceptible state, making the approximation simpler to implement and highly accurate.<br />
- The approach effectively determines the epidemic threshold and computes the quasi-stationary fraction of infected individuals above the threshold.<br />
- The method's accuracy is demonstrated for both finite graphs and infinite random graphs, highlighting its applicability in various network settings.<br />
- This research contributes to advancing the understanding of disease spread dynamics on complex networks, offering a valuable tool for predicting and managing infectious disease outbreaks. <br /> <div>
arXiv:2509.11706v1 Announce Type: new 
Abstract: We study the Susceptible-Infectious-Susceptible (SIS) model on arbitrary networks. The well-established pair approximation treats neighboring pairs of nodes exactly while making a mean field approximation for the rest of the network. We improve the method by expanding the state space dynamically, giving nodes a memory of when they last became susceptible. The resulting approximation is simple to implement and appears to be highly accurate, both in locating the epidemic threshold and in computing the quasi-stationary fraction of infected individuals above the threshold, for both finite graphs and infinite random graphs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Percolation and matrix spectrum through NIB message passing</title>
<link>https://arxiv.org/abs/2509.11730</link>
<guid>https://arxiv.org/abs/2509.11730</guid>
<content:encoded><![CDATA[
<div> belief propagation, message passing, KCN-method, NIB-method, percolation

Summary:<br />
- Belief propagation is a widely used message passing method due to its computational efficiency and versatility.
- The KCN-method was introduced to address the issue of loops affecting accuracy in belief propagation for networks, specifically in applications such as percolation and sparse matrix spectra calculation.
- The NIB-method is an improvement on the KCN-method, enhancing its performance in probabilistic graphical models on networks.
- This study demonstrates that the NIB-method's benefits can be realized not only in inference tasks in graphical models but also in its original applications of percolation and matrix spectra calculations.<br />  
Summary: <div>
arXiv:2509.11730v1 Announce Type: new 
Abstract: Given its computational efficiency and versatility, belief propagation is the most prominent message passing method in several applications. In order to diminish the damaging effect of loops on its accuracy, the first explicit version of generalized belief propagation for networks, the KCN-method, was recently introduced. This approach was originally developed in the context of two target problems: percolation and the calculation of the spectra of sparse matrices. Later on, the KCN-method was extended in order to deal with inference in the context of probabilistic graphical models on networks. It was in this scenario where an improvement on the KCN-method, the NIB-method, was conceived. We show here that this improvement can also achieved in the original applications of the KCN-method, namely percolation and matrix spectra.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fostering cultural change in research through innovative knowledge sharing, evaluation, and community engagement strategies</title>
<link>https://arxiv.org/abs/2509.12045</link>
<guid>https://arxiv.org/abs/2509.12045</guid>
<content:encoded><![CDATA[
<div> Keywords: research assessment, open knowledge, FAIR principles, metrics, scientific landscape 

Summary: 
The article discusses the need for a new system to value science and scientists, highlighting the importance of open knowledge and FAIR principles in research assessment. Despite efforts to promote more accurate assessment metrics and efficient knowledge sharing, outdated methods such as standardized metrics like h-index and journal impact factor still dominate evaluations, leading researchers to prioritize quantity over integrity and reproducibility. A global community of researchers, funding institutions, industrial partners, and publishers from 14 countries across 5 continents have come together to envision an evolved knowledge sharing and research evaluation system, aiming to bring about a cultural change in the scientific landscape towards fairness and equity. The proposed ideas set the groundwork for a more just and inclusive scientific environment. 

<br /><br />Summary: <div>
arXiv:2509.12045v1 Announce Type: new 
Abstract: Scientific research needs a new system that appropriately values science and scientists. Key innovations, within institutions and funding agencies, are driving better assessment of research, with open knowledge and FAIR (findable, accessible, interoperable, and reusable) principles as central pillars. Furthermore, coalitions, agreements, and robust infrastructures have emerged to promote more accurate assessment metrics and efficient knowledge sharing. However, despite these efforts, the system still relies on outdated methods where standardized metrics such as h-index and journal impact factor dominate evaluations. These metrics have had the unintended consequence of pushing researchers to produce more outputs at the expense of integrity and reproducibility. In this community paper, we bring together a global community of researchers, funding institutions, industrial partners, and publishers from 14 different countries across the 5 continents. We aim at collectively envision an evolved knowledge sharing and research evaluation along with the potential positive impact on every stakeholder involved. We imagine these ideas to set the groundwork for a cultural change to redefine a more fair and equitable scientific landscape.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset</title>
<link>https://arxiv.org/abs/2509.11136</link>
<guid>https://arxiv.org/abs/2509.11136</guid>
<content:encoded><![CDATA[
<div> Keywords: Persian names, gender detection, digital identity creation, dataset, frameworks<br />
Summary:<br />
This research addresses the challenges in natural language processing applications related to Persian names by introducing the PNGT-26K dataset, which consists of approximately 26,000 tuples of Persian names, their associated gender, and English transliterations. The dataset aims to improve performance in gender detection and digital identity creation for Persian names. Additionally, two frameworks are introduced: Open Gender Detection, a production-grade framework for probabilistic gender detection using user data, and Nominalist, an AI tool to help users choose usernames for social media accounts. These frameworks can enhance user experience and provide valuable tools for various applications. The PNGT-26K dataset, Nominalist, and Open Gender Detection frameworks are publicly available on Github.<br /> 
Summary: <div>
arXiv:2509.11136v1 Announce Type: cross 
Abstract: Persian names present unique challenges for natural language processing applications, particularly in gender detection and digital identity creation, due to transliteration inconsistencies and cultural-specific naming patterns. Existing tools exhibit significant performance degradation on Persian names, while the scarcity of comprehensive datasets further compounds these limitations. To address these challenges, the present research introduces PNGT-26K, a comprehensive dataset of Persian names, their commonly associated gender, and their English transliteration, consisting of approximately 26,000 tuples. As a demonstration of how this resource can be utilized, we also introduce two frameworks, namely Open Gender Detection and Nominalist. Open Gender Detection is a production-grade, ready-to-use framework for using existing data from a user, such as profile photo and name, to give a probabilistic guess about the person's gender. Nominalist, the second framework introduced by this paper, utilizes agentic AI to help users choose a username for their social media accounts on any platform. It can be easily integrated into any website to provide a better user experience. The PNGT-26K dataset, Nominalist and Open Gender Detection frameworks are publicly available on Github.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm</title>
<link>https://arxiv.org/abs/2509.11443</link>
<guid>https://arxiv.org/abs/2509.11443</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, 15-minute city concept, transformer models, multi-platform, social media<br />
Summary:<br />
This study conducted a multi-platform sentiment analysis of the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and annotation with Llama-3-8B, sentiment was classified across different text domains. Five models were benchmarked, with DistilRoBERTa achieving the highest F1 score. The study found that News data had inflated performance due to class imbalance, Reddit suffered from summarization loss, and Twitter presented a moderate challenge. The compressed models performed competitively, challenging the belief that larger models are necessary for sentiment analysis. Platform-specific trade-offs were identified, and directions for scalable sentiment classification in urban planning discourse were proposed.<br /> <div>
arXiv:2509.11443v1 Announce Type: cross 
Abstract: This study presents the first multi-platform sentiment analysis of public opinion on the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and Llama-3-8B for annotation, we classify sentiment across heterogeneous text domains. Our pipeline handles long-form and short-form text, supports consistent annotation, and enables reproducible evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1 (0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform consistency. Results show News data yields inflated performance due to class imbalance, Reddit suffers from summarization loss, and Twitter offers moderate challenge. Compressed models perform competitively, challenging assumptions that larger models are necessary. We identify platform-specific trade-offs and propose directions for scalable, real-world sentiment classification in urban planning discourse.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media</title>
<link>https://arxiv.org/abs/2509.11444</link>
<guid>https://arxiv.org/abs/2509.11444</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized social media, sentiment analysis, emotion analysis, narrative analysis, Bluesky

Summary:<br /><br />
The study introduces CognitiveSky, an open-source framework designed for sentiment, emotion, and narrative analysis on Bluesky, a decentralized social media platform. By using Bluesky's API, CognitiveSky applies transformer-based models to analyze large-scale user-generated content, generating structured outputs for dynamic visualization in a dashboard. The framework is built on free-tier infrastructure, ensuring low operational costs and high accessibility. It is showcased for monitoring mental health discourse but is adaptable for various domains like disinformation detection and crisis response. The modular design enables versatility, bridging large language models with decentralized networks transparently. CognitiveSky offers a transparent and extensible tool for computational social science in an evolving digital landscape.<br /><br /> <div>
arXiv:2509.11444v1 Announce Type: cross 
Abstract: The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Updating the Complex Systems Keyword Diagram Using Collective Feedback and Latest Literature Data</title>
<link>https://arxiv.org/abs/2509.11997</link>
<guid>https://arxiv.org/abs/2509.11997</guid>
<content:encoded><![CDATA[
<div> update, reorganization, complex systems, network science, keyword diagram

Summary:
The author reports on the update of a keyword diagram for complex systems generated in 2010. The update was based on feedback from social media, reference books, online resources, and keyword search hits. The data sources provided insight into both public perception and actual usage of keywords in complex systems publications. The resulting network visualization of complex systems keywords revealed differences and overlaps in keyword associations. Four topical communities were identified in the keyword association network, showing high interconnection among them. The updated diagram aims to provide a more accurate and current topic map of the complex systems field. <div>
arXiv:2509.11997v1 Announce Type: cross 
Abstract: The complex systems keyword diagram generated by the author in 2010 has been used widely in a variety of educational and outreach purposes, but it definitely needs a major update and reorganization. This short paper reports our recent attempt to update the keyword diagram using information collected from the following multiple sources: (a) collective feedback posted on social media, (b) recent reference books on complex systems and network science, (c) online resources on complex systems, and (d) keyword search hits obtained using OpenAlex, an open-access bibliographic catalogue of scientific publications. The data (a), (b) and (c) were used to incorporate the research community's internal perceptions of the relevant topics, whereas the data (d) was used to obtain more objective measurements of the keywords' relevance and associations from publications made in complex systems science. Results revealed differences and overlaps between public perception and actual usage of keywords in publications on complex systems. Four topical communities were obtained from the keyword association network, although they were highly intertwined with each other. We hope that the resulting network visualization of complex systems keywords provides a more up-to-date, accurate topic map of the field of complex systems as of today.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidencing preferential attachment in dependency network evolution</title>
<link>https://arxiv.org/abs/2509.12135</link>
<guid>https://arxiv.org/abs/2509.12135</guid>
<content:encoded><![CDATA[
<div> Keywords: Preferential attachment, network growth, scale-free, generalised linear model, Bayesian inference

Summary:
Preferential attachment is often seen as the driving force behind network growth, particularly in scale-free networks. However, traditional methods of determining scale-freeness may be questionable. By analyzing the evolution history of a network, a new approach using a generalised linear model allows for direct measurement of preferential attachment. This model considers in-degrees and their increments as covariates and responses, respectively. Parameters representing preferential attachment are integrated into the model, ensuring realistic tail heaviness of the degree distribution. Bayesian inference enables a hierarchical version of the model to be implemented. Application of this approach to the dependency network of R packages uncovers nuanced differences in behavior when considering new dependencies by new and existing packages, as well as when examining addition and removal of dependencies. <br /><br />Summary: <div>
arXiv:2509.12135v1 Announce Type: cross 
Abstract: Preferential attachment is often suggested to be the underlying mechanism of the growth of a network, largely due to that many real networks are, to a certain extent, scale-free. However, such attribution is usually made under debatable practices of determining scale-freeness and when only snapshots of the degree distribution are observed. In the presence of the evolution history of the network, modelling the increments of the evolution allows us to measure preferential attachment directly. Therefore, we propose a generalised linear model for such purpose, where the in-degrees and their increments are the covariate and response, respectively. Not only are the parameters that describe the preferential attachment directly incorporated, they also ensure that the tail heaviness of the asymptotic degree distribution is realistic. The Bayesian approach to inference enables the hierarchical version of the model to be implemented naturally. The application to the dependency network of R packages reveals subtly different behaviours between new dependencies by new and existing packages, and between addition and removal of dependencies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Follow Networks and Twitter's Content Recommender on Partisan Skew and Rumor Exposure during the 2022 U.S. Midterm Election</title>
<link>https://arxiv.org/abs/2509.09826</link>
<guid>https://arxiv.org/abs/2509.09826</guid>
<content:encoded><![CDATA[
<div> algorithmic systems, Twitter, social network, election, information<br />
<br />Summary:<br /> 
The study investigates the impact of Twitter's algorithmic content recommender and users' social networks during the 2022 U.S. midterm election. It is found that the algorithm significantly influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. The partisan makeup of a user's social network plays a crucial role, often exerting greater influence than the algorithm alone. The algorithmic feed reduces the proportion of election content shown to left-leaning accounts and skews content towards right-leaning sources compared to the reverse chronological feed. Additionally, the algorithm increases the prevalence of election-related rumors for right-leaning accounts and has mixed effects on the prevalence of low-quality information sources. This research sheds light on the complex outcomes of Twitter's recommender system during a critical election period and underscores the need for continued examination of algorithmic systems' impact on democratic processes. <div>
arXiv:2509.09826v1 Announce Type: new 
Abstract: Social media platforms shape users' experiences through the algorithmic systems they deploy. In this study, we examine to what extent Twitter's content recommender, in conjunction with a user's social network, impacts the topic, political skew, and reliability of information served on the platform during a high-stakes election. We utilize automated accounts to document Twitter's algorithmically curated and reverse chronological timelines throughout the U.S. 2022 midterm election. We find that the algorithmic timeline measurably influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. Critically, these impacts are mediated by the partisan makeup of one's personal social network, which often exerts greater influence than the algorithm alone. We find that the algorithmic feed decreases the proportion of election content shown to left-leaning accounts, and that it skews content toward right-leaning sources when compared to the reverse chronological feed. We additionally find evidence that the algorithmic system increases the prevalence of election-related rumors for right-leaning accounts, and has mixed effects on the prevalence of low-quality information sources. Our work provides insight into the outcomes of Twitter's complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Request a Note: How the Request Function Shapes X's Community Notes System</title>
<link>https://arxiv.org/abs/2509.09956</link>
<guid>https://arxiv.org/abs/2509.09956</guid>
<content:encoded><![CDATA[
<div> Keywords: X's Community Notes, fact-checking, scalability, misinformation, polarized

Summary:
The article discusses X's Community Notes, a fact-checking system that recently introduced a "Request Community Note" feature to solicit fact-checks from contributors on specific posts. The study examines 98,685 requested posts and their associated notes to evaluate how requests impact the system. Contributors prioritize posts with higher misleading content and from authors with greater misinformation exposure. However, they tend to neglect political content emphasized by requestors. Selection of posts for fact-checking also diverges along partisan lines, with more posts from Republicans being annotated. Only 12% of posts receive request-fostered notes from top contributors, which are rated as more helpful and less polarized. This reflects top contributors' selective fact-checking of misleading posts. The findings reveal the limitations and potential of requests for scaling high-quality community-based fact-checking.<br /><br />Summary:  <div>
arXiv:2509.09956v1 Announce Type: new 
Abstract: X's Community Notes is a crowdsourced fact-checking system. To improve its scalability, X recently introduced "Request Community Note" feature, enabling users to solicit fact-checks from contributors on specific posts. Yet, its implications for the system -- what gets checked, by whom, and with what quality -- remain unclear. Using 98,685 requested posts and their associated notes, we evaluate how requests shape the Community Notes system. We find that contributors prioritize posts with higher misleadingness and from authors with greater misinformation exposure, but neglect political content emphasized by requestors. Selection also diverges along partisan lines: contributors more often annotate posts from Republicans, while requestors surface more from Democrats. Although only 12% of posts receive request-fostered notes from top contributors, these notes are rated as more helpful and less polarized than others, partly reflecting top contributors' selective fact-checking of misleading posts. Our findings highlight both the limitations and promise of requests for scaling high-quality community-based fact-checking.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Higher-Order Interactions in Complex Networks: A U.S. Diplomacy Case Study</title>
<link>https://arxiv.org/abs/2509.10333</link>
<guid>https://arxiv.org/abs/2509.10333</guid>
<content:encoded><![CDATA[
<div> network structure, diplomatic communication, hypergraph, random-walk dynamics, interaction-prediction

Summary:<br />
The study explores the network structure of diplomatic communication using U.S. diplomatic cables from WikiLeaks. It adopts a hypergraph approach and develops a pipeline based on random-walk dynamics. The pipeline is evaluated on legislative co-sponsorship and organizational email data, showing superior performance in capturing higher-order, group-based interactions. Hypergraphs paired with random-walk dynamics provide a richer structural account of diplomacy and enable the inference of new diplomatic relationships. The study highlights the advantages of hypergraph modeling over traditional pairwise graphs and demonstrates the effectiveness of this approach in understanding and predicting diplomatic interactions. <div>
arXiv:2509.10333v1 Announce Type: new 
Abstract: Although diplomatic communication has long been examined in the social sciences, its network structure remains underexplored. Using the U.S. diplomatic cables released by WikiLeaks in 2010 as a case study, we adopt a network-science perspective. We represent diplomatic interactions as a hypergraph and develop a general, random-walk-based pipeline to evaluate this representation against traditional pairwise graphs. We further evaluate the pipeline on legislative co-sponsorship and organizational email data, finding improvements and empirical evidence that clarifies when hypergraph modeling is preferable to pairwise graphs. Overall, hypergraphs paired with appropriately specified random-walk dynamics more faithfully capture higher-order, group-based interactions, yielding a richer structural account of diplomacy and superior performance on interaction-prediction tasks that enables inferring new diplomatic relationships from existing patterns.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TikTok Rewards Divisive Political Messaging During the 2025 German Federal Election</title>
<link>https://arxiv.org/abs/2509.10336</link>
<guid>https://arxiv.org/abs/2509.10336</guid>
<content:encoded><![CDATA[
<div> Emotions, Outgroup animosity, Engagement, Political parties, TikTok<br />
Summary: <br />
A study analyzed German politicians' use of TikTok during the 2025 federal election, finding that videos expressing negative emotions and outgroup animosity were more engaging. Extreme parties, regardless of ideology, were more likely to post such content and achieve higher engagement compared to centrist parties. The findings suggest that TikTok's algorithmic curation rewards divisive political communication, potentially giving an advantage to extreme actors who exploit this dynamic. <div>
arXiv:2509.10336v1 Announce Type: new 
Abstract: Short-form video platforms like TikTok reshape how politicians communicate and have become important tools for electoral campaigning. Yet it remains unclear what kinds of political messages gain traction in these fast-paced, algorithmically curated environments, which are particularly popular among younger audiences. In this study, we use computational content analysis to analyze a comprehensive dataset of N=25,292 TikTok videos posted by German politicians in the run-up to the 2025 German federal election. Our empirical analysis shows that videos expressing negative emotions (e.g., anger, disgust) and outgroup animosity were significantly more likely to generate engagement than those emphasizing positive emotion, relatability, or identity. Furthermore, ideologically extreme parties (on both sides of the political spectrum) were both more likely to post this type of content and more successful in generating engagement than centrist parties. Taken together, these findings suggest that TikTok's platform dynamics systematically reward divisive over unifying political communication, thereby potentially benefiting extreme actors more inclined to capitalize on this logic.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beginner's Charm: Beginner-Heavy Teams Are Associated With High Scientific Disruption</title>
<link>https://arxiv.org/abs/2509.10389</link>
<guid>https://arxiv.org/abs/2509.10389</guid>
<content:encoded><![CDATA[
<div> disruption, innovation, beginners, collaboration, citations
<br />
Summary: 
Analyzing 28 million articles published between 1971 and 2020, a study found that teams with a higher fraction of beginners are more disruptive and innovative. These teams draw on broader and less canonical prior work, leading to atypical recombinations. Collaboration structure plays a role, with disruption being high when beginners work with early-career colleagues or co-authors with disruptive track records. While disruption and citations are typically negatively correlated, highly disruptive papers from beginner-heavy teams are highly cited. This highlights the "beginner's charm" in science, emphasizing the value of beginners in teams and suggesting strategies for fostering innovation in science and technology. <div>
arXiv:2509.10389v1 Announce Type: cross 
Abstract: Teams now drive most scientific advances, yet the impact of absolute beginners -- authors with no prior publications -- remains understudied. Analyzing over 28 million articles published between 1971 and 2020 across disciplines and team sizes, we uncover a universal and previously undocumented pattern: teams with a higher fraction of beginners are systematically more disruptive and innovative. Their contributions are linked to distinct knowledge-integration behaviors, including drawing on broader and less canonical prior work and producing more atypical recombinations. Collaboration structure further shapes outcomes: disruption is high when beginners work with early-career colleagues or with co-authors who have disruptive track records. Although disruption and citations are negatively correlated overall, highly disruptive papers from beginner-heavy teams are highly cited. These findings reveal a "beginner's charm" in science, highlighting the underrecognized yet powerful value of beginner fractions in teams and suggesting actionable strategies for fostering a thriving ecosystem of innovation in science and technology.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse</title>
<link>https://arxiv.org/abs/2412.16114</link>
<guid>https://arxiv.org/abs/2412.16114</guid>
<content:encoded><![CDATA[
<div> content moderation, online discourse, toxic speech, computational linguistics, social media  
Summary:  
- A methodology is proposed for measuring content-moderation-induced distortions in online discourse using text embeddings from computational linguistics.  
- Removing toxic Tweets alters the semantic composition of online content, not only due to toxic language but also the removal of topics often expressed toxically.  
- Using generative Large Language Models to rephrase toxic Tweets rather than removing them entirely can reduce toxicity while minimizing distortions in online content.  
- The study was conducted on a sample of 5 million US political Tweets and found consistent results across different embedding models, toxicity metrics, and samples.  
- This alternative approach to content moderation aims to preserve salvageable content in toxic Tweets, suggesting a more nuanced and effective strategy for addressing toxic speech on social media platforms.  

Summary: <div>
arXiv:2412.16114v2 Announce Type: replace 
Abstract: There is an ongoing debate about how to moderate toxic speech on social media and the impact of content moderation on online discourse. This paper proposes and validates a methodology for measuring the content-moderation-induced distortions in online discourse using text embeddings from computational linguistics. Applying the method to a representative sample of 5 million US political Tweets, we find that removing toxic Tweets alters the semantic composition of content. This finding is consistent across different embedding models, toxicity metrics, and samples. Importantly, we demonstrate that these effects are not solely driven by toxic language but by the removal of topics often expressed in toxic form. We propose an alternative approach to content moderation that uses generative Large Language Models to rephrase toxic Tweets, preserving their salvageable content rather than removing them entirely. We show that this rephrasing strategy reduces toxicity while minimizing distortions in online content.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster Synchronization via Graph Laplacian Eigenvectors</title>
<link>https://arxiv.org/abs/2503.18978</link>
<guid>https://arxiv.org/abs/2503.18978</guid>
<content:encoded><![CDATA[
<div> Equitable partitions, cluster synchronization, spectral framework, Laplacian spectrum, network dynamics <br />
Summary: Almost equitable partitions (AEPs) and cluster synchronization in oscillatory systems are linked through a spectral framework that utilizes eigenvectors to understand partition-induced synchronization behaviors. This framework reduces network dynamics to quotient graph projections, connecting transient hierarchical clustering and multi-frequency synchronization to network symmetry and community structure. The concept of quasi-equitable partitions at level $\delta$ ($\delta-$QEP) extends AEPs to account for structural imperfections and noise in real-world networks. This allows for a more realistic description of synchronization behavior in networks that lack perfect symmetries. The findings have significant implications for analyzing synchronization patterns in various networks, such as neural circuits and power grids. <br /> <br />Summary: <div>
arXiv:2503.18978v2 Announce Type: replace 
Abstract: Almost equitable partitions (AEPs) have been linked to cluster synchronization in oscillatory systems, highlighting the importance of structure in collective network dynamics. We provide a general spectral framework that formalizes this connection, showing how eigenvectors associated with AEPs span a subspace of the Laplacian spectrum that governs partition-induced synchronization behavior. This offers a principled reduction of network dynamics, allowing clustered states to be understood in terms of quotient graph projections. Our approach clarifies the conditions under which transient hierarchical clustering and multi-frequency synchronization emerge, and connects these dynamical phenomena directly to network symmetry and community structure. In doing so, we bridge a critical gap between static topology and dynamic behavior-namely, the lack of a spectral method for analyzing synchronization in networks that exhibit exact or approximate structural regularity. Perfect AEPs are rare in real-world networks since most have some degree of irregularity or noise. We define a relaxation of an AEP we call a quasi-equitable partition at level $\delta$ ($\delta-$QEP). $\delta-$QEPs can preserve many of the clustering-relevant properties of AEPs while tolerating structural imperfections and noise. This extension enables us to describe synchronization behavior in more realistic scenarios, where ideal symmetries are rarely present. Our findings have important implications for understanding synchronization patterns in real-world networks, from neural circuits to power grids.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Sheaf Neural Networks</title>
<link>https://arxiv.org/abs/2410.09590</link>
<guid>https://arxiv.org/abs/2410.09590</guid>
<content:encoded><![CDATA[
<div> Bayesian sheaf neural network, graph neural networks, convolution operation, cellular sheaf, reparameterizable probability distributions <br />
Summary:
The study introduces a Bayesian sheaf neural network for graph data, incorporating a convolution operation defined by a cellular sheaf to enhance expressive representations. By employing a variational approach, the network learns the sheaf as part of the architecture, resulting in improved performance on graph datasets. The innovative use of reparameterizable probability distributions on the rotation group improves model flexibility and robustness. Experimental results demonstrate that the Bayesian sheaf models outperform baseline models and exhibit reduced sensitivity to hyperparameters, particularly in limited training data scenarios. This approach offers a promising solution for learning representations of heterophilic graph data efficiently and effectively. <br /> <div>
arXiv:2410.09590v2 Announce Type: replace-cross 
Abstract: Equipping graph neural networks with a convolution operation defined in terms of a cellular sheaf offers advantages for learning expressive representations of heterophilic graph data. The most flexible approach to constructing the sheaf is to learn it as part of the network as a function of the node features. However, this leaves the network potentially overly sensitive to the learned sheaf. As a counter-measure, we propose a variational approach to learning cellular sheaves within sheaf neural networks, yielding an architecture we refer to as a Bayesian sheaf neural network. As part of this work, we define a novel family of reparameterizable probability distributions on the rotation group $SO(n)$ using the Cayley transform. We evaluate the Bayesian sheaf neural network on several graph datasets, and show that our Bayesian sheaf models achieve leading performance compared to baseline models and are less sensitive to the choice of hyperparameters under limited training data settings.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks</title>
<link>https://arxiv.org/abs/2509.09045</link>
<guid>https://arxiv.org/abs/2509.09045</guid>
<content:encoded><![CDATA[
<div> community detection, large graphs, graph mining, downstream tasks, algorithm evaluation

Summary: 
Large graphs in complex systems often require community detection methods to extract meaningful insights. These methods can uncover structural patterns by dividing graphs into subgraphs without relying on rich node attributes. However, the lack of ground truth community information and the absence of a universally optimal detection method pose challenges for practitioners. In this study, the impact of different community detection algorithms on downstream tasks is examined. A framework is proposed to systematically evaluate the performance of various algorithms, highlighting their varying effectiveness in different applications. The findings suggest that specific algorithms may yield superior results depending on the task at hand, emphasizing the significance of method selection in achieving optimal outcomes in graph mining applications. <div>
arXiv:2509.09045v1 Announce Type: new 
Abstract: In real-world scenarios, large graphs represent relationships among entities in complex systems. Mining these large graphs often containing millions of nodes and edges helps uncover structural patterns and meaningful insights. Dividing a large graph into smaller subgraphs facilitates complex system analysis by revealing local information. Community detection extracts clusters or communities of graphs based on statistical methods and machine learning models using various optimization techniques. Structure based community detection methods are more suitable for applying to graphs because they do not rely heavily on rich node or edge attribute information. The features derived from these communities can improve downstream graph mining tasks, such as link prediction and node classification. In real-world applications, we often lack ground truth community information. Additionally, there is neither a universally accepted gold standard for community detection nor a single method that is consistently optimal across diverse applications. In many cases, it is unclear how practitioners select community detection methods, and choices are often made without explicitly considering their potential impact on downstream tasks. In this study, we investigate whether the choice of community detection algorithm significantly influences the performance of downstream applications. We propose a framework capable of integrating various community detection methods to systematically evaluate their effects on downstream task outcomes. Our comparative analysis reveals that specific community detection algorithms yield superior results in certain applications, highlighting that method selection substantially affects performance.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-loop Learning Through Decentralized Communication Mechanisms</title>
<link>https://arxiv.org/abs/2509.09574</link>
<guid>https://arxiv.org/abs/2509.09574</guid>
<content:encoded><![CDATA[
<div> decentralized communication mechanism, multi-agent Markov decision process, game theory, human-in-the-loop learning, selfish agents<br />
Summary:<br />
The article discusses the challenges posed by selfish human agents in information sharing platforms and proposes a decentralized approach to incentivize exploration. By shifting from centralized to decentralized operation using a multi-agent Markov decision process (MA-MDP), the paper aims to regulate human behavior towards exploring unknown options for the benefit of all. An optimal decentralized communication mechanism is formulated, and an algorithm with linear complexity is presented to determine the timing of intermittent information sharing. The approach is further adapted for non-myopic agents to prevent over-exploration. Simulation experiments using real-world data demonstrate the effectiveness of the proposed decentralized mechanisms in various scenarios. <div>
arXiv:2509.09574v1 Announce Type: new 
Abstract: Information sharing platforms like TripAdvisor and Waze involve human agents as both information producers and consumers. All these platforms operate in a centralized way to collect agents' latest observations of new options (e.g., restaurants, hotels, travel routes) and share such information with all in real time. However, after hearing the central platforms' live updates, many human agents are found selfish and unwilling to further explore unknown options for the benefit of others in the long run. To regulate the human-in-the-loop learning (HILL) game against selfish agents' free-riding, this paper proposes a paradigm shift from centralized to decentralized way of operation that forces agents' local explorations through restricting information sharing. When game theory meets distributed learning, we formulate our decentralized communication mechanism's design as a new multi-agent Markov decision process (MA-MDP), and derive its analytical condition to outperform today's centralized operation. As the optimal decentralized communication mechanism in MA-MDP is NP-hard to solve, we present an asymptotically optimal algorithm with linear complexity to determine the mechanism's timing of intermittent information sharing. Then we turn to non-myopic agents who may revert to even over-explore, and adapt our mechanism design to work. Simulation experiments using real-world dataset demonstrate the effectiveness of our decentralized mechanisms for various scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2509.00389</link>
<guid>https://arxiv.org/abs/2509.00389</guid>
<content:encoded><![CDATA[
<div> DPG-Diff, cross-domain sequential recommendation, denoising framework, user preferences, knowledge transfer <br />
Summary:
DPG-Diff is a novel Disentangled Preference-Guided Diffusion Model for Cross-Domain Sequential Recommendation (CDSR). It addresses the challenge of conflicting domain-specific preferences in CDSR by decomposing user preferences into domain-invariant and domain-specific components. The model utilizes a reverse diffusion process guided by these disentangled preferences to enhance knowledge transfer and filter out noise in sequential signals. DPG-Diff outperforms existing baselines in CDSR, offering a more robust solution to capturing user preferences across diverse domains. The iterative refinement process of the model makes it effective at capturing subtle preference signals and improving recommendation quality. Overall, DPG-Diff provides a promising approach for enhancing the performance of cross-domain sequential recommendation systems. <br /> <div>
arXiv:2509.00389v1 Announce Type: cross 
Abstract: Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across domains to enhance recommendation quality. However, naive aggregation of sequential signals can introduce conflicting domain-specific preferences, leading to negative transfer. While Sequential Recommendation (SR) already suffers from noisy behaviors such as misclicks and impulsive actions, CDSR further amplifies this issue due to domain heterogeneity arising from diverse item types and user intents. The core challenge is disentangling three intertwined signals: domain-invariant preferences, domain-specific preferences, and noise. Diffusion Models (DMs) offer a generative denoising framework well-suited for disentangling complex user preferences and enhancing robustness to noise. Their iterative refinement process enables gradual denoising, making them effective at capturing subtle preference signals. However, existing applications in recommendation face notable limitations: sequential DMs often conflate shared and domain-specific preferences, while cross-domain collaborative filtering DMs neglect temporal dynamics, limiting their ability to model evolving user preferences. To bridge these gaps, we propose \textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the first diffusion-based approach tailored for CDSR, to or best knowledge. DPG-Diff decomposes user preferences into domain-invariant and domain-specific components, which jointly guide the reverse diffusion process. This disentangled guidance enables robust cross-domain knowledge transfer, mitigates negative transfer, and filters sequential noise. Extensive experiments on real-world datasets demonstrate that DPG-Diff consistently outperforms state-of-the-art baselines across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Engine Optimization: How to Dominate AI Search</title>
<link>https://arxiv.org/abs/2509.08919</link>
<guid>https://arxiv.org/abs/2509.08919</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, search engines, SEO, Earned media, GEO <br />
<br />
Summary: 
The article discusses the impact of generative AI-powered search engines on information retrieval and the need for a new approach called Generative Engine Optimization (GEO). Through a comparative analysis of AI Search and traditional web search like Google, the study reveals a bias towards Earned media by AI Search engines. The research also highlights differences in domain diversity, freshness, language stability, and sensitivity to phrasing among AI Search services. The strategic GEO agenda outlined includes recommendations for practitioners to optimize content for machine scannability, focus on earned media to establish authority, utilize engine-specific strategies, and overcome biases towards big brands. This empirical analysis provides a foundation and strategic framework for achieving visibility in the evolving generative search landscape. <br /> <div>
arXiv:2509.08919v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT, Perplexity, and Gemini is fundamentally reshaping information retrieval, moving from traditional ranked lists to synthesized, citation-backed answers. This shift challenges established Search Engine Optimization (SEO) practices and necessitates a new paradigm, which we term Generative Engine Optimization (GEO).
  This paper presents a comprehensive comparative analysis of AI Search and traditional web search (Google). Through a series of large-scale, controlled experiments across multiple verticals, languages, and query paraphrases, we quantify critical differences in how these systems source information. Our key findings reveal that AI Search exhibit a systematic and overwhelming bias towards Earned media (third-party, authoritative sources) over Brand-owned and Social content, a stark contrast to Google's more balanced mix. We further demonstrate that AI Search services differ significantly from each other in their domain diversity, freshness, cross-language stability, and sensitivity to phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We provide actionable guidance for practitioners, emphasizing the critical need to: (1) engineer content for machine scannability and justification, (2) dominate earned media to build AI-perceived authority, (3) adopt engine-specific and language-aware strategies, and (4) overcome the inherent "big brand bias" for niche players. Our work provides the foundational empirical analysis and a strategic framework for achieving visibility in the new generative search landscape.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>6G Resilience -- White Paper</title>
<link>https://arxiv.org/abs/2509.09005</link>
<guid>https://arxiv.org/abs/2509.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: 6G, resilience, sustainability, architecture, economics

Summary:
6G networks need to prioritize resilience, sustainability, and efficiency to withstand disruptions. The white paper advocates for a focus on resilience, with a 3R framework of reliability, robustness, and resilience. Measurable capabilities include graceful degradation, situational awareness, rapid reconfiguration, and learning-driven improvement. Architecturally, edge-native designs, open interfaces, and programmability are key, enabling islanded operations and diversity in radio, compute, energy, and timing layers. Techno-economic aspects include open platforms, complementors, and business model groups aligned with resilience objectives. Key enablers are AI-native control loops, zero-trust security, and networking techniques prioritizing critical traffic. The paper also highlights governance, standardization, and the importance of ecosystem externalities in enhancing resilience and opening new markets. This white paper aims to inspire stakeholders to shape the development of 6G resilience. 

<br /><br />Summary: 6G networks must prioritize resilience, using a 3R framework and measurable capabilities to ensure reliability and adaptation. Architectural designs, enablers like AI control loops and zero-trust security, and techno-economic aspects are crucial in enhancing network resilience and fostering new markets. Governance and standardization play a key role in shaping the development of 6G resilience. <div>
arXiv:2509.09005v1 Announce Type: cross 
Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged, complex disruptions. Mobile networks' shift from efficiency-first to sustainability-aware has motivated this white paper to assert that resilience is a primary design goal, alongside sustainability and efficiency, encompassing technology, architecture, and economics. We promote resilience by analysing dependencies between mobile networks and other critical systems, such as energy, transport, and emergency services, and illustrate how cascading failures spread through infrastructures. We formalise resilience using the 3R framework: reliability, robustness, resilience. Subsequently, we translate this into measurable capabilities: graceful degradation, situational awareness, rapid reconfiguration, and learning-driven improvement and recovery.
  Architecturally, we promote edge-native and locality-aware designs, open interfaces, and programmability to enable islanded operations, fallback modes, and multi-layer diversity (radio, compute, energy, timing). Key enablers include AI-native control loops with verifiable behaviour, zero-trust security rooted in hardware and supply-chain integrity, and networking techniques that prioritise critical traffic, time-sensitive flows, and inter-domain coordination.
  Resilience also has a techno-economic aspect: open platforms and high-quality complementors generate ecosystem externalities that enhance resilience while opening new markets. We identify nine business-model groups and several patterns aligned with the 3R objectives, and we outline governance and standardisation. This white paper serves as an initial step and catalyst for 6G resilience. It aims to inspire researchers, professionals, government officials, and the public, providing them with the essential components to understand and shape the development of 6G resilience.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls</title>
<link>https://arxiv.org/abs/2509.09063</link>
<guid>https://arxiv.org/abs/2509.09063</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet censorship, Iran, circumvention technologies, gamers, social networks <br />
Summary: 
This report discusses internet censorship in Iran and the use of circumvention technologies by Iranian internet users, particularly gamers. A mixed-methods study of 660 users was conducted, focusing on the digital literacy and social networking of gamers. Results show that younger users are more confident in circumvention techniques, with peer networks being a strong predictor of resilience. Gaming communities, especially those on platforms like Discord and Telegram, serve as hubs for sharing tactics. The study highlights the importance of both technical and social strategies for circumventing censorship. It concludes with implications for developers, researchers, and funders working in digital rights and information controls. <br /><br />Summary: <div>
arXiv:2509.09063v1 Announce Type: cross 
Abstract: Internet censorship in the Islamic Republic of Iran restricts access to global platforms and services, forcing users to rely on circumvention technologies such as VPNs, proxies, and tunneling tools. This report presents findings from a mixed-methods study of 660 Iranian internet users, with a focus on gamers as a digitally literate and socially networked community. Survey data are combined with network measurements of latency and VPN performance to identify both technical and social strategies of circumvention. Results show that while younger users report higher confidence with circumvention, peer networks, rather than formal training, are the strongest predictors of resilience. Gaming communities, particularly those active on platforms such as Discord and Telegram, serve as hubs for sharing tactics and lowering barriers to adoption. These findings extend existing work on usable security and censorship circumvention by highlighting the intersection of infrastructural conditions and social learning. The study concludes with design and policy implications for developers, researchers, and funders working on digital rights and information controls.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking</title>
<link>https://arxiv.org/abs/2509.09583</link>
<guid>https://arxiv.org/abs/2509.09583</guid>
<content:encoded><![CDATA[
<div> Personality detection, GPTs, Big-Five traits, online courses, SAMI <br />
Summary:<br />
The article discusses the importance of social connection in online learning environments and the limitations faced by tools like SAMI due to incomplete Theory of Mind. It proposes a personality detection model using GPTs to infer Big-Five traits from forum posts and integrates this into SAMI for personality-informed matchmaking. Benchmarking against established models shows the efficacy of this approach in inferring personality traits. Initial integration suggests that these traits can enhance social recommendations provided by SAMI, potentially improving student engagement and match quality. Further evaluation is required to fully understand the impact of personality traits on student interactions in online courses. <br /> <div>
arXiv:2509.09583v1 Announce Type: cross 
Abstract: Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applicability of the Minimal Dominating Set for Influence Maximization in Multilayer Networks</title>
<link>https://arxiv.org/abs/2502.15236</link>
<guid>https://arxiv.org/abs/2502.15236</guid>
<content:encoded><![CDATA[
<div> dominating set, influence maximization, multilayer networks, seed selection, Linear Threshold Model

Summary:<br /><br />
The study explores the use of minimal dominating sets (MDS) in enhancing seed selection for influence maximization in multilayer networks. By utilizing the Linear Threshold Model to represent influence spread, the researchers identify that incorporating MDS into the seed selection process can improve spread in certain situations. The improvement is prominent for larger seed set budgets, lower activation thresholds, and when using an "AND" strategy to aggregate influence across network layers. This enhancement is particularly beneficial when an individual's target opinion must be influenced across all social circles, rather than requiring the majority of acquaintances to hold that opinion. These findings shed light on the potential benefits of combining MDS with traditional seed selection methods in the context of influence maximization in multilayer networks. <div>
arXiv:2502.15236v3 Announce Type: replace 
Abstract: The minimal dominating set (MDS) is a well-established concept in network controllability and has been successfully applied in various domains, including sensor placement, network resilience, and epidemic containment. In this study, we adapt the local-improvement MDS routine and explore its potential for enhancing seed selection for influence maximization in multilayer networks (MLN). We employ the Linear Threshold Model (LTM), which offers an intuitive representation of influence spread or opinion dynamics by accounting for peer influence accumulation. To ensure interpretability, we utilize rank-refining seed selection methods, with the results further filtered with MDS. Our findings reveal that incorporating MDS into the seed selection process improves spread only within a specific range of situations. Notably, the improvement is observed for larger seed set budgets, lower activation thresholds, and when an "AND" strategy is used to aggregate influence across network layers. This scenario reflects situations where an individual does not require the majority of their acquaintances to hold a target opinion, but must be influenced across all social circles.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images</title>
<link>https://arxiv.org/abs/2405.03486</link>
<guid>https://arxiv.org/abs/2405.03486</guid>
<content:encoded><![CDATA[
<div> Image safety classifiers, Benchmarking framework, AI-generated images, Effectiveness, Robustness<br />
<br />
Summary: UnsafeBench is introduced as a benchmarking framework to assess the performance of image safety classifiers on a dataset consisting of real-world and AI-generated images categorized as safe or unsafe. Existing classifiers are found to be lacking in comprehensiveness and effectiveness in dealing with unsafe images, especially AI-generated ones. A distribution shift between real-world and AI-generated images leads to decreased effectiveness and robustness. PerspectiveVision is developed as a tool to enhance the performance of existing classifiers, particularly on AI-generated images. These findings highlight the challenges in image safety classification in the age of generative AI and provide a potential solution to improve the moderation of unsafe images.<br /> <div>
arXiv:2405.03486v3 Announce Type: replace-cross 
Abstract: With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which improves the effectiveness and robustness of existing classifiers, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Different Research Communities: Authorship Network</title>
<link>https://arxiv.org/abs/2409.00081</link>
<guid>https://arxiv.org/abs/2409.00081</guid>
<content:encoded><![CDATA[
<div> Google Scholar, data mining, software engineering, coauthorship network, influential authors<br />
<br />
Summary: 
In this study, Google Scholar data from 2000 to 2021 was collected for the research domains of Data Mining and Software Engineering in computer science. The advanced search option of Google Scholar allowed for the extraction of articles based on various criteria. The analysis focused on coauthorship networks in each domain, revealing distinct network structures with small communities of influential authors. Through extensive experiments, publication trends were analyzed, and influential authors and affiliated organizations were identified for each domain. The network analysis highlighted the unique features of each domain's network and showcased the presence of influential authors within specific communities. <div>
arXiv:2409.00081v2 Announce Type: replace-cross 
Abstract: Google Scholar is one of the top search engines to access research articles across multiple disciplines for scholarly literature. Google scholar advance search option gives the privilege to extract articles based on phrases, publishers name, authors name, time duration etc. In this work, we collected Google Scholar data (2000-2021) for two different research domains in computer science: Data Mining and Software Engineering. The scholar database resources are powerful for network analysis, data mining, and identify links between authors via authorship network. We examined coauthor-ship network for each domain and studied their network structure. Extensive experiments are performed to analyze publications trend and identifying influential authors and affiliated organizations for each domain. The network analysis shows that the networks features are distinct from one another and exhibit small communities within the influential authors of a particular domain.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Silence in Social Networks</title>
<link>https://arxiv.org/abs/2410.19685</link>
<guid>https://arxiv.org/abs/2410.19685</guid>
<content:encoded><![CDATA[
<div> Graph theory, opinion dynamics, Spiral of Silence, consensus, memory

Summary:
In this study, the classic multi-agent DeGroot model for opinion dynamics is extended to integrate the Spiral of Silence theory, where individuals may stay silent if they perceive their opinions to be in the minority. Two opinion update models are introduced: memoryless (SOM-) and memory-based (SOM+). The study shows that for the SOM- model, consensus is ensured for clique graphs but not for strongly connected aperiodic graphs. However, for the SOM+ model, consensus is not guaranteed even for clique graphs. Through simulations, the impact of silence dynamics on opinion formation is explored, revealing the limitations of achieving consensus in more complex social models. This research provides insights aligning with the key aspects of the Spiral of Silence theory, shedding light on how silence dynamics influence opinion dynamics in social networks. 

<br /><br />Summary: <div>
arXiv:2410.19685v2 Announce Type: replace-cross 
Abstract: We generalize the classic multi-agent DeGroot model for opinion dynamics to incorporate the Spiral of Silence theory from political science. This theory states that individuals may withhold their opinions when they perceive them to be in the minority. As in the DeGroot model, a community of agents is represented as a weighted directed graph whose edges indicate how much agents influence one another. However, agents whose current opinions are in the minority become silent (i.e., they do not express their opinion). Two models for opinion update are then introduced. In the memoryless opinion model (SOM-), agents update their opinion by taking the weighted average of their non-silent neighbors' opinions. In the memory based opinion model (SOM+), agents update their opinions by taking the weighted average of the opinions of all their neighbors, but for silent neighbors, their most recent opinion is considered. We show that for SOM- convergence to consensus is guaranteed for clique graphs but, unlike for the classic DeGroot, not guaranteed for strongly-connected aperiodic graphs. In contrast, we show that for SOM+ convergence to consensus is not guaranteed even for clique graphs. We showcase our models through simulations offering experimental insights that align with key aspects of the Spiral of Silence theory. These findings reveal the impact of silence dynamics on opinion formation and highlight the limitations of consensus in more nuanced social models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong</title>
<link>https://arxiv.org/abs/2509.08001</link>
<guid>https://arxiv.org/abs/2509.08001</guid>
<content:encoded><![CDATA[
<div> Keywords: employee turnover, professional networks, financial markets, temporal networks, machine learning

Summary:
The study explores the impact of professional networks on employee turnover in the financial sector using data from the Hong Kong Securities and Futures Commission. By analyzing temporal networks of professionals and firms, the research identifies a contagion effect where employees are more likely to leave if a significant portion of their peers depart within a short timeframe. The study introduces a graph-based feature propagation framework to capture peer influence and organizational stability, leading to a 30% improvement in turnover prediction compared to baseline models when incorporating network signals into machine learning algorithms. These findings demonstrate the predictive power of temporal network effects in understanding workforce dynamics and suggest the potential applications of network-based analytics in regulatory monitoring, talent management, and systemic risk assessment.<br /><br />Summary: <div>
arXiv:2509.08001v1 Announce Type: new 
Abstract: Employee turnover is a critical challenge in financial markets, yet little is known about the role of professional networks in shaping career moves. Using the Hong Kong Securities and Futures Commission (SFC) public register (2007-2024), we construct temporal networks of 121,883 professionals and 4,979 firms to analyze and predict employee departures. We introduce a graph-based feature propagation framework that captures peer influence and organizational stability. Our analysis shows a contagion effect: professionals are 23% more likely to leave when over 30% of their peers depart within six months. Embedding these network signals into machine learning models improves turnover prediction by 30% over baselines. These results highlight the predictive power of temporal network effects in workforce dynamics, and demonstrate how network-based analytics can inform regulatory monitoring, talent management, and systemic risk assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Dataset and Benchmark for Grounding Multimodal Misinformation</title>
<link>https://arxiv.org/abs/2509.08008</link>
<guid>https://arxiv.org/abs/2509.08008</guid>
<content:encoded><![CDATA[
<div> Keyword: online misinformation, multimodal content, GroundMM task, GroundLie360 dataset, VLM-based FakeMark baseline. 
Summary: 
The paper introduces the Grounding Multimodal Misinformation (GroundMM) task that aims to verify and localize misleading content across text, speech, and visuals. The researchers create the GroundLie360 dataset, which features a taxonomy of misinformation types, detailed annotations, and validation with evidence from Snopes and annotator reasoning. They propose the FakeMark baseline, a VLM-based system using single- and cross-modal cues for detection and grounding. The experiments demonstrate the challenges of the task and provide a foundation for interpretable multimodal misinformation detection. 
<br /><br />Summary: <div>
arXiv:2509.08008v1 Announce Type: new 
Abstract: The proliferation of online misinformation videos poses serious societal risks. Current datasets and detection methods primarily target binary classification or single-modality localization based on post-processed data, lacking the interpretability needed to counter persuasive misinformation. In this paper, we introduce the task of Grounding Multimodal Misinformation (GroundMM), which verifies multimodal content and localizes misleading segments across modalities. We present the first real-world dataset for this task, GroundLie360, featuring a taxonomy of misinformation types, fine-grained annotations across text, speech, and visuals, and validation with Snopes evidence and annotator reasoning. We also propose a VLM-based, QA-driven baseline, FakeMark, using single- and cross-modal cues for effective detection and grounding. Our experiments highlight the challenges of this task and lay a foundation for explainable multimodal misinformation detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Game is the Game: Dynamic network analysis and shifting roles in criminal networks</title>
<link>https://arxiv.org/abs/2509.08028</link>
<guid>https://arxiv.org/abs/2509.08028</guid>
<content:encoded><![CDATA[
<div> Keywords: criminal networks, key players, dynamic centrality, temporal analysis, network data uncertainty <br />
Summary: <br />
This paper introduces a novel approach to identifying key players in criminal networks by incorporating time as a crucial variable. Using network data from a two-year investigation of a drug trafficking network, the study applies dynamic Katz centrality to analyze changes in relationships and actors' relative importance over time. The results reveal actors who consistently hold central roles throughout the investigation and distinguish them from those who contribute significantly but for a limited period. Additionally, the study introduces a method to simulate missing data and assesses the impact of uncertainty on node rankings. The findings show that dynamic Katz centrality is effective in differentiating individual contributions within central nodes and tracking individual trajectories over time, even with incomplete data. This approach offers valuable insights for both organized crime scholars seeking to understand criminal collaboration complexity and law enforcement agencies targeting effective disruptions of criminal groups. <br /> <div>
arXiv:2509.08028v1 Announce Type: new 
Abstract: Objectives: This paper incorporates time as a crucial variable to identify key players in criminal networks and explores how actors' positions change over time. It then assesses the accuracy of the results against the uncertainty around network data collected from criminal justice records.
  Methods: Network data are from a judicial document for a two-year investigation targeting a drug trafficking and distribution network. We use Katz centrality in its dynamic version to explore changes in relationships and relative importance of network actors. We then use a novel method of introducing new edges to the network using Bernoulli random trials to simulate missing data and assess the extent to which node rankings based on Katz centrality change or remain the same when introducing some level of uncertainty to our observed network.
  Results: We identify actors who consistently held a central role over the course of the two-year investigation and differentiate them from actors who provided key contributions to the group's activities, but only for a limited period. We show that compared to centrality measures commonly used in criminal network analysis, dynamic Katz centrality is helpful to differentiate individual contributions even among central nodes and explore individual trajectories over time, even when data are incomplete.
  Conclusions: This paper demonstrates the value of key player identification using temporal network data and offers an additional analytical tool to both organised crime scholars trying to capture the complex nature of criminal collaboration and law enforcement agencies aiming at identifying appropriate targets and disrupting criminal groups.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals in the Noise: Decoding Unexpected Engagement Patterns on Twitter</title>
<link>https://arxiv.org/abs/2509.08128</link>
<guid>https://arxiv.org/abs/2509.08128</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, engagement, signaling theory, attention economy, Twitter

Summary:
This study investigates the factors influencing unexpected engagement patterns on Twitter, using a new metric called the "unexpectedness quotient." The analysis of over 600,000 tweets reveals that news, politics, and business tweets receive more retweets and comments, while games and sports content see higher likes and comments. Users prioritize sharing and discussing informational content, while emotional investment is higher in entertainment-related topics. The study highlights a relationship between content attributes and engagement types, with subjective tweets attracting more likes and objective tweets receiving more retweets. Additionally, longer and more complex tweets with URLs unexpectedly receive more retweets. These findings demonstrate how users use different engagement types as signals based on content characteristics, offering valuable insights for content creators, platform designers, and researchers studying online social behavior. <br /><br />Summary: <div>
arXiv:2509.08128v1 Announce Type: new 
Abstract: Social media platforms offer users multiple ways to engage with content--likes, retweets, and comments--creating a complex signaling system within the attention economy. While previous research has examined factors driving overall engagement, less is known about why certain tweets receive unexpectedly high levels of one type of engagement relative to others. Drawing on Signaling Theory and Attention Economy Theory, we investigate these unexpected engagement patterns on Twitter (now known as "X"), developing an "unexpectedness quotient" to quantify deviations from predicted engagement levels. Our analysis of over 600,000 tweets reveals distinct patterns in how content characteristics influence unexpected engagement. News, politics, and business tweets receive more retweets and comments than expected, suggesting users prioritize sharing and discussing informational content. In contrast, games and sports-related topics garner unexpected likes and comments, indicating higher emotional investment in these domains. The relationship between content attributes and engagement types follows clear patterns: subjective tweets attract more likes while objective tweets receive more retweets, and longer, complex tweets with URLs unexpectedly receive more retweets. These findings demonstrate how users employ different engagement types as signals of varying strength based on content characteristics, and how certain content types more effectively compete for attention in the social media ecosystem. Our results offer valuable insights for content creators optimizing engagement strategies, platform designers facilitating meaningful interactions, and researchers studying online social behavior.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cycle Walk for Sampling Measures on Spanning Forests for Redistricting</title>
<link>https://arxiv.org/abs/2509.08629</link>
<guid>https://arxiv.org/abs/2509.08629</guid>
<content:encoded><![CDATA[
<div> Markov Chain, Cycle Walk, graph partitions, political districts, sampling<br />
<br />
Summary:<br />
The article introduces a new Markov Chain called the Cycle Walk, designed for sampling measures of graph partitions with equally sized elements, particularly useful in the realm of generating and evaluating political districts. Through numerical evidence, it is shown that this new chain can efficiently sample target distributions that were previously challenging for existing sampling Markov chains. This advancement in sampling techniques holds significance for various applications where partition elements need to be of comparable sizes, such as in political districting. The Cycle Walk Markov Chain offers a promising solution for effectively sampling target distributions in scenarios where traditional methods may struggle. <div>
arXiv:2509.08629v1 Announce Type: new 
Abstract: We introduce a new Markov Chain called the Cycle Walk for sampling measures of graph partitions where the partition elements have roughly equal size. Such Markov Chains are of current interest in the generation and evaluation of political districts. We present numerical evidence that this chain can efficiently sample target distributions that have been difficult for existing sampling Markov chains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo Chambers and Information Brokers on Truth Social: A Study of Network Dynamics and Political Discourse</title>
<link>https://arxiv.org/abs/2509.08676</link>
<guid>https://arxiv.org/abs/2509.08676</guid>
<content:encoded><![CDATA[
<div> Keywords: Truth Social, political events, user interactions, polarization, central figures

Summary: 
This study analyzes the structural dynamics of the politically aligned social media platform, Truth Social, during significant political events. The research focuses on the U.S. Supreme Court's decision to overturn Roe v. Wade and the FBI's search of Mar-a-Lago. Through a dataset of user interactions based on re-truths, the study examines the network's evolution in terms of fragmentation, polarization, and user influence. The findings indicate a segmented and ideologically homogenous network dominated by a small group of central figures. Political events lead to temporary consolidation around shared narratives but quickly revert to fragmented, echo-chambered clusters. Key influencers, notably @realDonaldTrump, play a disproportionate role in shaping visibility and guiding discourse. Overall, the study sheds light on how Truth Social's infrastructure and community dynamics reinforce ideological boundaries and restrict diverse engagement. 

Summary: <div>
arXiv:2509.08676v1 Announce Type: new 
Abstract: This study examines the structural dynamics of Truth Social, a politically aligned social media platform, during two major political events: the U.S. Supreme Court's overturning of Roe v. Wade and the FBI's search of Mar-a-Lago. Using a large-scale dataset of user interactions based on re-truths (platform-native reposts), we analyze how the network evolves in relation to fragmentation, polarization, and user influence. Our findings reveal a segmented and ideologically homogenous structure dominated by a small number of central figures. Political events prompt temporary consolidation around shared narratives, followed by rapid returns to fragmented, echo-chambered clusters. Centrality metrics highlight the disproportionate role of key influencers, particularly @realDonaldTrump, in shaping visibility and directing discourse. These results contribute to research on alternative platforms, political communication, and online network behavior, demonstrating how infrastructure and community dynamics together reinforce ideological boundaries and limit cross-cutting engagement.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of geometric hypergraph embedding</title>
<link>https://arxiv.org/abs/2509.08772</link>
<guid>https://arxiv.org/abs/2509.08772</guid>
<content:encoded><![CDATA[
<div> Embedding, Hypergraph, Euclidean space, Spectral algorithms, Geometric structure  
Summary:  
- The article addresses the problem of embedding hypergraph nodes into Euclidean space using spectral algorithms based on the assumption of underlying geometric structure.  
- Two new spectral algorithms are proposed, leveraging the connection between hypergraphs and bipartite graphs to optimize the embedding through gradient descent.  
- Synthetic tests demonstrate the accuracy of the approach in revealing planted geometric structures in data, while real hypergraph tests show its usefulness in tasks such as detecting spurious or missing data and node clustering.  
- The algorithms are designed to tackle the inverse problem associated with generating geometric random hypergraphs and provide a measure of success to guide the optimization process.  
- By exploiting the assumption of interactions arising from closeness to unknown hyperedge centers, the proposed approach offers a promising solution for embedding hypergraphs for various applications.  

<br /><br />Summary:  <div>
arXiv:2509.08772v1 Announce Type: new 
Abstract: We consider the problem of embedding the nodes of a hypergraph into Euclidean space under the assumption that the interactions arose through closeness to unknown hyperedge centres. In this way, we tackle the inverse problem associated with the generation of geometric random hypergraphs. We propose two new spectral algorithms; both of these exploit the connection between hypergraphs and bipartite graphs. The assumption of an underlying geometric structure allows us to define a concrete measure of success that can be used to optimize the embedding via gradient descent. Synthetic tests show that this approach accurately reveals geometric structure that is planted in the data, and tests on real hypergraphs show that the approach is also useful for the downstream tasks of detecting spurious or missing data and node clustering.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Version: Security and Privacy Perceptions of Pakistani Facebook Matrimony Group Users</title>
<link>https://arxiv.org/abs/2509.08782</link>
<guid>https://arxiv.org/abs/2509.08782</guid>
<content:encoded><![CDATA[
<div> dating apps, censorship, Facebook matrimony groups, privacy concerns, security risks 

Summary: 
Participants in Pakistan use Facebook matrimony groups as alternatives to dating apps due to censorship. However, sharing personal information like photos and phone numbers exposes them to risks such as fraud and identity theft. Users have elevated privacy concerns, leading them to share limited information and creating mistrust among potential partners. Many are worried about profile authenticity, identity theft, harassment, and social judgment. Recommendations include stronger identity verification by group admins, stricter cybersecurity laws, clear platform guidelines for accountability, and technical enhancements like restricting screenshots and implementing anonymous chats to protect user data and build trust. <div>
arXiv:2509.08782v1 Announce Type: new 
Abstract: In Pakistan, where dating apps are subject to censorship, Facebook matrimony groups -- also referred to as marriage groups -- serve as alternative virtual spaces for members to search for potential life partners. To participate in these groups, members often share sensitive personal information such as photos, addresses, and phone numbers, which exposes them to risks such as fraud, blackmail, and identity theft. To better protect users of Facebook matrimony groups, we need to understand aspects related to user safety, such as how users perceive risks, what influences their trust in sharing personal information, and how they navigate security and privacy concerns when seeking potential partners online. In this study, through 23 semi-structured interviews, we explore how Pakistani users of Facebook matrimony groups perceive and navigate risks of sharing personal information, and how cultural norms and expectations influence their behavior in these groups.
  We find elevated privacy concerns among participants, leading them to share limited personal information and creating mistrust among potential partners. Many also expressed concerns about the authenticity of profiles and major security risks, such as identity theft, harassment, and social judgment. Our work highlights the challenges of safely navigating Facebook matrimony groups in Pakistan and offers recommendations for such as implementing stronger identity verification by group admins, enforcing stricter cybersecurity laws, clear platform guidelines to ensure accountability, and technical feature enhancements -- including restricting screenshots, picture downloads, and implementing anonymous chats -- to protect user data and build trust.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Truth: The Confidence Paradox in AI Fact-Checking</title>
<link>https://arxiv.org/abs/2509.08803</link>
<guid>https://arxiv.org/abs/2509.08803</guid>
<content:encoded><![CDATA[
<div> Misinformation, fact-checking solutions, language models, global contexts, Dunning-Kruger effect <br />
Summary: 
The study evaluates nine large language models for fact-checking effectiveness across global contexts. Findings suggest a pattern resembling the Dunning-Kruger effect, with smaller models showing high confidence but lower accuracy, and larger models demonstrating higher accuracy but lower confidence. This creates a risk of systemic bias in information verification, especially for resource-constrained organizations using smaller models. Performance gaps were most pronounced for non-English languages and claims from the Global South, potentially widening information inequalities. The study establishes a multilingual benchmark for future research and provides evidence for policy efforts aiming to ensure equitable access to trustworthy, AI-assisted fact-checking. <br /><br />Summary: <div>
arXiv:2509.08803v1 Announce Type: new 
Abstract: The rise of misinformation underscores the need for scalable and reliable fact-checking solutions. Large language models (LLMs) hold promise in automating fact verification, yet their effectiveness across global contexts remains uncertain. We systematically evaluate nine established LLMs across multiple categories (open/closed-source, multiple sizes, diverse architectures, reasoning-based) using 5,000 claims previously assessed by 174 professional fact-checking organizations across 47 languages. Our methodology tests model generalizability on claims postdating training cutoffs and four prompting strategies mirroring both citizen and professional fact-checker interactions, with over 240,000 human annotations as ground truth. Findings reveal a concerning pattern resembling the Dunning-Kruger effect: smaller, accessible models show high confidence despite lower accuracy, while larger models demonstrate higher accuracy but lower confidence. This risks systemic bias in information verification, as resource-constrained organizations typically use smaller models. Performance gaps are most pronounced for non-English languages and claims originating from the Global South, threatening to widen existing information inequalities. These results establish a multilingual benchmark for future research and provide an evidence base for policy aimed at ensuring equitable access to trustworthy, AI-assisted fact-checking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Elections in the Free State: Ensemble Analysis of Redistricting in New Hampshire</title>
<link>https://arxiv.org/abs/2509.07328</link>
<guid>https://arxiv.org/abs/2509.07328</guid>
<content:encoded><![CDATA[
<div> Keywords: legislative redistricting, New Hampshire, ensemble analysis, partisan outcomes, election data <br />
Summary: 
This article examines the legislative redistricting process in New Hampshire during the contentious 2020 census cycle. By utilizing an ensemble analysis of enacted districts, the study aims to provide mathematical context for evaluating claims about the redistricting maps in litigation. It operationalizes New Hampshire's redistricting rules to generate a variety of districting plans, establishing a baseline for expected districting plan behavior and assessing non-partisan justifications and geographic tradeoffs between districting criteria and partisan outcomes. The research also highlights the significance of selection and aggregation of election data in analyzing partisan symmetry measures. The findings shed light on the impact of redistricting decisions on partisan outcomes and underscore the importance of understanding the mathematical underpinnings of legislative redistricting efforts. <br /><br />Summary: <div>
arXiv:2509.07328v1 Announce Type: new 
Abstract: The process of legislative redistricting in New Hampshire, along with many other states across the country, was particularly contentious during the 2020 census cycle. In this paper we present an ensemble analysis of the enacted districts to provide mathematical context for claims made about these maps in litigation. Operationalizing the New Hampshire redistricting rules and algorithmically generating a large collection of districting plans allows us to construct a baseline for expected behavior of districting plans in the state and evaluate non-partisan justifications and geographic tradeoffs between districting criteria and partisan outcomes. In addition, our results demonstrate the impact of selection and aggregation of election data for analyzing partisan symmetry measures.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization Considering Influence, Cost and Time</title>
<link>https://arxiv.org/abs/2509.07625</link>
<guid>https://arxiv.org/abs/2509.07625</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence maximization, Social network analysis, Multi-objective optimization, Evolutionary algorithm, Viral marketing

Summary:
This paper introduces a new multi-objective influence maximization problem that considers influence spread, cost efficiency, and temporal urgency simultaneously. Existing studies often overlook the interconnectedness of these factors in scenarios like viral marketing and information campaigns. The proposed evolutionary variable-length search algorithm, EVEA, effectively searches for optimal node combinations to address this gap. Empirical evidence proves the feasibility and necessity of this multi-objective problem. The EVEA algorithm outperforms baseline methods by achieving higher hypervolume and faster convergence across real-world networks. It also maintains a diverse and balanced Pareto front among influence, cost, and time objectives. This research bridges the gap in the current literature by providing a holistic approach to optimize influence, cost, and time in social network analysis applications. 

<br /><br />Summary: <div>
arXiv:2509.07625v1 Announce Type: new 
Abstract: Influence maximization has been studied for social network analysis, such as viral marketing (advertising), rumor prevention, and opinion leader identification. However, most studies neglect the interplay between influence spread, cost efficiency, and temporal urgency. In practical scenarios such as viral marketing and information campaigns, jointly optimizing Influence, Cost, and Time is essential, yet remaining largely unaddressed in current literature. To bridge the gap, this paper proposes a new multi-objective influence maximization problem that simultaneously optimizes influence, cost, and time. We show the intuitive and empirical evidence to prove the feasibility and necessity of this multi-objective problem. We also develop an evolutionary variable-length search algorithm that can effectively search for optimal node combinations. The proposed EVEA algorithm outperforms all baselines, achieving up to 19.3% higher hypervolume and 25 to 40% faster convergence across four real-world networks, while maintaining a diverse and balanced Pareto front among influence, cost, and time objectives.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Topic Projected Opinion Dynamics for Resource Allocation</title>
<link>https://arxiv.org/abs/2509.07847</link>
<guid>https://arxiv.org/abs/2509.07847</guid>
<content:encoded><![CDATA[
<div> model, opinion formation, resource allocation, multiple agents, equilibrium 

Summary: 
The paper presents a model for opinion formation on resource allocation among multiple agents with hard budget constraints. Each agent has a utility function and seeks to maximize it within their constraints. Inter-agent and inter-topic coupling is defined through social networks and resource constraints. The study demonstrates that opinions always converge to equilibrium. In cases of weak antagonistic relations, opinions converge to a unique equilibrium point. The opinion formation game is shown to be a potential game, and the equilibria of the dynamics and Nash equilibria of the game are found to be related. The unique Nash equilibrium is characterized for networks with no antagonistic relations. Simulations are provided to illustrate the results. <div>
arXiv:2509.07847v1 Announce Type: cross 
Abstract: We propose a model of opinion formation on resource allocation among multiple topics by multiple agents, who are subject to hard budget constraints. We define a utility function for each agent and then derive a projected dynamical system model of opinion evolution assuming that each agent myopically seeks to maximize its utility subject to its constraints. Inter-agent coupling arises from an undirected social network, while inter-topic coupling arises from resource constraints. We show that opinions always converge to the equilibrium set. For special networks with very weak antagonistic relations, the opinions converge to a unique equilibrium point. We further show that the underlying opinion formation game is a potential game. We relate the equilibria of the dynamics and the Nash equilibria of the game and characterize the unique Nash equilibrium for networks with no antagonistic relations. Finally, simulations illustrate our findings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilevel User Credibility Assessment in Social Networks</title>
<link>https://arxiv.org/abs/2309.13305</link>
<guid>https://arxiv.org/abs/2309.13305</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, fake news, user credibility assessment, multilevel evaluation, deep learning

Summary:
Online social networks play a crucial role in spreading both real and fake news, with many users sharing harmful content and rumors. Existing methods for user credibility assessment have limitations in their binary approach and reliance on limited features. This paper introduces a new dataset and proposes the MultiCred model for multilevel credibility assessment. The model categorizes users into different credibility tiers based on a diverse range of features extracted from their profiles, tweets, and comments. MultiCred utilizes deep language models for analyzing text and deep neural networks for processing non-textual data. Extensive experiments show that MultiCred outperforms existing approaches in terms of accuracy metrics. The code for MultiCred is openly available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2309.13305v3 Announce Type: replace 
Abstract: Online social networks serve as major platforms for disseminating both real and fake news. Many users--intentionally or unintentionally--spread harmful content, misinformation, and rumors in domains such as politics and business. Consequently, user credibility assessment has become a prominent area of research in recent years. Most existing methods suffer from two key limitations. First, they treat credibility as a binary task, labeling users as either genuine or fake, whereas real-world applications often demand a more nuanced, multilevel evaluation. Second, they rely on only a subset of relevant features, which constrains their predictive performance. In this paper, we address the lack of a dataset suitable for multilevel credibility assessment by first devising a collection method tailored to this task. We then propose the \textit{MultiCred} model, which assigns users to one of several credibility tiers based on a rich and diverse set of features extracted from their profiles, tweets, and comments. MultiCred leverages deep language models for textual analysis and deep neural networks for non-textual data processing. Our extensive experiments demonstrate that MultiCred significantly outperforms existing approaches across multiple accuracy metrics. Our code is publicly available at https://github.com/Mohammad-Moradi/MultiCred.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Perplexity Predicts Scientific Surprise and Transformative Impact</title>
<link>https://arxiv.org/abs/2509.05591</link>
<guid>https://arxiv.org/abs/2509.05591</guid>
<content:encoded><![CDATA[
<div> surprise, deep neural networks, large language models, scientific breakthroughs, interdisciplinary engagement
<br />
Summary: 
Deep neural networks, specifically large language models, can be used to quantify surprise in scientific research. Analyzing over 2 million papers, higher perplexity scores from these models predict papers that receive more variable review ratings, longer editorial delays, and greater reviewer uncertainty. The most perplexing papers can result in significant scientific achievements or be discounted. They are published in journals with varying impact factors, receive fewer short-term citations, but generate more interdisciplinary engagement. These papers are often supported by speculative funders like DARPA and are published in prestigious venues. In contrast, in humanities research, the least surprising work is celebrated and cited more. Computational measures of linguistic surprise offer a scalable approach to identify potentially transformative research that challenges conventional scientific thinking.
<br /><br />Summary: <div>
arXiv:2509.05591v1 Announce Type: new 
Abstract: Scientific breakthroughs typically emerge through the surprising violation of established research ideas, yet quantifying surprise has remained elusive because it requires a coherent model of all contemporary scientific worldviews. Deep neural networks like large language models (LLMs) are arbitrary function approximators tuned to consistently expect the expressions and ideas on which they were trained and those semantically nearby. This suggests that as LLMs improve at generating plausible text, so the perplexity or improbability a text sequence would be generated by them should come to better predict scientific surprise and disruptive importance. Analyzing over 2 million papers across multiple disciplines published immediately following the training of 5 prominent open LLMs, here we show that higher perplexity scores systematically predict papers that receive more variable review ratings, longer editorial delays, and greater reviewer uncertainty. The most perplexing papers exhibit bimodal outcomes: disproportionately represented among the most celebrated scientific achievements and also the most discounted. High-perplexity papers tend to be published in journals with more variable impact factors and receive fewer short-term citations but in prestigious venues that bet on long-term impact. They also generate more interdisciplinary engagement portending long-term influence, and are more likely to have been supported by speculative funders like DARPA versus the NIH. Interestingly, we find the opposite pattern for humanities research, where the least surprising work is the most celebrated and cited. Our findings reveal that computational measures of corpus-wide linguistic surprise can forecast the reception and ultimate influence of scientific ideas, offering a scalable approach to recognize and generate potentially transformative research that challenge conventional scientific thinking.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spatiotemporal Adaptive Local Search Method for Tracking Congestion Propagation in Dynamic Networks</title>
<link>https://arxiv.org/abs/2509.06099</link>
<guid>https://arxiv.org/abs/2509.06099</guid>
<content:encoded><![CDATA[
<div> dynamic adjacency matrix learning, local search algorithm, congestion propagation, traffic networks, congestion mitigation

Summary:<br />
The study introduces the Spatiotemporal Adaptive Local Search (STALS) method to address challenges in traffic congestion propagation in urban areas. The method utilizes dynamic adjacency matrix learning to capture spatiotemporal relationships and a local search algorithm to identify congestion bottlenecks and propagation pathways. STALS outperforms existing methods in robustness and efficiency on benchmark networks and large-scale traffic networks in New York City, Shanghai, and Urumqi. The study demonstrates the scalability and adaptability of STALS in congestion mitigation by integrating dynamic graph learning with Geo-driven spatial analytics. The results highlight the method's ability to maintain high performance across different data granularities and spatial scales, showcasing its potential for optimizing urban sustainability and spatial accessibility.<br /> <div>
arXiv:2509.06099v1 Announce Type: new 
Abstract: Traffic congestion propagation poses significant challenges to urban sustainability, disrupting spatial accessibility. The cascading effect of traffic congestion propagation can cause large-scale disruptions to networks. Existing studies have laid a solid foundation for characterizing the cascading effects. However, they typically rely on predefined graph structures and lack adaptability to diverse data granularities. To address these limitations, we propose a spatiotemporal adaptive local search (STALS) method, which feeds the dynamically adaptive adjacency matrices into the local search algorithm to learn propagation rules. Specifically, the STALS is composed of two data-driven modules. One is a dynamic adjacency matrix learning module, which learns the spatiotemporal relationship from congestion graphs by fusing four node features. The other one is the local search module, which introduces local dominance to identify multi-scale congestion bottlenecks and search their propagation pathways. We test our method on the four benchmark networks with an average of 15,000 nodes. The STALS remains a Normalized Mutual Information (NMI) score at 0.97 and an average execution time of 27.66s, outperforming six state-of-the-art methods in robustness and efficiency. We also apply the STALS to three large-scale traffic networks in New York City, the United States, Shanghai, China, and Urumqi, China. The ablation study reveals an average modularity of 0.78 across three cities, demonstrating the spatiotemporal-scale invariance of frequencytransformed features and the spatial heterogeneity of geometric topological features. By integrating dynamic graph learning with Geo-driven spatial analytics, STALS provides a scalable tool for congestion mitigation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Such Thing as Free Brain Time: For a Pigouvian Tax on Attention Capture</title>
<link>https://arxiv.org/abs/2509.06453</link>
<guid>https://arxiv.org/abs/2509.06453</guid>
<content:encoded><![CDATA[
<div> attention economy, commodification, externalities, Pigouvian tax, digital regulation
<br />
Attention has become a valuable resource in the digital age, subject to market dynamics. This article explores the commodification of attention within the attention economy and argues for its recognition as a common good at risk of over-exploitation. Attention is not just an individual cognitive process but a collective resource susceptible to enclosure by digital platforms. Negative externalities of the attention economy include diminished agency, health issues, societal and political harms. These externalities, largely unpriced by markets, constitute a significant market failure. To address this, the article suggests a Pigouvian tax on attention capture to internalize the social cost of excessive digital engagement. Such a tax would encourage changes in platform design while protecting user autonomy. By viewing attention as a shared resource essential for human agency, health, and democracy, this article presents a new perspective on digital regulation.
<br /><br />Summary: <div>
arXiv:2509.06453v1 Announce Type: new 
Abstract: In our age of digital platforms, human attention has become a scarce and highly valuable resource, rivalrous, tradable, and increasingly subject to market dynamics. This article explores the commodification of attention within the framework of the attention economy, arguing that attention should be understood as a common good threatened by over-exploitation. Drawing from philosophical, economic, and legal perspectives, we first conceptualize attention not only as an individual cognitive process but as a collective and infrastructural phenomenon susceptible to enclosure by digital intermediaries. We then identify and analyze negative externalities of the attention economy, particularly those stemming from excessive screen time: diminished individual agency, adverse health outcomes, and societal and political harms, including democratic erosion and inequality. These harms are largely unpriced by market actors and constitute a significant market failure. In response, among a spectrum of public policy tools ranging from informational campaigns to outright restrictions, we propose a Pigouvian tax on attention capture as a promising regulatory instrument to internalize the externalities and, in particular, the social cost of compulsive digital engagement. Such a tax would incentivize structural changes in platform design while preserving user autonomy. By reclaiming attention as a shared resource vital to human agency, health, and democracy, this article contributes a novel economic and policy lens to the debate on digital regulation. Ultimately, this article advocates for a paradigm shift: from treating attention as a private, monetizable asset to protecting it as a collective resource vital for humanity.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Listener Structure Underlying K-pop's Global Success: A Large-Scale Listening Data Analysis</title>
<link>https://arxiv.org/abs/2509.06606</link>
<guid>https://arxiv.org/abs/2509.06606</guid>
<content:encoded><![CDATA[
<div> play counts, K-pop, Last.fm, genre tags, music listeners <br />
Summary:<br /> 
- K-pop has transitioned from a regionally popular genre in Asia to a global music genre with a growing fanbase worldwide.
- Analysis of Last.fm data shows a significant increase in K-pop plays between 2005 and 2019, driven by a small group of heavy listeners.
- The Gini coefficient for play counts in K-pop is higher than that of mainstream genres and other niche genres.
- User-assigned genre tags reveal that K-pop evolved from a local Asian genre to a distinct global music genre between 2005 and 2010.
- This study provides insights into how K-pop has gained recognition and popularity on a global scale, shedding light on its appeal and significance in the music industry. <br /> <div>
arXiv:2509.06606v1 Announce Type: new 
Abstract: From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from Last.fm. An analysis of the distribution of play counts reveals that K-pop experienced a significant increase in plays between 2005 and 2019, largely supported by a small group of heavy listeners. The Gini coefficient in play counts is notably greater than that of existing mainstream genres and other growing niche genres. Furthermore, an analysis based on user-assigned genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed its status as a local Asian genre and established itself as a distinct music genre in its own right.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How candidates evoke identity and issues on TikTok</title>
<link>https://arxiv.org/abs/2509.05310</link>
<guid>https://arxiv.org/abs/2509.05310</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, TikTok, campaign communication, political science theories, identity, issues

Summary:
Candidates in the 2024 US Presidential Election utilized TikTok for campaign communication, focusing on identity and issues to appeal to voters. Trump predominantly attacked opponent Harris and emphasized Republican identities and issues, while Harris highlighted Democratic identities and valued issues. Both candidates mentioned identities more than issues in their posts, with a majority of posts not referencing either. The study combines TikTok posts from the Harris and Trump campaigns with survey data to analyze campaign strategies and voter responses. Findings suggest a strategic use of social media platforms for political messaging, emphasizing the importance of identity and issues in campaign communication. The study provides insights into how social media platforms shape political discourse and influence voter perceptions during election campaigns.<br /><br />Summary: <div>
arXiv:2509.05310v1 Announce Type: cross 
Abstract: Social media platforms are increasingly central to campaign communication, with both paid (advertising) and earned (organic) posts used for fundraising, mobilization, and persuasion. TikTok, and other short-form video platforms, with its short-video format and content-driven algorithms, demand unique content. We examine the final six months before the 2024 US Presidential Election to understand how major campaigns used TikTok. We frame our analysis around two political science theories. The first is the expressive (identity) model, where voters are motivated by their group memberships and candidates appeal to those identities. Alternatively, the instrumental (issues) model argues voters align with politicians advocating their key issues. We also examine how often candidates attacked opponents, reflecting literature showing attacks are common in politics. We combine two datasets: posts from the Harris and Trump campaigns on TikTok (July-November 2024) and a two-wave 2022 survey of around 1,000 respondents. Results show Trump more often disparaged Harris and emphasized identities and issues distinguishing Republicans, while Harris more often highlighted Democratic identities and valued issues. Although issues predict party ID, both candidates referenced identities more (34 percent of posts) than issues (25 percent), with most posts mentioning neither (55 percent).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19</title>
<link>https://arxiv.org/abs/2509.05322</link>
<guid>https://arxiv.org/abs/2509.05322</guid>
<content:encoded><![CDATA[
<div> compression ratio, edge-centric network measures, pruning performance, COVID-19 chest x-ray image classification, random wired neural networks

Summary:
Randomly Wired Neural Networks (RWNNs) were studied to understand how network topology affects learning efficiency and model performance, focusing on three edge-centric measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC). These measures were used to compress RWNNs while maintaining performance in COVID-19 chest x-ray image classification. The study compared the pruning performance of FRC, ORC, and EBC across different network generators and analyzed the structural properties of pruned networks. Results indicated that FRC-based pruning could simplify RWNNs efficiently with comparable performance to ORC. This research offers insights into the trade-off between modular segregation and network efficiency in compressed RWNNs, highlighting the potential of FRC for effective network pruning. <br /><br />Summary: <div>
arXiv:2509.05322v1 Announce Type: cross 
Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for investigating the impact of network topology in deep learning by capturing how different connectivity patterns impact both learning efficiency and model performance. At the same time, they provide a natural framework for exploring edge-centric network measures as tools for pruning and optimization. In this study, we investigate three edge-centric network measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC), to compress RWNNs by selectively retaining important synapses (or edges) while pruning the rest. As a baseline, RWNNs are trained for COVID-19 chest x-ray image classification, aiming to reduce network complexity while preserving performance in terms of accuracy, specificity, and sensitivity. We extend prior work on pruning RWNN using ORC by incorporating two additional edge-centric measures, FRC and EBC, across three network generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the pruning performance of the three measures in terms of compression ratio and theoretical speedup. A central focus of our study is to evaluate whether FRC, which is computationally more efficient than ORC, can achieve comparable pruning effectiveness. Along with performance evaluation, we further investigate the structural properties of the pruned networks through modularity and global efficiency, offering insights into the trade-off between modular segregation and network efficiency in compressed RWNNs. Our results provide initial evidence that FRC-based pruning can effectively simplify RWNNs, offering significant computational advantages while maintaining performance comparable to ORC.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time social engineering, scams, privacy-preserving, AI-in-the-loop, federated learning

Summary:
This paper introduces a privacy-preserving, AI-in-the-loop framework for proactively detecting and disrupting scam conversations in real time. By combining instruction-tuned artificial intelligence with a safety-aware utility function, the system effectively balances engagement with harm minimization. Additionally, federated learning enables continual model updates without sharing raw data. Experimental evaluations show that the system generates fluent and engaging responses with low perplexity and high engagement levels. Human studies confirm the realism, safety, and effectiveness of the framework over existing baselines. Moreover, models trained with federated learning sustain high engagement and relevance while maintaining low personal information leakage. The evaluation of guard models indicates a trade-off between privacy risk and scam detection effectiveness, with stricter moderation settings limiting engagement but reducing privacy risk. This framework represents a novel approach that integrates real-time scam detection, federated privacy preservation, and calibrated safety moderation to create a proactive defense paradigm.<br /><br />Summary: <div>
arXiv:2509.05362v1 Announce Type: cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Hierarchical Networks and the Law of Functional Evolution: A Universal Framework for Complex Systems</title>
<link>https://arxiv.org/abs/2509.05567</link>
<guid>https://arxiv.org/abs/2509.05567</guid>
<content:encoded><![CDATA[
<div> Keywords: Recursive Hierarchical Network, evolution, complex systems, functional evolution, intelligent systems

Summary:
The article introduces the Recursive Hierarchical Network (RHN) framework for understanding and predicting the evolution of complex systems. It proposes a recursive encapsulation model involving nodes, modules, systems, and new nodes, with a progression from structure-dominated to regulation-dominated to intelligence-dominated stages. The law of functional evolution is formalized and proven, showing irreversible progression across different systems. Empirical analysis aligns life, cosmic, informational, and social systems on a scale of functional levels, revealing strong cross-system similarities. Trajectories are strictly monotonic, with high pairwise cosine similarities and robust stage resonance. Current system states are identified, and future transitions are projected. RHN offers a mathematically rigorous, multi-scale framework for reconstructing and predicting system evolution, providing theoretical guidance for developing next-generation intelligent systems. 

<br /><br />Summary: <div>
arXiv:2509.05567v1 Announce Type: cross 
Abstract: Understanding and predicting the evolution of across complex systems remains a fundamental challenge due to the absence of unified and computationally testable frameworks. Here we propose the Recursive Hierarchical Network(RHN), conceptualizing evolution as recursive encapsulation along a trajectory of node $\to$ module $\to$ system $\to$ new node, governed by gradual accumulation and abrupt transition. Theoretically, we formalize and prove the law of functional evolution, revealing an irreversible progression from structure-dominated to regulation-dominated to intelligence-dominated stages. Empirically, we operationalize functional levels and align life, cosmic, informational, and social systems onto this scale. The resulting trajectories are strictly monotonic and exhibit strong cross-system similarity, with high pairwise cosine similarities and robust stage resonance. We locate current system states and project future transitions. RHN provides a mathematically rigorous, multi-scale framework for reconstructing and predicting system evolution, offering theoretical guidance for designing next-generation intelligent systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stack Overflow Is Not Dead Yet: Crowd Answers Still Matter</title>
<link>https://arxiv.org/abs/2509.05879</link>
<guid>https://arxiv.org/abs/2509.05879</guid>
<content:encoded><![CDATA[
<div> ChatGPT, Stack Overflow, user contributions, programming help, user retention <br />
Summary:
The research paper examines the impact of ChatGPT on user-generated content on Stack Overflow after its introduction in November 2022. It identifies a decline in user contributions on the platform but notes a positive effect of ChatGPT on the length and difficulty of user questions and code examples. The study analyzes two years of data and finds that ChatGPT has led to an increase in question and answer length, code length, and question difficulty across different programming languages. This suggests that ChatGPT has raised the standard of questions on Stack Overflow, pushing users towards more complex and challenging problems. The results contribute to understanding the influence of advanced tools like ChatGPT on programming help-seeking and collaborative knowledge creation. The insights provided can help platform operators manage information effectively and enhance user retention post-ChatGPT launch. <br /><br /> <div>
arXiv:2509.05879v1 Announce Type: cross 
Abstract: Millions of users visit Stack Overflow regularly to ask community for answers to their programming questions. However, like many other platforms, Stack Overflow consistently struggles with low user retention and declining levels of user contributions to the platform. With the introduction of ChatGPT in November 2022, these ongoing difficulties on Stack Overflow were further magnified, as many users moved toward ChatGPT for programming help. In this paper, we build upon recent research on this phenomenon by analyzing the transformation of user-generated content on Stack Overflow during the post-ChatGPT period. Specifically, we analyze two years of Stack Overflow data and fit multiple causal regression models to estimate the effect of ChatGPT on the length and difficulty of user questions and code examples. We confirm an acceleration of decline in user contributions but find that ChatGPT had a significant positive effect on question and answer length, code length, and question difficulty on Stack Overflow across programming languages. Our results suggest that ChatGPT has effectively raised the bar for questions on Stack Overflow, as users increasingly turn to crowdsourced platforms for help with more complex and challenging problems. With our work we contribute to the ongoing discussion on the impact of tools such as ChatGPT on help-seeking in programming and, more broadly, on collaborative knowledge creation. Our results provide actionable insights for platform operators to support information management and user retention in the aftermath of ChatGPT's launch.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Productivity Gaps: Temporal Patterns of Gender Differences in Scientific Knowledge Creation</title>
<link>https://arxiv.org/abs/2509.06206</link>
<guid>https://arxiv.org/abs/2509.06206</guid>
<content:encoded><![CDATA[
<div> knowledge creation, gender differences, temporal dynamics, scientific careers, bibliometric data

Summary:
Female scientists exhibit higher stability in knowledge production but also greater year-to-year volatility, showcasing a paradox in career dynamics. They excel in persisting under moderate performance expectations but struggle with sustained peak performance demands. However, these patterns vary across disciplines, with humanities and social sciences showing stronger female advantages compared to STEM fields. Using a multi-dimensional framework, this study analyzes gender disparities in scientific careers through stability, volatility, and persistence dimensions. Data from 62.5 million authors starting their careers between 1960-2010 is used to construct knowledge creation capability measures, shedding light on the temporal dynamics underlying gender inequality in scientific knowledge creation. Female scientists' performance is nuanced, highlighting the complex nature of gender differences in scientific careers across various academic fields. 

<br /><br />Summary: <div>
arXiv:2509.06206v1 Announce Type: cross 
Abstract: Gender inequality in scientific careers has been extensively documented through aggregate measures such as total publications and cumulative citations, yet the temporal dynamics underlying these disparities remain largely unexplored. Here we developed a multi-dimensional framework to examine gender differences in scientific knowledge creation through three complementary temporal dimensions: stability (consistency of performance over time), volatility (degree of year-to-year fluctuation), and persistence (ability to maintain high performance for extended periods). Using comprehensive bibliometric data from SciSciNet covering 62.5 million authors whose careers began between 1960-2010, we constructed knowledge creation capability measures that captured how scientists absorb knowledge from diverse sources and contribute to field advancement. We found that female scientists demonstrated significantly higher knowledge production stability (0.170 vs. 0.119 for males) while simultaneously exhibiting greater year-to-year volatility (6.606 vs. 6.228), revealing a striking paradox in career dynamics. Female scientists showed persistence advantages under moderate performance requirements but faced disadvantages under extreme criteria demanding sustained peak performance. However, these patterns varied substantially across disciplines, with female advantages strongest in humanities and social sciences while STEM fields show mixed results.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergy, not size: How collaboration architecture shapes scientific disruption</title>
<link>https://arxiv.org/abs/2509.06212</link>
<guid>https://arxiv.org/abs/2509.06212</guid>
<content:encoded><![CDATA[
<div> collaboration, synergy factor, disruption, team composition, knowledge production<br />
<br />
Summary: 
The study analyzes the impact of collaboration on scientific innovation across 19 disciplines from 1960 to 2020, introducing the concept of the synergy factor to measure collaboration dynamics. It finds that Physics benefits from medium team sizes, while the humanities achieve optimal synergy through individual scholarship. Collaborative synergy, rather than team size alone, mediates a significant portion of the relationship between team composition and disruption. Papers featuring key authors exhibit higher levels of disruption, with exceptional researchers driving a 561% increase in disruptive potential. High-citation authors are found to decrease disruptive potential, whereas breakthrough track records enhance it, challenging traditional evaluation metrics. The study identifies four modes of knowledge production, highlighting the heterogeneity in optimal collaboration strategies across disciplines and offering evidence-based guidance for research organization and science policy development. <div>
arXiv:2509.06212v1 Announce Type: cross 
Abstract: The mechanisms driving different types of scientific innovation through collaboration remain poorly understood. Here we develop a comprehensive framework analyzing over 14 million papers across 19 disciplines from 1960 to 2020 to unpack how collaborative synergy shapes research disruption. We introduce the synergy factor to quantify collaboration cost-benefit dynamics, revealing discipline-specific architectures where Physics peaks at medium team sizes while humanities achieve maximal synergy through individual scholarship. Our mediation analysis demonstrates that collaborative synergy, not team size alone, mediates 75% of the relationship between team composition and disruption. Key authors play a catalytic role, with papers featuring exceptional researchers showing 561% higher disruption indices. Surprisingly, high-citation authors reduce disruptive potential while those with breakthrough track records enhance it, challenging traditional evaluation metrics. We identify four distinct knowledge production modes: elite-driven, baseline, heterogeneity-driven, and low-cost. These findings reveal substantial heterogeneity in optimal collaboration strategies across disciplines and provide evidence-based guidance for research organization, with implications for science policy and the design of research institutions in an increasingly collaborative scientific landscape.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features</title>
<link>https://arxiv.org/abs/2311.01661</link>
<guid>https://arxiv.org/abs/2311.01661</guid>
<content:encoded><![CDATA[
<div> deep learning, community resilience, socio-technical systems, urban development, machine intelligence

Summary:<br /><br />Community resilience is a complex and multifaceted phenomenon shaped by interactions within socio-technical systems. Current studies primarily focus on vulnerability assessment using index-based approaches, lacking the ability to capture the heterogeneous features and nonlinear interactions that influence resilience components. To address this gap, a three-layer deep learning model called Resili-Net is introduced. This model evaluates 12 measurable resilience features within community socio-technical systems related to robustness, redundancy, and resourcefulness. By utilizing public data from various U.S. metropolitan areas, Resili-Net categorizes resilience levels into five distinct levels, providing insights into resilience determinants and enhancement strategies. The model's interpretability allows for analyzing resilience profiles under different urban development patterns, offering unique perspectives on community resilience assessment through the integration of machine intelligence and urban big data. <div>
arXiv:2311.01661v2 Announce Type: replace 
Abstract: Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United States, Resili-Net characterizes the resilience levels of spatial areas into five distinct levels. The interpretability of the model outcomes enables feature analysis for specifying the determinants of resilience in areas within each resilience level, allowing for the identification of specific resilience enhancement strategies. Changes in community resilience profiles under urban development patterns are further examined by changing the value of related socio-technical systems features. Accordingly, the outcomes provide novel perspectives for community resilience assessment by harnessing machine intelligence and heterogeneous urban big data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities</title>
<link>https://arxiv.org/abs/2411.10868</link>
<guid>https://arxiv.org/abs/2411.10868</guid>
<content:encoded><![CDATA[
<div> social influence, generative AI, radicalization, social network, perturbations 
<br />Summary: 
The study investigates the impact of intentional perturbations on a social network using Taylor's model of social influence and robust control theory. By identifying subtle yet effective changes to the network structure, the research demonstrates how small alterations can lead to radicalization of all agents. The findings highlight the potential for significant shifts in collective behavior triggered by minimal adjustments in social influence. The study emphasizes the importance of analyzing real systems to identify existing dynamics and potential vulnerabilities within social networks. <div>
arXiv:2411.10868v5 Announce Type: replace 
Abstract: Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Right to Hide: Masking Community Affiliation via Minimal Graph Rewiring</title>
<link>https://arxiv.org/abs/2502.00432</link>
<guid>https://arxiv.org/abs/2502.00432</guid>
<content:encoded><![CDATA[
<div> community membership hiding, social graph, privacy, graph topology, node hiding

Summary:
The study addresses the challenge of protecting privacy in social graphs by concealing nodes' membership in sensitive communities while maintaining the underlying graph topology. The approach involves strategically modifying the graph structure to hide a target node's community affiliation from detection algorithms. The researchers introduce a new gradient-based method, $\nabla$-CMH, which minimizes structural changes within a defined modification budget to effectively hide community membership. Extensive experiments on various datasets and community detection methods show that the technique outperforms existing approaches by achieving a balance between node hiding effectiveness and graph rewiring cost while ensuring computational efficiency. <div>
arXiv:2502.00432v2 Announce Type: replace 
Abstract: Protecting privacy in social graphs may require obscuring nodes' membership in sensitive communities. However, doing so without significantly disrupting the underlying graph topology remains a key challenge. In this work, we address the community membership hiding problem, which involves strategically modifying the graph structure to conceal a target node's affiliation with a community, regardless of the detection algorithm used. We reformulate the original discrete, counterfactual graph search objective as a differentiable constrained optimisation task. To this end, we introduce $\nabla$-CMH, a new gradient-based method that operates within a feasible modification budget to minimise structural changes while effectively hiding a node's community membership. Extensive experiments on multiple datasets and community detection methods demonstrate that our technique outperforms existing baselines, achieving the best balance between node hiding effectiveness and graph rewiring cost, while preserving computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Hourly Neighborhood Population Using Mobile Phone Data in the United States</title>
<link>https://arxiv.org/abs/2504.01170</link>
<guid>https://arxiv.org/abs/2504.01170</guid>
<content:encoded><![CDATA[
<div> Keywords: population estimation, human mobility data, spatiotemporal dynamics, urban and rural movements, high resolution

Summary: 
Traditional population estimation methods often do not capture the dynamic fluctuations in urban and rural population movements. This study proposes a novel method that utilizes smartphone-based human mobility data to reconstruct hourly population estimates for neighborhoods across the US. By analyzing population fluctuations on an hourly, diurnal, daily, and seasonal basis, the study reveals the limitations of static population data in capturing temporal dynamics. This high spatiotemporal resolution dataset provides valuable insights for various studies, including air pollution exposure analysis and emergency response planning. This study represents one of the first attempts to create hourly population estimates on a large geographic scale, revolutionizing the way we understand and analyze dynamic populations.  <br /><br />Summary: <div>
arXiv:2504.01170v2 Announce Type: replace 
Abstract: Traditional population estimation techniques often fail to capture the dynamic fluctuations inherent in urban and rural population movements. Recognizing the need for a high spatiotemporal dynamic population dataset, we propose a method using smartphone-based human mobility data to reconstruct the hourly population for each neighborhood across the US. We quantify population fluctuations on an hourly, diurnal, daily, and seasonal basis, and compare these with static population data to highlight the limitations of traditional models in capturing temporal dynamics. This study is one of the first hourly population products at a large geographic extent (US), contributing to various studies that involve dynamic populations with high spatiotemporal resolution, such as air pollution exposure analysis and emergency response.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanNews: a Network-aware Fake News Mitigation Architecture for Social Media</title>
<link>https://arxiv.org/abs/2509.04489</link>
<guid>https://arxiv.org/abs/2509.04489</guid>
<content:encoded><![CDATA[
<div> Fake news detection, social media, deep learning, LSTM, GRU

Summary:
CleanNews is a new architecture proposed to accurately identify and combat fake news in real-time on social media platforms. It utilizes advanced deep learning techniques, including convolutional and bidirectional recurrent neural networks such as LSTM and GRU layers, to detect misinformation. A novel embedding technique that combines textual information with user network structure is employed, allowing the model to learn linguistic and relational cues associated with fake news. Additionally, two network immunization algorithms, SparseShield and NetShield, are used to prevent the spread of false information within networks. Extensive ablation studies and hyperparameter tuning are conducted to maximize performance. Experimental evaluation on real-world datasets demonstrates the effectiveness of CleanNews in addressing the dissemination of fake news. 

<br /><br />Summary: <div>
arXiv:2509.04489v1 Announce Type: new 
Abstract: With the widespread use of the internet and handheld devices, social media now holds a power similar to that of old newspapers. People use social media platforms for quick and accessible information. However, this convenience comes with a variety of risks. Anyone can freely post content, true or false, with the probability of remaining online forever. This makes it crucial to identify and tackle misinformation and disinformation on online platforms. In this article, we propose CleanNews, a comprehensive architecture to identify fake news in real-time accurately. CleanNews uses advanced deep learning architectures, combining convolutional and bidirectional recurrent neural networks, i.e., LSTM and GRU, layers to detect fake news. A key contribution of our work is a novel embedding technique that fuses textual information with user network structure, allowing the model to jointly learn linguistic and relational cues associated with misinformation. Furthermore, we use two network immunization algorithms, i.e., SparseShield and NetShield, to mitigate the spread of false information within networks. We conduct extensive ablation studies to evaluate the contribution of each model component and systematically tune hyperparameters to maximize performance. The experimental evaluation on two real-world datasets shows the efficacy of CleanNews in combating the spread of fake news.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThumbnailTruth: A Multi-Modal LLM Approach for Detecting Misleading YouTube Thumbnails Across Diverse Cultural Settings</title>
<link>https://arxiv.org/abs/2509.04714</link>
<guid>https://arxiv.org/abs/2509.04714</guid>
<content:encoded><![CDATA[
<div> LLMs, misleading thumbnails, detection pipeline, video platforms, content integrity <br />
<br />
Summary: This paper presents a novel multi-modal detection pipeline using Large Language Models (LLMs) to identify misleading video thumbnails on platforms like YouTube. A dataset of 2,843 videos from eight countries was analyzed, including 1,359 misleading thumbnails with over 7.6 billion views. The pipeline integrates video-to-text descriptions, thumbnail images, and subtitle transcripts for comprehensive analysis. State-of-the-art LLMs such as GPT-40, GPT-40 Mini, Claude 3.5 Sonnet, and Gemini-1.5 Flash were evaluated, with Claude 3.5 Sonnet performing well, achieving high accuracy, precision, and recall rates. The study emphasizes the importance of transparent and trustworthy video platforms, highlighting the implications for content moderation, user experience, and ethical considerations when deploying detection systems at scale. Through these findings, the research aims to enhance content integrity and user trust on video platforms globally. <br /> <div>
arXiv:2509.04714v1 Announce Type: new 
Abstract: Misleading video thumbnails on platforms like YouTube are a pervasive problem, undermining user trust and platform integrity. This paper proposes a novel multi-modal detection pipeline that uses Large Language Models (LLMs) to flag misleading thumbnails. We first construct a comprehensive dataset of 2,843 videos from eight countries, including 1,359 misleading thumbnail videos that collectively amassed over 7.6 billion views -- providing a unique cross-cultural perspective on this global issue. Our detection pipeline integrates video-to-text descriptions, thumbnail images, and subtitle transcripts to holistically analyze content and flag misleading thumbnails. Through extensive experimentation and prompt engineering, we evaluate the performance of state-of-the-art LLMs, including GPT-4o, GPT-4o Mini, Claude 3.5 Sonnet, and Gemini-1.5 Flash. Our findings show the effectiveness of LLMs in identifying misleading thumbnails, with Claude 3.5 Sonnet consistently showing strong performance, achieving an accuracy of 93.8\%, precision over 92\%, and recall exceeding 94\% in certain scenarios. We discuss the implications of our findings for content moderation, user experience, and the ethical considerations of deploying such systems at scale. Our findings pave the way for more transparent, trustworthy video platforms and stronger content integrity for audiences worldwide.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media</title>
<link>https://arxiv.org/abs/2509.04823</link>
<guid>https://arxiv.org/abs/2509.04823</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, cognitive-behavioral fixation, multimodal, topic extraction, user behavior

Summary:
This article introduces a novel framework for assessing cognitive-behavioral fixation on digital social media platforms. The framework involves analyzing users' multimodal social media engagement patterns using a combination of topic extraction and fixation quantification modules. The proposed approach aims to provide adaptive, hierarchical, and interpretable evaluation of user behavior, filling a gap in computational methods for detecting fixation in psychology. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of the framework. The code for this project is publicly available for research purposes on GitHub, enabling scalable computational analysis of cognitive fixation.<br /><br />Summary: <div>
arXiv:2509.04823v1 Announce Type: new 
Abstract: Digital social media platforms frequently contribute to cognitive-behavioral fixation, a phenomenon in which users exhibit sustained and repetitive engagement with narrow content domains. While cognitive-behavioral fixation has been extensively studied in psychology, methods for computationally detecting and evaluating such fixation remain underexplored. To address this gap, we propose a novel framework for assessing cognitive-behavioral fixation by analyzing users' multimodal social media engagement patterns. Specifically, we introduce a multimodal topic extraction module and a cognitive-behavioral fixation quantification module that collaboratively enable adaptive, hierarchical, and interpretable assessment of user behavior. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of our approach, laying the groundwork for scalable computational analysis of cognitive fixation. All code in this project is publicly available for research purposes at https://github.com/Liskie/cognitive-fixation-evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media</title>
<link>https://arxiv.org/abs/2509.04461</link>
<guid>https://arxiv.org/abs/2509.04461</guid>
<content:encoded><![CDATA[
<div> Keywords: Personality prediction, social media posts, Myers Briggs Type Indicator, Large Language Models, class imbalance

Summary: 
PostToPersonality (PtoP) is a novel framework for predicting personality types from social media posts using Large Language Models (LLMs). PtoP addresses two key challenges faced by LLMs in MBTI prediction: the hallucination problem and class imbalance. By utilizing Retrieval Augmented Generation and in context learning, PtoP mitigates hallucination in LLMs. Additionally, the framework fine-tunes a pretrained LLM with synthetic minority oversampling to balance the naturally imbalanced distribution of MBTI types in the population. Experiments on real-world social media data show that PtoP outperforms 10 traditional machine learning and deep learning baselines in MBTI prediction. PtoP not only demonstrates state-of-the-art performance but also showcases the potential of leveraging LLMs for understanding personality traits from social media content. 

<br /><br />Summary: <div>
arXiv:2509.04461v1 Announce Type: cross 
Abstract: Personality prediction from social media posts is a critical task that implies diverse applications in psychology and sociology. The Myers Briggs Type Indicator (MBTI), a popular personality inventory, has been traditionally predicted by machine learning (ML) and deep learning (DL) techniques. Recently, the success of Large Language Models (LLMs) has revealed their huge potential in understanding and inferring personality traits from social media content. However, directly exploiting LLMs for MBTI prediction faces two key challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. In this paper, we propose PostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from social media posts of individuals. Specifically, PtoP leverages Retrieval Augmented Generation with in context learning to mitigate hallucination in LLMs. Furthermore, we fine tune a pretrained LLM to improve model specification in MBTI understanding with synthetic minority oversampling, which balances the class imbalance by generating synthetic samples. Experiments conducted on a real world social media dataset demonstrate that PtoP achieves state of the art performance compared with 10 ML and DL baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Classroom Interaction Data Using Prompt Engineering and Network Analysis</title>
<link>https://arxiv.org/abs/2501.18912</link>
<guid>https://arxiv.org/abs/2501.18912</guid>
<content:encoded><![CDATA[
<div> Keywords: classroom interactions, prompt engineering, network analysis, interaction patterns, gender gap

Summary:
The study focuses on the analysis of classroom interactions to enhance learning outcomes. It introduces a framework that combines prompt engineering and network analysis to examine complex conversational data. The framework automates utterance classification through prompt engineering, enabling efficient dialogue analysis without pre-labeled datasets. The interactions are then transformed into network representations, allowing for the analysis of classroom dynamics as structured social networks. The study utilizes network mediation analysis to uncover complex interaction patterns and understand how underlying interaction structures impact student learning. In particular, the study investigates how the gender gap in mathematics performance may be mediated by students' classroom interaction structures. The research aims to improve educational practices by providing insights into the dynamics of classroom interactions and their impact on student outcomes.
<br /><br />Summary: <div>
arXiv:2501.18912v2 Announce Type: replace-cross 
Abstract: Classroom interactions play a vital role in developing critical thinking, collaborative problem-solving abilities, and enhanced learning outcomes. While analyzing these interactions is crucial for improving educational practices, the examination of classroom dialogues presents significant challenges due to the complexity and high-dimensionality of conversational data. This study presents an integrated framework that combines prompt engineering with network analysis to investigate classroom interactions comprehensively. Our approach automates utterance classification through prompt engineering, enabling efficient and scalable dialogue analysis without requiring pre-labeled datasets. The classified interactions are subsequently transformed into network representations, facilitating the analysis of classroom dynamics as structured social networks. To uncover complex interaction patterns and how underlying interaction structures relate to student learning, we utilize network mediation analysis. In this approach, latent interaction structures, derived from the additive and multiplicative effects network (AMEN) model that places students within a latent social space, act as mediators. In particular, we investigate how the gender gap in mathematics performance may be mediated by students' classroom interaction structures.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
<link>https://arxiv.org/abs/2509.03832</link>
<guid>https://arxiv.org/abs/2509.03832</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, echo chambers, confirmation bias, gravity well model, misinformation

Summary: 
The study focuses on the impact of confirmation bias in social media echo chambers and introduces a dynamic confirmation bias variable to the existing gravity well model. This variable adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content, improving the identification of echo chambers. By analyzing posting history and responses to various viewpoints, the model can better detect echo chambers in online communities. Validation on nineteen Reddit communities showed enhanced capabilities in identifying these high-risk environments. This framework allows for a systematic approach to understanding confirmation bias in online group dynamics, aiding in the efforts to combat the spread of misinformation at its core points of amplification. <div>
arXiv:2509.03832v1 Announce Type: new 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Analysis of Dissent and Self-Censorship</title>
<link>https://arxiv.org/abs/2509.03731</link>
<guid>https://arxiv.org/abs/2509.03731</guid>
<content:encoded><![CDATA[
<div> surveillance, censorship, dissent, authority, self-censorship
<br />
Summary:
The article discusses the dynamics of dissent and self-censorship in societies, particularly in the context of digital communications and authoritarian regimes. It introduces a model where individuals weigh the risks of expressing dissent against the consequences of punishment, while the authority adjusts its policies to suppress dissent. The study reveals that there is a strategic tradeoff between voicing dissent and avoiding punishment, leading to individuals either defiantly expressing dissent or self-censoring to comply with authority. The model highlights how the population's willingness to endure punishment influences the authority's choice of policy, ultimately affecting the extent and duration of dissent suppression. Additionally, the research demonstrates that any population can be led to total self-censorship by an authority's policies, but early resistance to punishment can deter the adoption of more extreme measures. 
<br /> 
Summary: <div>
arXiv:2509.03731v1 Announce Type: cross 
Abstract: Expressions of dissent against authority are an important feature of most societies, and efforts to suppress such expressions are common. Modern digital communications, social media, and Internet surveillance and censorship technologies are changing the landscape of public speech and dissent. Especially in authoritarian settings, individuals must assess the risk of voicing their true opinions or choose self-censorship, voluntarily moderating their behavior to comply with authority. We present a model in which individuals strategically manage the tradeoff between expressing dissent and avoiding punishment through self-censorship while an authority adapts its policies to minimize both total expressed dissent and punishment costs. We study the model analytically and in simulation to derive conditions separating defiant individuals who express their desired dissent in spite of punishment from self-censoring individuals who fully or partially limit their expression. We find that for any population, there exists an authority policy that leads to total self-censorship. However, the probability and time for an initially moderate, locally-adaptive authority to suppress dissent depend critically on the population's willingness to withstand punishment early on, which can deter the authority from adopting more extreme policies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Low Frequency Tweeters Have More to Say!" A New Approach to Identify Importance of Tweets</title>
<link>https://arxiv.org/abs/2509.03931</link>
<guid>https://arxiv.org/abs/2509.03931</guid>
<content:encoded><![CDATA[
<div> Keywords: Twitter, social media, information overload, tweet importance, tweet frequency

Summary:
This study focuses on addressing the issue of information overload on Twitter by personalizing tweet rankings based on the relationship between tweet importance and tweet frequency. The hypothesis tested suggests that infrequent tweeters may have more important messages to convey. Six new measures are proposed to evaluate tweet importance, including metrics like retweets, favorites, and comments. The study findings indicate that users who tweet less frequently tend to have more important messages according to their followers. This identified tweet-frequency band can be used to reorder user activity feeds, preventing important messages from getting lost. Additionally, it can serve as a scoring index to recognize users who frequently share significant messages on Twitter. <div>
arXiv:2509.03931v1 Announce Type: cross 
Abstract: Twitter is one of the most popular social media platforms.With a large number of tweets, the activity feed of users becomes noisy, challenging to read, and most importantly tweets often get lost. We present a new approach to personalise the ranking of the tweets toward solving the problem of information overload which is achieved by analysing the relationship between the importance of tweets to the frequency at which the author tweets. The hypothesis tested is that "low-frequency tweeters have more to say", i.e. if a user who tweets infrequently actually goes to the effort of tweeting, then it is more likely to be of more importance or contain more "meaning" than a tweet by a user who tweets continuously. We propose six new measures to evaluate the importance of tweets based on the ability of the tweet to drive interaction among its readers, which is measured through metrics such as retweets, favourites, and comments, and the extent of the author's network interacting with the tweet. Our study shows that users who tweeted less than ten tweets per week were more likely to be perceived as important by their followers and have the most important messages. This identified tweet-frequency band could be used to reorder the activity feed of users and such reordering would ensure the messages of low-frequency tweeters do not get lost in the stream of tweets. This could also serve as a scoring index for Twitter users to identify users frequently tweeting important messages.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit and Implicit Data Augmentation for Social Event Detection</title>
<link>https://arxiv.org/abs/2509.04202</link>
<guid>https://arxiv.org/abs/2509.04202</guid>
<content:encoded><![CDATA[
<div> Keywords: social event detection, augmentation framework, data diversity, model robustness, perturbation techniques

Summary:
The study introduces an Augmentation framework for Social Event Detection (SED-Aug) to improve the accuracy of identifying and categorizing important events from social media. The framework combines explicit text-based augmentation using large language models with implicit feature-space augmentation employing novel perturbation techniques on structural fused embeddings. This approach enhances data diversity and model robustness, resulting in a significant performance improvement over baseline models. SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code for SED-Aug is available on GitHub for further exploration and implementation. The framework showcases the effectiveness of combining explicit text-based augmentation with implicit feature-space perturbations for enhancing social event detection accuracy. 

<br /><br />Summary: <div>
arXiv:2509.04202v1 Announce Type: cross 
Abstract: Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Graph Structural Learning Beyond Homophily via Preserving Neighbor Similarity</title>
<link>https://arxiv.org/abs/2401.09754</link>
<guid>https://arxiv.org/abs/2401.09754</guid>
<content:encoded><![CDATA[
<div> Robust Models, Adversarial Attacks, Graph-based Learning Systems, Homophilic Graphs, Vulnerability <br />
Summary:<br />
- Graph-based learning systems are successful in handling structural data but are vulnerable to adversarial attacks on homophilic graphs.
- Existing robust models focus on homophilic graphs, leaving the security of graph-based learning on heterophilic graphs unexplored.
- The update of the negative classification loss is inversely related to pairwise similarities based on aggregated neighbor features.
- A novel robust graph structural learning strategy is proposed to address vulnerabilities in graph-based learning systems.
- The strategy incorporates a dual-kNN graph construction pipeline to supervise neighbor-similarity-preserved propagation for more reliable data management on both homophilic and heterophilic graphs. <br /> <div>
arXiv:2401.09754v2 Announce Type: replace-cross 
Abstract: Despite the tremendous success of graph-based learning systems in handling structural data, it has been widely investigated that they are fragile to adversarial attacks on homophilic graph data, where adversaries maliciously modify the semantic and topology information of the raw graph data to degrade the predictive performances. Motivated by this, a series of robust models are crafted to enhance the adversarial robustness of graph-based learning systems on homophilic graphs. However, the security of graph-based learning systems on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph-based learning systems regardless of the homophily degree, and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. The theoretical finding inspires us to craft a novel robust graph structural learning strategy that serves as a useful graph mining module in a robust model that incorporates a dual-kNN graph constructions pipeline to supervise the neighbor-similarity-preserved propagation, where the graph convolutional layer adaptively smooths or discriminates the features of node pairs according to their affluent local structures. In this way, the proposed methods can mine the ``better" topology of the raw graph data under diverse graph homophily and achieve more reliable data management on homophilic and heterophilic graphs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model</title>
<link>https://arxiv.org/abs/2503.23746</link>
<guid>https://arxiv.org/abs/2503.23746</guid>
<content:encoded><![CDATA[
<div> Keywords: Short-video, Propagation influence, Dataset, Cross-platform, Large Graph Model

Summary:
Short-video platforms have become immensely popular, prompting researchers to analyze their propagation dynamics for commercial and social insights. This paper introduces the Short-video Propagation Influence Rating (SPIR) task and presents a new Cross-platform Short-Video (XS-Video) dataset. This dataset, spanning 5 major Chinese platforms, contains over 117,000 videos annotated with propagation influences. Unique in its cross-platform coverage and detailed metrics, the XS-Video dataset facilitates research in short-video propagation. Additionally, the paper proposes the Large Graph Model (LGM) named NetGPT, leveraging a three-stage training mechanism to analyze short-video propagation graphs. Evaluations on the XS-Video dataset demonstrate the efficacy of NetGPT for SPIR, outperforming existing methods in classification and regression tasks. This work advances the analysis of short-video propagation, offering valuable insights for understanding and predicting the spread of videos across diverse platforms.<br /><br />Summary: <div>
arXiv:2503.23746v2 Announce Type: replace-cross 
Abstract: Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Optimization of Methods for Establishing Well-Connected Communities</title>
<link>https://arxiv.org/abs/2509.02590</link>
<guid>https://arxiv.org/abs/2509.02590</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Well-Connected Clusters, Connectivity Modifier, HPE Chapel programming language

Summary: <br />
Community detection is essential for identifying important structures within networks. Existing methods often struggle with disconnected clusters, hampering accuracy and reliability. The Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are effective but too computationally demanding for large graphs. This study introduces optimized parallel versions of WCC and CM using the HPE Chapel language. By leveraging Chapel's parallel capabilities, the algorithms achieve significant performance improvements and scalability on modern multicore systems. Integrated into the Arkouda/Arachne framework, these implementations allow for efficient community detection on massive graphs exceeding 2 billion edges. For example, the algorithms successfully analyze the Open-Alex dataset with over 2 billion edges in mere minutes using 128 cores, a task previously deemed impossible. <div>
arXiv:2509.02590v1 Announce Type: new 
Abstract: Community detection plays a central role in uncovering meso scale structures in networks. However, existing methods often suffer from disconnected or weakly connected clusters, undermining interpretability and robustness. Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing techniques that improve the accuracy of many clustering methods. However, they are computationally prohibitive on massive graphs. In this work, we present optimized parallel implementations of WCC and CM using the HPE Chapel programming language. First, we design fast and efficient parallel algorithms that leverage Chapel's parallel constructs to achieve substantial performance improvements and scalability on modern multicore architectures. Second, we integrate this software into Arkouda/Arachne, an open-source, high-performance framework for large-scale graph analytics. Our implementations uniquely enable well-connected community detection on massive graphs with more than 2 billion edges, providing a practical solution for connectivity-preserving clustering at web scale. For example, our implementations of WCC and CM enable community detection of the over 2-billion edge Open-Alex dataset in minutes using 128 cores, a result infeasible to compute previously.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
<link>https://arxiv.org/abs/2509.02609</link>
<guid>https://arxiv.org/abs/2509.02609</guid>
<content:encoded><![CDATA[
<div> Influential node identification, deep unsupervised framework, ReCC, contrastive learning mechanism, regular equivalence-based similarity<br />
<br />Summary: <br />Identifying influential nodes in complex networks is essential but challenging due to the lack of labeled data. The proposed ReCC framework introduces a novel deep unsupervised approach that redefines node influence detection as a clustering problem. By utilizing a contrastive learning mechanism based on regular equivalence-based similarity, ReCC generates positive and negative samples without the need for labeled data. This approach, integrated into a graph convolutional network, effectively distinguishes influential nodes from non-influential ones. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, enhancing node representations with structural metrics and regular equivalence-based similarities. Extensive experiments demonstrate ReCC outperforms existing methods across various benchmarks, offering a promising solution for influential node identification in real-world scenarios. <br /> <div>
arXiv:2509.02609v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based \textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic generation of online social networks through homophily</title>
<link>https://arxiv.org/abs/2509.02762</link>
<guid>https://arxiv.org/abs/2509.02762</guid>
<content:encoded><![CDATA[
<div> algorithm, synthetic social networks, homophily, microblogging, online social networks 

Summary:
The article introduces a new algorithm for generating synthetic microblogging social networks that mimic real-world online social networks. The algorithm takes into account attributes-based homophily, stochastic link formation, clustering through triadic closure, and global reachability via long-range connections. By calibrating five hyperparameters, the algorithm aims to replicate five key structural properties observed in real social networks. The framework is validated by generating synthetic networks ranging from 10^3 to 10^6 nodes and comparing them against a real-world network. Results indicate that the proposed algorithm outperforms existing techniques in capturing the structural properties of real networks and generating attribute-driven communities that align with sociological expectations. The generated synthetic networks provide a realistic and scalable testbed for social researchers to conduct controlled experiments without relying on live digital platforms. 

<br /><br />Summary: <div>
arXiv:2509.02762v1 Announce Type: new 
Abstract: Online social networks (OSNs) have become increasingly relevant for studying social behavior and information diffusion. Nevertheless, they are limited by restricted access to real OSN data due to privacy, legal, and platform-related constraints. In response, synthetic social networks serve as a viable approach to support controlled experimentation, but current generators reproduce only topology and overlook attribute-driven homophily and semantic realism.
  This work proposes a homophily-based algorithm that produces synthetic microblogging social networks such as X. The model creates a social graph for a given number of users, integrating semantic affinity among user attributes, stochastic variation in link formation, triadic closure to foster clustering, and long-range connections to ensure global reachability. A systematic grid search is used to calibrate five hyperparameters (affinity strength, noise, closure probability, distant link probability, and candidate pool size) for reaching five structural values observed in real social networks (density, clustering coefficient, LCC proportion, normalized shortest path, and modularity).
  The framework is validated by generating synthetic OSNs at four scales (10^3-10^6 nodes), and benchmarking them against a real-world Bluesky network comprising 4 million users. Comparative results show that the framework reliably reproduces the structural properties of the real network. Overall, the framework outperforms leading importance-sampling techniques applied to the same baseline. The generated graphs capture topological realism and yield attribute-driven communities that align with sociological expectations, providing a realistic, scalable testbed that liberates social researchers from relying on live digital platforms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Movie Success with Multi-Task Learning: A Hybrid Framework Combining GPT-Based Sentiment Analysis and SIR Propagation</title>
<link>https://arxiv.org/abs/2509.02809</link>
<guid>https://arxiv.org/abs/2509.02809</guid>
<content:encoded><![CDATA[
<div> Keywords: movie success prediction, multi-task learning, sentiment analysis, SIR modeling, feature integration<br />
<br />
Summary: <br />
This study introduces a hybrid framework for predicting the success of movies by combining multi-task learning, sentiment analysis using GPT, and SIR propagation modeling. The framework considers static production attributes, information dissemination, and audience sentiment simultaneously. Using 5,840 films and 300,000 user reviews, the framework achieves high predictive accuracy. Ablation analysis highlights the importance of selective feature combinations over a comprehensive model. Virality patterns between successful and unsuccessful films are identified, challenging existing assumptions. The framework's innovations include using epidemiological modeling for information diffusion, generating multidimensional sentiment features from GPT-based analysis, and employing a shared representation architecture to optimize success metrics. This research has implications for the film production industry and advances understanding of how audience engagement influences commercial outcomes. <br /> <div>
arXiv:2509.02809v1 Announce Type: new 
Abstract: This study presents a hybrid framework for predicting movie success. The framework integrates multi-task learning (MTL), GPT-based sentiment analysis, and Susceptible-Infected-Recovered (SIR) propagation modeling. The study examines limitations in existing approaches. It models static production attributes, information dissemination, and audience sentiment at the same time. The framework uses 5,840 films from 2004 to 2024 and approximate 300,000 user reviews. It shows predictive performance with classification accuracy of 0.964 and regression metrics of MAE 0.388. Ablation analysis indicates component interactions. Selective feature combinations perform better than the comprehensive model. This result questions assumptions about feature integration. The model shows virality patterns between successful and unsuccessful films. Innovations include epidemiological modeling for information diffusion, multidimensional sentiment features from GPT-based analysis, and a shared representation architecture that optimizes multiple success metrics. The framework provides applications in the film production lifecycle. It also contributes to understanding how audience engagement leads to commercial outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal social network modeling of mobile connectivity data with graph neural networks</title>
<link>https://arxiv.org/abs/2509.03319</link>
<guid>https://arxiv.org/abs/2509.03319</guid>
<content:encoded><![CDATA[
<div> GNN; temporal social networks; mobile connectivity data; ROLAND; EdgeBank
Summary:
Graph neural networks (GNNs) are powerful tools for analyzing complex networks, but their application to social networks using mobile connectivity data is understudied. This study explores four temporal GNN models for predicting phone call and SMS activity in a mobile communication network. The ROLAND temporal GNN outperforms a non-GNN baseline model, showcasing the potential of GNNs in analyzing temporal social networks. However, other GNN models performed worse than the baseline, highlighting the need for specialized GNN architectures. Further research is necessary to optimize GNNs for analyzing temporal social networks through mobile connectivity data.<br /><br />Summary: <div>
arXiv:2509.03319v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have emerged as a state-of-the-art data-driven tool for modeling connectivity data of graph-structured complex networks and integrating information of their nodes and edges in space and time. However, as of yet, the analysis of social networks using the time series of people's mobile connectivity data has not been extensively investigated. In the present study, we investigate four snapshot - based temporal GNNs in predicting the phone call and SMS activity between users of a mobile communication network. In addition, we develop a simple non - GNN baseline model using recently proposed EdgeBank method. Our analysis shows that the ROLAND temporal GNN outperforms the baseline model in most cases, whereas the other three GNNs perform on average worse than the baseline. The results show that GNN based approaches hold promise in the analysis of temporal social networks through mobile connectivity data. However, due to the relatively small performance margin between ROLAND and the baseline model, further research is required on specialized GNN architectures for temporal social network analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results</title>
<link>https://arxiv.org/abs/2509.02969</link>
<guid>https://arxiv.org/abs/2509.02969</guid>
<content:encoded><![CDATA[
<div> Challenge, Engagement Prediction, Short Videos, User-generated Content, Social Media Platforms
<br />
Summary:
The paper discusses the VQualA 2025 Challenge on Engagement Prediction for Short Videos, focusing on analyzing the popularity of user-generated content (UGC) short videos on social media platforms. The challenge utilizes a new UGC dataset with engagement metrics derived from real user interactions to promote robust modeling strategies capturing factors impacting user engagement. Participants explored various multi-modal features, including visual content, audio, and creator metadata. The challenge attracted 97 participants, receiving 15 valid test submissions, which significantly advanced progress in short-form UGC video engagement prediction. <div>
arXiv:2509.02969v1 Announce Type: cross 
Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict, Cluster, Refine: A Joint Embedding Predictive Self-Supervised Framework for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2502.01684</link>
<guid>https://arxiv.org/abs/2502.01684</guid>
<content:encoded><![CDATA[
<div> framework, graph SSL, contrastive objectives, semantic-aware objective, GMM-based pseudo-label scoring

Summary:
The paper introduces a novel joint embedding predictive framework for graph SSL that eliminates the need for contrastive objectives and negative sampling. This framework preserves semantic and structural information while enhancing node discriminability through a semantic-aware objective term that uses pseudo-labels derived from Gaussian Mixture Models (GMMs). The proposed framework outperforms existing graph SSL methods across benchmarks by leveraging a non-contrastive, view-invariant architecture, capturing single context and multiple targets relationships between subgraphs, and incorporating GMM-based pseudo-label scoring to evaluate semantic contributions. This approach offers a computationally efficient and collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. <div>
arXiv:2502.01684v4 Announce Type: replace-cross 
Abstract: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning of Local Updates for Maximum Independent Set in Dynamic Graphs</title>
<link>https://arxiv.org/abs/2505.13754</link>
<guid>https://arxiv.org/abs/2505.13754</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, unsupervised learning, Maximum Independent Sets, dynamic graphs, edge addition/deletion<br />
<br />
Summary:<br />
This paper introduces an unsupervised learning model to find Maximum Independent Sets (MaxIS) in dynamic graphs where edges evolve over time. The model combines graph neural networks (GNNs) with a learned update mechanism to determine nodes' MaxIS membership in response to edge changes. By parameterizing the model with an update radius, the tradeoff between performance and runtime for different graph topologies is explored. Evaluation on synthetic and real-world dynamic graphs shows competitive approximation ratios and scalability, with significant outperformance compared to existing learning-based methods. The model excels in solution quality, runtime, and memory usage, particularly on large graphs. When tested on graphs 100 times larger than the training data, the model produces MaxIS solutions larger than any other learning method while maintaining competitive runtimes. <div>
arXiv:2505.13754v2 Announce Type: replace-cross 
Abstract: We present the first unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our method combines structural learning from graph neural networks (GNNs) with a learned distributed update mechanism that, given an edge addition or deletion event, modifies nodes' internal memories and infers their MaxIS membership in a single, parallel step. We parameterize our model by the update mechanism's radius and investigate the resulting performance-runtime tradeoffs for various dynamic graph topologies. We evaluate our model against a mixed integer programming solver and the state-of-the-art learning-based methods for MaxIS on static graphs (ICML 2020; NeurIPS 2020, 2023). Across synthetic and empirical dynamic graphs of 50-1,000 nodes, our model achieves competitive approximation ratios with excellent scalability; on large graphs, it significantly outperforms the state-of-the-art learning methods in solution quality, runtime, and memory usage. When generalizing to graphs of 10,000 nodes (100x larger than the ones used for training), our model produces MaxIS solutions 1.05-1.18x larger than any other learning method, even while maintaining competitive runtimes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection using Fortunato's Performance Measure</title>
<link>https://arxiv.org/abs/2509.00938</link>
<guid>https://arxiv.org/abs/2509.00938</guid>
<content:encoded><![CDATA[
<div> performance measure, community detection, graph partition, greedy algorithm, heuristic algorithm 

Summary:
The paper introduces the fp measure for detecting communities in unweighted, undirected networks. It presents a greedy algorithm, fpGreed, that optimizes the fp measure by iteratively working at both vertex and community levels. Vertices only join a community if the fp value improves, and communities merge if the fp measure improves. The algorithm terminates when no further improvements can be made. A faster heuristic algorithm, fastFp, is also introduced, particularly suitable for large datasets. The quality of the communities and computation time on various datasets are presented. fastFp performs well on large datasets like youtube and livejournal, demonstrating both efficiency and solution quality.<br /><br />Summary: <div>
arXiv:2509.00938v1 Announce Type: new 
Abstract: In his paper on Community Detection [1], Fortunato introduced a quality function called performance to assess the goodness of a graph partition. This measure counts the number of correctly ``interpreted" pairs of vertices, i. e. two vertices belonging to the same community and connected by an edge, or two vertices belonging to different communities and not connected by an edge. In this paper, we explore Fortunato's performance measure (fp measure) for detecting communities in unweighted, undirected networks. First, we give a greedy algorithm fpGreed that tries to optimise the fp measure by working iteratively at two-levels, vertex-level and community-level. At the vertex level, a vertex joins a community only if the fp value improves. Once this is done, an initial set of communities are obtained. At the next stage, two communities merge only if the fp measure improves. Once there are no further improvements to be made, the algorithm switches back to the vertex level and so on. fpGreed terminates when there are no changes to any community. We then present a faster heuristic algorithm fastFp more suitable for running on large datasets. We present the quality of the communities and the time it takes to compute them on several well-known datasets. For some of the large datasets, such as youtube and livejournal, we find that Algorithm fastFP performs really well, both in terms of the time and the quality of the solution obtained.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Propagation-aware Representation Learning for Supervised Social Media Graph Analytics</title>
<link>https://arxiv.org/abs/2509.01124</link>
<guid>https://arxiv.org/abs/2509.01124</guid>
<content:encoded><![CDATA[
<div> representation learning, social media, graph analytics, propagation dynamics, transferability 

Summary:
The paper introduces a novel representation learning framework for social media graph analytics. The proposed framework combines a dual-encoder structure with a kinetic-guided propagation module to model both structural and contextual information in social media graphs. By incorporating principled kinetic knowledge, the framework captures information propagation dynamics within the graphs, enhancing robustness to noisy data. The approach achieves state-of-the-art performance on various social media graph mining tasks such as graph classification, node classification, and link prediction. It also demonstrates strong zero-shot and few-shot transferability across datasets, making it practical for handling data-scarce tasks. This framework offers a unified architecture that can be effectively reused on different tasks, reducing repetitive engineering efforts. <div>
arXiv:2509.01124v1 Announce Type: new 
Abstract: Social media platforms generate vast, complex graph-structured data, facilitating diverse tasks such as rumor detection, bot identification, and influence modeling. Real-world applications like public opinion monitoring and stock trading -- which have a strong attachment to social media -- demand models that are performant across diverse tasks and datasets. However, most existing solutions are purely data-driven, exhibiting vulnerability to the inherent noise within social media data. Moreover, the reliance on task-specific model design challenges efficient reuse of the same model architecture on different tasks, incurring repetitive engineering efforts. To address these challenges in social media graph analytics, we propose a general representation learning framework that integrates a dual-encoder structure with a kinetic-guided propagation module. In addition to jointly modeling structural and contextual information with two encoders, our framework innovatively captures the information propagation dynamics within social media graphs by integrating principled kinetic knowledge. By deriving a propagation-aware encoder and corresponding optimization objective from a Markov chain-based transmission model, the representation learning pipeline receives a boost in its robustness to noisy data and versatility in diverse tasks. Extensive experiments verify that our approach achieves state-of-the-art performance with a unified architecture on a variety of social media graph mining tasks spanning graph classification, node classification, and link prediction. Besides, our solution exhibits strong zero-shot and few-shot transferability across datasets, demonstrating practicality when handling data-scarce tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unnoticeable Community Deception via Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2509.01438</link>
<guid>https://arxiv.org/abs/2509.01438</guid>
<content:encoded><![CDATA[
<div> Community detection, graphs, privacy concerns, deception methods, optimization<br />
<br />
Summary:<br />
Community detection in graphs is essential for understanding node organization. However, it can pose privacy risks. Current community deception methods have limitations, such as ineffective evaluation metrics and detectable attacks. This study analyzes the shortcomings of modularity decrease as a deception metric and introduces a new metric. By combining this metric with an attack budget, an unnoticeable community deception task is formulated as a multi-objective optimization problem. Two enhanced methods, incorporating degree-biased and community-biased candidate node selection mechanisms, are proposed to improve deception performance. Extensive experiments on benchmark datasets validate the effectiveness of the proposed community deception strategies. <div>
arXiv:2509.01438v1 Announce Type: new 
Abstract: Community detection in graphs is crucial for understanding the organization of nodes into densely connected clusters. While numerous strategies have been developed to identify these clusters, the success of community detection can lead to privacy and information security concerns, as individuals may not want their personal information exposed. To address this, community deception methods have been proposed to reduce the effectiveness of detection algorithms. Nevertheless, several limitations, such as the rationality of evaluation metrics and the unnoticeability of attacks, have been ignored in current deception methods. Therefore, in this work, we first investigate the limitations of the widely used deception metric, i.e., the decrease of modularity, through empirical studies. Then, we propose a new deception metric, and combine this new metric together with the attack budget to model the unnoticeable community deception task as a multi-objective optimization problem. To further improve the deception performance, we propose two variant methods by incorporating the degree-biased and community-biased candidate node selection mechanisms. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed community deception strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic</title>
<link>https://arxiv.org/abs/2509.01954</link>
<guid>https://arxiv.org/abs/2509.01954</guid>
<content:encoded><![CDATA[
<div> weekday effects, COVID-19, YouTube, sentiment, engagement<br />
Summary:<br />
This study analyzed 10,000 COVID-19-related YouTube videos from January 2023 to October 2024 to understand factors influencing viewer engagement during the late pandemic. Publishing activity showed weekday peaks, with viewers shifting attention mid-week. High-frequency keywords in titles included COVID, coronavirus, shorts, and live. Videos titled with shorts attracted the highest views. Sentiment analysis of video descriptions had a weak correlation with views initially, but stronger correlations emerged after addressing outliers. Video duration and category also impacted engagement, with entertainment videos attracting the highest views. Overall, the study found that engagement patterns of COVID-19 videos on YouTube were influenced by factors such as publishing schedules, title vocabulary, topics, and genre-specific duration effects.<br /><br /> <div>
arXiv:2509.01954v1 Announce Type: new 
Abstract: This work investigated about 10,000 COVID-19-related YouTube videos published between January 2023 and October 2024 to evaluate how temporal, lexical, linguistic, and structural factors influenced engagement during the late pandemic period. Publishing activity showed consistent weekday effects: in the first window, average views peaked on Mondays at 92,658; in the second, on Wednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a shift in audience attention toward mid- and late week. Lexical analysis of video titles revealed recurring high-frequency keywords related to COVID-19 and YouTube features, including COVID, coronavirus, shorts, and live. Frequency analysis revealed sharp spikes, with COVID appearing in 799 video titles in August 2024, while engagement analysis showed that videos titled with shorts attracted very high views, peaking at 2.16 million average views per video in June 2023. Analysis of sentiment of video descriptions in English showed weak correlation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but stronger correlations emerged once outliers were addressed, with Spearman r = 0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis of video durations revealed contrasting outcomes: long videos focusing on people and blogs averaged 209,114 views, short entertainment videos averaged 288,675 views, and medium-to-long news and politics videos averaged 51,309 and 59,226 views, respectively. These results demonstrate that engagement patterns of COVID-19-related videos on YouTube during the late pandemic followed distinct characteristics driven by publishing schedules, title vocabulary, topics, and genre-specific duration effects.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics in Two-Sided Attention Markets: Objective, Optimization, and Control</title>
<link>https://arxiv.org/abs/2509.01970</link>
<guid>https://arxiv.org/abs/2509.01970</guid>
<content:encoded><![CDATA[
<div> Market Ecosystem, Content Creation, Platform, User-Platform Interaction, Two-Sided Market <br />
Summary: In this new study, the focus is on understanding the ecosystem of content creation and consumption within online platforms. The research delves into the dynamics of two-sided markets involving the platform, users, and creators. A potential function is designed to capture the interactions among these entities, with creators' best-response dynamics and users' choices being analyzed through mirror descent on this function. The study reveals that various platform ranking strategies correspond to different potential functions and their dynamics can still be captured through mirror descent. Additionally, the research offers a new local convergence result for mirror descent in non-convex functions. These findings provide a theoretical basis for explaining the diverse outcomes observed in attention markets. <div>
arXiv:2509.01970v1 Announce Type: new 
Abstract: With most content distributed online and mediated by platforms, there is a pressing need to understand the ecosystem of content creation and consumption. A considerable body of recent work shed light on the one-sided market on creator-platform or user-platform interactions, showing key properties of static (Nash) equilibria and online learning. In this work, we examine the {\it two-sided} market including the platform and both users and creators. We design a potential function for the coupled interactions among users, platform and creators. We show that such coupling of creators' best-response dynamics with users' multilogit choices is equivalent to mirror descent on this potential function. Furthermore, a range of platform ranking strategies correspond to a family of potential functions, and the dynamics of two-sided interactions still correspond to mirror descent. We also provide new local convergence result for mirror descent in non-convex functions, which could be of independent interest. Our results provide a theoretical foundation for explaining the diverse outcomes observed in attention markets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum entropy temporal networks</title>
<link>https://arxiv.org/abs/2509.02098</link>
<guid>https://arxiv.org/abs/2509.02098</guid>
<content:encoded><![CDATA[
<div> temporal networks, maximum entropy, inhomogeneous Poisson process, Hawkes process, log-likelihood<br />
Summary:<br />
This study introduces a maximum entropy approach to modeling temporal networks, where interactions are timestamped. The approach considers continuous-time interactions and yields a modular representation involving global time processes (such as inhomogeneous Poisson or Hawkes processes) and static maximum entropy edge probabilities. The factorization of time-edge labels allows for closed-form log-likelihood calculations, expectations of degree and unique edges, and the generation of various effective models for temporal networks. A log-linear Hawkes/NHPP intensity is derived through functional optimization over path entropy, linking inhomogeneous Poisson modeling to maximum entropy network ensembles. The use of global Hawkes time layers significantly improves log-likelihood compared to generic NHPP methods, while the maximum entropy edge labels help enforce strength constraints and reproduce expected unique-degree curves. The unified framework outlined in the study has potential integration with community tools, calibration procedures, and kernel estimation methods. <br /><br />Summary: <div>
arXiv:2509.02098v1 Announce Type: new 
Abstract: Temporal networks consist of timestamped directed interactions rather than static links. These links may appear continuously in time, yet few studies have directly tackled the continuous-time modeling of networks. Here, we introduce a maximum entropy approach to temporal networks and with basic assumptions on constraints, the corresponding network ensembles admit a modular and interpretable representation: a set of global time processes -an inhomogeneous Poisson or a Hawkes process- and a static maximum-entropy (MaxEnt) edge, e.g. node pair, probability. This time-edge labels factorization yields closed-form log-likelihoods, degree/unique-edge expectations, and yields a whole class of effective generative models. We provide maximum-entropy derivation of a log-linear Hawkes/NHPP intensity for temporal networks via functional optimization over path entropy, connecting inhomogeneous Poisson modeling -e.g. Hawkes models- to MaxEnt network ensembles. Global Hawkes time layers consistently improve log-likelihood over generic NHPP, while the MaxEnt edge labels recover strength constraints and reproduce expected unique-degree curves. We discuss the limitations of this unified framework and how it could be integrated with calibrated community/motif tools, Hawkes calibration procedures, and (neural) kernel estimation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RumorSphere: A Framework for Million-scale Agent-based Dynamic Simulation of Rumor Propagation</title>
<link>https://arxiv.org/abs/2509.02172</link>
<guid>https://arxiv.org/abs/2509.02172</guid>
<content:encoded><![CDATA[
<div> simulated rumor dynamics, large language models, social network simulation, opinion bias reduction, rumor spread<br />
<br />
Summary: 
This paper introduces a dynamic and hierarchical social network simulation framework utilizing large language models (LLMs) to model rumor propagation. The simulator, capable of handling millions of agents, demonstrates strong alignment with real-world rumor dynamics, reducing opinion bias by an average of 64%. The study identifies tightly connected local community structures as key drivers of rumor spread, where social pressure lead individuals to conform, perpetuating the dissemination. Counterfactual experiments suggest early and sustained efforts to correct misinformation are effective in mitigating rumors, with debunking through opinion leaders being the most successful strategy. The findings offer valuable insights for public opinion management and policymaking. <br /><br />Summary: <div>
arXiv:2509.02172v1 Announce Type: new 
Abstract: Rumor propagation modeling is critical for understanding the dynamics of misinformation spread. Previous models are either overly simplistic or static, making them ineffective for simulating real-world rumor dynamics. In this paper, leveraging the impressive human behavior imitation capabilities of large language models (LLMs), we present a novel dynamic and hierarchical social network simulation framework, which supports simulations with millions of agents. This simulator is used to explore the rumor dynamic in the real world. Experiments on real-world rumor propagation datasets reveal a strong alignment between simulated and real-world rumor dynamics, outperforming existing models with an average 64\% reduction in opinion bias. Our findings underscore the substantial potential of LLM-based multi-agent systems in social network simulations, offering critical insights for advancing social science research. Furthermore, our analysis reveals that the tightly connected local community structure within social networks is one of the key factors promoting the rapid spread of rumors. In these communities, as rumors propagate to a certain extent, some individuals, influenced by ''social pressure'', are often compelled to conform, while holders of minority opinions are further silenced, resulting in a vicious cycle that accelerates rumor dissemination. Through counterfactual experiments, we evaluate various intervention strategies and demonstrate that early and sustained efforts to correct misinformation are more effective in mitigating the spread of rumors, while debunking rumors through opinion leaders proves to be the most effective strategy. These findings provide valuable insights for public opinion management and policymaking.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Single-Linkage Clustering for Community Detection with Overlaps and Outliers</title>
<link>https://arxiv.org/abs/2509.02334</link>
<guid>https://arxiv.org/abs/2509.02334</guid>
<content:encoded><![CDATA[
<div> algorithm, community detection, HDBSCAN, hierarchical clustering, outlier detection

Summary: 
The article introduces a new approach to community detection that allows for outliers, departing from the traditional assumption of a partitioned community structure. Utilizing the Hierarchical Density Based Spatial Clustering for Applications with Noise (HDBSCAN) algorithm, the method redefines distance metrics to enable effective single-linkage clustering and noise robustness. By applying this hierarchical single-linkage clustering technique to various node/edge similarity metrics, the study explores its performance on synthetic and real-world datasets. Results demonstrate that no single method universally prevails across all graph types, implying the need for adaptive algorithms. Nevertheless, the promising outcomes suggest that hierarchical single-linkage clustering could serve as a viable model for graph clustering tasks. <div>
arXiv:2509.02334v1 Announce Type: new 
Abstract: Most community detection approaches make very strong assumptions about communities in the data, such as every vertex must belong to exactly one community (the communities form a partition). For vector data, Hierarchical Density Based Spatial Clustering for Applications with Noise (HDBSCAN) has emerged as a leading clustering algorithm that allows for outlier points that do not belong to any cluster. The first step in HDBSCAN is to redefine the distance between vectors in such a way that single-linkage clustering is effective and robust to noise. Many community detection algorithms start with a similar step that attempts to increase the weight of edges between similar nodes and decrease weights of noisy edges. In this paper, we apply the hierarchical single-linkage clustering algorithm from HDBSCAN to a variety of node/edge similarity scores to see if there is an algorithm that can effectively detect clusters while allowing for outliers. In experiments on synthetic and real world data sets, we find that no single method is optimal for every type of graph, but the admirable performance indicates that hierarchical single-linkage clustering is a viable paradigm for graph clustering.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Keyframe-Based Approach for Auditing Bias in YouTube Shorts Recommendations</title>
<link>https://arxiv.org/abs/2509.02543</link>
<guid>https://arxiv.org/abs/2509.02543</guid>
<content:encoded><![CDATA[
<div> keyframes, bias, drift, short-form video, recommendation systems 
Summary:
Using a keyframe-based approach, this study examines bias and drift in short-form video recommendations, focusing on politically sensitive topics. By analyzing perceptually salient keyframes and captions, the researchers identify shifts and clustering patterns in recommendation chains, indicating potential filtering and topic drift. A comparison between politically sensitive topics and general YouTube categories reveals noticeable differences in recommendation behavior. The findings emphasize the importance of understanding bias in short-form video algorithms and highlight the efficiency and interpretability of keyframes in this analysis.
<br /><br />Summary: <div>
arXiv:2509.02543v1 Announce Type: new 
Abstract: YouTube Shorts and other short-form video platforms now influence how billions engage with content, yet their recommendation systems remain largely opaque. Small shifts in promoted content can significantly impact user exposure, especially for politically sensitive topics. In this work, we propose a keyframe-based method to audit bias and drift in short-form video recommendations. Rather than analyzing full videos or relying on metadata, we extract perceptually salient keyframes, generate captions, and embed both into a shared content space. Using visual mapping across recommendation chains, we observe consistent shifts and clustering patterns that indicate topic drift and potential filtering. Comparing politically sensitive topics with general YouTube categories, we find notable differences in recommendation behavior. Our findings show that keyframes provide an efficient and interpretable lens for understanding bias in short-form video algorithms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse</title>
<link>https://arxiv.org/abs/2509.00696</link>
<guid>https://arxiv.org/abs/2509.00696</guid>
<content:encoded><![CDATA[
<div> toxicity, online conversations, emotion regulation, social media data, comment queuing mechanism<br />
Summary:<br />
The paper introduces a graph-based framework to identify the need for emotion regulation in online conversations, aiming to promote self-reflection and responsible behavior in real time. It also proposes a comment queuing mechanism to address intentional trolls by introducing a delay in publishing comments. Analysis of social media data from Twitter and Reddit shows that the framework reduces toxicity by 12% and decreases the spread of anger by 15%. The comment queuing mechanism holds only 4% of comments on average, allowing users time to self-regulate before engaging further. The combination of real-time emotion regulation and delayed moderation significantly improves well-being in online environments. <div>
arXiv:2509.00696v1 Announce Type: cross 
Abstract: The pervasiveness of online toxicity, including hate speech and trolling, disrupts digital interactions and online well-being. Previous research has mainly focused on post-hoc moderation, overlooking the real-time emotional dynamics of online conversations and the impact of users' emotions on others. This paper presents a graph-based framework to identify the need for emotion regulation within online conversations. This framework promotes self-reflection to manage emotional responses and encourage responsible behaviour in real time. Additionally, a comment queuing mechanism is proposed to address intentional trolls who exploit emotions to inflame conversations. This mechanism introduces a delay in publishing comments, giving users time to self-regulate before further engaging in the conversation and helping maintain emotional balance. Analysis of social media data from Twitter and Reddit demonstrates that the graph-based framework reduced toxicity by 12%, while the comment queuing mechanism decreased the spread of anger by 15%, with only 4% of comments being temporarily held on average. These findings indicate that combining real-time emotion regulation with delayed moderation can significantly improve well-being in online environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Community Detection and Novelty Scoring Reveal Underexplored Hub Genes in Rheumatoid Arthritis</title>
<link>https://arxiv.org/abs/2509.00897</link>
<guid>https://arxiv.org/abs/2509.00897</guid>
<content:encoded><![CDATA[
<div> Keywords: gene co-expression networks, rheumatoid arthritis, hub genes, network analysis, immune-related processes <br />
Summary: 
- The study focused on constructing gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue.
- Community detection algorithms revealed robust modules in the network, and node-strength ranking identified top hub genes globally and within communities.
- Integration of genome-wide association studies and literature-based evidence highlighted five high-centrality genes in RA with limited previous association.
- Functional enrichment analysis confirmed the roles of these hub genes in immune-related processes, particularly adaptive immune response and lymphocyte regulation.
- Strong correlations of these hub genes with T- and B-cell markers and negative correlations with NK-cell markers reflect the immunopathology of RA.
<br /><br />Summary: <div>
arXiv:2509.00897v1 Announce Type: cross 
Abstract: Understanding the modular structure and central elements of complex biological networks is critical for uncovering system-level mechanisms in disease. Here, we constructed weighted gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue, using pairwise correlation and a percolation-guided thresholding strategy. Community detection with Louvain and Leiden algorithms revealed robust modules, and node-strength ranking identified the top 50 hub genes globally and within communities. To assess novelty, we integrated genome-wide association studies (GWAS) with literature-based evidence from PubMed, highlighting five high-centrality genes with little to no prior RA-specific association. Functional enrichment confirmed their roles in immune-related processes, including adaptive immune response and lymphocyte regulation. Notably, these hubs showed strong positive correlations with T- and B-cell markers and negative correlations with NK-cell markers, consistent with RA immunopathology. Overall, our framework demonstrates how correlation-based network construction, modularity-driven clustering, and centrality-guided novelty scoring can jointly reveal informative structure in omics-scale data. This generalizable approach offers a scalable path to gene prioritization in RA and other autoimmune conditions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Gaussian Mixtures to Model Evolving Multi-Modal Beliefs Across Social Media</title>
<link>https://arxiv.org/abs/2509.01123</link>
<guid>https://arxiv.org/abs/2509.01123</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian mixtures, multi-modal beliefs, opinion uncertainty, social networks, opinion dynamics<br />
<br />
Summary: <br />
The article introduces a model using Gaussian mixtures to study the formation and evolution of multi-modal beliefs and opinion uncertainty in social networks. Opinions are influenced by both exogenous factors, such as news articles, and endogenous factors, like interactions on social media. This model allows for a comprehensive understanding of opinion dynamics while maintaining simplicity. The study focuses on the impact of stubborn individuals, or social influencers, on opinion formation and uncertainty. The research also introduces the concept of centrality based on an individual's ability to disrupt information flow in a social network. The preliminary results shed light on the complexities of opinion dynamics and the role of influential individuals in shaping beliefs across social networks. <br /> <div>
arXiv:2509.01123v1 Announce Type: cross 
Abstract: We use Gaussian mixtures to model formation and evolution of multi-modal beliefs and opinion uncertainty across social networks. In this model, opinions evolve by Bayesian belief update when incorporating exogenous factors (signals from outside sources, e.g., news articles) and by non-Bayesian mixing dynamics when incorporating endogenous factors (interactions across social media). The modeling enables capturing the richness of behavior observed in multi-modal opinion dynamics while maintaining interpretability and simplicity of scalar models. We present preliminary results on opinion formation and uncertainty to investigate the effect of stubborn individuals (as social influencers). This leads to a notion of centrality based on the ease with which an individual can disrupt the flow of information across the social network.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis</title>
<link>https://arxiv.org/abs/2509.02113</link>
<guid>https://arxiv.org/abs/2509.02113</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical graph, malware analysis, dataset, control flow graphs, function call graphs <br />
<br />
Summary: 
The article introduces a new dataset called HiGraph for malware analysis, consisting of over 200 million Control Flow Graphs nested within 595,000 Function Call Graphs. This hierarchical graph representation captures the structural semantics of software, enabling the development of robust detectors against code obfuscation and malware evolution. A large-scale analysis using HiGraph reveals distinct structural properties of benign and malicious software. The dataset and tools are publicly accessible at https://higraph.org, making it a valuable resource for the research community. <div>
arXiv:2509.02113v1 Announce Type: cross 
Abstract: The advancement of graph-based malware analysis is critically limited by the absence of large-scale datasets that capture the inherent hierarchical structure of software. Existing methods often oversimplify programs into single level graphs, failing to model the crucial semantic relationship between high-level functional interactions and low-level instruction logic. To bridge this gap, we introduce \dataset, the largest public hierarchical graph dataset for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs) nested within \textbf{595K} Function Call Graphs (FCGs). This two-level representation preserves structural semantics essential for building robust detectors resilient to code obfuscation and malware evolution. We demonstrate HiGraph's utility through a large-scale analysis that reveals distinct structural properties of benign and malicious software, establishing it as a foundational benchmark for the community. The dataset and tools are publicly available at https://higraph.org.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community as a Vague Operator: Epistemological Questions for a Critical Heuristics of Community Detection Algorithms</title>
<link>https://arxiv.org/abs/2210.02753</link>
<guid>https://arxiv.org/abs/2210.02753</guid>
<content:encoded><![CDATA[
<div> community, network science, vague operator, digital politics, social relations

Summary: 
- The article analyzes the nature and epistemic consequences of "communities" in network science.
- Communities are described as multi-faceted and ambivalent patterns, functioning as vague operators.
- Different modes of description, combining vagueness and hyper-precision, are crucial in digital politics and community analysis.
- The 'Louvain algorithm' is discussed as a widely-used community detection method, highlighting controversies in its applications.
- Communities can act as real abstractions reshaping social relations, such as creating echo chambers on social networking sites.
- The concept of 'critical heuristics' is proposed to reconsider community detection, emphasizing partiality, epistemic humbleness, reflexivity, and artificiality. <div>
arXiv:2210.02753v3 Announce Type: replace 
Abstract: In this article, we aim to analyse the nature and epistemic consequences of what figures in network science as patterns of nodes and edges called 'communities'. Tracing these patterns as multi-faceted and ambivalent, we propose to describe the concept of community as a 'vague operator', a variant of Susan Leigh Star's notion of the boundary object, and propose that the ability to construct different modes of description that are both vague in some registers and hyper-precise in others, is core both to digital politics and the analysis of 'communities'. Engaging with these formations in terms drawn from mathematics and software studies enables a wider mapping of their formation. Disentangling different lineages in network science then allows us to contextualise the founding account of 'community' popularised by Michelle Girvan and Mark Newman in 2002. After studying one particular community detection algorithm, the widely-used 'Louvain algorithm', we comment on controversies arising with some of their more ambiguous applications. We argue that 'community' can act as a real abstraction with the power to reshape social relations such as producing echo chambers in social networking sites. To rework the epistemological terms of community detection and propose a reconsideration of vague operators, we draw on debates and propositions within the literature of network science to imagine a 'critical heuristics' that embraces partiality, epistemic humbleness, reflexivity and artificiality.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Modelling in Analysing Cyber-related Graphs</title>
<link>https://arxiv.org/abs/2412.14375</link>
<guid>https://arxiv.org/abs/2412.14375</guid>
<content:encoded><![CDATA[
<div> influence spreading model, cyber attacks, attack graphs, causal graph, probabilistic analysis

Summary: 
The article introduces a novel network-based influence spreading model to analyze the structure and dynamics of cyber attacks. The model allows for detailed probabilistic analysis of directed, weighted, and cyclic attack and causal graphs, with a focus on self-avoiding attack chains in acyclic paths. By incorporating vulnerabilities, services, and exploitabilities, the model offers a quantitative approach beyond traditional visualizations, enabling cyber analysts to prioritize and analyze larger graphs effectively. The proposed model is demonstrated through three use cases involving cyber-related graphs, showcasing its utility in generating quantitative metrics for prioritization, summaries, and analysis. <div>
arXiv:2412.14375v2 Announce Type: replace 
Abstract: In order to improve the resilience of computer infrastructure against cyber attacks and finding ways to mitigate their impact we need to understand their structure and dynamics. Here we propose a novel network-based influence spreading model to investigate event trajectories or paths in various types of attack and causal graphs, which can be directed, weighted, and / or cyclic. In case of attack graphs with acyclic paths, only self-avoiding attack chains are allowed. In the framework of our model a detailed probabilistic analysis beyond the traditional visualisation of attack graphs, based on vulnerabilities, services, and exploitabilities, can be performed. In order to demonstrate the capabilities of the model, we present three use cases with cyber-related graphs, namely two attack graphs and a causal graph. The model can be of benefit to cyber analysts in generating quantitative metrics for prioritisation, summaries, or analysis of larger graphs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing News Engagement on Facebook: Tracking Ideological Segregation and News Quality in the Facebook URL Dataset</title>
<link>https://arxiv.org/abs/2409.13461</link>
<guid>https://arxiv.org/abs/2409.13461</guid>
<content:encoded><![CDATA[
<div> Keywords: Facebook, user engagement, news consumption, ideological gap, news quality

Summary:
The study analyzes user engagement with news domains on Facebook from 2017 to 2020, focusing on news URLs in the U.S. It considers ideological alignment, news source quality, and users' political preferences to track news consumption patterns for liberal, conservative, and moderate audiences. The analysis reveals two significant shifts in trends where the ideological gap widens and news quality declines, leading to changes in user engagement. The findings highlight the impact of two major Facebook News Feed updates on user behavior and news consumption. This empirical evidence helps understand user engagement with news, ideological leanings, and news reliability on Facebook during the study period.<br /><br />Summary: <div>
arXiv:2409.13461v3 Announce Type: replace-cross 
Abstract: The Facebook Privacy-Protected Full URLs Dataset was released to enable independent, academic research on the impact of Facebook's platform on society while ensuring user privacy. The dataset has been used in several studies to analyze the relationship between social media engagement and societal issues such as misinformation, polarization, and the quality of consumed news. In this paper, we conduct a comprehensive analysis of the engagement with popular news domains, covering four years from January 2017 to December 2020, with a focus on user engagement metrics related to news URLs in the U.S. By incorporating the ideological alignment and composite score of quality and reliability of news sources, along with users' political preferences, we construct weighted averages of ideology and quality of news consumption for liberal, conservative, and moderate audiences. This allows us to track the evolution of (i) the ideological gap in news consumption between liberals and conservatives and (ii) the average quality of each group's news consumption. We identify two major shifts in trends, each tied to engagement changes. In both, the ideological gap widens and news quality declines. However, engagement rises in the first shift but falls in the second. Finally, we contextualize these trends by linking them to two major Facebook News Feed updates. Our findings provide empirical evidence to better understand user behavior and engagement with news and their leaning and reliability during the period covered by the dataset.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation</title>
<link>https://arxiv.org/abs/2502.11649</link>
<guid>https://arxiv.org/abs/2502.11649</guid>
<content:encoded><![CDATA[
<div> Keywords: non-cooperative game, opinion formation, confirmation bias, misinformation, resource optimization 

Summary:<br /><br />
The article introduces a non-cooperative game to analyze opinion formation and resistance, considering factors like confirmation bias, resource constraints, and influence penalties. The simulation involves Large Language Model agents competing to influence a population, with penalties for generating messages that propagate or counter misinformation. The study shows that higher confirmation bias leads to stronger opinion alignment but also increases overall polarization. On the other hand, lower confirmation bias results in fragmented opinions and minimal shifts in beliefs. Investing heavily in debunking strategies can align the population temporarily but may deplete resources and reduce long-term influence. The study highlights the complex interplay between confirmation bias, resource allocation, and misinformation in shaping opinion dynamics and resistance mechanisms. <div>
arXiv:2502.11649v3 Announce Type: replace-cross 
Abstract: We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population-Scale Network Embeddings Expose Educational Divides in Network Structure Related to Right-Wing Populist Voting</title>
<link>https://arxiv.org/abs/2508.21236</link>
<guid>https://arxiv.org/abs/2508.21236</guid>
<content:encoded><![CDATA[
<div> Population-scale network, machine learning, embeddings, right-wing populist voting, Dutch population 
Summary: 
- Administrative registry data can be used to construct population-scale networks representing shared social contexts.
- Machine learning can encode these networks into numerical embeddings that capture individuals' positions.
- Embeddings for Dutch population from five shared contexts were created and used to predict right-wing populist voting.
- Embeddings alone predicted voting above chance level but individual characteristics performed better.
- Sparse and orthogonal transformations of embeddings revealed one dimension strongly associated with voting outcome.
- Mapping this dimension back to the population network showed structural differences related to education and voting patterns. 
<br /><br />Summary: <div>
arXiv:2508.21236v1 Announce Type: new 
Abstract: Administrative registry data can be used to construct population-scale networks whose ties reflect shared social contexts between persons. With machine learning, such networks can be encoded into numerical representations -- embeddings -- that automatically capture individuals' position within the network. We created embeddings for all persons in the Dutch population from a population-scale network that represents five shared contexts: neighborhood, work, family, household, and school. To assess the informativeness of these embeddings, we used them to predict right-wing populist voting. Embeddings alone predicted right-wing populist voting above chance-level but performed worse than individual characteristics. Combining the best subset of embeddings with individual characteristics only slightly improved predictions. However, after transforming the embeddings to make their dimensions more sparse and orthogonal, we found that one embedding dimension was strongly associated with the outcome. Mapping this dimension back to the population network revealed differences in network structure related to right-wing populist voting between different school ties and achieved education levels. Our study contributes methodologically by demonstrating how population-scale network embeddings can be made interpretable, and substantively by linking structural network differences in education to right-wing populist voting.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feminism, gender identity and polarization in TikTok and Twitter</title>
<link>https://arxiv.org/abs/2508.21301</link>
<guid>https://arxiv.org/abs/2508.21301</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, feminism, transsexuality, polarization, young people

Summary:
The research explores how social media platforms are shaping the debate on feminism and transsexuality, focusing on the term 'TERF'. Through Social Network Analysis, the study found that Twitter and TikTok communities discussing this topic are not cohesive, indicating isolation. Young people mostly support transinclusive feminism, highlighting a generational divide in the ideological debate. TikTok is seen as a more dialogue-based platform compared to Twitter's more partisan nature. The polarization observed suggests a strong divide within feminist activism online, with implications for broader discussions on sexual identity and activism on social media platforms.<br /><br />Summary: <div>
arXiv:2508.21301v1 Announce Type: new 
Abstract: The potential of social media to create open, collaborative and participatory spaces allows young women to engage and empower themselves in political and social activism. In this context, the objective of this research is to analyze the polarization in the debate at the intersection between the defense of feminism and transsexuality, preferably among the young population, symbolized in the use of the term 'TERF'. To do this, the existing communities on this subject on Twitter and TikTok have been analyzed with Social Network Analysis techniques, in addition to the presence of young people in them. The results indicate that the debates between both networks are not very cohesive, with a highly modularized structure that suggests isolation of each community. For this reason, it may be considered that the debate on sexual identity has resulted in a strong polarization of feminist activism in social media. Likewise, the positions of transinclusive feminism are very much in the majority among young people; this reinforces the idea of an ideological debate that can also be understood from a generational perspective. Finally, differential use between both social networks has been identified, where TikTok is a less partisan and more dialogue-based network than Twitter, which leads to discussions and participation in a more neutral tone.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Supported Content Analysis of Motivated Reasoning on Climate Change</title>
<link>https://arxiv.org/abs/2508.21305</link>
<guid>https://arxiv.org/abs/2508.21305</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, YouTube comments, social network analysis, motivated reasoning, large language model

Summary: 
This study analyzes YouTube comments related to climate change to understand the differences in discourse between believers and skeptics. Using a large language model, researchers identified ten distinct topics and examined engagement patterns through social network analysis. The findings show that discussions about government policy and natural cycles receive lower interaction compared to misinformation, indicating these topics are settled within their respective communities. This reflects motivated reasoning, where individuals selectively engage with content that aligns with their beliefs. The study highlights the role of cognitive and ideological motivations in shaping climate discourse, showcasing the potential of large language models for qualitative analysis on a large scale. <div>
arXiv:2508.21305v1 Announce Type: new 
Abstract: Public discourse around climate change remains polarized despite scientific consensus on anthropogenic climate change (ACC). This study examines how "believers" and "skeptics" of ACC differ in their YouTube comment discourse. We analyzed 44,989 comments from 30 videos using a large language model (LLM) as a qualitative annotator, identifying ten distinct topics. These annotations were combined with social network analysis to examine engagement patterns. A linear mixed-effects model showed that comments about government policy and natural cycles generated significantly lower interaction compared to misinformation, suggesting these topics are ideologically settled points within communities. These patterns reflect motivated reasoning, where users selectively engage with content that aligns with their identity and beliefs. Our findings highlight the utility of LLMs for large-scale qualitative analysis and highlight how climate discourse is shaped not only by content, but by underlying cognitive and ideological motivations.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Inference of Cell Complexes from Flows via Matrix Factorization</title>
<link>https://arxiv.org/abs/2508.21372</link>
<guid>https://arxiv.org/abs/2508.21372</guid>
<content:encoded><![CDATA[
<div> Keywords: edge-flow signals, cell complex, Hodge Laplacian, matrix-factorization-based heuristic, computational experiments

Summary:
In this paper, the authors address the problem of inferring a cell complex from edge-flow signals observed on a graph. They aim to represent these signals as a sparse combination of gradient and curl flows on the cell complex. While the general problem is known to be NP-hard, the authors propose a novel matrix-factorization-based heuristic to solve it efficiently. Through computational experiments, they show that their approach is less computationally expensive than previous heuristics, with comparable performance in most cases and even outperforming existing methods in noisy settings. By augmenting the observed graph with 2-cells and leveraging the eigenvectors of the Hodge Laplacian, their method provides a sparse and interpretable representation of edge flows on the graph. This work contributes to the field of graph inference by offering a practical and efficient solution to a challenging problem.

<br /><br />Summary: <div>
arXiv:2508.21372v1 Announce Type: new 
Abstract: We consider the following inference problem: Given a set of edge-flow signals observed on a graph, lift the graph to a cell complex, such that the observed edge-flow signals can be represented as a sparse combination of gradient and curl flows on the cell complex. Specifically, we aim to augment the observed graph by a set of 2-cells (polygons encircled by closed, non-intersecting paths), such that the eigenvectors of the Hodge Laplacian of the associated cell complex provide a sparse, interpretable representation of the observed edge flows on the graph. As it has been shown that the general problem is NP-hard in prior work, we here develop a novel matrix-factorization-based heuristic to solve the problem. Using computational experiments, we demonstrate that our new approach is significantly less computationally expensive than prior heuristics, while achieving only marginally worse performance in most settings. In fact, we find that for specifically noisy settings, our new approach outperforms the previous state of the art in both solution quality and computational speed.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology</title>
<link>https://arxiv.org/abs/2508.21740</link>
<guid>https://arxiv.org/abs/2508.21740</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social simulations, online platforms, toxicity dynamics, moderation strategies

Summary: 
The study focuses on creating a technology community simulation based on Voat, an alt-right platform, using Large Language Models (LLMs) to generate culturally informed interactions. The simulation is seeded with technology links from Voat and calibrated to replicate the platform's characteristics. Agents in the simulation utilize personas to generate posts, replies, and reactions following platform rules. The 30-day simulation shows similarities to Voat's activity patterns, interaction networks, and topical alignment, including elevated toxicity levels. The study highlights regular online phenomena such as heavy-tailed participation and sparse interaction networks. However, the study has limitations in agent design and the evaluation based on a single run. Despite these limitations, the simulation provides insights into toxicity dynamics and can be used to test moderation strategies within controlled environments. <br /><br />Summary: <div>
arXiv:2508.21740v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voat's shared URLs (covering 30+ domains) and calibrate parameters to Voat's v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model</title>
<link>https://arxiv.org/abs/2306.12968</link>
<guid>https://arxiv.org/abs/2306.12968</guid>
<content:encoded><![CDATA[
<div> Algorithm, Hidden communities, Labeled Stochastic Block Model, Instance-Adaptive Clustering, Spectral clustering <br />
<br />
Summary: 
The paper addresses the challenge of detecting hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters that increase proportionally with the total number of nodes. It establishes the conditions for reducing the expected number of misclassified nodes to less than $ s $ for any $ s = o(n) $. To accomplish this, the Instance-Adaptive Clustering (IAC) algorithm is introduced, which aligns with the instance-specific lower bounds in both expectation and high probability. IAC comprises a two-phase process involving one-shot spectral clustering and iterative likelihood-based cluster assignment improvements. Notably, it operates without prior knowledge of model parameters, such as the number of clusters, and maintains a computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $. This makes IAC suitable for large-scale applications, offering scalability and efficiency. <br /> <div>
arXiv:2306.12968v3 Announce Type: replace 
Abstract: In this paper, we investigate the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters whose sizes grow linearly with the total number of nodes. We derive the necessary and sufficient conditions under which the expected number of misclassified nodes is less than $ s $, for any number $ s = o(n) $. To achieve this, we propose IAC (Instance-Adaptive Clustering), the first algorithm whose performance matches the instance-specific lower bounds both in expectation and with high probability. IAC is a novel two-phase algorithm that consists of a one-shot spectral clustering step followed by iterative likelihood-based cluster assignment improvements. This approach is based on the instance-specific lower bound and notably does not require any knowledge of the model parameters, including the number of clusters. By performing the spectral clustering only once, IAC maintains an overall computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $, making it scalable and practical for large-scale problems.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed membership estimation for categorical data with weighted responses</title>
<link>https://arxiv.org/abs/2310.10989</link>
<guid>https://arxiv.org/abs/2310.10989</guid>
<content:encoded><![CDATA[
<div> Keywords: Grade of Membership model, Weighted Grade of Membership model, latent classes, categorical data, algorithm<br />
Summary:<br />
The paper introduces the Weighted Grade of Membership (WGoM) model as a more flexible alternative to the Grade of Membership (GoM) model for inferring latent classes in categorical data. While GoM is limited to nonnegative integer responses, WGoM can handle continuous and negative weighted responses by relaxing the distribution constraint. The proposed algorithm efficiently estimates latent mixed memberships and other WGoM parameters, with proven statistical consistency. A method for determining the number of latent classes for categorical data with weighted responses is also presented. Validation using synthetic and real-world datasets demonstrates the accuracy and efficiency of the algorithm in estimating latent mixed memberships and determining the number of latent classes, highlighting its practical application potential.<br /> 
Summary: <div>
arXiv:2310.10989v2 Announce Type: replace 
Abstract: The Grade of Membership (GoM) model, which allows subjects to belong to multiple latent classes, is a powerful tool for inferring latent classes in categorical data. However, its application is limited to categorical data with nonnegative integer responses, as it assumes that the response matrix is generated from Bernoulli or Binomial distributions, making it inappropriate for datasets with continuous or negative weighted responses. To address this, this paper proposes a novel model named the Weighted Grade of Membership (WGoM) model. Our WGoM is more general than GoM because it relaxes GoM's distribution constraint by allowing the response matrix to be generated from distributions like Bernoulli, Binomial, Normal, and Uniform as long as the expected response matrix has a block structure related to subjects' mixed memberships under the distribution. We show that WGoM can describe any response matrix with finite distinct elements. We then propose an algorithm to estimate the latent mixed memberships and other WGoM parameters. We derive the error bounds of the estimated parameters and show that the algorithm is statistically consistent. We also propose an efficient method for determining the number of latent classes $K$ for categorical data with weighted responses by maximizing fuzzy weighted modularity. The performance of our methods is validated through both synthetic and real-world datasets. The results demonstrate the accuracy and efficiency of our algorithm for estimating latent mixed memberships, as well as the high accuracy of our method for estimating $K$, indicating their high potential for practical applications.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Antivaccine Tweets on COVID-19 Vaccinations, Cases, and Deaths</title>
<link>https://arxiv.org/abs/2406.09142</link>
<guid>https://arxiv.org/abs/2406.09142</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19 vaccines, vaccine misinformation, vaccination rates, antivaccine content, social media moderation <br />
Summary: 
This study investigates the impact of exposure to antivaccine content on COVID-19 vaccination rates in the United States during 2021. Using an epidemic model and observational data, the researchers found a causal relationship between online antivaccine content on platforms like Twitter and reduced vaccine uptake in various US counties. The analysis estimates that around 14,000 individuals refused vaccination due to exposure to such content, leading to at least 510 additional cases and 8 additional deaths between February and August 2021. The study underscores the influence of social media in shaping public health decisions and highlights the importance of addressing vaccine misinformation online through effective moderation policies. These findings provide valuable insights for informing both social media regulation and public health interventions to promote higher vaccination rates and combat the spread of misinformation during health crises. <br /><br />Summary: <div>
arXiv:2406.09142v2 Announce Type: replace 
Abstract: Despite the wide availability of COVID-19 vaccines in the United States and their effectiveness in reducing hospitalizations and mortality during the pandemic, a majority of Americans chose not to be vaccinated during 2021. Recent work shows that vaccine misinformation affects intentions in controlled settings, but does not link it to real-world vaccination rates. Here, we present observational evidence of a causal relationship between exposure to antivaccine content and vaccination rates, and estimate the size of this effect. We present a compartmental epidemic model that includes vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the model to data to determine that a geographical pattern of exposure to online antivaccine content across US counties explains reduced vaccine uptake in the same counties. We find observational evidence that exposure to antivaccine content on Twitter caused about 14,000 people to refuse vaccination between February and August 2021 in the US, resulting in at least 510 additional cases and 8 additional deaths. This work provides a methodology for linking online speech with offline epidemic outcomes. Our findings should inform social media moderation policy as well as public health interventions.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifaceted polarisation and information reliability in climate change discussions on social media platforms</title>
<link>https://arxiv.org/abs/2410.21187</link>
<guid>https://arxiv.org/abs/2410.21187</guid>
<content:encoded><![CDATA[
<div> climate change, social media, Twitter, polarization, misinformation
Summary:
The study examines the interactions on Twitter related to climate change discussions across various topics. It finds that while retweets create echo chambers, mentions lead to exposure to opposing views, intensifying polarization. Ideological divides are evident in content differences and negative sentiments, particularly from right-leaning communities sharing low-reliability information. The study identifies a topological alignment between platforms, showing that ideological communities span multiple sites. Climate change polarisation involves ideological divides, structural isolation, and emotional engagement. The results highlight the need for climate policy discussions to address the emotional and identity-driven nature of public discourse and find strategies to bridge ideological divides. <div>
arXiv:2410.21187v3 Announce Type: replace-cross 
Abstract: Social media platforms like YouTube and Twitter play a key role in disseminating both reliable and unreliable information about climate change. This study analyses the topology of interactions in Twitter and their relation to cross-platform sharing, content discussions and emotional responses. We examined climate change discussions across four topics: the 27th United Nations Climate Change Conference, the Sixth Assessment Report of the United Nations Intergovernmental Panel on Climate Change, climate refugees, and Do\~nana Natural Park. While retweets reinforce in-group cohesion in the form of echo chambers, inter-group exposure is significant through mentions, suggesting that exposure to opposing views intensifies polarisation, rather than mitigates it. Ideological divides feature content differences accompanied by steeper negative sentiments, especially from right-leaning communities prone to share low-reliability information. We identified a topological alignment between platforms, indicating that ideological communities span multiple sites. Our findings show that climate change polarisation is multifaceted, involving ideological divides, structural isolation, and emotional engagement. These results suggest that effective climate policy discussions must address the emotional and identity-driven nature of public discourse and seek strategies to bridge ideological divides.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Confidence Models of Multidimensional Opinions with Topic-Weighted Discordance</title>
<link>https://arxiv.org/abs/2502.00284</link>
<guid>https://arxiv.org/abs/2502.00284</guid>
<content:encoded><![CDATA[
<div> opinion dynamics, multidimensional opinions, bounded-confidence model, topic-weighted discordance functions, steady-state opinion clusters <br />
Summary: <br />
This study explores how people's opinions on multiple topics evolve through interactions, considering interdependent and correlated opinions. The authors extend classical agent-based models to a multidimensional setting, where opinions on different topics are interconnected. Introducing topic-weighted discordance functions to measure opinion differences between agents, they define regions of receptiveness and analyze steady-state opinion clusters. Their models, simulated on various networks, show significant differences in results compared to baseline models when initial opinions are correlated across topics, impacting both transient and steady states. This research highlights the importance of considering multidimensional opinions in understanding opinion dynamics and provides a framework for analyzing opinion evolution in a multi-topic context. <br /> <div>
arXiv:2502.00284v2 Announce Type: replace-cross 
Abstract: People's opinions on a wide range of topics often evolve over time through their interactions with others. Models of opinion dynamics primarily focus on one-dimensional opinions, which represent opinions on one topic. However, opinions on various topics are rarely isolated; instead, they can be interdependent and correlated. In a bounded-confidence model (BCM) of opinion dynamics, agents are receptive to each other only if their opinions are sufficiently similar. We extend classical agent-based BCMs -- namely, the Hegselmann--Krause BCM, which has synchronous interactions, and the Deffuant--Weisbuch BCM, which has asynchronous interactions -- to a multidimensional setting, in which the opinions are multidimensional vectors representing opinions of different topics and opinions on different topics are interdependent. To measure opinion differences between agents, we introduce topic-weighted discordance functions that account for opinion differences in all topics. We define regions of receptiveness for our models, and we use them to characterize the steady-state opinion clusters and provide an analytical approach to compute these regions. In addition, we numerically simulate our models on various networks with initial opinions drawn from a variety of distributions. When initial opinions are correlated across different topics, our topic-weighted BCMs yield significantly different results in both transient and steady states compared to baseline models, where the dynamics of each opinion topic are independent.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of A National Digitally-Enabled Health Promotion Campaign for Mental Health Awareness using Social Media Platforms Tik Tok, Facebook, Instagram, and YouTube</title>
<link>https://arxiv.org/abs/2508.20142</link>
<guid>https://arxiv.org/abs/2508.20142</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health promotion, digital platforms, campaign effectiveness, Singapore, engagement

Summary:
This study evaluated the effectiveness of a digitally-enabled mental health promotion campaign in Singapore using various digital platforms. The campaign aimed to raise awareness of mental health issues and provide support to at-risk populations. The campaign materials, including narrative videos and infographics, were disseminated across YouTube, Facebook, Instagram, and TikTok. The primary outcomes showed that the campaign reached 1.39 million unique residents and generated 3.49 million total impressions. The cost-efficiency metrics indicated a Cost per Mille of $26.90, Cost per Click of $29.33, and Cost per Action of $6.06. Narrative videos received over 630,000 views and 18,768 engagements. Overall, the study demonstrates the effectiveness of digitally-enabled mental health promotion campaigns in engaging a national audience through multi-channel distribution and creative, narrative-driven designs.<br /><br />Summary: <div>
arXiv:2508.20142v1 Announce Type: new 
Abstract: Mental health disorders rank among the 10 leading contributors to the global burden of diseases, yet persistent stigma and care barriers delay early intervention. This has inspired efforts to leverage digital platforms for scalable health promotion to engage at-risk populations. To evaluate the effectiveness of a digitally-enabled mental health promotion (DEHP) campaign, we conducted an observational cross-sectional study of a 3-month (February-April 2025) nation-wide campaign in Singapore. Campaign materials were developed using a marketing funnel framework and disseminated across YouTube, Facebook, Instagram, and TikTok. This included narrative videos and infographics to promote symptom awareness, coping strategies, and/or patient navigation to mindline.sg, as the intended endpoint for user engagement and support. Primary outcomes include anonymised performance analytics (impressions, unique reach, video content view, engagements) stratified by demographics, device types, and sector. Secondary outcomes measured cost-efficiency metrics and traffic to mindline.sg respectively. This campaign generated 3.49 million total impressions and reached 1.39 million unique residents, with a Cost per Mille at \$26.90, Cost per Click at \$29.33, and Cost per Action at \$6.06. Narrative videos accumulated over 630,000 views and 18,768 engagements. Overall, we demonstrate that DEHP campaigns can achieve national engagement for mental health awareness through multi-channel distribution and creative, narrative-driven designs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whom We Trust, What We Fear: COVID-19 Fear and the Politics of Information</title>
<link>https://arxiv.org/abs/2508.20146</link>
<guid>https://arxiv.org/abs/2508.20146</guid>
<content:encoded><![CDATA[
<div> infodemic, fear, COVID-19, information sources, demographic factors
Summary:
The study examines the relationship between fear levels related to COVID-19 and the information sources individuals rely on during the pandemic. It finds that fear levels and information source usage mirror COVID-19 infection trends, are strongly correlated within each group, and vary across demographic groups, particularly age and education. The type of information source significantly impacts fear levels, highlighting the importance of the information ecosystem in shaping emotional responses during crises. Moreover, information source preferences align with the political orientation of U.S. states, indicating the influence of media on public perception. These findings underscore the critical role of information dissemination in shaping individuals' emotional and behavioral reactions during global health crises like the COVID-19 pandemic.
<br /><br /> <div>
arXiv:2508.20146v1 Announce Type: new 
Abstract: The COVID-19 pandemic triggered not only a global health crisis but also an infodemic, an overload of information from diverse sources influencing public perception and emotional responses. In this context, fear emerged as a central emotional reaction, shaped by both media exposure and demographic factors. In this study, we analyzed the relationship between individuals' self-reported levels of fear about COVID-19 and the information sources they rely on, across nine source categories, including medical experts, government institutions, media, and personal networks. In particular, we defined a score that ranks fear levels based on self-reported concerns about the pandemic, collected through the Delphi CTIS survey in the United States between May 2021 and June 2022. We found that both fear levels and information source usage closely follow COVID-19 infection trends, exhibit strong correlations within each group (fear levels across sources are strongly correlated, as are patterns of source usage), and vary significantly across demographic groups, particularly by age and education. Applying causal inference methods, we showed that the type of information source significantly affects individuals' fear levels. Furthermore, we demonstrated that information source preferences can reliably match the political orientation of U.S. states. These findings highlight the importance of information ecosystem dynamics in shaping emotional and behavioral responses during large-scale crises.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal vulnerability in strong modular networks with various degree distributions between inequality and equality</title>
<link>https://arxiv.org/abs/2508.20317</link>
<guid>https://arxiv.org/abs/2508.20317</guid>
<content:encoded><![CDATA[
<div> Keywords: networks, equality, fragmentation, connectivity, vulnerability

Summary: 
In a study comparing networks characterized by inequality and equality in the distribution of links, researchers found that networks with a more equal distribution of links demonstrate stronger connectivity tolerance to node malfunctions. However, the addition of a strong modular structure in networks with commonalities such as areas, interests, or purpose renders them extremely vulnerable to attacks or disasters. This highlights the importance of balanced resource allocation between intra- and inter-links of weak communities to avoid dense unions and maintain network resilience. The findings suggest that efficiency must be balanced with tolerance in network design, emphasizing the need to consider both aspects when creating network structures. Ultimately, the study underscores the importance of reevaluating network design strategies to enhance resilience and avoid potential vulnerabilities. 

Summary: <div>
arXiv:2508.20317v1 Announce Type: cross 
Abstract: Generally, networks are classified into two sides of inequality and equality with respect to the number of links at nodes by the types of degree distributions. One side includes many social, technological, and biological networks which consist of a few nodes with many links, and many nodes with a few links, whereas the other side consists of all nodes with an equal number of links. In comprehensive investigations between them, we have found that, as a more equal network, the tolerance of whole connectivity is stronger without fragmentation against the malfunction of nodes in a wide class of randomized networks. However, we newly find that all networks which include typical well-known network structures between them become extremely vulnerable, if a strong modular (or community) structure is added with commonalities of areas, interests, religions, purpose, and so on. These results will encourage avoiding too dense unions by connecting nodes and taking into account the balanced resource allocation between intra- and inter-links of weak communities. We must reconsider not only efficiency but also tolerance against attacks or disasters, unless no community that is really impossible.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyGenStability: Multiscale community detection with generalized Markov Stability</title>
<link>https://arxiv.org/abs/2303.05385</link>
<guid>https://arxiv.org/abs/2303.05385</guid>
<content:encoded><![CDATA[
<div> Python, PyGenStability, community detection, graphs, Markov Stability

Summary:
PyGenStability is a Python software package designed for analyzing and visualizing unsupervised multiscale community detection in graphs. It offers a range of tools for finding optimized partitions of graphs at different levels of resolution using algorithms such as Louvain or Leiden. The package focuses on maximizing the generalized Markov Stability quality function and includes features like automatic detection of robust graph partitions. Users can choose quality functions for various types of graphs and also incorporate custom quality functions. Overall, PyGenStability provides a flexible and efficient solution for analyzing and exploring community structures in graphs. <div>
arXiv:2303.05385v3 Announce Type: replace 
Abstract: We present PyGenStability, a general-use Python software package that provides a suite of analysis and visualisation tools for unsupervised multiscale community detection in graphs. PyGenStability finds optimized partitions of a graph at different levels of resolution by maximizing the generalized Markov Stability quality function with the Louvain or Leiden algorithms. The package includes automatic detection of robust graph partitions and allows the flexibility to choose quality functions for weighted undirected, directed and signed graphs, and to include other user-defined quality functions.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Formation and Dynamics Among Multi-LLMs</title>
<link>https://arxiv.org/abs/2402.10659</link>
<guid>https://arxiv.org/abs/2402.10659</guid>
<content:encoded><![CDATA[
<div> social networks, large language models, network dynamics, human decisions, network formation

Summary:
The study investigates how large language models (LLMs) interact in social networks in comparison to human behavior. LLM agents demonstrate behaviors such as preferential attachment, triadic closure, and homophily, adapting their emphasis based on the context (e.g., favoring homophily in friendship networks and heterophily in organizational settings). They also replicate community structure and small-world effects observed in real-world networks. A human survey confirms the similarity in link-formation decisions between LLMs and humans. The study suggests that LLMs can be effective tools for simulating social networks and generating synthetic data. However, it also raises concerns about bias, fairness, and the design of AI systems that engage in human networks. <div>
arXiv:2402.10659v5 Announce Type: replace 
Abstract: Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale mobility patterns and the restriction of human movement</title>
<link>https://arxiv.org/abs/2201.06323</link>
<guid>https://arxiv.org/abs/2201.06323</guid>
<content:encoded><![CDATA[
<div> Flow communities, human mobility, COVID-19 pandemic, UK lockdown, Facebook Movement Maps <br />
Summary: 
The study explores human mobility during the COVID-19 pandemic using Facebook Movement Maps before and during the first UK lockdown. By analyzing mobility patterns at multiple scales, the research identifies robust flow communities that better represent mobility than traditional administrative divisions. These flow communities capture patterns beyond commuting to work and demonstrate how mobility evolved under lockdown measures. The study shows that mobility initially reverted to fine-scale flow communities during lockdown and then expanded back to coarser scales as restrictions eased. A linear decay shock model is used to quantify regional differences in the impact of lockdown and recovery times. The findings highlight the dynamic nature of human mobility during the pandemic and the importance of understanding mobility patterns at different scales for effective policymaking. <br /> <div>
arXiv:2201.06323v5 Announce Type: replace-cross 
Abstract: From the perspective of human mobility, the COVID-19 pandemic constituted a natural experiment of enormous reach in space and time. Here, we analyse the inherent multiple scales of human mobility using Facebook Movement Maps collected before and during the first UK lockdown. First, we obtain the pre-lockdown UK mobility graph, and employ multiscale community detection to extract, in an unsupervised manner, a set of robust partitions into flow communities at different levels of coarseness. The partitions so obtained capture intrinsic mobility scales with better coverage than NUTS regions, which suffer from mismatches between human mobility and administrative divisions. Furthermore, the flow communities in the fine scale partition match well the UK Travel to Work Areas (TTWAs) but also capture mobility patterns beyond commuting to work. We also examine the evolution of mobility under lockdown, and show that mobility first reverted towards fine scale flow communities already found in the pre-lockdown data, and then expanded back towards coarser flow communities as restrictions were lifted. The improved coverage induced by lockdown is well captured by a linear decay shock model, which allows us to quantify regional differences both in the strength of the effect and the recovery time from the lockdown shock.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGDE: Local Graph-based Dictionary Expansion</title>
<link>https://arxiv.org/abs/2405.07764</link>
<guid>https://arxiv.org/abs/2405.07764</guid>
<content:encoded><![CDATA[
<div> Graph-based, Dictionary Expansion, Manifold Learning, Network Science, Word Embeddings
Summary:
The study introduces Local Graph-based Dictionary Expansion (LGDE) method for discovering semantic neighbourhood of words using manifold learning and network science tools. LGDE constructs a word similarity graph from word embeddings' geometry and identifies local communities through graph diffusion, enabling exploration of nonlinear geometry for capturing word similarities based on semantic associations paths beyond direct pairwise similarities. LGDE enriches pre-selected keyword dictionaries, crucial for information retrieval tasks such as database queries and online data collection, as evidenced by its superior performance compared to direct word similarity or co-occurrence methods. Validation on English corpora shows LGDE's efficacy in keyword enrichment, which is further supported by its successful application in expanding a conspiracy-related dictionary in communication science using online data, as confirmed by domain experts and empirical results.
<br /><br />Summary: <div>
arXiv:2405.07764v4 Announce Type: replace-cross 
Abstract: We present Local Graph-based Dictionary Expansion (LGDE), a method for data-driven discovery of the semantic neighbourhood of words using tools from manifold learning and network science. At the heart of LGDE lies the creation of a word similarity graph from the geometry of word embeddings followed by local community detection based on graph diffusion. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings to capture word similarities based on paths of semantic association, over and above direct pairwise similarities. Exploiting such semantic neighbourhoods enables the expansion of dictionaries of pre-selected keywords, an important step for tasks in information retrieval, such as database queries and online data collection. We validate LGDE on two user-generated English-language corpora and show that LGDE enriches the list of keywords with improved performance relative to methods based on direct word similarities or co-occurrences. We further demonstrate our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on the expansion of a conspiracy-related dictionary from online data collected and analysed by domain experts. Our empirical results and expert user assessment indicate that LGDE expands the seed dictionary with more useful keywords due to the manifold-learning-based similarity network.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections</title>
<link>https://arxiv.org/abs/2508.19737</link>
<guid>https://arxiv.org/abs/2508.19737</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph partitioning, community detection, graph signal processing, spectral GNN, negative correction <br />
Summary: 
InfraredGP is a novel approach for graph partitioning that leverages the amplification of low-frequency information beyond the conventional range [0, 2] through a negative correction mechanism. It employs a spectral Graph Neural Network (GNN) along with low-pass filters to derive graph embeddings without the need for training. By utilizing random inputs and a feed-forward propagation (FFP) process, InfraredGP can generate distinguishable embeddings that enable high-quality community detection results when combined with the BIRCH algorithm. The method demonstrates impressive efficiency improvements, being 16x-23x faster than various baselines, while maintaining competitive quality in both static and streaming graph partitioning tasks. The code for InfraredGP is publicly available on GitHub for further exploration and experimentation. <br /><br />Summary: <div>
arXiv:2508.19737v1 Announce Type: cross 
Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem that divides nodes of a graph into densely-connected blocks. From a perspective of graph signal processing, we find that graph Laplacian with a negative correction can derive graph frequencies beyond the conventional range $[0, 2]$. To explore whether the low-frequency information beyond this range can encode more informative properties about community structures, we propose InfraredGP. It (\romannumeral1) adopts a spectral GNN as its backbone combined with low-pass filters and a negative correction mechanism, (\romannumeral2) only feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings via one feed-forward propagation (FFP) without any training, and (\romannumeral4) obtains feasible GP results by feeding the derived embeddings to BIRCH. Surprisingly, our experiments demonstrate that based solely on the negative correction mechanism that amplifies low-frequency information beyond $[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard clustering modules (e.g., BIRCH) and obtain high-quality results for GP without any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate InfraredGP for both static and streaming GP, where InfraredGP can achieve much better efficiency (e.g., 16x-23x faster) and competitive quality over various baselines. We have made our code public at https://github.com/KuroginQin/InfraredGP
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Complexity of the Roman Empire</title>
<link>https://arxiv.org/abs/2508.19892</link>
<guid>https://arxiv.org/abs/2508.19892</guid>
<content:encoded><![CDATA[
<div> archaeological evidence, Roman Empire, economic complexity, inscriptions, provinces 

Summary: 
The study explores the evolution of economic complexity over long periods by analyzing archaeological evidence from the Roman Empire. By examining inscriptions that list occupations and their locations, researchers estimate the economic complexity of different provinces. Surprisingly, the most complex areas during the first four centuries of the Roman Empire align with the most complex countries in the present day. The study highlights the continuity of economic capabilities across centuries, suggesting that the development of economic complexity is a challenging and enduring process. While the reasons for this preservation remain unclear, the findings offer valuable insights into the historical and contemporary trends of economic complexity. <div>
arXiv:2508.19892v1 Announce Type: cross 
Abstract: Economic complexity is a powerful tool to estimate the productive capabilities and future growth of modern economies. Little is known of how economic complexity evolves over long periods in history. In this paper, we use archaeological evidence from the Roman Empire in the form of short texts preserved on a durable material (i.e. inscriptions) to estimate the economic complexity of the various provinces of the empire. By connecting the occupations listed in the text of inscriptions with the location in which the inscribed objects were found we can estimate that the most complex areas during the first four centuries of the Roman Empire have a remarkable and statistically significant overlap with the most complex countries today. While we lack an explanation for the reason of the preservation of economic complexity through the ages, this evidence provides a suggestion about how difficult the development of economic capabilities might be.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs</title>
<link>https://arxiv.org/abs/2508.19907</link>
<guid>https://arxiv.org/abs/2508.19907</guid>
<content:encoded><![CDATA[
<div> GegenNet, spectral convolutional neural network, link sign prediction, signed bipartite graph, Gegenbauer polynomial basis<br />
<br />
Summary: <br />
The paper introduces GegenNet, a spectral convolutional neural network model designed for link sign prediction in signed bipartite graphs (SBGs). GegenNet utilizes fast spectral decomposition techniques for node feature initialization, a spectral graph filter based on Gegenbauer polynomial basis, and multi-layer spectral convolutional networks that alternate between Gegenbauer polynomial filters for positive and negative edges. The model demonstrates improved performance compared to 11 competitors across 6 benchmark SBG datasets, achieving gains of up to 4.28% in AUC and 11.69% in F1 score. GegenNet's enhanced model capacity and predictive accuracy make it a promising solution for effectively predicting the signs of potential links in SBGs. <br /> <div>
arXiv:2508.19907v1 Announce Type: cross 
Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V, the goal of link sign prediction is to predict the signs of potential links connecting U and V based on known positive and negative edges in G. The majority of existing solutions towards link sign prediction mainly focus on unipartite signed graphs, which are sub-optimal due to the neglect of node heterogeneity and unique bipartite characteristics of SBGs. To this end, recent studies adapt graph neural networks to SBGs by introducing message-passing schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node pairs. However, the fundamental spectral convolutional operators were originally designed for positive links in unsigned graphs, and thus, are not optimal for inferring missing positive or negative links from known ones in SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective spectral convolutional neural network model for link sign prediction in SBGs. In particular, GegenNet achieves enhanced model capacity and high predictive accuracy through three main technical contributions: (i) fast and theoretically grounded spectral decomposition techniques for node feature initialization; (ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and (iii) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges. Our extensive empirical studies reveal that GegenNet can achieve significantly superior performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign prediction compared to 11 strong competitors over 6 benchmark SBG datasets.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks</title>
<link>https://arxiv.org/abs/2402.18235</link>
<guid>https://arxiv.org/abs/2402.18235</guid>
<content:encoded><![CDATA[
<div> Keywords: Ego Network Model, social relationships, sentiments, culture, topics of discussion

Summary:
The study explores the sentiment analysis of social relationships based on the Ego Network Model (ENM), focusing on positive and negative sentiments across different cultures, communities, and topics. Contrary to previous beliefs, culture does not easily override the influence of topic discussions. However, specific and polarizing topics lead to increased negativity across all cultures. These negative sentiments are consistent across different levels of the ENM, contradicting prior hypotheses. Furthermore, the number of generic topics discussed between users serves as a predictor for the overall positivity of their relationships. The research sheds light on the complex interplay between culture, topics of discussion, and sentiments in shaping social relationships within the ENM framework. 

<br /><br />Summary: <div>
arXiv:2402.18235v2 Announce Type: replace 
Abstract: Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Generative Diffusion Models for Social Recommendations</title>
<link>https://arxiv.org/abs/2412.15579</link>
<guid>https://arxiv.org/abs/2412.15579</guid>
<content:encoded><![CDATA[
<div> Keywords: social recommendation, social homophily, generative model, collaborative signals, self-supervised learning

Summary:
The paper addresses the challenge of low social homophily in social recommendations by proposing the Score-based Generative Model for Social Recommendation (SGSR). This innovative approach generates optimal user social representations that maximize consistency with collaborative signals, effectively adapting Stochastic Differential Equation (SDE)-based diffusion models for recommendations. SGSR utilizes a joint curriculum training strategy to handle missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Experimental results on real-world datasets show the effectiveness of SGSR in filtering redundant social information and enhancing recommendation performance. <div>
arXiv:2412.15579v2 Announce Type: replace 
Abstract: With the prevalence of social networks on online platforms, social recommendation has become a vital technique for enhancing personalized recommendations. The effectiveness of social recommendations largely relies on the social homophily assumption, which presumes that individuals with social connections often share similar preferences. However, this foundational premise has been recently challenged due to the inherent complexity and noise present in real-world social networks. In this paper, we tackle the low social homophily challenge from an innovative generative perspective, directly generating optimal user social representations that maximize consistency with collaborative signals. Specifically, we propose the Score-based Generative Model for Social Recommendation (SGSR), which effectively adapts the Stochastic Differential Equation (SDE)-based diffusion models for social recommendations. To better fit the recommendation context, SGSR employs a joint curriculum training strategy to mitigate challenges related to missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach in filtering redundant social information and improving recommendation performance.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization</title>
<link>https://arxiv.org/abs/2501.04820</link>
<guid>https://arxiv.org/abs/2501.04820</guid>
<content:encoded><![CDATA[
<div> Keywords: extremist discourse, online community forums, psychosocial model, incel community, radicalization

Summary:
The paper introduces a novel method for extracting and analyzing extremist discourse across online community forums, focusing on verbal behavioral traits to quantify extremism. It identifies 11 distinct factors, termed "The Extremist Eleven," as a psychosocial model of extremism. Applying this method to various online communities, the research demonstrates the ability to characterize diverse ideologies. By analyzing user histories from the incel community, the framework accurately predicts user entry up to 10 months in advance. Users tend to maintain their level of extremism within extremist forums while remaining distinguishable from general online discourse. The study contributes to understanding extremism by presenting a holistic, cross-ideological approach that goes beyond traditional models. 

<br /><br />Summary: <div>
arXiv:2501.04820v2 Announce Type: replace 
Abstract: The proliferation of ideological movements into extremist factions via social media has become a global concern. While radicalization has been studied extensively within the context of specific ideologies, our ability to accurately characterize extremism in more generalizable terms remains underdeveloped. In this paper, we propose a novel method for extracting and analyzing extremist discourse across a range of online community forums. By focusing on verbal behavioral signatures of extremist traits, we develop a framework for quantifying extremism at both user and community levels. Our research identifies 11 distinct factors, which we term ``The Extremist Eleven,'' as a generalized psychosocial model of extremism. Applying our method to various online communities, we demonstrate an ability to characterize ideologically diverse communities across the 11 extremist traits. We demonstrate the power of this method by analyzing user histories from members of the incel community. We find that our framework accurately predicts which users join the incel community up to 10 months before their actual entry with an AUC of $>0.6$, steadily increasing to AUC ~0.9 three to four months before the event. Further, we find that upon entry into an extremist forum, the users tend to maintain their level of extremism within the community, while still remaining distinguishable from the general online discourse. Our findings contribute to the study of extremism by introducing a more holistic, cross-ideological approach that transcends traditional, trait-specific models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing Distance-Count Matrices is Difficult</title>
<link>https://arxiv.org/abs/2508.18857</link>
<guid>https://arxiv.org/abs/2508.18857</guid>
<content:encoded><![CDATA[
<div> graph, centrality measures, distance-count matrix, NP-complete, counterexample

Summary:
This article introduces the topic of axiomatization of centrality measures and specifically focuses on the construction of counterexamples involving distance-count matrices in the context of geometric centralities. The study proves that determining whether a given matrix is the distance-count matrix of a graph is strongly NP-complete, indicating the complexity of this problem. This result highlights the limitations of brute-force methods for constructing counterexamples and emphasizes the need for more sophisticated approaches. The article underscores the challenges in proving properties of centrality indices and the importance of developing efficient strategies for tackling such problems in graph theory. <div>
arXiv:2508.18857v1 Announce Type: new 
Abstract: Axiomatization of centrality measures often involves proving that something cannot hold by providing a counterexample (i.e., a graph for which that specific centrality index fails to have a given property). In the context of geometric centralities, building such counterexamples requires constructing a graph with specific distance counts between nodes, as expressed by its distance-count matrix. We prove that deciding whether a matrix is the distance-count matrix of a graph is strongly NP-complete. This negative result implies that a brute-force approach to building this kind of counterexample is out of question, and cleverer approaches are required.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Skills Formation in Gendered Peer Networks: Exploring advice giving and taking in classrooms</title>
<link>https://arxiv.org/abs/2508.19102</link>
<guid>https://arxiv.org/abs/2508.19102</guid>
<content:encoded><![CDATA[
<div> Keywords: digital skills, peer relationships, advice networks, gender, education

Summary:
The study focuses on the role of peer relationships in the development of digital skills among children. By analyzing data from students in classrooms across three countries, the researchers found that digital skills spread through peer interactions, with higher-skilled students being sought for advice more frequently. Gender differences were also observed, with girls both seeking and giving more advice, and gender homophily playing a significant role in these interactions. The study highlights the importance of leveraging peer learning in formal education to enhance digital skills development and address existing divides. These findings emphasize the need for tailored digital skills education that considers peer dynamics and gender influences. <br /><br />Summary: <div>
arXiv:2508.19102v1 Announce Type: new 
Abstract: The digitalisation of childhood underscores the importance of early digital skill development. To understand how peer relationships shape this process, we draw on unique sociocentric network data from students in classrooms across three countries, focusing on peer-to-peer advice-giving and advice-seeking networks related to digital skills. Using exponential random graph models, we find that digital skills systematically spread through peer interactions: higher-skilled students are more likely to be sought for advice while less likely to seek it themselves. Students perceived as highly skilled are more likely to seek and offer advice, but it has limited influence on being sought out by others. Gender plays a significant role: girls both seek and give more advice, with strong gender homophily shaping these interactions. We suggest that digital skills education should leverage the potential of peer learning within formal education and consider how such approaches can address persistent divides.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urn Modeling of Random Graphs Across Granularity Scales: A Framework for Origin-Destination Human Mobility Networks</title>
<link>https://arxiv.org/abs/2508.18544</link>
<guid>https://arxiv.org/abs/2508.18544</guid>
<content:encoded><![CDATA[
<div> modeling human mobility, combinatorial allocation process, origin-destination networks, three-scale framework, mixed-Poisson law

Summary:
The article presents a novel approach to modeling human mobility by treating trips as distinguishable balls assigned to location-bins. This approach generates origin-destination networks and establishes a unified three-scale framework comprising enumerative, probabilistic, and continuum graphon ensembles. A renormalization theorem is proven, showing convergence to a universal mixed-Poisson law in the large sparse regime. The framework allows for the calculation of key mobility observables such as destination occupancy, coverage, and overflow beyond finite capacities. Simulations using gravity-like kernels, calibrated on empirical OD data, validate the asymptotic predictions. By combining exact combinatorial models with continuum analysis, the results provide a systematic toolkit for generating synthetic networks, evaluating congestion, and developing sustainable urban mobility policies. <div>
arXiv:2508.18544v1 Announce Type: cross 
Abstract: We model human mobility as a combinatorial allocation process, treating trips as distinguishable balls assigned to location-bins and generating origin-destination (OD) networks. From this analogy, we construct a unified three-scale framework, enumerative, probabilistic, and continuum graphon ensembles, and prove a renormalization theorem showing that, in the large sparse regime, these representations converge to a universal mixed-Poisson law. The framework yields compact formulas for key mobility observables, including destination occupancy, vacancy of unvisited sites, coverage (a stopping-time extension of the coupon collector problem), and overflow beyond finite capacities. Simulations with gravity-like kernels, calibrated on empirical OD data, closely match the asymptotic predictions. By connecting exact combinatorial models with continuum analysis, the results offer a principled toolkit for synthetic network generation, congestion assessment, and the design of sustainable urban mobility policies.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection</title>
<link>https://arxiv.org/abs/2508.18819</link>
<guid>https://arxiv.org/abs/2508.18819</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, self-supervised framework, Abstract Meaning Representation (AMR), Large Language Model (LLM), news propagation dynamics

Summary: 
This study introduces a new self-supervised framework for detecting misinformation that incorporates complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. The framework includes an LLM-based graph contrastive loss (LGCL) that improves feature separability with negative anchor points from a Large Language Model (LLM). Additionally, a multi-view graph masked autoencoder is used to learn news propagation features from social context graphs. By combining semantic and propagation-based features, the framework effectively distinguishes fake news from real news in a self-supervised manner. Experimental results show that this approach outperforms existing methodologies, even with limited labelled datasets, and enhances generalizability.<br /><br />Summary: <div>
arXiv:2508.18819v1 Announce Type: cross 
Abstract: The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective Polarization across European Parliaments</title>
<link>https://arxiv.org/abs/2508.18916</link>
<guid>https://arxiv.org/abs/2508.18916</guid>
<content:encoded><![CDATA[
<div> Keywords: affective polarization, European parliaments, natural language processing, sentiment analysis, reciprocity

Summary: 
The study explores affective polarization in six European parliaments using automated methods. It analyzes sentiment in parliamentary speeches to identify negativity towards opposing groups compared to one's own. The results show consistent affective polarization across all parliaments, indicating increased negativity and hostility towards opposing groups. There is no significant difference in affective polarization between less active and more active parliamentarians. The study reveals that reciprocity plays a role in affecting polarization among parliamentarians in the European context. This research sheds light on the presence and mechanisms of affective polarization in political discourse, providing insights into the dynamics of intergroup attitudes within European parliaments. <br /><br />Summary: <div>
arXiv:2508.18916v1 Announce Type: cross 
Abstract: Affective polarization, characterized by increased negativity and hostility towards opposing groups, has become a prominent feature of political discourse worldwide. Our study examines the presence of this type of polarization in a selection of European parliaments in a fully automated manner. Utilizing a comprehensive corpus of parliamentary speeches from the parliaments of six European countries, we employ natural language processing techniques to estimate parliamentarian sentiment. By comparing the levels of negativity conveyed in references to individuals from opposing groups versus one's own, we discover patterns of affectively polarized interactions. The findings demonstrate the existence of consistent affective polarization across all six European parliaments. Although activity correlates with negativity, there is no observed difference in affective polarization between less active and more active members of parliament. Finally, we show that reciprocity is a contributing mechanism in affective polarization between parliamentarians across all six parliaments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing graphs and their connectivity using graphlets</title>
<link>https://arxiv.org/abs/2508.19189</link>
<guid>https://arxiv.org/abs/2508.19189</guid>
<content:encoded><![CDATA[
<div> graphlets, subgraphs, topological description, vertex-deleted subgraphs, trees <br />
Summary:
graphlets are small subgraphs rooted at a fixed vertex, providing a topological description of the surrounding vertex. The article investigates properties and uniqueness of graphlet degree sequences and their use in analyzing asymmetric vertex-deleted subgraphs in graphs. It demonstrates the reconstruction of trees from their (<= n-1) - graphlet degree sequences, offering a simpler method compared to standard vertex-deleted subgraph reconstruction. This approach utilizes information from graphlets up to size (n-1) to capture the structural characteristics of the analyzed graphs. <div>
arXiv:2508.19189v1 Announce Type: cross 
Abstract: Graphlets are small subgraphs rooted at a fixed vertex. The number of occurrences of graphlets aligned to a particular vertex, called graphlet degree sequence, gives a topological description of the surrounding of the analyzed vertex. In this article, we study properties and uniqueness of graphlet degree sequences. The information given by graphlets up to size (n-1) is utilized graphs having certain type of asymmetric vertex-deleted subgraphs. Moreover, we show a reconstruction of trees from their (<= n-1)-graphlet degree sequences, which is much easier compared to the standard reconstruction from vertex-deleted subgraphs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gender Gap in Science Communication on TikTok and YouTube: How Platform Dynamics Shape the Visibility of Female Science Communicators</title>
<link>https://arxiv.org/abs/2508.16865</link>
<guid>https://arxiv.org/abs/2508.16865</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, gender inequality, science communication, YouTube, TikTok

Summary:
On social media platforms like YouTube and TikTok, gender inequalities persist in science communication. A study compared the reach and audience response of the top science accounts on these platforms, finding that men received more likes and views on YouTube, while audience response on TikTok was more balanced. Women's participation on both platforms had varying impacts, with women's engagement on YouTube negatively affecting interaction levels, while on TikTok, their impact was slightly positive. Overall, TikTok was found to be a more inclusive space for scientific communication compared to YouTube. However, structural challenges still exist on both platforms, highlighting the need for further research on strategies to promote gender equity in online science communication.<br /><br />Summary: Social media platforms like YouTube and TikTok play a significant role in science communication, but gender inequalities persist. Men tend to receive more likes and views on YouTube, while audience response on TikTok is more balanced. Women's participation on both platforms has varying impacts, with women on YouTube negatively affecting interaction levels, and on TikTok having a slightly positive impact. Despite being a more inclusive space, TikTok still faces challenges in promoting gender equity in online science communication, indicating a need for further research in this area. <div>
arXiv:2508.16865v1 Announce Type: new 
Abstract: Social media platforms facilitate the dissemination of science and access to it. However, gender inequalities in the participation and visibility of communicators persist. This study examined the differences in reach and audience response between YouTube and TikTok from a gender perspective. To do so, the ten most influential science accounts on YouTube and TikTok were selected, with the sample divided equally between men and women, to conduct a comparative study. A total of 4293 videos on TikTok and 4825 on YouTube were analyzed, along with 277,528 comments, considering metrics of views and interaction. The results show that on YouTube, men received more likes and views, while on TikTok, audience response was more balanced. The participation of women on both platforms also had a differential impact, as the number of women engaging with content on YouTube negatively correlated with interaction levels, whereas on TikTok, their impact was slightly positive. In conclusion, TikTok emerges as a more inclusive space for scientific communication, though structural challenges remain on both platforms, encouraging further research into strategies that promote gender equity in online science communication.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Subgraph Clustering and a New Cluster Ensemble Method</title>
<link>https://arxiv.org/abs/2508.17013</link>
<guid>https://arxiv.org/abs/2508.17013</guid>
<content:encoded><![CDATA[
<div> community detection algorithm, DSC-Flow-Iter, dense subgraphs, cluster ensemble technique, modularity-based clustering

Summary:
The article introduces a new community detection algorithm called DSC-Flow-Iter, which iteratively extracts dense subgraphs. This algorithm, while leaving some nodes unclustered, shows competitiveness with leading methods and exhibits high precision and low recall. It complements modularity-based methods that usually have high recall but lower precision. The authors propose a cluster ensemble technique that combines DSC-Flow-Iter with modularity-based clustering to enhance accuracy. Through experiments on synthetic networks, the proposed pipeline, utilizing the ensemble technique, outperforms both individual components and baseline techniques. The study demonstrates the effectiveness of the approach in improving community detection accuracy. <div>
arXiv:2508.17013v1 Announce Type: new 
Abstract: We propose DSC-Flow-Iter, a new community detection algorithm that is based on iterative extraction of dense subgraphs. Although DSC-Flow-Iter leaves many nodes unclustered, it is competitive with leading methods and has high-precision and low-recall, making it complementary to modularity-based methods that typically have high recall but lower precision. Based on this observation, we introduce a novel cluster ensemble technique that combines DSC-Flow-Iter with modularity-based clustering, to provide improved accuracy. We show that our proposed pipeline, which uses this ensemble technique, outperforms its individual components and improves upon the baseline techniques on a large collection of synthetic networks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks</title>
<link>https://arxiv.org/abs/2508.17236</link>
<guid>https://arxiv.org/abs/2508.17236</guid>
<content:encoded><![CDATA[
<div> Keywords: high-order relationships, real-world networks, dynamics, LINCOLN, hyperedge prediction

Summary:<br /><br />
Real-world networks exhibit high-order relationships among objects and evolve over time. Two key characteristics of high-order dynamics are identified: short-term structural and temporal influence, and long-term periodic re-appearance. To address these dynamics, LINCOLN, a method for Learning hIgh-order dyNamiCs Of reaL-world Networks, is proposed. LINCOLN incorporates bi-interactional hyperedge encoding for short-term patterns, periodic time injection, and intermediate node representation for long-term patterns. Experimental results demonstrate that LINCOLN outperforms nine existing methods in predicting dynamic hyperedges. <div>
arXiv:2508.17236v1 Announce Type: new 
Abstract: Real-world networks have high-order relationships among objects and they evolve over time. To capture such dynamics, many works have been studied in a range of fields. Via an in-depth preliminary analysis, we observe two important characteristics of high-order dynamics in real-world networks: high-order relations tend to (O1) have a structural and temporal influence on other relations in a short term and (O2) periodically re-appear in a long term. In this paper, we propose LINCOLN, a method for Learning hIgh-order dyNamiCs Of reaL-world Networks, that employs (1) bi-interactional hyperedge encoding for short-term patterns, (2) periodic time injection and (3) intermediate node representation for long-term patterns. Via extensive experiments, we show that LINCOLN outperforms nine state-of-the-art methods in the dynamic hyperedge prediction task.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Straddling Two Platforms: From Twitter to Mastodon, an Analysis of the Evolution of an Unfinished Social Media Migration</title>
<link>https://arxiv.org/abs/2508.17563</link>
<guid>https://arxiv.org/abs/2508.17563</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, content moderation, personal data management, commercial exploitation, decentralised platform <br />
Summary: 
The study examines the migration of users from Twitter to Mastodon following Elon Musk's acquisition of Twitter in 2022. It analyzes the onboarding process of 19,000 users who switched to Mastodon, mainly consisting of academics, scientists, and journalists. The migration was a response to concerns about Twitter's direction under Musk's ownership. However, users chose to maintain a presence on both platforms due to the difficulty in replicating Twitter's communities on Mastodon's decentralised platform. This led to a partial loss of social capital and greater fragmentation of user communities. The study underscores the challenges of transitioning between centralized and decentralized social media platforms, highlighting the unique characteristics and limitations of each platform. <br /><br /> <div>
arXiv:2508.17563v1 Announce Type: new 
Abstract: Social media have been fundamental in the daily lives of millions of people, but they have raised concerns about content moderation policies, the management of personal data, and their commercial exploitation. The acquisition of Twitter (now X) by Elon Musk in 2022 generated concerns among Twitter users regarding changes in the platform's direction, prompting a migration campaign by some user groups to the federated network Mastodon. This study reviews the onboarding of users to this decentralised platform between 2016 and 2022 and analyses the migration of 19,000 users who identified themselves as supporters of the platform switch. The results show that the migration campaign was a reactive response to Elon Musk's acquisition of Twitter and was led by a group of highly active academics, scientists, and journalists. However, a complete transition was not realised, as users preferred to straddle their presence on both platforms. Mastodon's decentralisation made it difficult to exactly replicate Twitter's communities, resulting in a partial loss of these users' social capital and greater fragmentation of these user communities, which highlights the intrinsic differences between both platforms.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connected Theorems: A Graph-Based Approach to Evaluating Mathematical Results</title>
<link>https://arxiv.org/abs/2508.17596</link>
<guid>https://arxiv.org/abs/2508.17596</guid>
<content:encoded><![CDATA[
<div> Keyword: mathematical results, evaluation, data-driven approach, citation relationships, influence scores
Summary: 
The article proposes a data-driven approach for evaluating mathematical results, aiming to complement traditional human judgment methods. By constructing a hierarchical graph linking theorems, papers, and fields and using a PageRank-style algorithm to compute influence scores, the framework analyzes the evolution of field rankings over time and quantifies the impact between fields. The goal is to develop more advanced, quantitative methods for evaluating mathematical research and provide a complementary tool for expert assessment. The approach provides a systematic way to assess researchers' contributions and shape the direction of the field, leveraging citation relationships to determine influence scores and track the evolution of research impact. This data-driven method offers a new perspective on evaluating mathematical research and has the potential to enhance the assessment process in the field. 
<br /><br />Summary: <div>
arXiv:2508.17596v1 Announce Type: new 
Abstract: The evaluation of mathematical results plays a central role in assessing researchers' contributions and shaping the direction of the field. Currently, such evaluations rely primarily on human judgment, whether through journal peer review or committees at research institutions. To complement these traditional processes, we propose a data-driven approach. We construct a hierarchical graph linking theorems, papers, and fields to capture their citation relationships. We then introduce a PageRank-style algorithm to compute influence scores for these entities. Using these scores, we analyze the evolution of field rankings over time and quantify the impact between fields. We hope this framework can contribute to the development of more advanced, quantitative methods for evaluating mathematical research and serve as a complement to expert assessment.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-Based Social Bot via an Adversarial Learning Framework</title>
<link>https://arxiv.org/abs/2508.17711</link>
<guid>https://arxiv.org/abs/2508.17711</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model (LLM), social media, generative capabilities, adversarial learning framework, human-like behavior

Summary:<br />
Developing Large Language Model agents that exhibit human-like behavior is a challenging research task. EvoBot, an Evolving LLM-based social Bot, enhances generative capabilities through an adversarial learning framework. It refines its content generation through Supervised Fine-Tuning and Direct Preference Optimization, guided by feedback from a co-adapting Detector. EvoBot generates human-like content aligned with diverse user profiles and demonstrates strong social responsiveness in modeling opinion dynamics and information spread in simulations. The framework produces a robust Detector with broader utility for agent development and detection tasks. <div>
arXiv:2508.17711v1 Announce Type: new 
Abstract: Developing Large Language Model (LLM) agents that exhibit human-like behavior, encompassing not only individual heterogeneity rooted in unique user profiles but also adaptive response to socially connected neighbors, is a significant research challenge. Social media platforms, with their diverse user data and explicit social structures, provide an ideal testbed for such investigations. This paper introduces EvoBot, an \textbf{Evo}lving LLM-based social \textbf{Bot} that significantly enhances human-like generative capabilities through a novel adversarial learning framework. EvoBot is initialized by Supervised Fine-Tuning (SFT) on representative data from social media and then iteratively refines its generation of sophisticated, human-like content via Direct Preference Optimization (DPO). This refinement is guided by feedback from a co-adapting \textbf{Detector} which concurrently improves its ability to distinguish EvoBot from humans, thereby creating an increasingly challenging learning environment for EvoBot. Experiments demonstrate that EvoBot generates content aligned with diverse user profiles, increasingly bypassing the co-adapting Detector through human-like expression. Moreover, it exhibits strong social responsiveness, more accurately modeling real-world opinion dynamics and information spread in multi-agent simulations. The framework also yields a more robust Detector, underscoring its broader utility for both advanced agent development and related detection tasks. The code is available at https://github.com/kfq20/EvoBot.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games</title>
<link>https://arxiv.org/abs/2508.16596</link>
<guid>https://arxiv.org/abs/2508.16596</guid>
<content:encoded><![CDATA[
<div> Keywords: video games, player enjoyment, generative AI, machine learning, game reviews

Summary:
This study utilizes generative AI and machine learning techniques, specifically Microsoft Phi-4 LLM and XGBoost, to analyze and quantify game reviews from the Steam and Meta Quest stores. By converting qualitative feedback into structured data, the study evaluates various game design elements, monetization models, and platform-specific trends. The analysis reveals distinct patterns in player preferences between PC and VR games, identifying factors that contribute to increased player satisfaction. Leveraging Google Cloud for data storage and processing, the study establishes a scalable framework for large-scale game review analysis. The insights obtained from this analysis offer actionable guidance for game developers to optimize game mechanics, pricing strategies, and player engagement. <div>
arXiv:2508.16596v1 Announce Type: cross 
Abstract: As video games continue to evolve, understanding what drives player enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost, to quantify and analyze game reviews from Steam and Meta Quest stores. The approach converts qualitative feedback into structured data, enabling comprehensive evaluation of key game design elements, monetization models, and platform-specific trends. The findings reveal distinct patterns in player preferences across PC and VR games, highlighting factors that contribute to higher player satisfaction. By integrating Google Cloud for largescale data storage and processing, this study establishes a scalable framework for game review analysis. The study's insights offer actionable guidance for game developers, helping optimize game mechanics, pricing strategies, and player engagement.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Multimodal LLMs See Sentiment?</title>
<link>https://arxiv.org/abs/2508.16873</link>
<guid>https://arxiv.org/abs/2508.16873</guid>
<content:encoded><![CDATA[
<div> Keywords: visual content, sentiment analysis, Multimodal Large Language Models, fine-tuning, affective computing 

Summary: 
The paper introduces MLLMsent, a framework that explores how Multimodal Large Language Models (MLLMs) can reason about sentiment in visual content. The framework includes three perspectives: direct sentiment classification from images using MLLMs, sentiment analysis on automatically generated image descriptions, and fine-tuning Language Models on sentiment-labeled image descriptions. Experiments on a benchmark dataset show that the fine-tuned approach achieves state-of-the-art results, outperforming baseline methods across different sentiment polarity categories and levels of evaluators' agreement. Notably, in a cross-dataset test, the model performs well even without training on the new data, surpassing the best runner-up method. These results demonstrate the potential of the proposed visual reasoning scheme for advancing affective computing and set new benchmarks for future research. 

Summary: <div>
arXiv:2508.16873v1 Announce Type: cross 
Abstract: Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-of-the-art results outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Pricing Through Strategic User Profiling in Social Networks</title>
<link>https://arxiv.org/abs/2508.17111</link>
<guid>https://arxiv.org/abs/2508.17111</guid>
<content:encoded><![CDATA[
<div> privacy-enhancing technologies, user profiling, personalized pricing, online social networks, Bayesian game

Summary:
This paper investigates how users can strategically manage their social activities to avoid personalized pricing based on their profiles derived from online social network interactions. A dynamic Bayesian game model is formulated to capture the interactions between the seller and users under asymmetric information. The equilibrium analysis reveals that with improving profiling technology, sellers tend to increase uniform prices to incentivize user engagement on social networks for better profiling. However, the implementation of informed consent policies to inform users of data access and profiling practices may result in most users being worse off. This suggests that regulatory efforts aimed at enhancing user privacy awareness could unintentionally lead to reduced payoffs for users. <div>
arXiv:2508.17111v1 Announce Type: cross 
Abstract: Traditional user profiling techniques rely on browsing history or purchase records to identify users' willingness to pay. This enables sellers to offer personalized prices to profiled users while charging only a uniform price to non-profiled users. However, the emergence of privacy-enhancing technologies has caused users to actively avoid on-site data tracking. Today, major online sellers have turned to public platforms such as online social networks to better track users' profiles from their product-related discussions. This paper presents the first analytical study on how users should best manage their social activities against potential personalized pricing, and how a seller should strategically adjust her pricing scheme to facilitate user profiling in social networks. We formulate a dynamic Bayesian game played between the seller and users under asymmetric information. The key challenge of analyzing this game comes from the double couplings between the seller and the users as well as among the users. Furthermore, the equilibrium analysis needs to ensure consistency between users' revealed information and the seller's belief under random user profiling. We address these challenges by alternately applying backward and forward induction, and successfully characterize the unique perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as the accuracy of profiling technology improves, the seller tends to raise the equilibrium uniform price to motivate users' increased social activities and facilitate user profiling. However, this results in most users being worse off after the informed consent policy is imposed to ensure users' awareness of data access and profiling practices by potential sellers. This finding suggests that recent regulatory evolution towards enhancing users' privacy awareness may have unintended consequences of reducing users' payoffs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Clustering for Large Multi-Relational Graphs</title>
<link>https://arxiv.org/abs/2508.17388</link>
<guid>https://arxiv.org/abs/2508.17388</guid>
<content:encoded><![CDATA[
<div> feature vectors, optimization, clustering, multi-relational graphs, scalability

Summary: 
DEMM and DEMM+ are novel algorithms proposed in this paper to address the challenges of partitioning node sets in multi-relational graphs (MRGs) into clusters. These algorithms focus on optimizing node feature vectors and minimizing the Dirichlet energy of clustering results over the node affinity graph. DEMM+ stands out for its scalability and efficiency, achieved through innovative optimizations. Key contributions include an approximation solver for constructing node feature vectors and a problem transformation technique for linear-time clustering without the dense affinity matrix. DEMM+ is extended to handle attribute-less MRGs as well. Extensive experiments demonstrate that DEMM+ outperforms 20 baseline methods in clustering quality while also being faster in many cases. <div>
arXiv:2508.17388v1 Announce Type: cross 
Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling diverse interactions/relations among real objects (i.e., nodes), which pervade extensive applications and scenarios. Given an MRG G with N nodes, partitioning the node set therein into K disjoint clusters (MRGC) is a fundamental task in analyzing MRGs, which has garnered considerable attention. However, the majority of existing solutions towards MRGC either yield severely compromised result quality by ineffective fusion of heterogeneous graph structures and attributes, or struggle to cope with sizable MRGs with millions of nodes and billions of edges due to the adoption of sophisticated and costly deep learning models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to address the limitations above. Specifically, our algorithms are built on novel two-stage optimization objectives, where the former seeks to derive high-caliber node feature vectors by optimizing the multi-relational Dirichlet energy specialized for MRGs, while the latter minimizes the Dirichlet energy of clustering results over the node affinity graph. In particular, DEMM+ achieves significantly higher scalability and efficiency over our based method DEMM through a suite of well-thought-out optimizations. Key technical contributions include (i) a highly efficient approximation solver for constructing node feature vectors, and (ii) a theoretically-grounded problem transformation with carefully-crafted techniques that enable linear-time clustering without explicitly materializing the NxN dense affinity matrix. Further, we extend DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit that DEMM+ is consistently superior in terms of clustering quality measured against ground-truth labels, while often being remarkably faster.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Price of Uncertainty for Consensus Games</title>
<link>https://arxiv.org/abs/2508.17557</link>
<guid>https://arxiv.org/abs/2508.17557</guid>
<content:encoded><![CDATA[
<div> game-theoretic models, uncertainty, social cost, consensus games, adversarial perturbations

Summary:
This paper examines the impact of uncertainty in observed data on game-theoretic models. It introduces adversarial perturbations to players' observed costs and measures the resulting increase in social cost as the price of uncertainty. The study focuses on consensus games and establishes a tight bound on the price of uncertainty, showing that it is proportional to $\varepsilon^2 n^2$ for all $\varepsilon = \Omega\mathopen{}\left(n^{-1/4}\right)$. This result represents an improvement over previous bounds, with a lower bound of $\Omega(\varepsilon^3 n^2)$ and an upper bound of $O(\varepsilon n^2). By considering uncertainty in real-world settings, this research sheds light on the importance of accurate information in game theory applications. <div>
arXiv:2508.17557v1 Announce Type: cross 
Abstract: Many game-theoretic models assume that players have access to accurate information, but uncertainty in observed data is frequently present in real-world settings. In this paper, we consider a model of uncertainty where adversarial perturbations of relative magnitude $1+\varepsilon$ are introduced to players' observed costs. The effect of uncertainty on social cost is denoted as the price of uncertainty. We prove a tight bound on the price of uncertainty for consensus games of $\Theta(\varepsilon^2 n^2)$ for all $\varepsilon = \Omega\mathopen{}\left(n^{-1/4}\right)$. This improves a previous lower bound of $\Omega(\varepsilon^3 n^2)$ as well as a previous upper bound of $O(\varepsilon n^2)$.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does node centrality in a financial network affect asset price prediction?</title>
<link>https://arxiv.org/abs/2305.03245</link>
<guid>https://arxiv.org/abs/2305.03245</guid>
<content:encoded><![CDATA[
<div> Keywords: financial networks, node centrality, price forecasting, random forest algorithm, deep learning <br />
Summary: <br />
1. Systemically important nodes in complex financial networks play crucial roles in asset price forecasting. <br />
2. Contrary to intuition, factors with low centrality demonstrate better forecasting ability in global asset networks. <br />
3. Nodes with low centrality can be predicted with greater accuracy in terms of price direction. <br />
4. Information theory provides a framework for explaining the unexpected observations regarding node centrality and price forecasting. <br />
5. Factor selection for asset price prediction in complex systems should prioritize factors with low centrality over those with high centrality. <br />
6. The study's findings are validated using a hybrid random forest algorithm and an alternative deep learning method. <br /> <div>
arXiv:2305.03245v2 Announce Type: replace-cross 
Abstract: In complex financial networks, systemically important nodes usually play crucial roles. Asset price forecasting is important for describing the evolution of a financial network. Naturally, the question arises as to whether node centrality impacts the effectiveness of price forecasting. To explore this, we examine networks composed of major global assets and investigate how node centrality affects price forecasting using a hybrid random forest algorithm. Our findings reveal two counterintuitive phenomena: (i) factors with low centrality usually have better forecasting ability, and (ii) nodes with low centrality can be predicted more accurately in direction. These unexpected observations can be explained from the perspective of information theory. Moreover, our research suggests a criterion for factor selection: when predicting an asset price in a complex system, factors with low centrality should be selected rather than only factors with high centrality. Finally, we verify the robustness of our results using an alternative deep learning method.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reinforcing cascades: A spreading model for beliefs or products of varying intensity or quality</title>
<link>https://arxiv.org/abs/2411.00714</link>
<guid>https://arxiv.org/abs/2411.00714</guid>
<content:encoded><![CDATA[
<div> social contagions, spread, self-reinforcement, cascade dynamics, power-law distributions <br />
Summary:<br />
The study explores the impact of self-reinforcement mechanisms in cascade dynamics, focusing on the spread of ideas, beliefs, and innovations in social networks. Unlike traditional models that assume fixed transmission mechanisms, the research considers the recursive nature of the process, where ideas can be reinforced and beliefs strengthened as they spread. The findings reveal a critical regime characterized by power-law cascade size distributions with non-universal scaling exponents. This regime challenges classic models by demonstrating critical-like behavior across a wide range of parameters, rather than requiring precise fine-tuning at a specific critical point. The results suggest that self-reinforced cascades may explain the prevalence of power-law distributions observed in empirical social data, shedding light on the dynamics of social contagions. <br /> 
Summary: <div>
arXiv:2411.00714v2 Announce Type: replace-cross 
Abstract: Models of how things spread often assume that transmission mechanisms are fixed over time. However, social contagions--the spread of ideas, beliefs, innovations--can lose or gain in momentum as they spread: ideas can get reinforced, beliefs strengthened, products refined. We study the impacts of such self-reinforcement mechanisms in cascade dynamics. We use different mathematical modeling techniques to capture the recursive, yet changing nature of the process. We find a critical regime with a range of power-law cascade size distributions with non-universal scaling exponents. This regime clashes with classic models, where criticality requires fine tuning at a precise critical point. Self-reinforced cascades produce critical-like behavior over a wide range of parameters, which may help explain the ubiquity of power-law distributions in empirical social data.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Word-of-Mouth and Private-Prior Sequential Social Learning</title>
<link>https://arxiv.org/abs/2504.02913</link>
<guid>https://arxiv.org/abs/2504.02913</guid>
<content:encoded><![CDATA[
<div> Word-of-Mouth, social learning, rational agents, dynamical system, noise<br />
Summary:<br />
The paper explores the Word-of-Mouth (WoM) social learning paradigm, where agents estimate a dynamical system's state based on each other's actions. The first agent receives noisy measurements, and subsequent agents use a degraded version of the previous estimate. The final agent's belief is publicly broadcast for all agents to adopt. Through theoretical analysis and simulations, it is found that some agents benefit from the final agent's belief, while others experience performance deterioration. The study highlights the complexity of social learning dynamics and the varying effects on different agents within the WoM framework. <div>
arXiv:2504.02913v3 Announce Type: replace-cross 
Abstract: Social learning constitutes a fundamental framework for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and subsequently adopted by all agents, in place of their own. We analyze this setting theoretically and through numerical simulations, noting that some agents benefit from using the belief of the last agent, while others experience performance deterioration.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dac-Fake: A Divide and Conquer Framework for Detecting Fake News on Social Media</title>
<link>https://arxiv.org/abs/2508.16223</link>
<guid>https://arxiv.org/abs/2508.16223</guid>
<content:encoded><![CDATA[
<div> detecting fake news, social media, automated detection, linguistic features, accuracy rate
<br />
<br />
Summary: 
The article introduces DaCFake, a novel fake news detection model that utilizes a divide and conquer strategy combining content and context-based features for rapid and automated detection of fake news on social media platforms. The model extracts over eighty linguistic features from news articles and integrates them with either a continuous bag of words or a skipgram model to enhance detection accuracy. Impressive accuracy rates of 97.88%, 96.05%, and 97.32% were achieved on three datasets including Kaggle, McIntire + PolitiFact, and Reuter. Ten-fold cross-validation was employed to further enhance the model's robustness and accuracy. The results demonstrate the effectiveness of DaCFake in early detection of fake news, offering a promising solution to curb misinformation on social media platforms. 
<br /> <div>
arXiv:2508.16223v1 Announce Type: new 
Abstract: With the rapid evolution of technology and the Internet, the proliferation of fake news on social media has become a critical issue, leading to widespread misinformation that can cause societal harm. Traditional fact checking methods are often too slow to prevent the dissemination of false information. Therefore, the need for rapid, automated detection of fake news is paramount. We introduce DaCFake, a novel fake news detection model using a divide and conquer strategy that combines content and context based features. Our approach extracts over eighty linguistic features from news articles and integrates them with either a continuous bag of words or a skipgram model for enhanced detection accuracy. We evaluated the performance of DaCFake on three datasets including Kaggle, McIntire + PolitiFact, and Reuter achieving impressive accuracy rates of 97.88%, 96.05%, and 97.32%, respectively. Additionally, we employed a ten-fold cross validation to further enhance the model's robustness and accuracy. These results highlight the effectiveness of DaCFake in early detection of fake news, offering a promising solution to curb misinformation on social media platforms.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media</title>
<link>https://arxiv.org/abs/2508.16453</link>
<guid>https://arxiv.org/abs/2508.16453</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, anti-establishment sentiment, TikTok, conspiracy theories, engagement

Summary: 
The study investigates the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok, focusing on content related to finance, wellness, and conspiracy theories. Content creators on TikTok often position themselves as experts and may promote anti-establishment views to gain followers. The research reveals that AES is most common in conspiracy theory content, but less so in finance and wellness topics. However, engagement with anti-establishment content varies across different areas. Despite the relatively low prevalence of AES in finance and wellness content, there are indications of platform incentives encouraging users to post such content. The findings suggest a potential connection between social media environments, anti-establishment views, and distrust of institutions, highlighting the importance of understanding how social media influences people's attitudes towards public institutions. 

<br /><br />Summary: <div>
arXiv:2508.16453v1 Announce Type: new 
Abstract: Distrust of public serving institutions and anti-establishment views are on the rise (especially in the U.S.). As people turn to social media for information, it is imperative to understand whether and how social media environments may be contributing to distrust of institutions. In social media, content creators, influencers, and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics, and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility. However, the extent to which this content appears and whether such content increases engagement is unclear. This study analyzes the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok. Despite its popularity as a source of information, TikTok remains relatively understudied and may provide important insights into how people form attitudes towards institutions. We employ a computational approach to label TikTok posts as containing AES or not across topical domains where content creators tend to frame themselves as experts: finance and wellness. As a comparison, we also consider the topic of conspiracy theories, where AES is expected to be common. We find that AES is most prevalent in conspiracy theory content, and relatively rare in content related to the other two topics. However, we find that engagement patterns with such content varies by area, and that there may be platform incentives for users to post content that expresses anti-establishment sentiment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embarrassed to observe: The effects of directive language in brand conversation</title>
<link>https://arxiv.org/abs/2508.15826</link>
<guid>https://arxiv.org/abs/2508.15826</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, directive language, brand-consumer interactions, engagement, brand relationship <br />
Summary: This study explores the effects of directive brand language on consumers in social media conversations. The use of directive language by brands can lead to reduced engagement among consumers who observe such interactions. This is because consumers may perceive brands using directive language as face-threatening, leading to feelings of vicarious embarrassment and decreased engagement. The impact of directive language is stronger in nonproduct-centered conversations compared to product-centered ones, as consumers expect more freedom in mundane conversations. However, a strong brand relationship can mitigate the negative effects of directive language. This study underscores the significance of context in interactive communication, particularly in the realm of social media and brand management. <br /><br />Summary: <div>
arXiv:2508.15826v1 Announce Type: cross 
Abstract: In social media, marketers attempt to influence consumers by using directive language, that is, expressions designed to get consumers to take action. While the literature has shown that directive messages in advertising have mixed results for recipients, we know little about the effects of directive brand language on consumers who see brands interacting with other consumers in social media conversations. On the basis of a field study and three online experiments, this study shows that directive language in brand conversation has a detrimental downstream effect on engagement of consumers who observe such exchanges. Specifically, in line with Goffman's facework theory, because a brand that encourages consumers to react could be perceived as face-threatening, consumers who see a brand interacting with others in a directive way may feel vicarious embarrassment and engage less (compared with a conversation without directive language). In addition, we find that when the conversation is nonproduct-centered (vs. product-centered), consumers expect more freedom, as in mundane conversations, even for others; therefore, directive language has a stronger negative effect. However, in this context, the strength of the brand relationship mitigates this effect. Thus, this study contributes to the literature on directive language and brand-consumer interactions by highlighting the importance of context in interactive communication, with direct relevance for social media and brand management.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian framework for opinion dynamics models</title>
<link>https://arxiv.org/abs/2508.16539</link>
<guid>https://arxiv.org/abs/2508.16539</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, opinion dynamics models, signal score, rational foundations, cognitive constraints <br />
Summary:<br />
This work presents a Bayesian framework that unifies various opinion dynamics models by considering individuals' opinions as expected values of beliefs represented as random variables with prior distributions. When individuals receive a signal, their belief distribution is updated using Bayes' rule, incorporating prior belief, bias, and noise. By manipulating the prior, bias, and noise distributions, a wide range of models such as DeGroot, bounded confidence, and bounded shift are recovered. The signal score plays a crucial role in determining the mathematical structure of each model, influencing their behavior for both small and large signals. While all models converge to DeGroot's linear update rule for small signals, they exhibit different behavior for large signals. This framework not only reveals connections among previously disparate models but also provides a systematic approach for creating new models, shedding light on the rational basis of opinion formation within cognitive limitations. <br /> <div>
arXiv:2508.16539v1 Announce Type: cross 
Abstract: This work introduces a Bayesian framework that unifies a wide class of opinion dynamics models. In this framework, an individual's opinion on a topic is the expected value of their belief, represented as a random variable with a prior distribution. Upon receiving a signal, modeled as the prior belief plus a bias term and subject to zero-mean noise with a known distribution, the individual updates their belief distribution via Bayes' rule. By systematically varying the prior, bias, and noise distributions, this approach recovers a broad array of opinion dynamics models, including DeGroot, bounded confidence, bounded shift, and models exhibiting overreaction or backfire effects. Our analysis shows that the signal score is the key determinant of each model's mathematical structure, governing both small- and large-signal behavior. All models converge to DeGroot's linear update rule for small signals, but diverge in their tail behavior for large signals. This unification not only reveals theoretical linkages among previously disconnected models but also provides a systematic method for generating new ones, offering insights into the rational foundations of opinion formation under cognitive constraints.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From chambers to echo chambers: Quantifying polarization with a second-neighbor approach applied to Twitter's climate discussion</title>
<link>https://arxiv.org/abs/2206.14501</link>
<guid>https://arxiv.org/abs/2206.14501</guid>
<content:encoded><![CDATA[
<div> polarization, social media, echo chambers, climate change, ideological patterns
<br />
Summary: 
The study examines polarization on social media platforms, particularly focusing on discussions about climate change on X (formerly Twitter) in 2019. Using chambers as second-order information sources, the researchers uncover ideological patterns and polarization dynamics. They identify echo chambers of both climate believers and skeptics, showing strong alignment within groups and minimal overlap between them. Their method allows for the classification of high-impact users based on their audience's chamber alignment, with coverage surpassing existing models. The study finds stable echo chamber structures over time, indicating persistent ideological polarization. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019, highlighting the fluidity of ideological beliefs on social media platforms. The analysis offers valuable insights into the dynamics of polarization and belief reinforcement in online discourse.
<br /> <div>
arXiv:2206.14501v3 Announce Type: replace 
Abstract: Social media platforms often foster environments where users primarily engage with content that aligns with their existing beliefs, thereby reinforcing their views and limiting exposure to opposing viewpoints. In this paper, we analyze X (formerly Twitter) discussions on climate change throughout 2019, using an unsupervised method centered on chambers--second-order information sources--to uncover ideological patterns at scale. Beyond direct connections, chambers capture shared sources of influence, revealing polarization dynamics efficiently and effectively. Analyzing retweet patterns, we identify echo chambers of climate believers and skeptics, revealing strong chamber overlap within ideological groups and minimal overlap between them, resulting in a robust bimodal structure that characterizes polarization. Our method enables us to infer the stance of high-impact users based on their audience's chamber alignment, allowing for the classification of over half the retweeting population with minimal cross-group interaction, in what we term augmented echo chamber classification. We benchmark our approach against manual labeling and a state-of-the-art latent ideology model, finding comparable performance but with nearly four times greater coverage. Moreover, we find that echo chamber structures remain stable over time, even as their members change significantly, suggesting that these structures are a persistent and emergent property of the system. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019. This chamber-based analysis offers valuable insights into the persistence and fluidity of ideological polarization on social media.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Chilling: Identifying Strategic Antisocial Behavior Online and Examining the Impact on Journalists</title>
<link>https://arxiv.org/abs/2508.15061</link>
<guid>https://arxiv.org/abs/2508.15061</guid>
<content:encoded><![CDATA[
<div> classification, online behavior, social media, Twitter, targeted attacks
<br />
Summary:<br />
The article introduces a new tree structured Transformer model for categorizing replies on social platforms like Twitter based on hierarchical conversation structures. This model can effectively detect different user groups such as attackers, supporters, and bystanders, and their latent strategies. The approach allows for exploration of strategic behaviors and their impact on journalists, other users, and conversational outcomes. The study reveals a correlation between the presence of attackers' interactions and chilling effects, leading to a slowdown in journalists' posting behavior. The findings emphasize the importance of social platforms developing tools to address coordinated toxicity, early detection of patterns of attacks, and providing journalists and users with real-time reporting tools to manage hostile interactions effectively. <div>
arXiv:2508.15061v1 Announce Type: new 
Abstract: On social platforms like Twitter, strategic targeted attacks are becoming increasingly common, especially against vulnerable groups such as female journalists. Two key challenges in identifying strategic online behavior are the complex structure of online conversations and the hidden nature of potential strategies that drive user behavior. To address these, we develop a new tree structured Transformer model that categorizes replies based on their hierarchical conversation structures. Extensive experiments demonstrate that our proposed classification model can effectively detect different user groups, namely attackers, supporters, and bystanders, and their latent strategies. To demonstrate the utility of our approach, we apply this classifier to real time Twitter data and conduct a series of quantitative analyses on the interactions between journalists with different groups of users. Our classification approach allows us to not only explore strategic behaviors of attackers but also those of supporters and bystanders who engage in online interactions. When examining the impact of online attacks, we find a strong correlation between the presence of attackers' interactions and chilling effects, where journalists tend to slow their subsequent posting behavior. This paper provides a deeper understanding of how different user groups engage in online discussions and highlights the detrimental effects of attacker presence on journalists, other users, and conversational outcomes. Our findings underscore the need for social platforms to develop tools that address coordinated toxicity. By detecting patterns of coordinated attacks early, platforms could limit the visibility of toxic content to prevent escalation. Additionally, providing journalists and users with tools for real time reporting could empower them to manage hostile interactions more effectively.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIP: Model-Agnostic Hypergraph Influence Prediction via Distance-Centrality Fusion and Neural ODEs</title>
<link>https://arxiv.org/abs/2508.15312</link>
<guid>https://arxiv.org/abs/2508.15312</guid>
<content:encoded><![CDATA[
<div> Keywords: influence prediction, hypergraphs, centrality indicators, neural networks, LSTM

Summary: 
The study introduces HIP, a framework for predicting user influence in social networks using hypergraphs. HIP combines multi-dimensional centrality indicators and a temporally reinterpreted distance matrix to represent node-level diffusion capacity. A multi-hop Hypergraph Neural Network (HNN) captures higher-order structural dependencies, while temporal correlations are modeled using a hybrid module of LSTM networks and Neural Ordinary Differential Equations (Neural ODEs). HIP is modular, allowing for the substitution of different components without compromising performance. Empirical evaluations on 14 real-world hypergraph datasets show that HIP outperforms existing baselines in prediction accuracy, resilience, and identification of top influencers. Importantly, HIP does not require knowledge of the spreading model or diffusion trajectories. The findings highlight HIP's effectiveness and adaptability as a general-purpose solution for influence prediction in complex hypergraph environments.

<br /><br />Summary: <div>
arXiv:2508.15312v1 Announce Type: new 
Abstract: Predicting user influence in social networks is a critical problem, and hypergraphs, as a prevalent higher-order modeling approach, provide new perspectives for this task. However, the absence of explicit cascade or infection probability data makes it particularly challenging to infer influence in hypergraphs. To address this, we introduce HIP, a unified and model-independent framework for influence prediction without knowing the underlying spreading model. HIP fuses multi-dimensional centrality indicators with a temporally reinterpreted distance matrix to effectively represent node-level diffusion capacity in the absence of observable spreading. These representations are further processed through a multi-hop Hypergraph Neural Network (HNN) to capture complex higher-order structural dependencies, while temporal correlations are modeled using a hybrid module that combines Long Short-Term Memory (LSTM) networks and Neural Ordinary Differential Equations (Neural ODEs). Notably, HIP is inherently modular: substituting the standard HGNN with the advanced DPHGNN, and the LSTM with xLSTM, yields similarly strong performance, showcasing its architectural generality and robustness. Empirical evaluations across 14 real-world hypergraph datasets demonstrate that HIP consistently surpasses existing baselines in prediction accuracy, resilience, and identification of top influencers, all without relying on any diffusion trajectories or prior knowledge of the spreading model. These findings underline HIP's effectiveness and adaptability as a general-purpose solution for influence prediction in complex hypergraph environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDBMS: A Context-Aware Hybrid Graph Traversal Algorithm for Efficient Information Discovery in Social Networks</title>
<link>https://arxiv.org/abs/2508.14092</link>
<guid>https://arxiv.org/abs/2508.14092</guid>
<content:encoded><![CDATA[
<div> Algorithm, Graph traversal, Hybrid, Probabilistic, Information retrieval

Summary: 
The paper introduces a new graph-searching algorithm, the Hybrid Depth-Breadth Meaningful Search (HDBMS), that combines Depth-First Search and Breadth-First Search techniques with probabilistic node transitions. HDBMS dynamically adapts its exploration strategy based on the likelihood of nodes containing desired information, leading to contextually relevant search paths. Experimental results on directed graphs show that HDBMS maintains computational efficiency while outperforming traditional algorithms in identifying meaningful paths. By integrating probabilistic decision-making, HDBMS creates an adaptive traversal order that balances exploration across depth and breadth, making it effective for applications like information retrieval, social network analysis, and recommendation systems. The algorithm's robustness in scenarios with unpredictable valuable connections positions it as a powerful alternative to conventional graph-searching methods. 

<br /><br />Summary: <div>
arXiv:2508.14092v1 Announce Type: new 
Abstract: Graph-searching algorithms play a crucial role in various computational domains, enabling efficient exploration and pathfinding in structured data. Traditional approaches, such as Depth-First Search (DFS) and Breadth-First Search (BFS), follow rigid traversal patterns -- DFS explores branches exhaustively, while BFS expands level by level. In this paper, we propose the Hybrid Depth-Breadth Meaningful Search (HDBMS) algorithm, a novel graph traversal method that dynamically adapts its exploration strategy based on probabilistic node transitions. Unlike conventional methods, HDBMS prioritizes traversal paths by estimating the likelihood that a node contains the desired information, ensuring a more contextually relevant search. Through extensive experimentation on diverse directed graphs with varying structural properties, we demonstrate that HDBMS not only maintains competitive computational efficiency but also outperforms traditional algorithms in identifying meaningful paths. By integrating probabilistic decision-making, HDBMS constructs an adaptive and structured traversal order that balances exploration across depth and breadth, making it particularly effective in applications such as information retrieval, social network analysis, and recommendation systems. Our results highlight the robustness of HDBMS in scenarios where the most valuable connections emerge unpredictably, positioning it as a powerful alternative to traditional graph-searching techniques.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Dissipative Graph Propagation for Non-Local Community Detection</title>
<link>https://arxiv.org/abs/2508.14097</link>
<guid>https://arxiv.org/abs/2508.14097</guid>
<content:encoded><![CDATA[
<div> Community detection, graphs, heterophilic, graph neural networks, unsupervised<br />
Summary:<br />
Community detection in heterophilic graphs is challenging due to distantly connected similar nodes. Graph neural networks struggle with this due to local message passing. The Unsupervised Antisymmetric Graph Neural Network (uAGNN) tackles this by propagating long-range information effectively using non-dissipative dynamical systems. uAGNN utilizes antisymmetric weight matrices to capture both local and global structures, outperforming traditional methods in high and medium heterophilic settings. Extensive experiments show uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.<br /> <div>
arXiv:2508.14097v1 Announce Type: new 
Abstract: Community detection in graphs aims to cluster nodes into meaningful groups, a task particularly challenging in heterophilic graphs, where nodes sharing similarities and membership to the same community are typically distantly connected. This is particularly evident when this task is tackled by graph neural networks, since they rely on an inherently local message passing scheme to learn the node representations that serve to cluster nodes into communities. In this work, we argue that the ability to propagate long-range information during message passing is key to effectively perform community detection in heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric Graph Neural Network (uAGNN), a novel unsupervised community detection approach leveraging non-dissipative dynamical systems to ensure stability and to propagate long-range information effectively. By employing antisymmetric weight matrices, uAGNN captures both local and global graph structures, overcoming the limitations posed by heterophilic scenarios. Extensive experiments across ten datasets demonstrate uAGNN's superior performance in high and medium heterophilic settings, where traditional methods fail to exploit long-range dependencies. These results highlight uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.14088</link>
<guid>https://arxiv.org/abs/2508.14088</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, human mobility, collective behaviors, spatiotemporal dependencies, attention mechanism

Summary:<br />
Anomalies in human mobility play a crucial role in applications like public safety and urban planning. This study introduces the concept of collective anomaly detection, which focuses on irregularities in collective mobility behaviors rather than individual patterns. A novel model called CoBAD is proposed to address this challenge by leveraging Collective Event Sequences and a two-stage attention mechanism to capture spatiotemporal dependencies among individuals. CoBAD is trained on large-scale collective behavior data and can detect unexpected co-occurrence anomalies and absence anomalies, which have not been extensively explored in previous work. Experimental results on large mobility datasets show that CoBAD outperforms existing baselines by a significant margin, demonstrating improvements in AUCROC and AUCPR. The source code for CoBAD is publicly available for further research and development.<br /> 

Summary: <div>
arXiv:2508.14088v1 Announce Type: cross 
Abstract: Detecting anomalies in human mobility is essential for applications such as public safety and urban planning. While traditional anomaly detection methods primarily focus on individual movement patterns (e.g., a child should stay at home at night), collective anomaly detection aims to identify irregularities in collective mobility behaviors across individuals (e.g., a child is at home alone while the parents are elsewhere) and remains an underexplored challenge. Unlike individual anomalies, collective anomalies require modeling spatiotemporal dependencies between individuals, introducing additional complexity. To address this gap, we propose CoBAD, a novel model designed to capture Collective Behaviors for human mobility Anomaly Detection. We first formulate the problem as unsupervised learning over Collective Event Sequences (CES) with a co-occurrence event graph, where CES represents the event sequences of related individuals. CoBAD then employs a two-stage attention mechanism to model both the individual mobility patterns and the interactions across multiple individuals. Pre-trained on large-scale collective behavior data through masked event and link reconstruction tasks, CoBAD is able to detect two types of collective anomalies: unexpected co-occurrence anomalies and absence anomalies, the latter of which has been largely overlooked in prior work. Extensive experiments on large-scale mobility datasets demonstrate that CoBAD significantly outperforms existing anomaly detection baselines, achieving an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is available at https://github.com/wenhaomin/CoBAD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations</title>
<link>https://arxiv.org/abs/2508.14335</link>
<guid>https://arxiv.org/abs/2508.14335</guid>
<content:encoded><![CDATA[
<div> small-world characteristics, GS relays, feeder links, betweenness analysis, spatial coverage

Summary:<br /><br />This paper examines a six-shell mega-constellation with over 10,000 satellites and 198 gateway stations, focusing on routing efficiency and inter-shell communication. The analysis reveals that the constellation displays strong small-world properties, facilitating efficient routing despite its large size. Gateway stations play a crucial role in enhancing connectivity between shells by bridging disconnected components. Feeder links help reduce average path length, improving long-distance communication feasibility. Further, betweenness analysis highlights load imbalances among gateway stations, emphasizing the need for traffic-aware management strategies. The mega-constellation demonstrates excellent spatial coverage and resilience, maintaining connectivity and low routing costs even in the event of gateway station failures. These findings shed light on the design principles of existing mega-constellations and offer valuable insights for the future development of satellite network infrastructures.<br /> <div>
arXiv:2508.14335v1 Announce Type: cross 
Abstract: In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Messengers: Breaking Echo Chambers in Collective Opinion Dynamics with Homophily</title>
<link>https://arxiv.org/abs/2406.06440</link>
<guid>https://arxiv.org/abs/2406.06440</guid>
<content:encoded><![CDATA[
<div> consensus, collective estimation, social interactions, echo chambers, agent-based simulations
<br />
Summary:
Collective estimation in decision-making involves agents reaching consensus on a continuous quantity through social interactions. However, achieving precise consensus is challenging due to the co-evolution of opinions and the interaction network. Homophilic networks may facilitate estimation in well-connected systems, but disproportionate interactions with like-minded neighbors can lead to the emergence of echo chambers, hindering consensus. Limited exposure to differing opinions and seeking reaffirming information can trap agents in these echo chambers. To address this, agents can adopt a stubborn state (Messengers) to physically transport their opinions and connect clusters. A Dichotomous Markov Process is proposed to govern probabilistic switching between behavioral states, promoting diverse collective behaviors ranging from task specialization to generalization. Messengers help the collective escape local minima, break echo chambers, and ultimately promote consensus. 
<br /> <div>
arXiv:2406.06440v3 Announce Type: replace 
Abstract: Collective estimation is a variant of collective decision-making where agents reach consensus on a continuous quantity through social interactions. Achieving precise consensus is complex due to the co-evolution of opinions and the interaction network. While homophilic networks may facilitate estimation in well-connected systems, disproportionate interactions with like-minded neighbors lead to the emergence of echo chambers and prevent consensus. Our agent-based simulations confirm that, besides limited exposure to attitude-challenging opinions, seeking reaffirming information entrap agents in echo chambers. To overcome this, agents can adopt a stubborn state (Messengers) that carry data and connect clusters by physically transporting their opinion. We propose a generic approach based on a Dichotomous Markov Process, which governs probabilistic switching between behavioral states and generates diverse collective behaviors. We study a continuum between task specialization (no switching), to generalization (slow or rapid switching). Messengers help the collective escape local minima, break echo chambers, and promote consensus.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Profile of U.S. Neighborhoods: Datasets of Time Use at Social Infrastructure Places</title>
<link>https://arxiv.org/abs/2508.13295</link>
<guid>https://arxiv.org/abs/2508.13295</guid>
<content:encoded><![CDATA[
<div> social infrastructure, time use, spatial accessibility, foot traffic data, population representation

Summary: 
The study focuses on analyzing time use at social infrastructure places, which play a crucial role in neighborhood well-being. The researchers developed Social-Infrastructure Time Use (STU) measures using foot traffic data collected across 49 continental U.S. states from 2019 to 2024. The STU measures capture the length and depth of engagement, activity diversity, and spatial inequality at different geographic scales. The data description highlights variations in STU across time, space, and neighborhood characteristics. Validation results show consistent population representation with national survey findings and reveal nuanced patterns. Future analyses could explore the link between STU and public health outcomes and environmental factors to guide interventions for enhancing population well-being and inform social infrastructure planning and usage. <div>
arXiv:2508.13295v1 Announce Type: new 
Abstract: Social infrastructure plays a critical role in shaping neighborhood well-being by fostering social and cultural interaction, enabling service provision, and encouraging exposure to diverse environments. Despite the growing knowledge of its spatial accessibility, time use at social infrastructure places is underexplored due to the lack of a spatially resolved national dataset. We address this gap by developing scalable Social-Infrastructure Time Use measures (STU) that capture length and depth of engagement, activity diversity, and spatial inequality, supported by first-of-their-kind datasets spanning multiple geographic scales from census tracts to metropolitan areas. Our datasets leverage anonymized and aggregated foot traffic data collected between 2019 and 2024 across 49 continental U.S. states. The data description reveals variances in STU across time, space, and differing neighborhood sociodemographic characteristics. Validation demonstrates generally robust population representation, consistent with established national survey findings while revealing more nuanced patterns. Future analyses could link STU with public health outcomes and environmental factors to inform targeted interventions aimed at enhancing population well-being and guiding social infrastructure planning and usage.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State &amp; Geopolitical Censorship on Twitter (X): Detection &amp; Impact Analysis of Withheld Content</title>
<link>https://arxiv.org/abs/2508.13375</link>
<guid>https://arxiv.org/abs/2508.13375</guid>
<content:encoded><![CDATA[
<div> Keywords: state censorship, social media, user-level classifier, platform governance, digital repression

Summary: 
State censorship on social media platforms like Twitter has become increasingly common, leading to concerns about the balance between criminal content and freedom of speech. This study analyzes the impact of state censorship on Twitter accounts, focusing on Russian and Turkish accounts withheld in specific regions. The research shows that while censorship does not significantly reduce posting frequency, it does decrease likes, retweets, and follower growth, particularly when the censored region aligns with the account's primary audience. A user-level binary classifier using tweet content is developed to predict whether an account is likely to be withheld, achieving high accuracy. This analysis sheds light on the complexities of platform governance, free speech, and digital repression in the context of state censorship on social media. 

<br /><br />Summary: <div>
arXiv:2508.13375v1 Announce Type: new 
Abstract: State and geopolitical censorship on Twitter, now X, has been turning into a routine, raising concerns about the boundaries between criminal content and freedom of speech. One such censorship practice, withholding content in a particular state has renewed attention due to Elon Musk's apparent willingness to comply with state demands. In this study, we present the first quantitative analysis of the impact of state censorship by withholding on social media using a dataset in which two prominent patterns emerged: Russian accounts censored in the EU for spreading state-sponsored narratives, and Turkish accounts blocked within Turkey for promoting militant propaganda. We find that censorship has little impact on posting frequency but significantly reduces likes and retweets by 25%, and follower growth by 90%-especially when the censored region aligns with the account's primary audience. Meanwhile, some Russian accounts continue to experience growth as their audience is outside the withholding jurisdictions. We develop a user-level binary classifier with a transformer backbone and temporal aggregation strategies, aiming to predict whether an account is likely to be withheld. Through an ablation study, we find that tweet content is the primary signal in predicting censorship, while tweet metadata and profile features contribute marginally. Our best model achieves an F1 score of 0.73 and an AUC of 0.83. This work informs debates on platform governance, free speech, and digital repression.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a general diffusion-based information quality assessment model</title>
<link>https://arxiv.org/abs/2508.13927</link>
<guid>https://arxiv.org/abs/2508.13927</guid>
<content:encoded><![CDATA[
<div> diffusion dynamics, information quality, academic publications, Generalized Additive Model, citation gain <br />
<br />
Summary: 
The article introduces a framework for assessing information quality based on diffusion dynamics, focusing on academic publications. By analyzing a dataset of STEM and social science papers, the framework uses three key features: diversity, timeliness, and salience. A Generalized Additive Model trained on these features shows high accuracy in predicting high-impact papers and next-year citation gain. Timeliness and salience are identified as the most important predictors, while diversity has more variable benefits. The framework's transparency and domain-agnostic design make it a scalable tool for assessing information quality globally. It also suggests moving towards richer evaluation metrics based on diffusion dynamics, providing a more nuanced approach to assessing information credibility beyond binary labels. <br /> <div>
arXiv:2508.13927v1 Announce Type: new 
Abstract: The rapid and unregulated dissemination of information in the digital era has amplified the global "infodemic," complicating the identification of high quality information. We present a lightweight, interpretable and non-invasive framework for assessing information quality based solely on diffusion dynamics, demonstrated here in the context of academic publications. Using a heterogeneous dataset of 29,264 sciences, technology, engineering, mathematics (STEM) and social science papers from ArnetMiner and OpenAlex, we model the diffusion network of each paper as a set of three theoretically motivated features: diversity, timeliness, and salience. A Generalized Additive Model (GAM) trained on these features achieved Pearson correlations of 0.8468 for next-year citation gain and up to 97.8% accuracy in predicting high-impact papers. Feature relevance studies reveal timeliness and salience as the most robust predictors, while diversity offers less stable benefits in the academic setting but may be more informative in social media contexts. The framework's transparency, domain-agnostic design, and minimal feature requirements position it as a scalable tool for global information quality assessment, opening new avenues for moving beyond binary credibility labels toward richer, diffusion-informed evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust and Reputation in Data Sharing: A Survey</title>
<link>https://arxiv.org/abs/2508.14028</link>
<guid>https://arxiv.org/abs/2508.14028</guid>
<content:encoded><![CDATA[
<div> Keywords: data sharing, trust, reputation management systems, trustworthiness, evaluation metrics

Summary:
Trust plays a crucial role in facilitating data sharing, yet concerns about privacy and misuse hinder such initiatives. Trust and Reputation Management Systems (TRMSs) have emerged to address these challenges in various domains, but there is a lack of dedicated approaches to data sharing. This survey examines TRMSs from a data-sharing perspective, proposing novel taxonomies for system designs and trust evaluation frameworks. The study analyzes the applicability of existing TRMSs in assessing the trustworthiness of both data and entities in different environments. By identifying open challenges and suggesting future research directions, the goal is to enhance the accuracy and comprehensiveness of TRMSs in large-scale data-sharing ecosystems. This research aims to improve the explainability and reliability of TRMSs to foster trust and facilitate data sharing in the rapidly advancing artificial intelligence economy. 

<br /><br />Summary: 
- Data sharing trust issues hinder advancements in AI. 
- TRMSs address trustworthiness challenges in various domains. 
- Lack of dedicated TRMSs for data sharing necessitates novel approaches. 
- Proposed taxonomies and evaluation frameworks enhance trust assessment. 
- Future research aims to improve TRMS accuracy and comprehensiveness. <div>
arXiv:2508.14028v1 Announce Type: new 
Abstract: Data sharing is the fuel of the galloping artificial intelligence economy, providing diverse datasets for training robust models. Trust between data providers and data consumers is widely considered one of the most important factors for enabling data sharing initiatives. Concerns about data sensitivity, privacy breaches, and misuse contribute to reluctance in sharing data across various domains. In recent years, there has been a rise in technological and algorithmic solutions to measure, capture and manage trust, trustworthiness, and reputation in what we collectively refer to as Trust and Reputation Management Systems (TRMSs). Such approaches have been developed and applied to different domains of computer science, such as autonomous vehicles, or IoT networks, but there have not been dedicated approaches to data sharing and its unique characteristics. In this survey, we examine TRMSs from a data-sharing perspective, analyzing how they assess the trustworthiness of both data and entities across different environments. We develop novel taxonomies for system designs, trust evaluation framework, and evaluation metrics for both data and entity, and we systematically analyze the applicability of existing TRMSs in data sharing. Finally, we identify open challenges and propose future research directions to enhance the explainability, comprehensiveness, and accuracy of TRMSs in large-scale data-sharing ecosystems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Conversational Recommender System Considering Consumer Types</title>
<link>https://arxiv.org/abs/2508.13209</link>
<guid>https://arxiv.org/abs/2508.13209</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational Recommender Systems, Consumer Type Modeling, Personalization, Inverse Reinforcement Learning, User Categories

Summary: 
CT-CRS is a framework that integrates consumer type modeling into dialogue recommendation for Conversational Recommender Systems. It defines four user categories based on decision-making style and knowledge level, automatically inferring user types in real time. The system adjusts recommendation granularity, diversity, and attribute query complexity based on user type. Inverse Reinforcement Learning is used to approximate expert-like strategies conditioned on consumer type, improving recommendation success rate and reducing interaction turns. Experiments on LastFM, Amazon-Book, and Yelp show significant performance gains with CT-CRS. This approach offers a scalable and interpretable solution for enhancing CRS personalization through psychological modeling and advanced policy optimization.<br /><br /> <div>
arXiv:2508.13209v1 Announce Type: cross 
Abstract: Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Influence Maximization in User Recommendation</title>
<link>https://arxiv.org/abs/2508.13517</link>
<guid>https://arxiv.org/abs/2508.13517</guid>
<content:encoded><![CDATA[
<div> recommendation systems, user engagement, information propagation, Influence-Maximization, HeteroIR

Summary:
User recommendation systems play a crucial role in enhancing user engagement by encouraging interaction between users, leading to information propagation. Traditional recommendation methods focus on modeling interaction willingness, while Influence-Maximization (IM) methods aim to maximize information spread. However, existing methods face challenges in fully utilizing candidates' dissemination potential and accounting for interaction willingness. To address these issues, the HeteroIR and HeteroIM models are proposed. HeteroIR unleashes the spread capability of user recommendation systems, while HeteroIM bridges the gap between IM methods and recommendation tasks by improving interaction willingness and maximizing spread coverage. Experimental results demonstrate the superior performance of HeteroIR and HeteroIM compared to baseline methods. Deployment in Tencent's online gaming platforms resulted in significant improvements in online A/B tests. The implementation codes for HeteroIR and HeteroIM are available at https://github.com/socialalgo/HIM.<br /><br />Summary: <div>
arXiv:2508.13517v1 Announce Type: cross 
Abstract: User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B test, respectively. Implementation codes are available at https://github.com/socialalgo/HIM.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exit Stories: Using Reddit Self-Disclosures to Understand Disengagement from Problematic Communities</title>
<link>https://arxiv.org/abs/2508.13837</link>
<guid>https://arxiv.org/abs/2508.13837</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, exit stories, disengagement, conspiracy theories, manosphere

Summary:
This paper explores the phenomenon of individuals sharing their experiences of leaving social, ideological, and political groups on online platforms like Reddit. It focuses on 15,000 exit stories across 131 subreddits, including themes of religion, manosphere, conspiracy theories, politics, and lifestyle. By drawing on theories from social psychology, organizational behavior, and violent extremism studies, the study identifies factors contributing to disengagement. It distinguishes the process of disengaging from conspiracy theories and the manosphere from more established structures like religions or political ideologies. The research emphasizes the need for interventions that go beyond treating conspiracy theorizing as merely an information issue, suggesting the importance of mental health support in exit communities. This study provides valuable insights for designing interventions targeting disengagement from harmful ideologies. <br /><br />Summary: This study examines individuals sharing their experiences of leaving groups on Reddit, analyzing exit stories across various subreddits. It identifies factors influencing disengagement and highlights differences in disengaging from problematic groups versus established structures. The research calls for interventions beyond addressing information problems, emphasizing mental health support in exit communities. <div>
arXiv:2508.13837v1 Announce Type: cross 
Abstract: Online platforms like Reddit are increasingly becoming popular for individuals sharing personal experiences of leaving behind social, ideological, and political groups. Specifically, a series of "ex-" subreddits on Reddit allow users to recount their departures from commitments such as religious affiliations, manosphere communities, conspiracy theories or political beliefs, and lifestyle choices. Understanding the natural process through which users exit, especially from problematic groups such as conspiracy theory communities and the manosphere, can provide valuable insights for designing interventions targeting disengagement from harmful ideologies. This paper presents an in-depth exploration of 15K exit stories across 131 subreddits, focusing on five key areas: religion, manosphere, conspiracy theories, politics, and lifestyle. Using a transdisciplinary framework that incorporates theories from social psychology, organizational behavior, and violent extremism studies, this work identifies a range of factors contributing to disengagement. The results describe how disengagement from problematic groups, such as conspiracy theories and the manosphere, is a multi-faceted process that is qualitatively different than disengaging from more established social structures, such as religions or political ideologies. This research further highlights the need for moving beyond interventions that treat conspiracy theorizing solely as an information problem and contributes insights for future research focusing on offering mental health interventions and support in exit communities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Community Formation in Response to Extreme Weather Events through Human Mobility Networks</title>
<link>https://arxiv.org/abs/2205.04981</link>
<guid>https://arxiv.org/abs/2205.04981</guid>
<content:encoded><![CDATA[
<div> Keywords: community formation, human mobility networks, natural disasters, managed power outages, socio-spatial networks

Summary:<br /><br />
The study explores community formation in human mobility networks during natural disasters, focusing on the 2021 Winter Storm Uri in Harris County, Texas. Three key characteristics were examined in the formed communities: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The findings show that population movements were influenced by these factors, with communities being shaped by socio-demographic similarities, exposure to hazards, and social connections. The study highlights that communities formed during managed power outages are spatially co-located, suggesting the importance of avoiding prolonged outages in high-impact areas within these communities. The results have practical implications for power utility operators in effectively managing power outages by considering the characteristics of socio-spatial human networks. <div>
arXiv:2205.04981v2 Announce Type: replace 
Abstract: Community formation in socio-spatial human networks is one of the important mechanisms for mitigating hazard impacts of extreme weather events. Research is scarce regarding latent network characteristics shaping community formation in human mobility networks during natural disasters. Here, we examined human mobility networks in Harris County, Texas, in the context of the managed power outage forced by 2021 Winter Storm Uri to detect communities and to evaluate latent characteristics in those communities. We examined three characteristics in the communities formed within human mobility networks: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The results show that population movements were shaped by socio-demographic homophily, heterophilic hazard exposure, and social connectedness strength. Our results also indicate that a community encompassing more high-impact areas would motivate population movements to areas with weaker social connectedness. Our findings reveal important characteristics shaping community formation in human mobility networks in hazard response. Specific to managed power outages, formed communities are spatially co-located, underscoring a best management practice to avoid prolonged power outages among areas within communities, thus improving hazard exposure heterophily. The findings have implications for power utility operators to account for the characteristics of socio-spatial human networks when determining the patterns of managed power outages.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Attributes and Multi-Scale Structures for Heterogeneous Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2503.13911</link>
<guid>https://arxiv.org/abs/2503.13911</guid>
<content:encoded><![CDATA[
<div> contrastive learning, heterogeneous graphs, self-supervised learning, node representation learning, positive sample selection

Summary:
ASHGCL is a novel contrastive learning framework designed for heterogeneous graphs. It incorporates three distinct views to capture attribute information, high-order structures, and low-order structures for node representation learning. The framework addresses the challenge of limited labeling data in real-world scenarios by utilizing self-supervised learning. ASHGCL introduces an attribute-enhanced positive sample selection strategy to tackle sampling bias effectively. Experimental results on four real-world datasets demonstrate that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks. <div>
arXiv:2503.13911v3 Announce Type: replace-cross 
Abstract: Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges, making it more effective in capturing the complex relational structures inherent in the real world. However, in real-world scenarios, labeled data is often difficult to obtain, which limits the applicability of semi-supervised approaches. Self-supervised learning aims to enable models to automatically learn useful features from data, effectively addressing the challenge of limited labeling data. In this paper, we propose a novel contrastive learning framework for heterogeneous graphs (ASHGCL), which incorporates three distinct views, each focusing on node attributes, high-order and low-order structural information, respectively, to effectively capture attribute information, high-order structures, and low-order structures for node representation learning. Furthermore, we introduce an attribute-enhanced positive sample selection strategy that combines both structural information and attribute information, effectively addressing the issue of sampling bias. Extensive experiments on four real-world datasets show that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs</title>
<link>https://arxiv.org/abs/2508.11863</link>
<guid>https://arxiv.org/abs/2508.11863</guid>
<content:encoded><![CDATA[
<div> sparse network, random K-out graphs, reliable connectivity, finite nodes, adversarial nodes<br />
<br />
Summary:
The article focuses on designing sparse networks with reliable connectivity in distributed systems. It introduces random K-out graphs as a model to balance connectivity and sparsity, particularly in settings with limited trust. The study presents theorems to guide the selection of network parameters for ensuring reliable connectivity in scenarios with finite or unreliable nodes. It establishes upper and lower bounds for connectivity probability in random K-out graphs with finite node numbers and explores r-robustness for resilient consensus in the presence of malicious nodes. Additionally, the impact of adversarial nodes on connectivity and giant component size is analyzed, considering deletions as a modeling approach. These findings contribute to providing performance guarantees for algorithms aimed at reliable inference on networks. <br /><br /> <div>
arXiv:2508.11863v1 Announce Type: new 
Abstract: In several applications in distributed systems, an important design criterion is ensuring that the network is sparse, i.e., does not contain too many edges, while achieving reliable connectivity. Sparsity ensures communication overhead remains low, while reliable connectivity is tied to reliable communication and inference on decentralized data reservoirs and computational resources. A class of network models called random K-out graphs appear widely as a heuristic to balance connectivity and sparsity, especially in settings with limited trust, e.g., privacy-preserving aggregation of networked data in which networks are deployed. However, several questions remain regarding how to choose network parameters in response to different operational requirements, including the need to go beyond asymptotic results and the ability to model the stochastic and adversarial environments. To address this gap, we present theorems to inform the choice of network parameters that guarantee reliable connectivity in regimes where nodes can be finite or unreliable. We first derive upper and lower bounds for probability of connectivity in random K-out graphs when the number of nodes is finite. Next, we analyze the property of r-robustness, a stronger notion than connectivity that enables resilient consensus in the presence of malicious nodes. Finally, motivated by aggregation mechanisms based on pairwise masking, we model and analyze the impact of a subset of adversarial nodes, modeled as deletions, on connectivity and giant component size - metrics that are closely tied to privacy guarantees. Together, our results pave the way for end-to-end performance guarantees for a suite of algorithms for reliable inference on networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust@Health: A Trust-Based Multilayered Network for Scalable Healthcare Service Management</title>
<link>https://arxiv.org/abs/2508.11942</link>
<guid>https://arxiv.org/abs/2508.11942</guid>
<content:encoded><![CDATA[
<div> trust relationships, healthcare systems, optimization, evolutionary graph framework, key healthcare entities

Summary:
The study explores healthcare systems' intricate relationships focusing on doctors, departments, and hospitals. Using an evolutionary graph framework, the model incorporates intra-layer and inter-layer trust relationships to optimize healthcare services. By integrating social and professional interactions, a trust-based network identifies key healthcare entities. The proposed trust algorithm effectively quantifies entity importance, showing a strong correlation (0.91) with hospital and department ratings. However, doctor ratings may be biased, leading to skewed distributions. This framework enables scalable healthcare infrastructure, supporting patient referrals, personalized recommendations, and improved decision-making processes. <div>
arXiv:2508.11942v1 Announce Type: new 
Abstract: We study the intricate relationships within healthcare systems, focusing on interactions among doctors, departments, and hospitals. Leveraging an evolutionary graph framework, the proposed model emphasizes both intra-layer and inter-layer trust relationships to better understand and optimize healthcare services. The trust-based network facilitates the identification of key healthcare entities by integrating their social and professional interactions, culminating in a trust-based algorithm that quantifies the importance of these entities. Validation with a real-world dataset reveals a strong correlation (0.91) between the proposed trust measures and the ratings of hospitals and departments, though doctor ratings demonstrate skewed distributions due to potential biases. By modeling these relationships and trust dynamics, the framework supports scalable healthcare infrastructure, enabling effective patient referrals, personalized recommendations, and enhanced decision-making pathways.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Network-aware Direct Search Method for Influence Maximization</title>
<link>https://arxiv.org/abs/2508.12164</link>
<guid>https://arxiv.org/abs/2508.12164</guid>
<content:encoded><![CDATA[
<div> direct search, influence maximization, social network analysis, network structure, computational efficiency 

Summary:
Influence Maximization (IM) is crucial in social network analysis, aiming to identify influential nodes to maximize influence spread. Direct search methods, suitable for such problems, evaluate the objective function without gradient information. Network-aware Direct Search (NaDS) integrates network structure into its formulation to address IM problems efficiently. Utilizing a mixed-integer programming IM model, NaDS outperformed existing state-of-the-art approaches in large-scale network tests. By incorporating graph structure, NaDS demonstrated improved computational efficiency, confirming the benefits of leveraging network information in algorithmic frameworks for the IM problem.<br /><br />Summary: <div>
arXiv:2508.12164v1 Announce Type: new 
Abstract: Influence Maximization (IM) is a pivotal concept in social network analysis, involving the identification of influential nodes within a network to maximize the number of influenced nodes, and has a wide variety of applications that range from viral marketing and information dissemination to public health campaigns. IM can be modeled as a combinatorial optimization problem with a black-box objective function, where the goal is to select $B$ seed nodes that maximize the expected influence spread. Direct search methods, which do not require gradient information, are well-suited for such problems. Unlike gradient-based approaches, direct search algorithms, in fact, only evaluate the objective function at a suitably chosen set of trial points around the current solution to guide the search process. However, these methods often suffer from scalability issues due to the high cost of function evaluations, especially when applied to combinatorial problems like IM. This work, therefore, proposes the Network-aware Direct Search (NaDS) method, an innovative direct search approach that integrates the network structure into its neighborhood formulation and is used to tackle a mixed-integer programming formulation of the IM problem, the so-called General Information Propagation model. We tested our method on large-scale networks, comparing it to existing state-of-the-art approaches for the IM problem, including direct search methods and various greedy techniques and heuristics. The results of the experiments empirically confirm the assumptions underlying NaDS, demonstrating that exploiting the graph structure of the IM problem in the algorithmic framework can significantly improve its computational efficiency in the considered context.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD: A Benchmark for Multi-Turn Audio Dialogue Fact-Checking</title>
<link>https://arxiv.org/abs/2508.12186</link>
<guid>https://arxiv.org/abs/2508.12186</guid>
<content:encoded><![CDATA[
<div> fact-checking, audio platforms, misinformation, multi-turn dialogues, dataset

Summary:
The article introduces the MAD (Multi-turn Audio Dialogues) dataset, focused on fact-checking spoken content in multi-turn dialogues. It addresses the lack of comprehensive datasets capturing the complexity of misinformation in speech, including speaker interactions, emotional tone, and overlapping speech. The dataset includes annotations for speaker turns, dialogue scenarios, check-worthiness, and veracity at both sentence and dialogue levels. Two core tasks supported are check-worthy claim detection and claim verification. Benchmarking shows the difficulty of these tasks, with pretrained models achieving only around 72-74% accuracy for sentence-level verification and 71-72% for dialogue-level verification. MAD serves as a benchmark for advancing multimodal and conversational fact-checking and highlights challenges in reasoning over speech and dialogue dynamics. <div>
arXiv:2508.12186v1 Announce Type: new 
Abstract: Despite the growing popularity of audio platforms, fact-checking spoken content remains significantly underdeveloped. Misinformation in speech often unfolds across multi-turn dialogues, shaped by speaker interactions, disfluencies, overlapping speech, and emotional tone-factors that complicate both claim detection and verification. Existing datasets fall short by focusing on isolated sentences or text transcripts, without modeling the conversational and acoustic complexity of spoken misinformation. We introduce MAD (Multi-turn Audio Dialogues), the first fact-checking dataset aligned with multi-turn spoken dialogues and corresponding audio. MAD captures how misinformation is introduced, contested, and reinforced through natural conversation. Each dialogue includes annotations for speaker turns, dialogue scenarios, information spread styles, sentence-level check-worthiness, and both sentence- and dialogue-level veracity. The dataset supports two core tasks: check-worthy claim detection and claim verification. Benchmarking shows that even strong pretrained models reach only 72-74% accuracy at the sentence level and 71-72% at the dialogue level in claim verification, underscoring MAD's difficulty. MAD offers a high-quality benchmark for advancing multimodal and conversational fact-checking, while also surfacing open challenges related to reasoning over speech and dialogue dynamics.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Physicians: Social and Familial Norms Driving Cesarean Section Decisions in Bangladesh</title>
<link>https://arxiv.org/abs/2508.12240</link>
<guid>https://arxiv.org/abs/2508.12240</guid>
<content:encoded><![CDATA[
<div> Keywords: women's health, cesarean section, Bangladesh, Health Belief Model, Theory of Planned Behavior 

Summary: 
The study examines the high rate of cesarean sections (CS) in Bangladesh, which exceeds the WHO's recommendation. It investigates the socio-cultural factors influencing women's decisions regarding childbirth mode. Findings show that 91% of CS cases occurred against initial preferences, indicating a gap between health beliefs and behavior. The subjective norms, specifically family influence and social expectations, play a significant role in shaping CS decisions, outweighing physician recommendations. This highlights the need to consider socio-cultural factors in addressing the rising CS rates in Bangladesh, to ensure women's health and well-being are prioritized. <br /><br /> <div>
arXiv:2508.12240v1 Announce Type: new 
Abstract: Women's health in Bangladesh faces risks due to an alarming rise in cesarean section (CS) rates, exceeding 72% in hospital-based deliveries, far surpassing the WHO's recommended limit of 15%. This study, guided by the Health Belief Model (HBM) and the Theory of Planned Behavior (TPB), explored socio-cultural factors influencing childbirth mode decisions. Among 503 survey participants, 91% of CS cases occurred against initial preferences, revealing a disconnect between health beliefs and behavior. Subjective norms, particularly family influence and social expectations, emerged as more critical in shaping CS decisions than physician recommendations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network</title>
<link>https://arxiv.org/abs/2508.12574</link>
<guid>https://arxiv.org/abs/2508.12574</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, rumor detection, bidirectional Mamba2 Network, dot-product attention, Conditional Random Fields

Summary:
The paper introduces the Insight Rumors model, aiming to locate and mark specific rumor content in textual data. The model utilizes the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2) to enhance the representation of high-dimensional rumor features. A Rumor Locating and Marking module is designed to accurately locate and mark rumors using a skip-connection network and Conditional Random Fields (CRF). A labeled dataset is created for rumor locating and marking, and comprehensive experiments show that the proposed model outperforms existing schemes by accurately detecting, locating, and marking rumors in context. This innovative approach addresses the limitation of current models that focus solely on classifying contexts as rumors or not, providing a more precise and effective method for rumor detection in social media networks. 

<br /><br />Summary: <div>
arXiv:2508.12574v1 Announce Type: new 
Abstract: With the development of social media networks, rumor detection models have attracted more and more attention. Whereas, these models primarily focus on classifying contexts as rumors or not, lacking the capability to locate and mark specific rumor content. To address this limitation, this paper proposes a novel rumor detection model named Insight Rumors to locate and mark rumor content within textual data. Specifically, we propose the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a bidirectional Mamba2 model and applies dot-product attention to weight and combine the outputs from both directions, thereby enhancing the representation of high-dimensional rumor features. Simultaneously, a Rumor Locating and Marking module is designed to locate and mark rumors. The module constructs a skip-connection network to project high-dimensional rumor features onto low-dimensional label features. Moreover, Conditional Random Fields (CRF) is employed to impose strong constraints on the output label features, ensuring accurate rumor content location. Additionally, a labeled dataset for rumor locating and marking is constructed, with the effectiveness of the proposed model is evaluated through comprehensive experiments. Extensive experiments indicate that the proposed scheme not only detects rumors accurately but also locates and marks them in context precisely, outperforming state-of-the-art schemes that can only discriminate rumors roughly.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Prediction in Collaboration Networks: An Empirical Study on arXiv</title>
<link>https://arxiv.org/abs/2508.13029</link>
<guid>https://arxiv.org/abs/2508.13029</guid>
<content:encoded><![CDATA[
<div> influence prediction, Social Sphere Model, link prediction, centrality-based selection, General Relativity, Quantum Cosmology<br />
<br />
Summary:<br />
This empirical study evaluates the Social Sphere Model for influence prediction on the arXiv General Relativity and Quantum Cosmology collaboration network. The model combines link prediction with top-k centrality-based selection and is tested under varying edge sampling rates and prediction horizons. Results show the model effectively identifies latent influencers in evolving networks, performing best with denser initial graphs. The newly introduced RA-2 metric consistently produces the lowest prediction errors among the similarity measures tested. The study demonstrates the practical applicability of the Social Sphere Model in predicting real-world influence in evolving networks. <div>
arXiv:2508.13029v1 Announce Type: new 
Abstract: This paper provides an empirical study of the Social Sphere Model for influence prediction, previously introduced by the authors, combining link prediction with top-k centrality-based selection. We apply the model to the temporal arXiv General Relativity and Quantum Cosmology collaboration network, evaluating its performance under varying edge sampling rates and prediction horizons to reflect different levels of initial data completeness and network evolution. Accuracy is assessed using mean squared error in both link prediction and influence maximization tasks. The results show that the model effectively identifies latent influencers, i.e., nodes that are not initially central but later influential, and performs best with denser initial graphs. Among the similarity measures tested, the newly introduced RA-2 metric consistently yields the lowest prediction errors. These findings support the practical applicability of the model to predict real-world influence in evolving networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation</title>
<link>https://arxiv.org/abs/2508.12674</link>
<guid>https://arxiv.org/abs/2508.12674</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic relational structures, node embeddings, stability properties, Laplacian matrix, spectral graph theory

Summary: 
The paper introduces the Unfolded Laplacian Spectral Embedding method for representing dynamic relational structures in AI tasks. This method extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians, ensuring both cross-sectional and longitudinal stability. Formal proofs are provided to confirm the stability conditions of the proposed method. Additionally, leveraging the Laplacian matrix leads to a new Cheeger-style inequality linking the embeddings to the conductance of dynamic graphs. Empirical evaluations on synthetic and real-world datasets validate the theoretical findings and showcase the effectiveness of the approach. Overall, the Unfolded Laplacian Spectral Embedding method establishes a stable and principled framework for dynamic network representation rooted in spectral graph theory.<br /><br />Summary: <div>
arXiv:2508.12674v1 Announce Type: cross 
Abstract: Dynamic relational structures play a central role in many AI tasks, but their evolving nature presents challenges for consistent and interpretable representation. A common approach is to learn time-varying node embeddings, whose effectiveness depends on satisfying key stability properties. In this paper, we propose Unfolded Laplacian Spectral Embedding, a new method that extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians while preserving both cross-sectional and longitudinal stability. We provide formal proof that our method satisfies these stability conditions. In addition, as a bonus of using the Laplacian matrix, we establish a new Cheeger-style inequality that connects the embeddings to the conductance of the underlying dynamic graphs. Empirical evaluations on synthetic and real-world datasets support our theoretical findings and demonstrate the strong performance of our method. These results establish a principled and stable framework for dynamic network representation grounded in spectral graph theory.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps</title>
<link>https://arxiv.org/abs/2508.11516</link>
<guid>https://arxiv.org/abs/2508.11516</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender systems, echo chambers, user homogenization, algorithmic recommendations, psychological mechanisms <br />
Summary: 
The study focuses on the impact of echo chambers and user homogenization in recommender systems, identifying them as results of the interaction between algorithmic recommendations and user behavior. The Confirmation-Aware Social Dynamic Model is proposed to incorporate user psychology and social relationships to simulate user-recommender interactions. The theoretical analysis demonstrates that echo chambers and homogenization traps are inevitable outcomes. Empirical simulations on real-world datasets and a synthetic dataset support this claim, highlighting factors influencing these phenomena at system, user, and platform levels. Moreover, practical mitigation strategies are proposed to address echo chambers and user homogenization. These findings offer insights into the emergence and drivers of these issues in recommender systems, providing guidance for designing more human-centered recommendation systems.<br /><br />Summary: <div>
arXiv:2508.11516v1 Announce Type: new 
Abstract: Recommender systems increasingly suffer from echo chambers and user homogenization, systemic distortions arising from the dynamic interplay between algorithmic recommendations and human behavior. While prior work has studied these phenomena through the lens of algorithmic bias or social network structure, we argue that the psychological mechanisms of users and the closed-loop interaction between users and recommenders are critical yet understudied drivers of these emergent effects. To bridge this gap, we propose the Confirmation-Aware Social Dynamic Model which incorporates user psychology and social relationships to simulate the actual user and recommender interaction process. Our theoretical analysis proves that echo chambers and homogenization traps, defined respectively as reduced recommendation diversity and homogenized user representations, will inevitably occur. We also conduct extensive empirical simulations on two real-world datasets and one synthetic dataset with five well-designed metrics, exploring the root factors influencing the aforementioned phenomena from three level perspectives: the stochasticity and social integration degree of recommender (system-level), the psychological mechanisms of users (user-level), and the dataset scale (platform-level). Furthermore, we demonstrate four practical mitigation strategies that help alleviate echo chambers and user homogenization at the cost of some recommendation accuracy. Our findings provide both theoretical and empirical insights into the emergence and drivers of echo chambers and user homogenization, as well as actionable guidelines for human-centered recommender design.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.11197</link>
<guid>https://arxiv.org/abs/2508.11197</guid>
<content:encoded><![CDATA[
<div> framework, misinformation, social media, multimodal, detection<br />
<br />
Summary: 
The article proposes E-CaTCH, a framework for detecting multimodal misinformation on social media. E-CaTCH clusters posts into pseudo-events based on similarity and proximity, processes each event independently, and extracts textual and visual features using pre-trained encoders. It models temporal evolution by segmenting events into time windows and uses a trend-aware LSTM to encode narrative progression over time. The model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining to address class imbalance and promote stable learning. Extensive experiments on multiple datasets show that E-CaTCH outperforms existing methods and demonstrates robustness and generalizability in detecting misinformation across different scenarios. <div>
arXiv:2508.11197v1 Announce Type: cross 
Abstract: Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Many existing methods treat posts independently and fail to capture the event-level structure that connects them across time and modality. We propose E-CaTCH, an interpretable and scalable framework for robustly detecting misinformation. If needed, E-CaTCH clusters posts into pseudo-events based on textual similarity and temporal proximity, then processes each event independently. Within each event, textual and visual features are extracted using pre-trained BERT and ResNet encoders, refined via intra-modal self-attention, and aligned through bidirectional cross-modal attention. A soft gating mechanism fuses these representations to form contextualized, content-aware embeddings of each post. To model temporal evolution, E-CaTCH segments events into overlapping time windows and uses a trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode narrative progression over time. Classification is performed at the event level, enabling better alignment with real-world misinformation dynamics. To address class imbalance and promote stable learning, the model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining. The total loss is aggregated across all events. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH consistently outperforms state-of-the-art baselines. Cross-dataset evaluations further demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Invariant Neighborhood Patterns for Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2403.10572</link>
<guid>https://arxiv.org/abs/2403.10572</guid>
<content:encoded><![CDATA[
<div> adaptie neighborhood propagation, invariant neighborhood pattern learning, non-homophilous graphs, distribution shifts, graph neural network<br />
<br />
Summary:<br />
This paper addresses distribution shifts on non-homophilous graphs, where the homophilous assumption doesn't always apply. Existing graph neural network methods may struggle with the diverse distribution shifts of neighborhood patterns on such graphs. The proposed Invariant Neighborhood Pattern Learning (INPL) approach introduces an Adaptive Neighborhood Propagation (ANP) module to capture adaptive neighborhood information and alleviate neighborhood pattern distribution shifts. Additionally, the Invariant Non-Homophilous Graph Learning (INHGL) module helps constrain the ANP and learn invariant graph representation on non-homophilous graphs. Experimental results on real-world non-homophilous graphs demonstrate that the INPL method achieves state-of-the-art performance for learning on large non-homophilous graphs. <br /> <div>
arXiv:2403.10572v2 Announce Type: replace-cross 
Abstract: This paper studies the problem of distribution shifts on non-homophilous graphs Mosting existing graph neural network methods rely on the homophilous assumption that nodes from the same class are more likely to be linked. However, such assumptions of homophily do not always hold in real-world graphs, which leads to more complex distribution shifts unaccounted for in previous methods. The distribution shifts of neighborhood patterns are much more diverse on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern Learning (INPL) to alleviate the distribution shifts problem on non-homophilous graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP) module to capture the adaptive neighborhood information, which could alleviate the neighborhood pattern distribution shifts problem on non-homophilous graphs. We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain the ANP and learn invariant graph representation on non-homophilous graphs. Extensive experimental results on real-world non-homophilous graphs show that INPL could achieve state-of-the-art performance for learning on large non-homophilous graphs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spectral Framework for Evaluating Geodesic Distances Between Graphs</title>
<link>https://arxiv.org/abs/2406.10500</link>
<guid>https://arxiv.org/abs/2406.10500</guid>
<content:encoded><![CDATA[
<div> spectral framework, graph geodesic distance, spectral graph matching, resistance-based spectral graph coarsening, GGD metric <br />
<br />
Summary: 
This paper introduces a spectral framework for measuring the dissimilarity between graph data samples, using a novel metric called Graph Geodesic Distance (GGD). The framework includes spectral graph matching to find node correspondence and compute geodesic distance between graphs, as well as a coarsening scheme for graphs of different sizes. The GGD metric captures differences in spectral properties like resistances, cuts, and mixing time of random walks, and outperforms state-of-the-art metrics like Tree-Mover's Distance (TMD) for graph classification, especially with partial node features. GGD is also applicable for stability analysis of Graph Neural Networks (GNNs) and measuring distances between datasets, demonstrating its versatility in various machine learning applications. <div>
arXiv:2406.10500v3 Announce Type: replace-cross 
Abstract: This paper presents a spectral framework for quantifying the differentiation between graph data samples by introducing a novel metric named Graph Geodesic Distance (GGD). For two different graphs with the same number of nodes, our framework leverages a spectral graph matching procedure to find node correspondence so that the geodesic distance between them can be subsequently computed by solving a generalized eigenvalue problem associated with their Laplacian matrices. For graphs of different sizes, a resistance-based spectral graph coarsening scheme is introduced to reduce the size of the larger graph while preserving the original spectral properties. We show that the proposed GGD metric can effectively quantify dissimilarities between two graphs by encapsulating their differences in key structural (spectral) properties, such as effective resistances between nodes, cuts, and the mixing time of random walks. Through extensive experiments comparing with state-of-the-art metrics, such as the latest Tree-Mover's Distance (TMD), the proposed GGD metric demonstrates significantly improved performance for graph classification, particularly when only partial node features are available. Furthermore, we extend the application of GGD beyond graph classification to stability analysis of GNNs and the quantification of distances between datasets, highlighting its versatility in broader machine learning contexts.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Content and Social Connections of Fake News with Explainable Text and Graph Learning</title>
<link>https://arxiv.org/abs/2508.10040</link>
<guid>https://arxiv.org/abs/2508.10040</guid>
<content:encoded><![CDATA[
<div>  Keywords: misinformation, fact-checking, social media, graph-based features, explainable framework

Summary: 
This paper discusses the challenges of misinformation and the importance of incorporating social media dynamics in fact-checking systems. The proposed framework combines content analysis, social media features, and graph-based features to improve fact-checking accuracy. By integrating a misinformation classifier with explainability techniques, the framework provides interpretable insights to support classification decisions. Experiments on English, Spanish, and Portuguese datasets show that multimodal information enhances performance compared to using single modalities. The framework's explanations are evaluated for interpretability, trustworthiness, and robustness, demonstrating that it generates human-understandable justifications for its predictions. Overall, the framework offers a comprehensive approach to fact-checking that considers the complexities of social media dynamics and provides transparent explanations for its decisions. 

<br /><br />Summary: <div>
arXiv:2508.10040v1 Announce Type: new 
Abstract: The global spread of misinformation and concerns about content trustworthiness have driven the development of automated fact-checking systems. Since false information often exploits social media dynamics such as "likes" and user networks to amplify its reach, effective solutions must go beyond content analysis to incorporate these factors. Moreover, simply labelling content as false can be ineffective or even reinforce biases such as automation and confirmation bias. This paper proposes an explainable framework that combines content, social media, and graph-based features to enhance fact-checking. It integrates a misinformation classifier with explainability techniques to deliver complete and interpretable insights supporting classification decisions. Experiments demonstrate that multimodal information improves performance over single modalities, with evaluations conducted on datasets in English, Spanish, and Portuguese. Additionally, the framework's explanations were assessed for interpretability, trustworthiness, and robustness with a novel protocol, showing that it effectively generates human-understandable justifications for its predictions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABIA: An AI-Powered Tool for Detecting Opioid-Related Behaviors on Social Media</title>
<link>https://arxiv.org/abs/2508.10046</link>
<guid>https://arxiv.org/abs/2508.10046</guid>
<content:encoded><![CDATA[
<div> BERT, social media, opioid misuse, deep learning model, public health<br />
<br />
Summary: 
The study focuses on detecting opioid-related user behavior on social media. It addresses challenges like informal language and coded communication that can obscure opioid misuse detection. The researchers developed a hybrid deep learning model called SABIA, combining BERT and other techniques to classify user behavior into five categories: Dealers, Active Opioid Users, Recovered Users, Prescription Users, and Non-Users. They created a new dataset from Reddit posts and achieved benchmark performance with SABIA, outperforming Logistic Regression by 9.30%. The study showcases the effectiveness of hybrid deep learning models in detecting complex opioid-related behaviors on social media, offering insights for public health monitoring and intervention efforts.<br /><br /> <div>
arXiv:2508.10046v1 Announce Type: new 
Abstract: Social media platforms have become valuable tools for understanding public health challenges by offering insights into patient behaviors, medication use, and mental health issues. However, analyzing such data remains difficult due to the prevalence of informal language, slang, and coded communication, which can obscure the detection of opioid misuse. This study addresses the issue of opioid-related user behavior on social media, including informal expressions, slang terms, and misspelled or coded language. We analyzed the existing Bidirectional Encoder Representations from Transformers (BERT) technique and developed a BERT-BiLSTM-3CNN hybrid deep learning model, named SABIA, to create a single-task classifier that effectively captures the features of the target dataset. The SABIA model demonstrated strong capabilities in capturing semantics and contextual information. The proposed approach includes: (1) data preprocessing, (2) data representation using the SABIA model, (3) a fine-tuning phase, and (4) classification of user behavior into five categories. A new dataset was constructed from Reddit posts, identifying opioid user behaviors across five classes: Dealers, Active Opioid Users, Recovered Users, Prescription Users, and Non-Users, supported by detailed annotation guidelines. Experiments were conducted using supervised learning. Results show that SABIA achieved benchmark performance, outperforming the baseline (Logistic Regression, LR = 0.86) and improving accuracy by 9.30%. Comparisons with seven previous studies confirmed its effectiveness and robustness. This study demonstrates the potential of hybrid deep learning models for detecting complex opioid-related behaviors on social media, supporting public health monitoring and intervention efforts.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization in Multi-layer Social Networks Based on Differentiated Graph Embeddings</title>
<link>https://arxiv.org/abs/2508.10289</link>
<guid>https://arxiv.org/abs/2508.10289</guid>
<content:encoded><![CDATA[
<div> Keywords: influential nodes, social network analysis, graph neural networks, multi-layer networks, influence maximization

Summary:
Inf-MDE is a novel method for identifying influential nodes in social networks. It addresses shortcomings in current techniques by leveraging differentiated graph embeddings and a multi-layer network structure. The model captures self-influence propagation to eliminate representation bias and incorporates an adaptive local influence aggregation mechanism in its graph neural network design. This mechanism adjusts influence feature aggregation based on local context and intensity, allowing for effective capturing of inter-layer propagation heterogeneity and intra-layer diffusion dynamics. Extensive experiments on various multi-layer social network datasets show that Inf-MDE outperforms state-of-the-art methods in influence maximization. <div>
arXiv:2508.10289v1 Announce Type: new 
Abstract: Identifying influential nodes is crucial in social network analysis. Existing methods often neglect local opinion leader tendencies, resulting in overlapping influence ranges for seed nodes. Furthermore, approaches based on vanilla graph neural networks (GNNs) struggle to effectively aggregate influence characteristics during message passing, particularly with varying influence intensities. Current techniques also fail to adequately address the multi-layer nature of social networks and node heterogeneity. To address these issues, this paper proposes Inf-MDE, a novel multi-layer influence maximization method leveraging differentiated graph embedding. Inf-MDE models social relationships using a multi-layer network structure. The model extracts a self-influence propagation subgraph to eliminate the representation bias between node embeddings and propagation dynamics. Additionally, Inf-MDE incorporates an adaptive local influence aggregation mechanism within its GNN design. This mechanism dynamically adjusts influence feature aggregation during message passing based on local context and influence intensity, enabling it to effectively capture both inter-layer propagation heterogeneity and intra-layer diffusion dynamics. Extensive experiments across four distinct multi-layer social network datasets demonstrate that Inf-MDE significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Homogeneity Can Emerge Without Filtering Algorithms or Homophily Preferences</title>
<link>https://arxiv.org/abs/2508.10466</link>
<guid>https://arxiv.org/abs/2508.10466</guid>
<content:encoded><![CDATA[
<div> Keywords: online environments, echo chambers, filter bubbles, agent-based model, polarization

Summary: 
This study challenges the conventional belief that online echo chambers and filter bubbles are solely created by algorithmic curation or user preferences. Through an agent-based model inspired by Schelling's model of segregation, it demonstrates that homogeneity can emerge without these factors. Weak individual preferences and group-based interactions can lead to feedback loops that drive communities towards segregation. Once a slight imbalance forms, cascades of user exits and regrouping can intensify homogeneity. Surprisingly, algorithmic filtering, often blamed for filter bubbles, can actually help maintain diversity by stabilizing mixed communities. The research sheds light on online polarization as an emergent system-level dynamic, emphasizing the importance of applying a complexity lens to the study of digital public spheres. <div>
arXiv:2508.10466v1 Announce Type: new 
Abstract: Ideologically homogeneous online environments - often described as "echo chambers" or "filter bubbles" - are widely seen as drivers of polarization, radicalization, and misinformation. A central debate asks whether such homophily stems primarily from algorithmic curation or users' preference for like-minded peers. This study challenges that view by showing that homogeneity can emerge in the absence of both filtering algorithms and user preferences. Using an agent-based model inspired by Schelling's model of residential segregation, we demonstrate that weak individual preferences, combined with simple group-based interaction structures, can trigger feedback loops that drive communities toward segregation. Once a small imbalance forms, cascades of user exits and regrouping amplify homogeneity across the system. Counterintuitively, algorithmic filtering - often blamed for "filter bubbles" - can in fact sustain diversity by stabilizing mixed communities. These findings highlight online polarization as an emergent system-level dynamic and underscore the importance of applying a complexity lens to the study of digital public spheres.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Integration of Multi-View Attributed Graphs for Clustering and Embedding</title>
<link>https://arxiv.org/abs/2508.09452</link>
<guid>https://arxiv.org/abs/2508.09452</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view attributed graph, Laplacian aggregation, clustering, embedding, efficiency

Summary: 
The paper introduces a new approach called spectrum-guided Laplacian aggregation for clustering and embedding tasks in multi-view attributed graphs (MVAGs). By integrating all views of the MVAG into a Laplacian matrix, classic graph algorithms can be effectively applied for superior performance. The method combines eigengap and connectivity objectives in an integrated objective formulation to ensure spectral properties align with community and connectivity properties. Two algorithms, SGLA and SGLA+, are introduced to address the computational complexity of the optimization problem. SGLA+ further enhances efficiency using sampling and approximation techniques. Experimental results on 8 MVAG datasets demonstrate the superior performance and efficiency of SGLA and SGLA+ compared to existing methods, with significant speed improvements. This novel approach holds promise for various applications like recommendation systems, anomaly detection, and social network analysis. 

<br /><br />Summary: <div>
arXiv:2508.09452v1 Announce Type: new 
Abstract: A multi-view attributed graph (MVAG) G captures the diverse relationships and properties of real-world entities through multiple graph views and attribute views. Effectively utilizing all views in G is essential for MVAG clustering and embedding, which are important for applications like recommendation systems, anomaly detection, social network analysis, etc. Existing methods either achieve inferior result quality or incur significant computational costs to handle large-scale MVAGs.
  In this paper, we present a spectrum-guided Laplacian aggregation scheme with an effective objective formulation and two efficient algorithms SGLA and SGLA+, to cohesively integrate all views of G into an MVAG Laplacian matrix, which readily enables classic graph algorithms to handle G with superior performance in clustering and embedding tasks. We begin by conducting a theoretical analysis to design an integrated objective that consists of two components, the eigengap and connectivity objectives, aiming to link the spectral properties of the aggregated MVAG Laplacian with the underlying community and connectivity properties of G. A constrained optimization problem is then formulated for the integration, which is computationally expensive to solve. Thus, we first develop the SGLA algorithm, which already achieves excellent performance compared with existing methods. To further enhance efficiency, we design SGLA+ to reduce the number of costly objective evaluations via sampling and approximation to quickly find an approximate optimum. Extensive experiments compare our methods against 12 baselines for clustering and 8 baselines for embedding on 8 multi-view attributed graphs, validating the superior performance of SGLA and SGLA+ in terms of result quality and efficiency. Compared with the most effective baselines, our methods are significantly faster, often by up to orders of magnitude.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS-Agent: LLM-based Community Search via Dual-agent Collaboration</title>
<link>https://arxiv.org/abs/2508.09549</link>
<guid>https://arxiv.org/abs/2508.09549</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Graph Structure Analysis, Community Search, CS-Agent, Benchmark

Summary: 
Large Language Models (LLMs) have shown promise in natural language processing but have not been extensively applied to graph structure analysis, specifically in community search tasks. The proposed GraphCS benchmark evaluates LLMs in community search, revealing limitations such as output bias and lack of meaningful results. To mitigate these issues, the CS-Agent framework is introduced, utilizing two LLMs as Solver and Validator in a collaborative manner. CS-Agent refines initial results iteratively without additional training, with the Decider module selecting the optimal community. Experimental results demonstrate the effectiveness of CS-Agent in improving the quality and stability of identified communities compared to baseline methods. This work represents a novel application of LLMs to community search, offering a robust and adaptive solution for real-world graph analysis tasks.

<br /><br />Summary: <div>
arXiv:2508.09549v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet their application to graph structure analysis, particularly in community search, remains underexplored. Community search, a fundamental task in graph analysis, aims to identify groups of nodes with dense interconnections, which is crucial for understanding the macroscopic structure of graphs. In this paper, we propose GraphCS, a comprehensive benchmark designed to evaluate the performance of LLMs in community search tasks. Our experiments reveal that while LLMs exhibit preliminary potential, they frequently fail to return meaningful results and suffer from output bias. To address these limitations, we introduce CS-Agent, a dual-agent collaborative framework to enhance LLM-based community search. CS-Agent leverages the complementary strengths of two LLMs acting as Solver and Validator. Through iterative feedback and refinement, CS-Agent dynamically refines initial results without fine-tuning or additional training. After the multi-round dialogue, Decider module selects the optimal community. Extensive experiments demonstrate that CS-Agent significantly improves the quality and stability of identified communities compared to baseline methods. To our knowledge, this is the first work to apply LLMs to community search, bridging the gap between LLMs and graph analysis while providing a robust and adaptive solution for real-world applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</title>
<link>https://arxiv.org/abs/2508.09665</link>
<guid>https://arxiv.org/abs/2508.09665</guid>
<content:encoded><![CDATA[
<div> cloning, identity detection, social-sensor cloud, deep forest model, cryptography<br />
<br />
Summary:
This research addresses the issue of identity cloning in social-sensor cloud service providers. The proposed method comprises two main components: a similar identity detection approach and a cryptography-based authentication protocol. A weakly supervised deep forest model is used to identify similar identities based on non-privacy-sensitive user profile features. Additionally, a cryptography-based authentication protocol is developed to verify whether similar identities are from the same provider. Experiments conducted on a large real-world dataset demonstrate the effectiveness and superior performance of the proposed technique compared to existing methods for detecting identity clones. <div>
arXiv:2508.09665v1 Announce Type: cross 
Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Human Values in Online Communities</title>
<link>https://arxiv.org/abs/2402.14177</link>
<guid>https://arxiv.org/abs/2402.14177</guid>
<content:encoded><![CDATA[
<div> Reddit, human values, computational analysis, online communities, Schwartz values <br />
<br />
Summary: 
The study focuses on analyzing human values on Reddit to understand society's preferences and behaviors. The researchers propose a method to computationally analyze values on the platform, allowing for large-scale analysis complementing survey-based approaches. They train classifiers to annotate over six million posts across 12k subreddits with Schwartz values. The analysis reveals insights into the values prevalent in various online communities, such as a negative stance towards conformity in certain subreddits. The study also highlights the correlation between traditional values and conservative U.S. states in geographically specific subreddits. The dataset and method developed can serve as a valuable tool for qualitative studies of online communication. <br /><br /> <div>
arXiv:2402.14177v4 Announce Type: replace 
Abstract: Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over six million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, we discover a very negative stance towards conformity in the Vegan and AbolishTheMonarchy subreddits. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states. Through our work, we demonstrate how our dataset and method can be used as a complementary tool for qualitative study of online communication.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catch Me If You Can: Finding the Source of Infections in Temporal Networks</title>
<link>https://arxiv.org/abs/2412.10877</link>
<guid>https://arxiv.org/abs/2412.10877</guid>
<content:encoded><![CDATA[
<div> detection, spreading process, temporal graphs, SIR model, algorithms<br />
<br />
Summary:
Source detection (SD) is crucial for identifying the origin of spreading processes in networks. Previous work in this area has assumed a static network structure, which may not be realistic in many scenarios such as disease spread. To address this limitation, a model of SD on temporal graphs is proposed, where links between nodes exist only at certain time steps. This extends the traditional SD framework to account for dynamic network structures. The study utilizes the SIR model for spreading processes and provides algorithms and lower bounds for the SD problem in various settings, including consistent or dynamic source behavior on general graphs and trees. This work fills a gap in the existing literature by formalizing SD on temporal graphs and offering insights into source identification in time-varying networks. <br /><br /> <div>
arXiv:2412.10877v2 Announce Type: replace-cross 
Abstract: Source detection (SD) is the task of finding the origin of a spreading process in a network. Algorithms for SD help us combat diseases, misinformation, pollution, and more, and have been studied by physicians, physicists, sociologists, and computer scientists. The field has received considerable attention and been analyzed in many settings (e.g., under different models of spreading processes), yet all previous work shares the same assumption that the network the spreading process takes place in has the same structure at every point in time. For example, if we consider how a disease spreads through a population, it is unrealistic to assume that two people can either never or at every time infect each other, rather such an infection is possible precisely when they meet. Therefore, we propose an extended model of SD based on temporal graphs, where each link between two nodes is only present at some time step. Temporal graphs have become a standard model of time-varying graphs, and, recently, researchers have begun to study infection problems (such as influence maximization) on temporal graphs (arXiv:2303.11703, [Gayraud et al., 2015]). We give the first formalization of SD on temporal graphs. For this, we employ the standard SIR model of spreading processes ([Hethcote, 1989]). We give both lower bounds and algorithms for the SD problem in a number of different settings, such as with consistent or dynamic source behavior and on general graphs as well as on trees.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Conversational Structure and Style Shape Online Community Experiences</title>
<link>https://arxiv.org/abs/2508.08596</link>
<guid>https://arxiv.org/abs/2508.08596</guid>
<content:encoded><![CDATA[
<div> Keywords: Sense of Virtual Community, online communities, social interaction, Reddit, linguistic style

Summary: 
This study investigates the relationship between social interactions and Sense of Virtual Community (SOVC) in online communities, specifically focusing on Reddit. By analyzing conversational structure and linguistic style of 2,826 Reddit users across 281 subreddits, the study identifies patterns such as reciprocal reply chains and use of prosocial language that predict stronger community attachment. Three primary dimensions of SOVC within Reddit are identified: Membership & Belonging, Cooperation & Shared Values, and Connection & Influence. The findings provide valuable insights for fostering stronger community bonds and designing features to enhance the online community experience. This research contributes to the understanding of online communities and offers practical strategies for maximizing the positive impacts of online community participation. <div>
arXiv:2508.08596v1 Announce Type: new 
Abstract: Sense of Community (SOC) is vital to individual and collective well-being. Although social interactions have moved increasingly online, still little is known about the specific relationships between the nature of these interactions and Sense of Virtual Community (SOVC). This study addresses this gap by exploring how conversational structure and linguistic style predict SOVC in online communities, using a large-scale survey of 2,826 Reddit users across 281 varied subreddits. We develop a hierarchical model to predict self-reported SOVC based on automatically quantifiable and highly generalizable features that are agnostic to community topic and that describe both individual users and entire communities. We identify specific interaction patterns (e.g., reciprocal reply chains, use of prosocial language) associated with stronger communities and identify three primary dimensions of SOVC within Reddit -- Membership & Belonging, Cooperation & Shared Values, and Connection & Influence. This study provides the first quantitative evidence linking patterns of social interaction to SOVC and highlights actionable strategies for fostering stronger community attachment, using an approach that can generalize readily across community topics, languages, and platforms. These insights offer theoretical implications for the study of online communities and practical suggestions for the design of features to help more individuals experience the positive benefits of online community participation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Efficient Attributed Hypergraph Embedding on Nodes and Hyperedges</title>
<link>https://arxiv.org/abs/2508.08807</link>
<guid>https://arxiv.org/abs/2508.08807</guid>
<content:encoded><![CDATA[
<div> hypergraph, attributed hypergraph, embedding, node, hyperedge

Summary:
SAHE is proposed as an efficient and effective approach for computing node and hyperedge embeddings in attributed hypergraphs. The approach introduces higher-order similarity measures, HMS-N and HMS-E, to capture similarities between node pairs and hyperedge pairs, respectively. SAHE formulates the embedding objective to jointly preserve all-pair HMS-N and HMS-E similarities. The approach also includes optimizations to improve efficiency and avoid the computational expense of direct optimization. Extensive experiments on diverse attributed hypergraphs demonstrate that SAHE outperforms existing methods in embedding quality and is significantly faster in computation. <div>
arXiv:2508.08807v1 Announce Type: new 
Abstract: An attributed hypergraph comprises nodes with attributes and hyperedges that connect varying numbers of nodes. Attributed hypergraph node and hyperedge embedding (AHNEE) maps nodes and hyperedges to compact vectors for use in important tasks such as node classification, hyperedge link prediction, and hyperedge classification. Generating high-quality embeddings is challenging due to the complexity of attributed hypergraphs and the need to embed both nodes and hyperedges, especially in large-scale data. Existing solutions often fall short by focusing only on nodes or lacking native support for attributed hypergraphs, leading to inferior quality, and struggle with scalability on large attributed hypergraphs.
  We propose SAHE, an efficient and effective approach that unifies node and hyperedge embeddings for AHNEE computation, advancing the state of the art via comprehensive embedding formulations and algorithmic designs. First, we introduce two higher-order similarity measures, HMS-N and HMS-E, to capture similarities between node pairs and hyperedge pairs, respectively. These measures consider multi-hop connections and global topology within an extended hypergraph that incorporates attribute-based hyperedges. SAHE formulates the AHNEE objective to jointly preserve all-pair HMS-N and HMS-N similarities. Direct optimization is computationally expensive, so we analyze and unify core approximations of all-pair HMS-N and HMS-N to solve them simultaneously. To enhance efficiency, we design several non-trivial optimizations that avoid iteratively materializing large dense matrices while maintaining high-quality results. Extensive experiments on diverse attributed hypergraphs and 3 downstream tasks, compared against 11 baselines, show that SAHE consistently outperforms existing methods in embedding quality and is up to orders of magnitude faster.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents</title>
<link>https://arxiv.org/abs/2508.08837</link>
<guid>https://arxiv.org/abs/2508.08837</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, opinion evolution, bias, media exposure, cross-cultural tolerance

Summary:<br /><br />The article introduces a novel framework utilizing Language Model Machines (LLMs) to model the evolution of attitudes towards China among US citizens over a two-decade period. By incorporating media data collection, user profile creation, and cognitive architecture, the framework successfully replicates the real trend of US attitudes towards China. The inclusion of debiased media exposure and a devil's advocate agent sheds light on the factors influencing opinion formation and the rare reversal from negative to positive attitudes. The simulation results highlight the impact of biased framing and selection bias on attitude shaping. This work contributes to the advancement of LLM-based modeling of cognitive behaviors in a large-scale, cross-border social context, providing insights into international biases and offering valuable implications for media consumers to understand the factors influencing their perspectives. Ultimately, the study aims to contribute to bias reduction and promote cross-cultural tolerance in society. <div>
arXiv:2508.08837v1 Announce Type: new 
Abstract: The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another -- US citizens' perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMs' capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks</title>
<link>https://arxiv.org/abs/2508.09043</link>
<guid>https://arxiv.org/abs/2508.09043</guid>
<content:encoded><![CDATA[
<div> Keywords: faculty hiring networks, GIScience, placement patterns, global knowledge dissemination, hiring practices

Summary: 
The study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping connections between PhD-granting institutions and current affiliations. The dataset, though volunteer-contributed, provides valuable insight into global GIScience faculty placement. Hiring is concentrated in western countries with influential programs identified. The analysis reveals significant internal retention at continental and country levels, with a high non-self-hired ratio at the institutional level. Research themes have evolved over time, with emphasis on spatial data analytics, cartography, geovisualization, geocomputation, and environmental sciences. These results shed light on hiring practices' impact on global knowledge dissemination and promote academic equity within GIScience and Geography.<br /><br />Summary: <div>
arXiv:2508.09043v1 Announce Type: cross 
Abstract: Academia is profoundly influenced by faculty hiring networks, which serve as critical conduits for knowledge dissemination and the formation of collaborative research initiatives. While extensive research in various disciplines has revealed the institutional hierarchies inherent in these networks, their impacts within GIScience remain underexplored. To fill this gap, this study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping the connections between PhD-granting institutions and current faculty affiliations. Our dataset, which is compiled from volunteer-contributed information, is the most comprehensive collection available in this field. While there may be some limitations in its representativeness, its scope and depth provide a unique and valuable perspective on the global placement patterns of GIScience faculty. Our analysis reveals several influential programs in placing GIScience faculty, with hiring concentrated in the western countries. We examined the diversity index to assess the representation of regions and institutions within the global GIScience faculty network. We observe significant internal retention at both the continental and country levels, and a high level of non-self-hired ratio at the institutional level. Over time, research themes have also evolved, with growing research clusters emphasis on spatial data analytics, cartography and geovisualization, geocomputation, and environmental sciences, etc. These results illuminate the influence of hiring practices on global knowledge dissemination and contribute to promoting academic equity within GIScience and Geography.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning optimizes predictions of missing links in real-world networks</title>
<link>https://arxiv.org/abs/2508.09069</link>
<guid>https://arxiv.org/abs/2508.09069</guid>
<content:encoded><![CDATA[
<div> Benchmark, Link prediction, Model stacking, Graph neural networks, Meta-learning <br />
<br />
Summary: 
Relational data often have missing links, making predicting these links crucial in various applications. This study compares different algorithms for link prediction on a diverse set of 550 real-world networks. The research shows that no single algorithm performs best on all networks, with model stacking using a random forest outperforming or competing with graph neural networks. Additionally, the performance of algorithms varies based on network characteristics such as degree distribution and triangle density. A meta-learning algorithm is introduced to optimize link predictions by selecting the best algorithm for individual networks, outperforming all existing methods and scaling effectively for large networks. Overall, model stacking with a random forest is scalable and shows promising results in predicting missing links, with the meta-learning approach proving to be the most effective method for optimizing link predictions. <div>
arXiv:2508.09069v1 Announce Type: cross 
Abstract: Relational data are ubiquitous in real-world data applications, e.g., in social network analysis or biological modeling, but networks are nearly always incompletely observed. The state-of-the-art for predicting missing links in the hard case of a network without node attributes uses model stacking or neural network techniques. It remains unknown which approach is best, and whether or how the best choice of algorithm depends on the input network's characteristics. We answer these questions systematically using a large, structurally diverse benchmark of 550 real-world networks under two standard accuracy measures (AUC and Top-k), comparing four stacking algorithms with 42 topological link predictors, two of which we introduce here, and two graph neural network algorithms. We show that no algorithm is best across all input networks, all algorithms perform well on most social networks, and few perform well on economic and biological networks. Overall, model stacking with a random forest is both highly scalable and surpasses on AUC or is competitive with graph neural networks on Top-k accuracy. But, algorithm performance depends strongly on network characteristics like the degree distribution, triangle density, and degree assortativity. We introduce a meta-learning algorithm that exploits this variability to optimize link predictions for individual networks by selecting the best algorithm to apply, which we show outperforms all state-of-the-art algorithms and scales to large networks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Vertex-Attribute-Constrained Densest $k$-Subgraph Problem</title>
<link>https://arxiv.org/abs/2508.06655</link>
<guid>https://arxiv.org/abs/2508.06655</guid>
<content:encoded><![CDATA[
<div> Attribute values, Densest k-Subgraph, Vertex-Attribute-Constrained Densest k-Subgraph, NP-hardness, Frank-Wolfe algorithm <br />
Summary: <br />
- Dense subgraph mining is crucial in various applications such as fraud detection and community detection.
- A new problem variant, VAC-DkS, incorporates vertex attribute values for more meaningful results.
- VAC-DkS retains NP-hardness and inapproximability but can be efficiently solved using a continuous relaxation approach and the Frank-Wolfe algorithm.
- The optimized landscape of VAC-DkS is analyzed, showing its effectiveness in large graph applications.
- In a political network mining scenario, VAC-DkS identifies a balanced set of politicians representing different ideologies, unlike classical DkS, which yields unbalanced results. <div>
arXiv:2508.06655v1 Announce Type: new 
Abstract: Dense subgraph mining is a fundamental technique in graph mining, commonly applied in fraud detection, community detection, product recommendation, and document summarization. In such applications, we are often interested in identifying communities, recommendations, or summaries that reflect different constituencies, styles or genres, and points of view. For this task, we introduce a new variant of the Densest $k$-Subgraph (D$k$S) problem that incorporates the attribute values of vertices. The proposed Vertex-Attribute-Constrained Densest $k$-Subgraph (VAC-D$k$S) problem retains the NP-hardness and inapproximability properties of the classical D$k$S. Nevertheless, we prove that a suitable continuous relaxation of VAC-D$k$S is tight and can be efficiently tackled using a projection-free Frank--Wolfe algorithm. We also present an insightful analysis of the optimization landscape of the relaxed problem. Extensive experimental results demonstrate the effectiveness of our proposed formulation and algorithm, and its ability to scale up to large graphs. We further elucidate the properties of VAC-D$k$S versus classical D$k$S in a political network mining application, where VAC-D$k$S identifies a balanced and more meaningful set of politicians representing different ideological camps, in contrast to the classical D$k$S solution which is unbalanced and rather mundane.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face</title>
<link>https://arxiv.org/abs/2508.06811</link>
<guid>https://arxiv.org/abs/2508.06811</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, artificial intelligence, model fine-tuning, model family trees, genetic similarity<br />
<br />
Summary: 
This paper examines the development and deployment of generative machine learning and artificial intelligence models, focusing on the fine-tuning process. Analyzing 1.86 million models on Hugging Face, the study explores the structure of model family trees and genetic similarities among models. The findings indicate that models within the same family exhibit more genetic resemblance. However, mutations occur rapidly and in a directed manner, leading sibling models to show more similarity than parent/child pairs. The analysis also reveals insights into the evolution of models within the machine learning ecosystem, such as shifts in licenses and language compatibility, as well as changes in model card lengths and standardization. Overall, this work offers a novel perspective on model fine-tuning and suggests that ecological models and methods can provide valuable scientific insights. <br /><br />Summary: <div>
arXiv:2508.06811v1 Announce Type: new 
Abstract: Many have observed that the development and deployment of generative machine learning (ML) and artificial intelligence (AI) models follow a distinctive pattern in which pre-trained models are adapted and fine-tuned for specific downstream tasks. However, there is limited empirical work that examines the structure of these interactions. This paper analyzes 1.86 million models on Hugging Face, a leading peer production platform for model development. Our study of model family trees -- networks that connect fine-tuned models to their base or parent -- reveals sprawling fine-tuning lineages that vary widely in size and structure. Using an evolutionary biology lens to study ML models, we use model metadata and model cards to measure the genetic similarity and mutation of traits over model families. We find that models tend to exhibit a family resemblance, meaning their genetic markers and traits exhibit more overlap when they belong to the same model family. However, these similarities depart in certain ways from standard models of asexual reproduction, because mutations are fast and directed, such that two `sibling' models tend to exhibit more similarity than parent/child pairs. Further analysis of the directional drifts of these mutations reveals qualitative insights about the open machine learning ecosystem: Licenses counter-intuitively drift from restrictive, commercial licenses towards permissive or copyleft licenses, often in violation of upstream license's terms; models evolve from multi-lingual compatibility towards english-only compatibility; and model cards reduce in length and standardize by turning, more often, to templates and automatically generated text. Overall, this work takes a step toward an empirically grounded understanding of model fine-tuning and suggests that ecological models and methods can yield novel scientific insights.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.07201</link>
<guid>https://arxiv.org/abs/2508.07201</guid>
<content:encoded><![CDATA[
<div> Graph-based models, especially those focused on rumor detection on social media, assume rumor propagation trees have deep structures and learn sequential stance features along branches. However, research shows that most nodes in these trees are shallow 1-level replies, implying wide structures rather than deep ones. To address this, the Rumor Adaptive Graph Contrastive Learning (RAGCL) method is proposed, which uses adaptive view augmentation guided by node centralities. The method follows three principles for RPT augmentation: exempting root nodes, retaining deep reply nodes, and preserving lower-level nodes in deep sections. By leveraging node dropping, attribute masking, and edge dropping based on centrality scores, RAGCL generates views to learn robust rumor representations. Experimental results on benchmark datasets demonstrate that RAGCL outperforms existing methods, suggesting its effectiveness in tailored rumor detection for wide-structure RPTs. The proposed principles and augmentation techniques could have broader applications in tree-structured graph analysis.<br /><br />Keywords: rumor detection, social media, graph-based models, contrastive learning, wide-structure RPTs <br /><br />Summary: Rumor Adaptive Graph Contrastive Learning (RAGCL) is introduced to address the wide structures of rumor propagation trees (RPTs) on social media. By using centrality-guided adaptive view augmentation and following key principles for RPT augmentation, RAGCL outperforms existing methods in rumor detection. The approach sheds light on the nature of RPTs and demonstrates the potential for broader applications in tree-structured graph analysis. <div>
arXiv:2508.07201v1 Announce Type: new 
Abstract: Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.07205</link>
<guid>https://arxiv.org/abs/2508.07205</guid>
<content:encoded><![CDATA[
<div> Anomaly Detection, Graph Supervised Contrastive Learning, Rumor Detection, Imbalanced Data, Social Media<br />
<br />
Summary: <br />
The study discusses the challenges of rumor detection in social media, where imbalanced data distribution poses a problem for traditional classification methods. By analyzing datasets from Weibo and Twitter, it is observed that rumors are concentrated in news domains, while non-rumors are mainly in entertainment domains, suggesting a shift towards anomaly detection for rumor identification. The proposed Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL) leverages graph contrastive learning to address the imbalanced data issue and heuristically treats unlabeled data as non-rumors in the detection process. The framework demonstrates superior performance in various conditions, including class-balanced, imbalanced, and few-shot scenarios. This approach provides valuable insights for improving real-world rumor detection methodologies on social media platforms with imbalanced data distributions. <div>
arXiv:2508.07205v1 Announce Type: new 
Abstract: Current rumor detection methods based on propagation structure learning predominately treat rumor detection as a class-balanced classification task on limited labeled data. However, real-world social media data exhibits an imbalanced distribution with a minority of rumors among massive regular posts. To address the data scarcity and imbalance issues, we construct two large-scale conversation datasets from Weibo and Twitter and analyze the domain distributions. We find obvious differences between rumor and non-rumor distributions, with non-rumors mostly in entertainment domains while rumors concentrate in news, indicating the conformity of rumor detection to an anomaly detection paradigm. Correspondingly, we propose the Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats unlabeled data as non-rumors and adapts graph contrastive learning for rumor detection. Extensive experiments demonstrate AD-GSCL's superiority under class-balanced, imbalanced, and few-shot conditions. Our findings provide valuable insights for real-world rumor detection featuring imbalanced data distributions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning</title>
<link>https://arxiv.org/abs/2508.07264</link>
<guid>https://arxiv.org/abs/2508.07264</guid>
<content:encoded><![CDATA[
<div> fusion, multimodal, classification, robustness, scalability

Summary:
FLUID proposes a token-level pipeline for multimodal classification, enhancing robust integration of visual and textual signals. It introduces Q-transforms to distill salient token-level features, a two-stage fusion scheme for cross-modal consistency and adaptive fusion, and a Mixture-of-Experts for efficient specialization. FLUID achieves 91% accuracy on the GLAMI-1M benchmark, surpassing prior baselines and demonstrating resilience to noise, class imbalance, and semantic heterogeneity. Ablation studies validate the effectiveness of individual components and their synergistic benefits, positioning FLUID as a scalable, noise-resilient solution for multimodal product classification.<br /><br />Summary: <div>
arXiv:2508.07264v1 Announce Type: new 
Abstract: Multimodal classification requires robust integration of visual and textual signals, yet common fusion strategies are brittle and vulnerable to modality-specific noise. In this paper, we present \textsc{FLUID}-Flow-Latent Unified Integration via Token Distillation for Expert Specialization, a principled token-level pipeline that improves cross-modal robustness and scalability. \textsc{FLUID} contributes three core elements: (1) \emph{Q-transforms}, learnable query tokens that distill and retain salient token-level features from modality-specific backbones; (2) a two-stage fusion scheme that enforces cross-modal consistency via contrastive alignment and then performs adaptive, task-aware fusion through a gating mechanism and a \emph{Q-bottleneck} that selectively compresses information for downstream reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at prediction time that enables efficient specialization to diverse semantic patterns. Extensive experiments demonstrate that \textsc{FLUID} attains \(91\%\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior baselines and exhibiting strong resilience to label noise, long-tail class imbalance, and semantic heterogeneity. Targeted ablation studies corroborate both the individual and synergistic benefits of the proposed components, positioning \textsc{FLUID} as a scalable, noise-resilient solution for multimodal product classification.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering link-weight structure in complex networks with weight-aware random walks</title>
<link>https://arxiv.org/abs/2508.07489</link>
<guid>https://arxiv.org/abs/2508.07489</guid>
<content:encoded><![CDATA[
<div> Keywords: edge weights, node embeddings, random walk strategies, network models, real-world networks

Summary: 
This paper explores the impact of different random walk strategies on preserving edge weight information in low-dimensional node embeddings. The study compares traditional unweighted, strength-based, and fully weight-aware random walks using network models, real-world graphs, and networks with low-weight edge removal. Results show that weight-aware random walks outperform other strategies, achieving high correlations with original edge weights in network models. However, performance in real-world networks varies based on topology and weight distribution. Removing weak edges through thresholding can initially improve correlation by reducing noise but excessive pruning degrades representation quality. Overall, using a weight-aware random walk is the most effective approach for preserving node weight information in embeddings, but the solution may not be universally applicable. <div>
arXiv:2508.07489v1 Announce Type: new 
Abstract: Using edge weights is essential for modeling real-world systems where links possess relevant information, and preserving this information in low-dimensional representations is relevant for classification and prediction tasks. This paper systematically investigates how different random walk strategies - traditional unweighted, strength-based, and fully weight-aware - keeps edge weight information when generating node embeddings. Using network models, real-world graphs, and networks subjected to low-weight edge removal, we measured the correlation between original edge weights and the similarity of node pairs in the embedding space generated by random walk strategies. Our results consistently showed that weight-aware random walks significantly outperform other strategies, achieving correlations above 0.90 in network models. However, performance in real-world networks was more heterogeneous, influenced by factors like topology and weight distribution. Our analysis also revealed that removing weak edges via thresholding can initially improve correlation by reducing noise, but excessive pruning degrades representation quality. Our findings suggest that simply using a weight-aware random walk is generally the best approach for preserving node weight information in embeddings, but it is not a universal solution.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women</title>
<link>https://arxiv.org/abs/2508.07579</link>
<guid>https://arxiv.org/abs/2508.07579</guid>
<content:encoded><![CDATA[
<div> Keywords: Hashtags, queer communities, cross-cultural communication, social media, content analysis

Summary:
The study explores the rise of the #wlw hashtag in the Chinese lesbian community on RedNote following user migration triggered by the temporary US TikTok ban. Through a two-phase content analysis of 418 #wlw posts, the research examines the usage patterns during the hashtag's introduction and diffusion. The successful introduction of #wlw was facilitated by bold importation by TikTok immigrants, mutual interpretation between different populations, and discussions among RedNote natives. The hashtag has become recognized on RedNote as a tool for sharing queer life and has expanded to support feminism discourse. This case study provides empirical insights into cross-cultural communication among marginalized communities on social media platforms. <div>
arXiv:2508.07579v1 Announce Type: new 
Abstract: Hashtags serve as identity markers and connection tools in online queer communities. Recently, the Western-origin #wlw (women-loving-women) hashtag has risen in the Chinese lesbian community on RedNote, coinciding with user migration triggered by the temporary US TikTok ban. This event provides a unique lens to study cross-cultural hashtag ingress and diffusion through the populations' responsive behaviors in cyber-migration. In this paper, we conducted a two-phase content analysis of 418 #wlw posts from January and April, examining different usage patterns during the hashtag's ingress and diffusion. Results indicate that the successful introduction of #wlw was facilitated by TikTok immigrants' bold importation, both populations' mutual interpretation, and RedNote natives' discussions. In current manifestation of diffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer life, and semantically expands to support feminism discourse. Our findings provide empirical insights for enhancing the marginalized communities' cross-cultural communication.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fabricating Holiness: Characterizing Religious Misinformation Circulators on Arabic Social Media</title>
<link>https://arxiv.org/abs/2508.07845</link>
<guid>https://arxiv.org/abs/2508.07845</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, social media, fabricated Hadith, religious content, quantitative analysis

Summary: 
This study focuses on the spread of fabricated Hadith, or quotes attributed to Prophet Muhammad, on Arabic social media as a form of religious misinformation. The researchers use quantitative methods to analyze the characteristics of social media users who interact with fabricated Hadith. They identify two main groups: circulators who share fabricated Hadith and debunkers who challenge them. The study reveals that circulators are linked to Shia and Sunni Islamic accounts, while debunkers follow academic Islamic scholars and have diverse non-religious interests. Logistic Regression is used to predict user behaviors, providing insights into the differences between the two groups. The findings suggest that addressing religious misinformation on social media requires understanding the dynamics of different user groups and their interactions with fabricated content.<br /><br />Summary: <div>
arXiv:2508.07845v1 Announce Type: new 
Abstract: Misinformation is a growing concern in a decade involving critical global events. While social media regulation is mainly dedicated towards the detection and prevention of fake news and political misinformation, there is limited research about religious misinformation which has only been addressed through qualitative approaches. In this work, we study the spread of fabricated quotes (Hadith) that are claimed to belong to Prophet Muhammad (the prophet of Islam) as a case study demonstrating one of the most common religious misinformation forms on Arabic social media. We attempt through quantitative methods to understand the characteristics of social media users who interact with fabricated Hadith. We spotted users who frequently circulate fabricated Hadith and others who frequently debunk it to understand the main differences between the two groups. We used Logistic Regression to automatically predict their behaviors and analyzed its weights to gain insights about the characteristics and interests of each group. We find that both fabricated Hadith circulators and debunkers have generally a lot of ties to religious accounts. However, circulators are identified by many accounts that follow the Shia branch of Islam, Sunni Islamic public figures from the gulf countries, and many Sunni non-professional pages posting Islamic content. On the other hand, debunkers are identified by following academic Islamic scholars from multiple countries and by having more intellectual non-religious interests like charity, politics, and activism.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Network Games: $\alpha$-Potential Function and Learning</title>
<link>https://arxiv.org/abs/2508.06619</link>
<guid>https://arxiv.org/abs/2508.06619</guid>
<content:encoded><![CDATA[
<div> network games, asymmetric networks, alpha-potential games, Nash equilibrium, social welfare <br />
Summary: This paper examines static network games in asymmetric networks using alpha-potential games. It introduces the concept of the alpha-potential function to analyze convergence to 2alpha-Nash equilibrium using specific algorithms. The study focuses on linear-quadratic network games and discusses the behavior of alpha in relation to network asymmetry. Bounds on social welfare at the alpha-Nash equilibrium are determined. Numerical simulations demonstrate the convergence of proposed algorithms and properties of learned 2alpha-Nash equilibria. <div>
arXiv:2508.06619v1 Announce Type: cross 
Abstract: In a network game, players interact over a network and the utility of each player depends on his own action and on an aggregate of his neighbours' actions. Many real world networks of interest are asymmetric and involve a large number of heterogeneous players. This paper analyzes static network games using the framework of $\alpha$-potential games. Under mild assumptions on the action sets (compact intervals) and the utility functions (twice continuously differentiable) of the players, we derive an expression for an inexact potential function of the game, called the $\alpha$-potential function. Using such a function, we show that modified versions of the sequential best-response algorithm and the simultaneous gradient play algorithm achieve convergence of players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network games, we show that $\alpha$ depends on the maximum asymmetry in the network and is well-behaved for a wide range of networks of practical interest. Further, we derive bounds on the social welfare of the $\alpha$-Nash equilibrium corresponding to the maximum of the $\alpha$-potential function, under suitable assumptions. We numerically illustrate the convergence of the proposed algorithms and properties of the learned $2\alpha$-Nash equilibria.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model</title>
<link>https://arxiv.org/abs/2508.07209</link>
<guid>https://arxiv.org/abs/2508.07209</guid>
<content:encoded><![CDATA[
<div> Keywords: Pretrained Language Models, social media, rumor detection, Post Engagement Prediction, TwitterCorpus

Summary: 
The study focuses on enhancing the performance of Pretrained Language Models (PLMs) in social media tasks, particularly rumor detection. The researchers propose a Post Engagement Prediction (PEP) strategy to incorporate information from propagation structures into PLMs, improving their ability to capture interactions crucial for rumor detection. They curate and release large-scale Twitter corpus and unlabeled claim conversation datasets with propagation structures. By utilizing these resources and implementing the PEP strategy, a Twitter-tailored PLM called SoLM is trained, demonstrating significant performance improvements in rumor detection tasks. The experiments show that PEP boosts baseline models by 1.0-3.7% accuracy and even outperforms current state-of-the-art methods on multiple datasets. SoLM, without high-level modules, achieves competitive results, highlighting the effective learning of discriminative post interaction features. 

<br /><br />Summary: <div>
arXiv:2508.07209v1 Announce Type: cross 
Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanism's ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategy's effectiveness in learning discriminative post interaction features.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Agentic Service Ecosystems: Measurement, Analysis, and Optimization</title>
<link>https://arxiv.org/abs/2508.07343</link>
<guid>https://arxiv.org/abs/2508.07343</guid>
<content:encoded><![CDATA[
<div> autonomous agents, Agentic Service Ecosystems, swarm intelligence, emergent behavior, decentralized systems
Summary:<br /><br />The Agentic Service Ecosystem comprises diverse autonomous agents engaging in resource and service interactions, leading to complex systems. Traditional linear analysis methods are inadequate due to the autonomy and capabilities of these agents. Swarm intelligence, characterized by decentralized self-organization, offers a novel approach to understanding and optimizing such ecosystems. However, existing research lacks a unified methodology to comprehensively analyze swarm intelligence emergence in agentic contexts. A proposed framework advocates for a three-step approach - measurement, analysis, and optimization - to uncover the cyclic mechanisms fostering emergence. The review of current technologies highlights their strengths and limitations, pinpointing unresolved challenges. The framework not only offers theoretical support but also actionable methods for real-world applications, enhancing the understanding and optimization of Agentic Service Ecosystems. <div>
arXiv:2508.07343v1 Announce Type: cross 
Abstract: The Agentic Service Ecosystem consists of heterogeneous autonomous agents (e.g., intelligent machines, humans, and human-machine hybrid systems) that interact through resource exchange and service co-creation. These agents, with distinct behaviors and motivations, exhibit autonomous perception, reasoning, and action capabilities, which increase system complexity and make traditional linear analysis methods inadequate. Swarm intelligence, characterized by decentralization, self-organization, emergence, and dynamic adaptability, offers a novel theoretical lens and methodology for understanding and optimizing such ecosystems. However, current research, owing to fragmented perspectives and cross-ecosystem differences, fails to comprehensively capture the complexity of swarm-intelligence emergence in agentic contexts. The lack of a unified methodology further limits the depth and systematic treatment of the research. This paper proposes a framework for analyzing the emergence of swarm intelligence in Agentic Service Ecosystems, with three steps: measurement, analysis, and optimization, to reveal the cyclical mechanisms and quantitative criteria that foster emergence. By reviewing existing technologies, the paper analyzes their strengths and limitations, identifies unresolved challenges, and shows how this framework provides both theoretical support and actionable methods for real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Densest $k$-Subgraph Mining and Diagonal Loading</title>
<link>https://arxiv.org/abs/2410.07388</link>
<guid>https://arxiv.org/abs/2410.07388</guid>
<content:encoded><![CDATA[
<div> Keywords: Densest $k$-Subgraph, continuous relaxation, diagonal loading, Frank--Wolfe algorithm, subgraph density <br />
<br />
Summary: 
This article focuses on the Densest $k$-Subgraph (D$k$S) problem and introduces a continuous relaxation of the binary quadratic D$k$S problem with a diagonal loading term. The study shows that this non-convex relaxation is tight for certain diagonal loading parameters and analyzes the impact of these parameters on the optimization landscape. Two projection-free algorithms, based on Frank--Wolfe and explicit constraint parameterization, are proposed to solve the relaxed problem efficiently. Experimental results indicate that both algorithms have advantages compared to existing methods, with the Frank--Wolfe algorithm particularly excelling in subgraph density, computational complexity, and scalability to large datasets. This research provides valuable insights into optimizing subgraph density and offers effective algorithms for addressing the D$k$S problem. <br /> <div>
arXiv:2410.07388v3 Announce Type: replace 
Abstract: The Densest $k$-Subgraph (D$k$S) problem aims to find a subgraph comprising $k$ vertices with the maximum number of edges between them. A continuous relaxation of the binary quadratic D$k$S problem is considered, which incorporates a diagonal loading term. It is shown that this non-convex, continuous relaxation is tight for a range of diagonal loading parameters, and the impact of the diagonal loading parameter on the optimization landscape is studied. On the algorithmic side, two projection-free algorithms are proposed to tackle the relaxed problem, based on Frank--Wolfe and explicit constraint parameterization, respectively. Experiments suggest that both algorithms have merits relative to the state-of-art, while the Frank--Wolfe-based algorithm stands out in terms of subgraph density, computational complexity, and ability to scale up to very large datasets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delayed takedown of illegal content on social media makes moderation ineffective</title>
<link>https://arxiv.org/abs/2502.08841</link>
<guid>https://arxiv.org/abs/2502.08841</guid>
<content:encoded><![CDATA[
<div> takedown delay, illegal content, social media, agent-based model, diffusion<br />
Summary:<br />
This study examines the impact of takedown delays on the prevalence, reach, and exposure to illegal content on social media. Analyzing data from the EU Digital Services Act Transparency Database, the research finds significant variation in takedown delays across major platforms. Using an agent-based model calibrated to empirical data, the study reveals that rapid content removal within hours significantly reduces the spread of illegal content, while longer delays do not effectively curb its spread. The simulations show how takedown speed plays a crucial role in shaping the diffusion of illegal content online. The findings suggest that faster content removal is essential for combating the spread of illegal content, highlighting the need for timely enforcement measures. However, the study also acknowledges the limitations of strict enforcement policies in addressing the challenges posed by illegal content on social media platforms. <div>
arXiv:2502.08841v2 Announce Type: replace 
Abstract: Illegal content on social media poses significant societal harm and necessitates timely removal. However, the impact of the speed of content removal on prevalence, reach, and exposure to illegal content remains underexplored. This study examines the relationship with a systematic analysis of takedown delays using data from the EU Digital Services Act Transparency Database, covering five major platforms over a one-year period. We find substantial variation in takedown delay, with some content remaining online for weeks or even months. To evaluate how these delays affect the prevalence and reach of illegal content and exposure to it, we develop an agent-based model and calibrate it to empirical data. We simulate illegal content diffusion, revealing that rapid takedown (within hours) significantly reduces prevalence, reach, and exposure to illegal content, while longer delays fail to reduce its spread. Though the effect of delay may seem intuitive, our simulations quantify exactly how takedown speed shapes the spread of illegal content. Building on these results, we point to the benefits of faster content removal to effectively curb the spread of illegal content, while also considering the limitations of strict enforcement policies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content</title>
<link>https://arxiv.org/abs/2503.04773</link>
<guid>https://arxiv.org/abs/2503.04773</guid>
<content:encoded><![CDATA[
<div> Keywords: segregation prediction, Large Language Models (LLMs), online review mining, Reflective LLM Coder, REasoning-and-EMbedding (RE'EM) framework

Summary: 
The study focuses on using Large Language Models (LLMs) to automate online review mining for predicting segregation in urban daily life. They introduce a Reflective LLM Coder to extract insights from social media content related to segregation experience dimensions like cultural resonance, accessibility, and community engagement. A codebook is created to guide LLMs in generating review summaries and ratings for segregation prediction. The RE'EM framework combines language model capabilities to enhance prediction accuracy with a significant improvement in R2 and MSE. The codebook's effectiveness is validated across different cities, indicating its generalizability. A user study confirms the cognitive benefits of codebook-guided summaries in understanding Points of Interest (POIs) social inclusiveness. Overall, the study showcases the potential of AI in addressing societal inequalities and promoting social inclusiveness. 

<br /><br />Summary: <div>
arXiv:2503.04773v3 Announce Type: replace-cross 
Abstract: Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizations, teams, and job mobility: A social microdynamics approach</title>
<link>https://arxiv.org/abs/2503.24117</link>
<guid>https://arxiv.org/abs/2503.24117</guid>
<content:encoded><![CDATA[
<div> team composition, worker reunions, job mobility, social capital, organizational structure

Summary: 
Workers in large organizations are influenced by past social connections when changing jobs, with a high percentage of moves resulting in reunions with former coworkers. The study of a US Army organization reveals that social ties within teams play a significant role in shaping internal job changes. Worker reunions are more informative predictors of job moves than labor supply and demand or occupational specialization. The likelihood of reunions increases with time spent together and smaller team sizes, indicating the importance of familiarity and trust in these reunions. This highlights the key role of social capital and team structures in understanding organizational worker mobility. <div>
arXiv:2503.24117v2 Announce Type: replace-cross 
Abstract: Most of the modeling approaches used to understand organizational worker mobility are highly stylized, using idealizations such as structureless organizations, indistinguishable workers, and a lack of social bonding of the workers. In this article, aided by a decade of precise, temporally resolved data of a large civilian organization of the US Army in which employees can change jobs in a similar way to many private organizations, we introduce a new framework to describe organizations as composites of teams within which individuals perform specific tasks and where social connections develop. By tracking the personnel composition of organizational teams, we find that workers who change jobs are highly influenced by preferring to reunite with past coworkers. In this organization, 34% of all moves across temporally stable teams (and 32% of the totality of moves) lead to worker reunions, percentages that have not been reported and are well-above intuitive expectation. To assess the importance of worker reunions in determining job moves, we compare them to labor supply and demand with or without occupational specialization. The comparison shows that the most consistent information about job change is provided by reunions. We find that the greater the time workers spend together or the smaller the team they share both increase their likelihood to reunite, supporting the notion of increased familiarity and trust behind such reunions and the dominant role of social capital in the evolution of large organizations. Our study of this organization supports the idea that to correctly forecast job mobility inside large organizations, their teams structures and the social ties formed in those teams play a key role in shaping internal job change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</title>
<link>https://arxiv.org/abs/2508.06342</link>
<guid>https://arxiv.org/abs/2508.06342</guid>
<content:encoded><![CDATA[
<div> Keywords: social interactions, street view imagery, urban planning, sociability, place attachment

Summary: 
The study aims to analyze street view imagery to extract information on social interactions, guided by Mehta's taxonomy of sociability. Through linear regression models, the researchers found that factors such as sky view index, green view index, and place attachment scores were associated with different types of social interactions. The results supported urban planning theories, indicating that environmental factors play a role in shaping sociability in cities. This research suggests that street view images can provide valuable insights into the relationship between social interactions and the built environment, offering a scalable and privacy-preserving tool for studying urban sociability. Further exploration of street view imagery could help in cross-cultural theory testing and inform the design of socially vibrant cities. 

<br /><br />Summary: <div>
arXiv:2508.06342v1 Announce Type: cross 
Abstract: Designing socially active streets has long been a goal of urban planning, yet existing quantitative research largely measures pedestrian volume rather than the quality of social interactions. We hypothesize that street view imagery -- an inexpensive data source with global coverage -- contains latent social information that can be extracted and interpreted through established social science theory. As a proof of concept, we analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehta's taxonomy of passive, fleeting, and enduring sociability -- one illustrative example of a theory grounded in urban design that could be substituted or complemented by other sociological frameworks. We then used linear regression models, controlling for factors like weather, time of day, and pedestrian counts, to test whether the inferred sociability measures correlate with city-level place attachment scores from the World Values Survey and with environmental predictors (e.g., green, sky, and water view indices) derived from individual street view images. Results aligned with long-standing urban planning theory: the sky view index was associated with all three sociability types, the green view index predicted enduring sociability, and place attachment was positively associated with fleeting sociability. These results provide preliminary evidence that street view images can be used to infer relationships between specific types of social interactions and built environment variables. Further research could establish street view imagery as a scalable, privacy-preserving tool for studying urban sociability, enabling cross-cultural theory testing and evidence-based design of socially vibrant cities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Graph Theory of Majority Illusions: Theoretical Results and Computational Experiments</title>
<link>https://arxiv.org/abs/2304.02258</link>
<guid>https://arxiv.org/abs/2304.02258</guid>
<content:encoded><![CDATA[
<div> illusion, opinion, social network, majority, network structure

Summary:
The study explores how network structures can lead to distortions in the perception of opinions within a community. It focuses on the 'majority illusion', where an individual's direct circles may show a different majority opinion from the global one. The research indicates that no network structure can ensure that most agents perceive the correct majority opinion. Computational experiments are conducted to assess the likelihood of majority illusions in various network classes. The findings suggest that certain network configurations can allow for a significant portion of the population to have misconceptions about the overall distribution of opinions. The study also considers the potential for other types of illusions beyond the majority illusion, emphasizing the impact of network wiring on information distortion within social networks. <div>
arXiv:2304.02258v4 Announce Type: replace 
Abstract: The popularity of an opinion in one's direct circles is not necessarily a good indicator of its popularity in one's entire community. Network structures make local information about global properties of the group potentially inaccurate, and the way a social network is wired constrains what kind of information distortion can actually occur. In this paper, we discuss which classes of networks allow for a large enough proportion of the population to get a wrong enough impression about the overall distribution of opinions. We start by focusing on the 'majority illusion', the case where one sees a majority opinion in one's direct circles that differs from the global majority. We show that no network structure can guarantee that most agents see the correct majority. We then perform computational experiments to study the likelihood of majority illusions in different classes of networks. Finally, we generalize to other types of illusions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Random Field model for Hypergraph-based Machine Learning</title>
<link>https://arxiv.org/abs/2308.14172</link>
<guid>https://arxiv.org/abs/2308.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph, data generation, machine learning, structure inference, node classification

Summary: 
This paper introduces a novel approach for modelling data generation processes on hypergraphs, using a hypergraph Markov random field. The model captures the joint distribution of node and hyperedge features in a hypergraph through a multivariate Gaussian distribution, uniquely determined by the hypergraph structure. The proposed data-generating process serves as an inductive bias for hypergraph machine learning tasks, enhancing algorithm design. The paper focuses on two tasks: hypergraph structure inference (HGSI) and node classification (Hypergraph-MLP). Empirical evaluation shows that HGSI outperforms existing methods on synthetic and real data, while Hypergraph-MLP excels in six hypergraph node classification benchmarks, offering improved runtime efficiency and robustness against structural perturbations during inference. This framework advances the understanding of hypergraph data generation and informs the design of effective machine learning algorithms for hypergraphs. 

<br /><br />Summary: <div>
arXiv:2308.14172v4 Announce Type: replace-cross 
Abstract: Understanding the data-generating process is essential for building machine learning models that generalise well while ensuring robustness and interpretability. This paper addresses the fundamental challenge of modelling the data generation processes on hypergraphs and explores how such models can inform the design of machine learning algorithms for hypergraph data. The key to our approach is the development of a hypergraph Markov random field that models the joint distribution of the node features and hyperedge features in a hypergraph through a multivariate Gaussian distribution whose covariance matrix is uniquely determined by the hypergraph structure. The proposed data-generating process provides a valuable inductive bias for various hypergraph machine learning tasks, thus enhancing the algorithm design. In this paper, we focus on two representative downstream tasks: structure inference and node classification. Accordingly, we introduce two novel frameworks: 1) an original hypergraph structure inference framework named HGSI, and 2) a novel learning framework entitled Hypergraph-MLP for node classification on hypergraphs. Empirical evaluation of the proposed frameworks demonstrates that: 1) HGSI outperforms existing hypergraph structure inference methods on both synthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in six hypergraph node classification benchmarks, at the same time promoting runtime efficiency and robustness against structural perturbations during inference.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications</title>
<link>https://arxiv.org/abs/2508.04889</link>
<guid>https://arxiv.org/abs/2508.04889</guid>
<content:encoded><![CDATA[
<div> API, decentralized, Vue.js, interoperate, ecosystem
Summary:
Graffiti is a system designed to create personalized social applications that can easily coexist and interact with each other. Through the concept of total reification, conflicting designs and moderation rules can still interoperate, while the use of channels prevents accidental context collapse. The system operates through a minimal client-side API, allowing for decentralized implementations. A Vue.js plugin enables the development of various applications, like Twitter or Wikipedia, using only client-side code. Case studies demonstrate how these applications can interoperate and the broader ecosystem Graffiti enables. Overall, Graffiti offers a flexible and interoperable solution for building unique social applications that maintain a user's friends and data across different designs and features. 

<br /><br />Summary: <div>
arXiv:2508.04889v1 Announce Type: new 
Abstract: Most social applications, from Twitter to Wikipedia, have rigid one-size-fits-all designs, but building new social applications is both technically challenging and results in applications that are siloed away from existing communities. We present Graffiti, a system that can be used to build a wide variety of personalized social applications with relative ease that also interoperate with each other. People can freely move between a plurality of designs -- each with its own aesthetic, feature set, and moderation -- all without losing their friends or data.
  Our concept of total reification makes it possible for seemingly contradictory designs, including conflicting moderation rules, to interoperate. Conversely, our concept of channels prevents interoperation from occurring by accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we show admits at least two decentralized implementations. Above the API, we built a Vue.js plugin, which we use to develop applications similar to Twitter, Messenger, and Wikipedia using only client-side code. Our case studies explore how these and other novel applications interoperate, as well as the broader ecosystem that Graffiti enables.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Aware Social Community Recommendation</title>
<link>https://arxiv.org/abs/2508.05107</link>
<guid>https://arxiv.org/abs/2508.05107</guid>
<content:encoded><![CDATA[
<div> community recommendation, social recommendation, personalized services, recommender systems, CASO

Summary:
CASO is a novel model designed for social community recommendation, addressing the limitations of existing models in recommending communities. It utilizes three encoders to extract user embeddings and incorporates social network structures and user preferences. CASO introduces a mutual exclusion mechanism to eliminate feature redundancy and includes a community detection loss in the optimization process. Furthermore, extensive experiments on real-world social networks demonstrate CASO's impressive performance in community recommendation compared to nine strong baselines. Overall, CASO shows superior effectiveness in recommending communities by leveraging social network structures and user preferences effectively. <div>
arXiv:2508.05107v1 Announce Type: new 
Abstract: Social recommendation, which seeks to leverage social ties among users to alleviate the sparsity issue of user-item interactions, has emerged as a popular technique for elevating personalized services in recommender systems. Despite being effective, existing social recommendation models are mainly devised for recommending regular items such as blogs, images, and products, and largely fail for community recommendations due to overlooking the unique characteristics of communities. Distinctly, communities are constituted by individuals, who present high dynamicity and relate to rich structural patterns in social networks. To our knowledge, limited research has been devoted to comprehensively exploiting this information for recommending communities.
  To bridge this gap, this paper presents CASO, a novel and effective model specially designed for social community recommendation. Under the hood, CASO harnesses three carefully-crafted encoders for user embedding, wherein two of them extract community-related global and local structures from the social network via social modularity maximization and social closeness aggregation, while the third one captures user preferences using collaborative filtering with observed user-community affiliations. To further eliminate feature redundancy therein, we introduce a mutual exclusion between social and collaborative signals. Finally, CASO includes a community detection loss in the model optimization, thereby producing community-aware embeddings for communities. Our extensive experiments evaluating CASO against nine strong baselines on six real-world social networks demonstrate its consistent and remarkable superiority over the state of the art in terms of community recommendation performance.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling roles and trade-offs in multiplex networks</title>
<link>https://arxiv.org/abs/2508.05488</link>
<guid>https://arxiv.org/abs/2508.05488</guid>
<content:encoded><![CDATA[
<div> social network, multiplex, MLT, roles, interdependence

Summary:<br /><br />This article introduces the Multiplex Latent Trade-off Model (MLT) for analyzing multiplex social networks, considering independence, dependence, and interdependence. The approach identifies roles as trade-offs, taking into account individual attributes, status/resources of others, and mutual influence. By applying the MLT framework to real-world multiplex networks from villages in western Honduras, the study uncovers core social exchange principles and identifies local, layer-specific, and multi-scale communities. The analysis shows that modeling interdependence significantly improves link prediction performance in social networks, implying that social ties are deeply embedded structurally. In contrast, health and economic ties are more influenced by individual status and behaviors. This research provides valuable insights into the structure of human social systems and showcases the complexity of relationships across different layers in multiplex networks. <div>
arXiv:2508.05488v1 Announce Type: new 
Abstract: A multiplex social network captures multiple types of social relations among the same set of people, with each layer representing a distinct type of relationship. Understanding the structure of such systems allows us to identify how social exchanges may be driven by a person's own attributes and actions (independence), the status or resources of others (dependence), and mutual influence between entities (interdependence). Characterizing structure in multiplex networks is challenging, as the distinct layers can reflect different yet complementary roles, with interdependence emerging across multiple scales. Here, we introduce the Multiplex Latent Trade-off Model (MLT), a framework for extracting roles in multiplex social networks that accounts for independence, dependence, and interdependence. MLT defines roles as trade-offs, requiring each node to distribute its source and target roles across layers while simultaneously distributing community memberships within hierarchical, multi-scale structures. Applying the MLT approach to 176 real-world multiplex networks, composed of social, health, and economic layers, from villages in western Honduras, we see core social exchange principles emerging, while also revealing local, layer-specific, and multi-scale communities. Link prediction analyses reveal that modeling interdependence yields the greatest performance gains in the social layer, with subtler effects in health and economic layers. This suggests that social ties are structurally embedded, whereas health and economic ties are primarily shaped by individual status and behavioral engagement. Our findings offer new insights into the structure of human social systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAEx: A Plug-and-Play Framework for Explaining Network Alignment</title>
<link>https://arxiv.org/abs/2508.04731</link>
<guid>https://arxiv.org/abs/2508.04731</guid>
<content:encoded><![CDATA[
<div> Explanations, Network Alignment, Interpretability, Subgraphs, Features <br />
Summary:<br />
The paper introduces NAEx, a framework designed to explain network alignment models by identifying key subgraphs and features that influence predictions. The framework aims to improve the interpretability of alignment decisions, particularly in high-stakes domains where trust is crucial. NAEx addresses the challenge of preserving cross-network dependencies by jointly parameterizing graph structures and feature spaces through learnable edge and feature masks. It also introduces an optimization objective that ensures explanations are faithful to the original predictions and enable meaningful comparisons of structural and feature-based similarities between networks. NAEx is an inductive framework that efficiently generates explanations for previously unseen data. Evaluation metrics tailored to alignment explainability are introduced, and the effectiveness and efficiency of NAEx are demonstrated on benchmark datasets by integrating it with four representative network alignment models. <br /> <div>
arXiv:2508.04731v1 Announce Type: cross 
Abstract: Network alignment (NA) identifies corresponding nodes across multiple networks, with applications in domains like social networks, co-authorship, and biology. Despite advances in alignment models, their interpretability remains limited, making it difficult to understand alignment decisions and posing challenges in building trust, particularly in high-stakes domains. To address this, we introduce NAEx, a plug-and-play, model-agnostic framework that explains alignment models by identifying key subgraphs and features influencing predictions. NAEx addresses the key challenge of preserving the joint cross-network dependencies on alignment decisions by: (1) jointly parameterizing graph structures and feature spaces through learnable edge and feature masks, and (2) introducing an optimization objective that ensures explanations are both faithful to the original predictions and enable meaningful comparisons of structural and feature-based similarities between networks. NAEx is an inductive framework that efficiently generates NA explanations for previously unseen data. We introduce evaluation metrics tailored to alignment explainability and demonstrate NAEx's effectiveness and efficiency on benchmark datasets by integrating it with four representative NA models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration</title>
<link>https://arxiv.org/abs/2508.04780</link>
<guid>https://arxiv.org/abs/2508.04780</guid>
<content:encoded><![CDATA[
<div> hurricanes, power system restoration, equity, predict-then-optimize framework, reinforcement learning<br />
<br />
Summary:<br />
The article discusses the need for an equitable and efficient power system restoration strategy, addressing disparities in request submission volumes from different communities. The proposed Equity-Conformalized Quantile Regression and Spatial-Temporal Attentional RL framework, EPOPR, aims to balance restoration efficiency and equity. EPOPR overcomes challenges such as dataset heteroscedasticity and reinforcement learning agent biases by predicting repair durations and adapting decision-making to varying uncertainty levels across regions. Experimental results demonstrate that EPOPR reduces average power outage duration by 3.60% and decreases inequity between communities by 14.19%, outperforming existing baselines. <div>
arXiv:2508.04780v1 Announce Type: cross 
Abstract: The increasing frequency of extreme weather events, such as hurricanes, highlights the urgent need for efficient and equitable power system restoration. Many electricity providers make restoration decisions primarily based on the volume of power restoration requests from each region. However, our data-driven analysis reveals significant disparities in request submission volume, as disadvantaged communities tend to submit fewer restoration requests. This disparity makes the current restoration solution inequitable, leaving these communities vulnerable to extended power outages. To address this, we aim to propose an equity-aware power restoration strategy that balances both restoration efficiency and equity across communities. However, achieving this goal is challenging for two reasons: the difficulty of predicting repair durations under dataset heteroscedasticity, and the tendency of reinforcement learning agents to favor low-uncertainty actions, which potentially undermine equity. To overcome these challenges, we design a predict-then-optimize framework called EPOPR with two key components: (1) Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction, and (2) Spatial-Temporal Attentional RL that adapts to varying uncertainty levels across regions for equitable decision-making. Experimental results show that our EPOPR effectively reduces the average power outage duration by 3.60% and decreases inequity between different communities by 14.19% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing</title>
<link>https://arxiv.org/abs/2508.04811</link>
<guid>https://arxiv.org/abs/2508.04811</guid>
<content:encoded><![CDATA[
<div> passenger fairness, driver preference, ride-hailing system, multi-agent reinforcement learning, system efficiency

Summary:<br /><br />Order dispatch systems in ride-hailing services are crucial for operator revenue, driver profit, and passenger experience. Existing approaches focus on system efficiency, possibly neglecting passenger and driver satisfaction. To address this, a human-centered ride-hailing system, HCRide, is developed. HCRide utilizes a novel multi-agent reinforcement learning algorithm called Habic, which includes a multi-agent competition mechanism, dynamic Actor network, and Bi-Critic network to balance passenger fairness and driver preference while optimizing efficiency. Evaluation using real-world datasets shows that HCRide improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to existing methods. <div>
arXiv:2508.04811v1 Announce Type: cross 
Abstract: Order dispatch systems play a vital role in ride-hailing services, which directly influence operator revenue, driver profit, and passenger experience. Most existing work focuses on improving system efficiency in terms of operator revenue, which may cause a bad experience for both passengers and drivers. Hence, in this work, we aim to design a human-centered ride-hailing system by considering both passenger fairness and driver preference without compromising the overall system efficiency. However, it is nontrivial to achieve this target due to the potential conflicts between passenger fairness and driver preference since optimizing one may sacrifice the other. To address this challenge, we design HCRide, a Human-Centered Ride-hailing system based on a novel multi-agent reinforcement learning algorithm called Harmonization-oriented Actor-Bi-Critic (Habic), which includes three major components (i.e., a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network) to optimize system efficiency and passenger fairness with driver preference consideration. We extensively evaluate our HCRide using two real-world ride-hailing datasets from Shenzhen and New York City. Experimental results show our HCRide effectively improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2508.04894</link>
<guid>https://arxiv.org/abs/2508.04894</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, graph-structured data, adversarial attacks, robustness, defense framework 

Summary: 
Large Language Models (LLMs) integrated with graph data are vulnerable to adversarial attacks, including poisoning and evasion attacks. The models LLAGA and GRAPHPROMPTER were analyzed for vulnerabilities, with LLAGA showing susceptibility to malicious node injection while GRAPHPROMPTER demonstrated greater robustness due to its GNN encoder. Both models were found to be susceptible to imperceptible feature perturbations. A new attack surface was discovered for LLAGA where attackers can severely impact performance by injecting malicious nodes. To address these vulnerabilities, an end-to-end defense framework called GALGUARD was proposed. GALGUARD combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.
<br /><br />Summary: <div>
arXiv:2508.04894v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated with graph-structured data for tasks like node classification, a domain traditionally dominated by Graph Neural Networks (GNNs). While this integration leverages rich relational information to improve task performance, their robustness against adversarial attacks remains unexplored. We take the first step to explore the vulnerabilities of graph-aware LLMs by leveraging existing adversarial attack methods tailored for graph-based models, including those for poisoning (training-time attacks) and evasion (test-time attacks), on two representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al. 2024). Additionally, we discover a new attack surface for LLAGA where an attacker can inject malicious nodes as placeholders into the node sequence template to severely degrade its performance. Our systematic analysis reveals that certain design choices in graph encoding can enhance attack success, with specific findings that: (1) the node sequence template in LLAGA increases its vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness; and (3) both approaches remain susceptible to imperceptible feature perturbation attacks. Finally, we propose an end-to-end defense framework GALGUARD, that combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Identification in Peer Effects Estimation</title>
<link>https://arxiv.org/abs/2508.04897</link>
<guid>https://arxiv.org/abs/2508.04897</guid>
<content:encoded><![CDATA[
<div> identification, estimation, network linear-in-means model, peer effects, infill asymptotic setting

Summary:
- The article discusses the network linear-in-means model, which is used to incorporate peer effects in statistical models.
- It explores whether the parameters of the model are reliably estimated in the infill asymptotic setting, where a single network grows in size.
- The study shows that when covariates are i.i.d and the average network degree of nodes increases with the population size, standard estimators can suffer from bias or slow convergence rates due to asymptotic collinearity induced by network averaging.
- Linear-in-sums models, based on aggregate neighborhood characteristics, are proposed as an alternative that does not exhibit these issues as long as network degrees have nontrivial variation.
- Most network models satisfy the condition of nontrivial variation in network degrees, making linear-in-sums models a viable option for reliable estimation in social phenomena analysis.<br /><br />Summary: <div>
arXiv:2508.04897v1 Announce Type: cross 
Abstract: It is commonly accepted that some phenomena are social: for example, individuals' smoking habits often correlate with those of their peers. Such correlations can have a variety of explanations, such as direct contagion or shared socioeconomic circumstances. The network linear-in-means model is a workhorse statistical model which incorporates these peer effects by including average neighborhood characteristics as regressors. Although the model's parameters are identifiable under mild structural conditions on the network, it remains unclear whether identification ensures reliable estimation in the "infill" asymptotic setting, where a single network grows in size. We show that when covariates are i.i.d. and the average network degree of nodes increases with the population size, standard estimators suffer from bias or slow convergence rates due to asymptotic collinearity induced by network averaging. As an alternative, we demonstrate that linear-in-sums models, which are based on aggregate rather than average neighborhood characteristics, do not exhibit such issues as long as the network degrees have some nontrivial variation, a condition satisfied by most network models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide</title>
<link>https://arxiv.org/abs/2311.17627</link>
<guid>https://arxiv.org/abs/2311.17627</guid>
<content:encoded><![CDATA[
<div> gender bias, women diplomats, social media, online visibility, retweets
Summary:
Women diplomats face online gender bias, with a global study revealing disparities in online visibility. Despite mild gendered language in responses to diplomatic tweets, women ambassadors receive 66.4% fewer retweets than men, highlighting a significant form of gender bias. Negativity in tweets directed at diplomats is not significantly higher for women than for men. The study introduces a unique methodology for analyzing online gender bias, emphasizing the need for further research on bias in international politics. This research sheds light on the challenges faced by women in foreign policy in dealing with online hostility and calls for greater attention to addressing gender disparities in digital diplomacy. Women's visibility on social media remains a crucial area for improvement in promoting gender equality in diplomatic circles. <br /><br />Summary: <div>
arXiv:2311.17627v2 Announce Type: replace 
Abstract: Despite mounting evidence that women in foreign policy often bear the brunt of online hostility, the extent of online gender bias against diplomats remains unexplored. This paper offers the first global analysis of the treatment of women diplomats on social media. Introducing a multidimensional and multilingual methodology for studying online gender bias, it focuses on three critical elements: gendered language, negativity in tweets directed at diplomats, and the visibility of women diplomats. Our unique dataset encompasses ambassadors from 164 countries, their tweets, and the direct responses to these tweets in 65 different languages. Using automated content and sentiment analysis, our findings reveal a crucial gender bias. The language in responses to diplomatic tweets is only mildly gendered and largely pertains to international affairs and, generally, women ambassadors do not receive more negative reactions to their tweets than men, yet the pronounced discrepancy in online visibility stands out as a significant form of gender bias. Women receive a staggering 66.4% fewer retweets than men. By unraveling the invisibility that obscures women diplomats on social media, we hope to spark further research on online bias in international politics.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems</title>
<link>https://arxiv.org/abs/2508.03747</link>
<guid>https://arxiv.org/abs/2508.03747</guid>
<content:encoded><![CDATA[
<div> machine learning, human mobility, periodicity, COVID-19 impact, urban systems <br />
Summary: <br />
This study focuses on uncovering the temporal regularity of human mobility using a sparse identification approach in time series autoregression. By applying this framework to real-world data on metro passenger flow in Hangzhou and ridesharing trips in NYC and Chicago, the researchers were able to identify and quantify significant weekly periodic patterns. The analysis of ridesharing data from 2019 to 2024 revealed the disruptive impact of the COVID-19 pandemic on mobility regularity, with both NYC and Chicago experiencing a reduction in weekly periodicity in 2020. The recovery of mobility regularity in NYC was found to be faster than in Chicago. The interpretability of sparse autoregression provided valuable insights into the underlying temporal patterns of human mobility, highlighting the potential of interpretable machine learning in understanding urban systems. <div>
arXiv:2508.03747v1 Announce Type: new 
Abstract: Uncovering the temporal regularity of human mobility is crucial for discovering urban dynamics and has implications for various decision-making processes and urban system applications. This study formulates the periodicity quantification problem in complex and multidimensional human mobility data as a sparse identification of dominant positive auto-correlations in time series autoregression, allowing one to discover and quantify significant periodic patterns such as weekly periodicity from a data-driven and interpretable machine learning perspective. We apply our framework to real-world human mobility data, including metro passenger flow in Hangzhou, China and ridesharing trips in New York City (NYC) and Chicago, USA, revealing the interpretable weekly periodicity across different spatial locations over past several years. In particular, our analysis of ridesharing data from 2019 to 2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility regularity and the subsequent recovery trends, highlighting differences in the recovery pattern percentages and speeds between NYC and Chicago. We explore that both NYC and Chicago experienced a remarkable reduction of weekly periodicity in 2020, and the recovery of mobility regularity in NYC is faster than Chicago. The interpretability of sparse autoregression provides insights into the underlying temporal patterns of human mobility, offering a valuable tool for understanding urban systems. Our findings highlight the potential of interpretable machine learning to unlock crucial insights from real-world mobility data.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Stochastic Block Models for Community Detection: The issue of edge-connectivity</title>
<link>https://arxiv.org/abs/2508.03843</link>
<guid>https://arxiv.org/abs/2508.03843</guid>
<content:encoded><![CDATA[
<div> Well-Connected Clusters, Community detection, Graphs, Stochastic Block Models, Connectivity<br />
Summary:<br />
The study focuses on the importance of connectivity in community detection within graphs. Existing community detection methods often produce poorly connected or disconnected communities. A technique called Well-Connected Clusters (WCC) was introduced to address this issue by identifying and removing small edge cuts within clusters. The study evaluated various Stochastic Block Model (SBM) clustering methods and found that all tested methods generated disconnected communities. Graph-tool was shown to perform better than PySBM but still had issues with connectivity due to its description length formula. Modifications to this formula were explored. WCC was proven to enhance accuracy in both flat and nested SBMs and was scalable to networks with millions of nodes. The research provides valuable insights into cluster connectivity and offers a practical solution to improve community detection accuracy. <br /> <div>
arXiv:2508.03843v1 Announce Type: new 
Abstract: A relevant, sometimes overlooked, quality criterion for communities in graphs is that they should be well-connected in addition to being edge-dense. Prior work has shown that leading community detection methods can produce poorly-connected communities, and some even produce internally disconnected communities. A recent study by Park et al. in Complex Networks and their Applications 2024 showed that this problem is evident in clusterings from three Stochastic Block Models (SBMs) in graph-tool, a popular software package. To address this issue, Park et al. presented a simple technique, Well-Connected Clusters (WCC), that repeatedly finds and removes small edge cuts of size at most $\log_{10}n$ in clusters, where $n$ is the number of nodes in the cluster, and showed that treatment of graph-tool SBM clusterings with WCC improves accuracy. Here we examine the question of cluster connectivity for clusterings computed using other SBM software or nested SBMs within graph-tool. Our study, using a wide range of real-world and synthetic networks, shows that all tested SBM clustering methods produce communities that are disconnected, and that graph-tool improves on PySBM. We provide insight into why graph-tool degree-corrected SBM clustering produces disconnected clusters by examining the description length formula it uses, and explore the impact of modifications to the description length formula. Finally, we show that WCC provides an improvement in accuracy for both flat and nested SBMs and establish that it scales to networks with millions of nodes.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical community detection via maximum entropy partitions and the renormalization group</title>
<link>https://arxiv.org/abs/2508.04034</link>
<guid>https://arxiv.org/abs/2508.04034</guid>
<content:encoded><![CDATA[
<div> Hierarchical Clustering Entropy, scales, community structures, network science, dendrograms <br /> 
Summary: <br />
The article introduces Hierarchical Clustering Entropy (HCE), a framework for identifying informative levels in hierarchical community structures in networks. HCE operates on dendrograms without edge-level statistics, selecting resolution levels based on the entropy of community size distribution and number of communities. It works with various clustering algorithms and distance metrics. Evaluation on synthetic benchmarks shows HCE accurately identifies partitions aligned with ground truth, even with noise. Real-world applications in social and neuroscience networks demonstrate HCE's ability to reveal modular hierarchies consistent with existing knowledge. As a scalable and principled method, HCE offers a domain-independent approach to hierarchical community detection with possible applications in biological, social, and technological systems. <br /> <div>
arXiv:2508.04034v1 Announce Type: new 
Abstract: Identifying meaningful structure across multiple scales remains a central challenge in network science. We introduce Hierarchical Clustering Entropy (HCE), a general and model-agnostic framework for detecting informative levels in hierarchical community structures. Unlike existing approaches, HCE operates directly on dendrograms without relying on edge-level statistics. It selects resolution levels that maximize a principled trade-off between the entropy of the community size distribution and the number of communities, corresponding to scales of high structural heterogeneity. This criterion applies to dendrograms produced by a wide range of clustering algorithms and distance metrics, including modularity-based and correlation-based methods. We evaluate HCE on synthetic benchmarks with varying degrees of hierarchy, size imbalance, and noise, including LFR and both symmetric and asymmetric multiscale models, and show that it consistently identifies partitions closely aligned with ground truth. Applied to real-world networks in social and neuroscience systems, HCE reveals interpretable modular hierarchies that align with known structural and functional organizations. As a scalable and principled method, HCE offers a general, domain-independent approach to hierarchical community detection with potential applications across biological, social, and technological systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Clique Discovery via Energy Diffusion</title>
<link>https://arxiv.org/abs/2508.04174</link>
<guid>https://arxiv.org/abs/2508.04174</guid>
<content:encoded><![CDATA[
<div> Keywords: quasi-clique discovery, graph mining, energy diffusion, dense subgraph, real-world datasets

Summary: 
The paper introduces a new algorithm, EDQC, for discovering quasi-cliques in graphs. Inspired by energy diffusion, EDQC performs stochastic diffusion from source vertices, concentrating energy in structurally cohesive regions and enabling efficient dense subgraph discovery without exhaustive search or dataset-specific tuning. Experimental results on 30 real-world datasets show that EDQC consistently finds larger quasi-cliques than existing methods on most datasets, with lower variance in solution quality. This approach, which incorporates energy diffusion, is a novel contribution to the field of quasi-clique discovery.<br /><br />Summary: <div>
arXiv:2508.04174v1 Announce Type: new 
Abstract: Discovering quasi-cliques -- subgraphs with edge density no less than a given threshold -- is a fundamental task in graph mining, with broad applications in social networks, bioinformatics, and e-commerce. Existing heuristics often rely on greedy rules, similarity measures, or metaheuristic search, but struggle to maintain both efficiency and solution consistency across diverse graphs. This paper introduces EDQC, a novel quasi-clique discovery algorithm inspired by energy diffusion. Instead of explicitly enumerating candidate subgraphs, EDQC performs stochastic energy diffusion from source vertices, naturally concentrating energy within structurally cohesive regions. The approach enables efficient dense subgraph discovery without exhaustive search or dataset-specific tuning. Experimental results on 30 real-world datasets demonstrate that EDQC consistently discovers larger quasi-cliques than state-of-the-art baselines on the majority of datasets, while also yielding lower variance in solution quality. To the best of our knowledge, EDQC is the first method to incorporate energy diffusion into quasi-clique discovery.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tweets vs Pathogen Spread: A Case Study of COVID-19 in American States</title>
<link>https://arxiv.org/abs/2508.04187</link>
<guid>https://arxiv.org/abs/2508.04187</guid>
<content:encoded><![CDATA[
<div> awareness, disease, SIR dynamics, Twitter data, epidemic

Summary:
- The article discusses the mutual influence between awareness and disease, affecting the spread dynamics.
- A null model coupling two SIR dynamics is proposed and analyzed using a mean-field approach.
- Effects of mutual influence on various observables are quantified by exploring the parameter space.
- Empirical analysis of Twitter data related to COVID-19 and confirmed cases in American states is conducted.
- Findings suggest that increasing awareness can suppress the epidemic in specific parameter regions and investigate phase transitions.
- The model can alter the dominant population group by adjusting parameters during the outbreak.
- Parameters assigned to each state based on the model show changes at different pandemic peaks.
- A correlation is observed between states' Twitter activity ranking and the immunity parameters assigned using the model, highlighting the importance of sustained awareness in disease progression.<br /><br />Summary: <div>
arXiv:2508.04187v1 Announce Type: new 
Abstract: The concept of the mutual influence that awareness and disease may exert on each other has recently presented significant challenges. The actions individuals take to prevent contracting a disease and their level of awareness can profoundly affect the dynamics of its spread. Simultaneously, disease outbreaks impact how people become aware. In response, we initially propose a null model that couples two Susceptible-Infectious-Recovered (SIR) dynamics and analyze it using a mean-field approach. Subsequently, we explore the parameter space to quantify the effects of this mutual influence on various observables. Finally, based on this null model, we conduct an empirical analysis of Twitter data related to COVID-19 and confirmed cases within American states. Our findings indicate that in specific regions of the parameter space, it is possible to suppress the epidemic by increasing awareness, and we investigate phase transitions. Furthermore, our model demonstrates the ability to alter the dominant population group by adjusting parameters throughout the course of the outbreak. Additionally, using the model, we assign a set of parameters to each state, revealing that these parameters change at different pandemic peaks. Notably, a robust correlation emerges between the ranking of states' Twitter activity, as gathered from empirical data, and the immunity parameters assigned to each state using our model. This observation underscores the pivotal role of sustained awareness transitioning from the initial to the subsequent peaks in the disease progression.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Representation Learning with Massive Unlabeled Data for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.04252</link>
<guid>https://arxiv.org/abs/2508.04252</guid>
<content:encoded><![CDATA[
<div> detecting rumors, social media, propagation structure learning, large-scale unlabeled datasets, graph representation learning <br />
Summary:<br />
The article discusses the challenges of rumor detection in social media and the importance of effective methods in combating the spread of false information. It highlights the limitations of existing rumor detection methods, particularly the difficulty in obtaining large labeled datasets and the need for generalization on new events. The study addresses these issues by utilizing large-scale unlabeled datasets from social media platforms and incorporating claim propagation structure. By applying graph self-supervised learning methods to improve semantic learning on various topics and leveraging a decade-spanning rumor dataset, the study demonstrates the enhanced performance of general graph methods in rumor detection tasks. The research shows that these methods outperform previous specialized approaches and exhibit strong generalization capabilities, proving valuable in combating the rapid dissemination of rumors on social media platforms. <br /> <div>
arXiv:2508.04252v1 Announce Type: new 
Abstract: With the development of social media, rumors spread quickly, cause great harm to society and economy. Thereby, many effective rumor detection methods have been developed, among which the rumor propagation structure learning based methods are particularly effective compared to other methods. However, the existing methods still suffer from many issues including the difficulty to obtain large-scale labeled rumor datasets, which leads to the low generalization ability and the performance degeneration on new events since rumors are time-critical and usually appear with hot topics or newly emergent events. In order to solve the above problems, in this study, we used large-scale unlabeled topic datasets crawled from the social media platform Weibo and Twitter with claim propagation structure to improve the semantic learning ability of a graph reprentation learing model on various topics. We use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE in two commonly used training strategies, to verify the performance of general graph semi-supervised methods in rumor detection tasks. In addition, for alleviating the time and topic difference between unlabeled topic data and rumor data, we also collected a rumor dataset covering a variety of topics over a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our experiments show that these general graph self-supervised learning methods outperform previous methods specifically designed for rumor detection tasks and achieve good performance under few-shot conditions, demonstrating the better generalization ability with the help of our massive unlabeled topic dataset.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assortativity in geometric and scale-free networks</title>
<link>https://arxiv.org/abs/2508.04608</link>
<guid>https://arxiv.org/abs/2508.04608</guid>
<content:encoded><![CDATA[
<div> assortative behavior, network, degree assortativity, real-world networks, generative models <br />
Summary:<br />
This paper investigates degree assortativity in real-world networks and generative models for networks with heavy-tailed degree distributions. It challenges the traditional use of the Pearson assortativity coefficient, showing it does not adequately capture assortativity in networks with heavy-tailed degree distributions. Instead, the study adopts a more detailed approach, examining a variety of conditional and joint weight and degree distributions of connected nodes. The analysis reveals that generative models like Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs are assortativity-neutral, while many real-world networks display assortative behavior. To address this, the study proposes an extension to the GIRG model that allows for tunable assortativity. This new model maintains desired properties related to degree distribution and latent space while also exhibiting assortative behavior. The paper offers mathematical analyses and visualization methods to support these findings. <br /> <div>
arXiv:2508.04608v1 Announce Type: new 
Abstract: The assortative behavior of a network is the tendency of similar (or dissimilar) nodes to connect to each other. This tendency can have an influence on various properties of the network, such as its robustness or the dynamics of spreading processes. In this paper, we study degree assortativity both in real-world networks and in several generative models for networks with heavy-tailed degree distribution based on latent spaces. In particular, we study Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs).
  Previous research on assortativity has primarily focused on measuring the degree assortativity in real-world networks using the Pearson assortativity coefficient, despite reservations against this coefficient. We rigorously confirm these reservations by mathematically proving that the Pearson assortativity coefficient does not measure assortativity in any network with sufficiently heavy-tailed degree distributions, which is typical for real-world networks. Moreover, we find that other single-valued assortativity coefficients also do not sufficiently capture the wiring preferences of nodes, which often vary greatly by node degree. We therefore take a more fine-grained approach, analyzing a wide range of conditional and joint weight and degree distributions of connected nodes, both numerically in real-world networks and mathematically in the generative graph models. We provide several methods of visualizing the results.
  We show that the generative models are assortativity-neutral, while many real-world networks are not. Therefore, we also propose an extension of the GIRG model which retains the manifold desirable properties induced by the degree distribution and the latent space, but also exhibits tunable assortativity. We analyze the resulting model mathematically, and give a fine-grained quantification of its assortativity.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layers of a City: Network-Based Insights into San Diego's Transportation Ecosystem</title>
<link>https://arxiv.org/abs/2508.04694</link>
<guid>https://arxiv.org/abs/2508.04694</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation networks, multi-modal analysis, network accessibility, resilience, equity

Summary:
The paper analyzes the urban transportation system of San Diego using network science, constructing a multi-layer graph encompassing driving, walking, and public transit layers. The analysis reveals a core-periphery divide, with significant equity gaps in suburban and rural access to public transit. Centrality analysis highlights the reliance on critical freeways as bottlenecks, indicating low network resilience. The study also finds that San Diego is not a broadly walkable city. Community detection shows that mobility scales differ based on transportation mode, with compact, local clusters for walking and broad, regional clusters for driving. The comprehensive framework presented can guide interventions to improve transportation equity and infrastructure resilience in San Diego.<br /><br />Summary: <div>
arXiv:2508.04694v1 Announce Type: new 
Abstract: Analyzing the structure and function of urban transportation networks is critical for enhancing mobility, equity, and resilience. This paper leverages network science to conduct a multi-modal analysis of San Diego's transportation system. We construct a multi-layer graph using data from OpenStreetMap (OSM) and the San Diego Metropolitan Transit System (MTS), representing driving, walking, and public transit layers. By integrating thousands of Points of Interest (POIs), we analyze network accessibility, structure, and resilience through centrality measures, community detection, and a proposed metric for walkability.
  Our analysis reveals a system defined by a stark core-periphery divide. We find that while the urban core is well-integrated, 30.3% of POIs are isolated from public transit within a walkable distance, indicating significant equity gaps in suburban and rural access. Centrality analysis highlights the driving network's over-reliance on critical freeways as bottlenecks, suggesting low network resilience, while confirming that San Diego is not a broadly walkable city. Furthermore, community detection demonstrates that transportation mode dictates the scale of mobility, producing compact, local clusters for walking and broad, regional clusters for driving. Collectively, this work provides a comprehensive framework for diagnosing urban mobility systems, offering quantitative insights that can inform targeted interventions to improve transportation equity and infrastructure resilience in San Diego.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Social Data-Driven System for Identifying Estate-related Events and Topics</title>
<link>https://arxiv.org/abs/2508.03711</link>
<guid>https://arxiv.org/abs/2508.03711</guid>
<content:encoded><![CDATA[
<div> Keyword: social media, estate-related events, classification, geolocation, urban management
<br />
Summary:
<br />
This study introduces a language model-based system designed to detect and classify estate-related events from social media posts. The system utilizes a hierarchical classification framework to filter and categorize relevant content, enabling the identification of actionable estate-related topics. Moreover, a transformer-based geolocation module is employed to infer posting locations for posts lacking explicit geotags, enhancing the accuracy and usefulness of the data. By integrating these components, the system provides valuable insights for urban management, operational response, and situational awareness. Overall, the system offers a data-driven approach to leveraging social media platforms for monitoring and addressing estate-related issues in urban environments. <div>
arXiv:2508.03711v1 Announce Type: cross 
Abstract: Social media platforms such as Twitter and Facebook have become deeply embedded in our everyday life, offering a dynamic stream of localized news and personal experiences. The ubiquity of these platforms position them as valuable resources for identifying estate-related issues, especially in the context of growing urban populations. In this work, we present a language model-based system for the detection and classification of estate-related events from social media content. Our system employs a hierarchical classification framework to first filter relevant posts and then categorize them into actionable estate-related topics. Additionally, for posts lacking explicit geotags, we apply a transformer-based geolocation module to infer posting locations at the point-of-interest level. This integrated approach supports timely, data-driven insights for urban management, operational response and situational awareness.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Game-Theoretic Framework for Network Formation in Large Populations</title>
<link>https://arxiv.org/abs/2508.03847</link>
<guid>https://arxiv.org/abs/2508.03847</guid>
<content:encoded><![CDATA[
<div> interaction strength, Nash equilibrium, graphon games, stochastic differential equations, uniqueness

Summary:
This paper explores a model of network formation in large populations where agents choose their interaction strength to reach a Nash equilibrium. The model considers the agents' control based on both their own index and the indices of other agents. Special attention is given to a case with piecewise constant graphs, for which optimality conditions are derived through a system of forward-backward stochastic differential equations. The paper establishes results on uniqueness and existence in this context. Numerical experiments are conducted to investigate the impact of different model settings. <div>
arXiv:2508.03847v1 Announce Type: cross 
Abstract: In this paper, we study a model of network formation in large populations. Each agent can choose the strength of interaction (i.e. connection) with other agents to find a Nash equilibrium. Different from the recently-developed theory of graphon games, here each agent's control depends not only on her own index but also on the index of other agents. After defining the general model of the game, we focus on a special case with piecewise constant graphs and we provide optimality conditions through a system of forward-backward stochastic differential equations. Furthermore, we show the uniqueness and existence results. Finally, we provide numerical experiments to discuss the effects of different model settings.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</title>
<link>https://arxiv.org/abs/2508.04542</link>
<guid>https://arxiv.org/abs/2508.04542</guid>
<content:encoded><![CDATA[
<div> Keywords: personal information, identity theft, fraud cases, privacy risks, graph-based model 

Summary:
By analyzing thousands of identity theft and fraud cases, this research explores the exposure of personal data, frequency of exposures, and consequences. The researchers developed an Identity Ecosystem graph, a model representing relationships between personally identifiable information attributes. Using graph theory and neural networks, they created a privacy risk prediction framework to estimate the likelihood of further disclosures when certain attributes are compromised. The framework effectively determines if the exposure of one identity attribute could lead to the disclosure of another. This comprehensive analysis provides insights into privacy risks and helps individuals and organizations protect personal information more effectively. <div>
arXiv:2508.04542v1 Announce Type: cross 
Abstract: It is difficult for individuals and organizations to protect personal information without a fundamental understanding of relative privacy risks. By analyzing over 5,000 empirical identity theft and fraud cases, this research identifies which types of personal data are exposed, how frequently exposures occur, and what the consequences of those exposures are. We construct an Identity Ecosystem graph--a foundational, graph-based model in which nodes represent personally identifiable information (PII) attributes and edges represent empirical disclosure relationships between them (e.g., the probability that one PII attribute is exposed due to the exposure of another). Leveraging this graph structure, we develop a privacy risk prediction framework that uses graph theory and graph neural networks to estimate the likelihood of further disclosures when certain PII attributes are compromised. The results show that our approach effectively answers the core question: Can the disclosure of a given identity attribute possibly lead to the disclosure of another attribute?
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Patterns in the Blockchain: Analysis of EOAs and Smart Contracts in ERC20 Token Networks</title>
<link>https://arxiv.org/abs/2508.04671</link>
<guid>https://arxiv.org/abs/2508.04671</guid>
<content:encoded><![CDATA[
<div> Keywords: ERC20 tokens, Ethereum blockchain, scaling laws, power law distributions, decentralized systems

Summary:
This study explores the transactional dynamics of ERC20 tokens on the Ethereum blockchain from July 2017 to March 2018. The transactions are categorized into four types based on the interacting addresses. EOA-driven transactions exhibit consistent statistical behavior with stable power law exponents and adherence to Taylor's law. In contrast, interactions involving Smart Contracts (SCs) show sublinear scaling, unstable power-law exponents, and fluctuating Taylor coefficients. SC-driven activity displays heavier-tailed distributions, indicating algorithm-driven behavior. The findings highlight the differences between human-controlled and automated transaction behaviors in blockchain ecosystems. By integrating complex systems theory and blockchain data analytics, this work establishes a framework for understanding decentralized financial systems.<br /><br />Summary: <div>
arXiv:2508.04671v1 Announce Type: cross 
Abstract: Scaling laws offer a powerful lens to understand complex transactional behaviors in decentralized systems. This study reveals distinctive statistical signatures in the transactional dynamics of ERC20 tokens on the Ethereum blockchain by examining over 44 million token transfers between July 2017 and March 2018 (9-month period). Transactions are categorized into four types: EOA--EOA, EOA--SC, SC-EOA, and SC-SC based on whether the interacting addresses are Externally Owned Accounts (EOAs) or Smart Contracts (SCs), and analyzed across three equal periods (each of 3 months). To identify universal statistical patterns, we investigate the presence of two canonical scaling laws: power law distributions and temporal Taylor's law (TL). EOA-driven transactions exhibit consistent statistical behavior, including a near-linear relationship between trade volume and unique partners with stable power law exponents ($\gamma \approx 2.3$), and adherence to TL with scaling coefficients ($\beta \approx 2.3$). In contrast, interactions involving SCs, especially SC-SC, exhibit sublinear scaling, unstable power-law exponents, and significantly fluctuating Taylor coefficients (variation in $\beta$ to be $\Delta\beta = 0.51$). Moreover, SC-driven activity displays heavier-tailed distributions ($\gamma < 2$), indicating bursty and algorithm-driven activity. These findings reveal the characteristic differences between human-controlled and automated transaction behaviors in blockchain ecosystems. By uncovering universal scaling behaviors through the integration of complex systems theory and blockchain data analytics, this work provides a principled framework for understanding the underlying mechanisms of decentralized financial systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Fix Social Media? Testing Prosocial Interventions using Generative Social Simulation</title>
<link>https://arxiv.org/abs/2508.03385</link>
<guid>https://arxiv.org/abs/2508.03385</guid>
<content:encoded><![CDATA[
<div> Keywords: social media platforms, generative social simulation, Large Language Models, political discourse, platform architecture 
Summary: 
Through generative social simulation, researchers investigated the impact of interventions on addressing societal harms associated with social media platforms. The study utilized Agent-Based Models embedded with Large Language Models to create a synthetic platform. The findings revealed three main dysfunctions: partisan echo chambers, concentrated influence among a small elite, and amplification of polarized voices. Six interventions were tested, including chronological feeds and bridging algorithms, with mixed results. The study suggested that core dysfunctions may be influenced by the feedback between reactive engagement and network growth, highlighting the need for rethinking the foundational dynamics of platform architecture to achieve meaningful reform. <div>
arXiv:2508.03385v1 Announce Type: new 
Abstract: Social media platforms have been widely linked to societal harms, including rising polarization and the erosion of constructive debate. Can these problems be mitigated through prosocial interventions? We address this question using a novel method - generative social simulation - that embeds Large Language Models within Agent-Based Models to create socially rich synthetic platforms. We create a minimal platform where agents can post, repost, and follow others. We find that the resulting following-networks reproduce three well-documented dysfunctions: (1) partisan echo chambers; (2) concentrated influence among a small elite; and (3) the amplification of polarized voices - creating a 'social media prism' that distorts political discourse. We test six proposed interventions, from chronological feeds to bridging algorithms, finding only modest improvements - and in some cases, worsened outcomes. These results suggest that core dysfunctions may be rooted in the feedback between reactive engagement and network growth, raising the possibility that meaningful reform will require rethinking the foundational dynamics of platform architecture.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</title>
<link>https://arxiv.org/abs/2508.03599</link>
<guid>https://arxiv.org/abs/2508.03599</guid>
<content:encoded><![CDATA[
<div> Keywords: Open Source Intelligence, Twitter, Russo-Ukrainian war, misinformation, sentiment analysis

Summary:<br /><br />This paper examines the role of Open Source Intelligence (OSINT) on Twitter during the Russo-Ukrainian war, identifying genuine OSINT and misinformation efforts. Analyzing millions of tweets from over a thousand users, the study reveals a prevailing negative sentiment linked to war events. It uncovers a diverse mix of pro-Ukrainian and pro-Russian partisanship, along with potential strategic manipulation of information. Named Entity Recognition (NER) and community detection techniques help identify clusters related to partisanship, topics, and misinformation. This research sheds light on the complexities of information dissemination within the OSINT community in the context of geopolitical conflicts, providing valuable insights into digital warfare and misinformation dynamics. <div>
arXiv:2508.03599v1 Announce Type: new 
Abstract: This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed "BULLSHINT." Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.03280</link>
<guid>https://arxiv.org/abs/2508.03280</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph Embedding, Hyper-relational Knowledge Graphs, Qualifiers, Graph Neural Networks, Performance evaluation 

Summary:
1. Researchers have been exploring Hyper-relational Knowledge Graphs (HKGs) as an extension of traditional Knowledge Graphs.
2. The performance of Hyper-relational Knowledge Graph Embedding (HKGE) models has been attributed to both the base model and the design of extension modules.
3. Conversion of HKGs to KG format using different methods revealed that some classical Knowledge Graph Embedding (KGE) models perform comparably to HKGE models. 
4. Existing HKGE models may struggle with capturing long-range dependencies and integrating main-triple and qualifier information effectively.
5. The FormerGNN framework proposed in this study outperforms existing HKGE models by employing a qualifier integrator, GNN-based graph encoder, and an improved approach for integrating main-triple and qualifier information.  

<br /><br />Summary: <div>
arXiv:2508.03280v1 Announce Type: cross 
Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an extension of traditional Knowledge Graphs (KGs) to better represent real-world facts with additional qualifiers. As a result, researchers have attempted to adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing extra qualifier processing modules. However, it remains unclear whether the superior performance of Hyper-relational KGE (HKGE) models arises from their base KGE model or the specially designed extension module. Hence, in this paper, we data-wise convert HKGs to KG format using three decomposition methods and then evaluate the performance of several classical KGE models on HKGs. Our results show that some KGE models achieve performance comparable to that of HKGE models. Upon further analysis, we find that the decomposition methods alter the original HKG topology and fail to fully preserve HKG information. Moreover, we observe that current HKGE models are either insufficient in capturing the graph's long-range dependency or struggle to integrate main-triple and qualifier information due to the information compression issue. To further justify our findings and offer a potential direction for future HKGE research, we propose the FormerGNN framework. This framework employs a qualifier integrator to preserve the original HKG topology, and a GNN-based graph encoder to capture the graph's long-range dependencies, followed by an improved approach for integrating main-triple and qualifier information to mitigate compression issues. Our experimental results demonstrate that FormerGNN outperforms existing HKGE models.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</title>
<link>https://arxiv.org/abs/2508.03420</link>
<guid>https://arxiv.org/abs/2508.03420</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, dynamic environmental representations, LSTM model, continuous dynamics equation, pre-trained dynamics system

Summary:
Misinformation Detection (MD) has become a crucial research topic due to the spread of misinformation on social media platforms. Traditional MD methods assume a static learning paradigm, but in reality, the veracity of news articles can change over time. To address this issue, the proposed framework, MISDER, focuses on learning social environmental representations for different periods and using a temporal model to predict future representations. Three variants of MISDER are introduced: MISDER-LSTM, MISDER-ODE, and MISDER-PT, utilizing LSTM models, continuous dynamics equations, and pre-trained dynamics systems, respectively. Experimental results on two datasets demonstrate the effectiveness of MISDER compared to baseline MD methods. This novel approach considers the dynamic nature of misinformation in evolving social environments, showcasing promising results in detecting misinformation accurately over time.<br /><br />Summary: <div>
arXiv:2508.03420v1 Announce Type: cross 
Abstract: The proliferation of misinformation across diverse social media platforms has drawn significant attention from both academic and industrial communities due to its detrimental effects. Accordingly, automatically distinguishing misinformation, dubbed as Misinformation Detection (MD), has become an increasingly active research topic. The mainstream methods formulate MD as a static learning paradigm, which learns the mapping between the content, links, and propagation of news articles and the corresponding manual veracity labels. However, the static assumption is often violated, since in real-world scenarios, the veracity of news articles may vacillate within the dynamically evolving social environment. To tackle this problem, we propose a novel framework, namely Misinformation detection with Dynamic Environmental Representations (MISDER). The basic idea of MISDER lies in learning a social environmental representation for each period and employing a temporal model to predict the representation for future periods. In this work, we specify the temporal model as the LSTM model, continuous dynamics equation, and pre-trained dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM, MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER, we compare it to various MD baselines across 2 prevalent datasets, and the experimental results can indicate the effectiveness of our proposed model.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterophily-Aware Fair Recommendation using Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2402.03365</link>
<guid>https://arxiv.org/abs/2402.03365</guid>
<content:encoded><![CDATA[
<div> fair GNN-based recommender system, fairness-aware embeddings, HetroFair, item-side fairness, unfairness, popularity bias

Summary:
HetroFair is a novel fair GNN-based recommender system designed to address unfairness and popularity bias on the item side. It utilizes fairness-aware attention to reduce the impact of nodes' degrees during the normalization process and employs heterophily feature weighting to assign varying weights to different features during aggregation. Experimental results on six real-world datasets demonstrate that HetroFair not only mitigates unfairness and popularity bias but also enhances accuracy on the user side. The proposed system offers a promising approach to improving the fairness and effectiveness of graph neural network-based recommender systems in complex multi-stakeholder environments. The implementation of HetroFair is open-source and available for further research and application. 

Summary: <div>
arXiv:2402.03365v4 Announce Type: replace-cross 
Abstract: In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve end users, but also to benefit other participants, such as items and item providers. These participants may have different or conflicting goals and interests, which raises the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias, and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve item-side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) Fairness-aware attention, which incorporates the dot product in the normalization process of GNNs to decrease the effect of nodes' degrees. ii) Heterophily feature weighting, to assign distinct weights to different features during the aggregation process. To evaluate the effectiveness of HetroFair, we conduct extensive experiments over six real-world datasets. Our experimental results reveal that HetroFair not only alleviates unfairness and popularity bias on the item side but also achieves superior accuracy on the user side. Our implementation is publicly available at https://github.com/NematGH/HetroFair.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Condensation with Evolving Capabilities</title>
<link>https://arxiv.org/abs/2502.17614</link>
<guid>https://arxiv.org/abs/2502.17614</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Condensation, Continual Learning, Evolving Graph Data, Scalability, Clustering<br />
<br />
Summary: <br />
Graph Condensation methods aim to address scalability challenges by learning a smaller graph from a larger one. However, existing approaches struggle with dynamic and evolving graph data. This paper introduces GECC (Graph Evolving Clustering Condensation), a novel framework for continual graph condensation that efficiently updates distilled graphs to handle evolving data streams. GECC utilizes class-wise clustering on aggregated features and can adapt to expanding condensed graphs by inheriting previous results as clustering centroids. The method is theoretically sound and demonstrates superior empirical performance, with experiments showing around 1000$\times$ speedup on large datasets compared to state-of-the-art methods. GECC provides a scalable solution for handling large-scale and evolving graph data efficiently. <br /> <div>
arXiv:2502.17614v2 Announce Type: replace-cross 
Abstract: The rapid growth of graph data creates significant scalability challenges as most graph algorithms scale quadratically with size. To mitigate these issues, Graph Condensation (GC) methods have been proposed to learn a small graph from a larger one, accelerating downstream tasks. However, existing approaches critically assume a static training set, which conflicts with the inherently dynamic and evolving nature of real-world graph data. This work introduces a novel framework for continual graph condensation, enabling efficient updates to the distilled graph that handle data streams without requiring costly retraining. This limitation leads to inefficiencies when condensing growing training sets. In this paper, we introduce GECC (\underline{G}raph \underline{E}volving \underline{C}lustering \underline{C}ondensation), a scalable graph condensation method designed to handle large-scale and evolving graph data. GECC employs a traceable and efficient approach by performing class-wise clustering on aggregated features. Furthermore, it can inherit previous condensation results as clustering centroids when the condensed graph expands, thereby attaining an evolving capability. This methodology is supported by robust theoretical foundations and demonstrates superior empirical performance. Comprehensive experiments including real world scenario show that GECC achieves better performance than most state-of-the-art graph condensation methods while delivering an around 1000$\times$ speedup on large datasets.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Community Spectral Clustering for Geometric Graphs</title>
<link>https://arxiv.org/abs/2508.00893</link>
<guid>https://arxiv.org/abs/2508.00893</guid>
<content:encoded><![CDATA[
<div> Spectral clustering algorithm, community recovery, dense regime, weak consistency, strong consistency <br />
<br />
Summary: 
The paper introduces a spectral clustering algorithm for community recovery in the soft geometric block model (SGBM) with a fixed number of homogeneous communities in the dense regime. The algorithm embeds the graph into $\mathbb{R}^{k-1}$ using eigenvectors associated with specific eigenvalues of the adjacency matrix and applies $k$-means clustering. Weak consistency is proven, and strong consistency is achieved through a local refinement step. The non-standard Davis-Kahan theorem is used to control eigenspace perturbations when eigenvalues are not simple. The limiting spectrum of the adjacency matrix is analyzed through combinatorial and matrix techniques. <div>
arXiv:2508.00893v1 Announce Type: new 
Abstract: In this paper, we consider the soft geometric block model (SGBM) with a fixed number $k \geq 2$ of homogeneous communities in the dense regime, and we introduce a spectral clustering algorithm for community recovery on graphs generated by this model. Given such a graph, the algorithm produces an embedding into $\mathbb{R}^{k-1}$ using the eigenvectors associated with the $k-1$ eigenvalues of the adjacency matrix of the graph that are closest to a value determined by the parameters of the model. It then applies $k$-means clustering to the embedding. We prove weak consistency and show that a simple local refinement step ensures strong consistency. A key ingredient is an application of a non-standard version of Davis-Kahan theorem to control eigenspace perturbations when eigenvalues are not simple. We also analyze the limiting spectrum of the adjacency matrix, using a combination of combinatorial and matrix techniques.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WOCD: A Semi-Supervised Method for Overlapping Community Detection Using Weak Cliques</title>
<link>https://arxiv.org/abs/2508.00927</link>
<guid>https://arxiv.org/abs/2508.00927</guid>
<content:encoded><![CDATA[
<div> community detection, overlapping, graph, deep learning, semi-supervised 

Summary:
- The paper introduces a Weak-clique based Overlapping Community Detection method (WOCD) for identifying overlapping communities in graphs.
- WOCD incorporates prior information and optimizes link information to enhance detection accuracy.
- The method utilizes pseudo-labels within a semi-supervised framework to improve generalization ability and versatility.
- Pseudo-labels are initialized using weak cliques to leverage link and prior information for better accuracy.
- WOCD combines a single-layer Graph Transformer with GNN to achieve significant performance improvements while maintaining efficiency. 

<br /><br />Summary: <div>
arXiv:2508.00927v1 Announce Type: new 
Abstract: Overlapping community detection (OCD) is a fundamental graph data analysis task for extracting graph patterns. Traditional OCD methods can be broadly divided into node clustering and link clustering approaches, both of which rely solely on link information to identify overlapping communities. In recent years, deep learning-based methods have made significant advancements for this task. However, existing GNN-based approaches often face difficulties in effectively integrating link, attribute, and prior information, along with challenges like limited receptive fields and over-smoothing, which hinder their performance on complex overlapping community detection. In this paper, we propose a Weak-clique based Overlapping Community Detection method, namely WOCD, which incorporates prior information and optimizes the use of link information to improve detection accuracy. Specifically, we introduce pseudo-labels within a semi-supervised framework to strengthen the generalization ability, making WOCD more versatile. Furthermore, we initialize pseudo-labels using weak cliques to fully leverage link and prior information, leading to better detection accuracy. Additionally, we employ a single-layer Graph Transformer combined with GNN, which achieves significant performance improvements while maintaining efficiency. We evaluate WOCD on eight real-world attributed datasets, and the results demonstrate that it outperforms the state-of-the-art semi-supervised OCD method by a significant margin in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Network Motifs on X during COVID-19</title>
<link>https://arxiv.org/abs/2508.00975</link>
<guid>https://arxiv.org/abs/2508.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: social network motifs, COVID-19 discourse, star network, bot users, human users

Summary:
In this study, the authors analyze social network motifs, specifically focusing on the simple star network patterns that appear in the discourse surrounding COVID-19 on social media platform X. They examine the different manifestations of the star motif among bot and human users. The analysis reveals six primary patterns of the star motif, distinguished by whether the bots and humans are egos or alters within the network. By detailing the prevalence of these six patterns in the data, the authors illustrate how motif patterns can be used to gain insights into social media behavioral patterns. This research highlights the potential of studying social network motifs to understand communication dynamics during significant events like the COVID-19 pandemic.<br /><br />Summary: <div>
arXiv:2508.00975v1 Announce Type: new 
Abstract: Social network motifs are recurring patterns of small subgraphs that indicate fundamental patterns of social communication. In this work, we study the simple star network motifs that recur on X during the COVID-19 discourse. We study the profile of the manifestation of the star network among bot and human users. There are six primary patterns of the star motif, differentiating by the bots and humans being either egos and alters. We describe the presentation of each of these six patterns in our data, demonstrating how the motif patterns can inform social media behavioral analysis.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-Powered Social Media Bots Realistic?</title>
<link>https://arxiv.org/abs/2508.00998</link>
<guid>https://arxiv.org/abs/2508.00998</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media bots, network science, interactions, detection

Summary:
Large Language Models (LLMs) have the potential to be utilized in creating sophisticated social media bots. This study explores the feasibility of generating LLM-Powered social media bot networks through a combination of manual effort, network science, and LLMs. Synthetic bot agent personas, their tweets, and interactions were created to simulate social media networks. A comparison between the generated networks and empirical bot/human data revealed differences in network and linguistic properties of LLM-Powered Bots compared to Wild Bots/Humans. These findings have implications for the detection and effectiveness of LLM-Powered Bots. 

<br /><br />Summary: 
1. Investigates the feasibility of using Large Language Models (LLMs) to power social media bots. 
2. Utilizes manual effort, network science, and LLMs to create synthetic bot personas and simulate social media networks. 
3. Compares generated LLM-Powered bot networks with empirical data to observe differences in properties compared to Wild Bots/Humans. 
4. Highlights implications for the detection and effectiveness of LLM-Powered Bots. <div>
arXiv:2508.00998v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Prebunking Problem: Optimizing Prebunking Targets to Suppress the Spread of Misinformation in Social Networks</title>
<link>https://arxiv.org/abs/2508.01124</link>
<guid>https://arxiv.org/abs/2508.01124</guid>
<content:encoded><![CDATA[
<div> prebunking, misinformation, social network, combinatorial optimization, approximation algorithm

Summary:
The article introduces prebunking as a preventive intervention to combat misinformation on social media. It aims to enhance cognitive resistance by exposing individuals to weakened misinformation or manipulation techniques before they encounter actual misinformation. The focus is on identifying optimal targets for prebunking interventions in social networks to curb the spread of misinformation. A combinatorial optimization problem, known as the network prebunking problem, is formulated to address this issue. The problem is proven to be NP-hard, and an approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence approach, is proposed to tackle it. Numerical experiments using real-world social network datasets show that MIA-NPP effectively reduces misinformation spread under different model parameter settings. <div>
arXiv:2508.01124v1 Announce Type: new 
Abstract: As a countermeasure against misinformation that undermines the healthy use of social media, a preventive intervention known as prebunking has recently attracted attention in the field of psychology. Prebunking aims to strengthen individuals' cognitive resistance to misinformation by presenting weakened doses of misinformation or by teaching common manipulation techniques before they encounter actual misinformation. Despite the growing body of evidence supporting its effectiveness in reducing susceptibility to misinformation at the individual level, an important open question remains: how best to identify the optimal targets for prebunking interventions to mitigate the spread of misinformation in a social network. To address this issue, we formulate a combinatorial optimization problem, called the network prebunking problem, to identify optimal prebunking targets for minimizing the spread of misinformation in a social network. We prove that this problem is NP-hard and propose an approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence (MIA) approach, which restricts influence propagation around each node to a local directed tree rooted at that node. Through numerical experiments using real-world social network datasets, we demonstrate that MIA-NPP effectively suppresses the spread of misinformation under both fully observed and uncertain model parameter settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shooting the Messenger? Harassment and Hate Speech Directed at Journalists on Social Media</title>
<link>https://arxiv.org/abs/2508.01125</link>
<guid>https://arxiv.org/abs/2508.01125</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, journalists, harassment, hate speech, gender

Summary:
This study examines the presence of harassment and hate speech towards journalists on social networks, particularly women, before and after the 2023 general elections in Spain. The analysis of responses to messages posted by journalists and media outlets on X (formerly Twitter) accounts revealed that insults and political hate were common forms of harassment, with personal accounts being targeted more than institutional ones. Although the total number of harassing messages was similar for men and women, women journalists received a greater number of sexist messages. Additionally, hate speech directed at women journalists had an ideological dimension, particularly from extremists or right-wing populists. The study highlights the role of political polarization in shaping the hostility faced by journalists, especially during election periods. It emphasizes the need for media to implement proactive policies and protective actions at both institutional and individual levels to address this systemic issue. 

<br /><br />Summary: <div>
arXiv:2508.01125v1 Announce Type: new 
Abstract: Journalists have incorporated social networks into their work as a standard tool, enhancing their ability to produce and disseminate information and making it easier for them to connect more directly with their audiences. However, this greater presence in the digital public sphere has also increased their exposure to harassment and hate speech, particularly in the case of women journalists. This study analyzes the presence of harassment and hate speech in responses (n = 60,684) to messages that 200 journalists and media outlets posted on X (formerly Twitter) accounts during the days immediately preceding and following the July 23 (23-J) general elections held in Spain in 2023. The results indicate that the most common forms of harassment were insults and political hate, which were more frequently aimed at personal accounts than institutional ones, highlighting the significant role of political polarization-particularly during election periods-in shaping the hostility that journalists face. Moreover, although, generally speaking, the total number of harassing messages was similar for men and women, it was found that a greater number of sexist messages were aimed at women journalists, and an ideological dimension was identified in the hate speech that extremists or right-wing populists directed at them. This study corroborates that this is a minor but systemic issue, particularly from a political and gender perspective. To counteract this, the media must develop proactive policies and protective actions extending even to the individual level, where this issue usually applies.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Efficient Conductance-based Community Search at Billion Scale</title>
<link>https://arxiv.org/abs/2508.01244</link>
<guid>https://arxiv.org/abs/2508.01244</guid>
<content:encoded><![CDATA[
<div> Algorithm, Conductance, Community search, Graph clustering, Semi-supervised

Summary: 
The paper introduces Conductance-based Community Search (CCS), a novel approach to finding a connected subgraph with the lowest conductance that includes a user-specified query vertex. The CCS problem is proven to be NP-hard. The proposed SCCS algorithm addresses this challenge through a four-stage process. First, the graph is reduced using local sampling techniques. Then, a three-stage local optimization strategy is employed to improve community quality. This strategy involves seeding, expansion, and verification stages to enhance internal cohesiveness and external sparsity of the community. Experimental results on real-world and synthetic datasets show the effectiveness, efficiency, and scalability of the proposed approach. <div>
arXiv:2508.01244v1 Announce Type: new 
Abstract: Community search is a widely studied semi-supervised graph clustering problem, retrieving a high-quality connected subgraph containing the user-specified query vertex. However, existing methods primarily focus on cohesiveness within the community but ignore the sparsity outside the community, obtaining sub-par results. Inspired by this, we adopt the well-known conductance metric to measure the quality of a community and introduce a novel problem of conductance-based community search (CCS). CCS aims at finding a subgraph with the smallest conductance among all connected subgraphs that contain the query vertex. We prove that the CCS problem is NP-hard. To efficiently query CCS, a four-stage subgraph-conductance-based community search algorithm, SCCS, is proposed. Specifically, we first greatly reduce the entire graph using local sampling techniques. Then, a three-stage local optimization strategy is employed to continuously refine the community quality. Namely, we first utilize a seeding strategy to obtain an initial community to enhance its internal cohesiveness. Then, we iteratively add qualified vertices in the expansion stage to guarantee the internal cohesiveness and external sparsity of the community. Finally, we gradually remove unqualified vertices during the verification stage. Extensive experiments on real-world datasets containing one billion-scale graph and synthetic datasets show the effectiveness, efficiency, and scalability of our solutions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A graph neural network based on feature network for identifying influential nodes</title>
<link>https://arxiv.org/abs/2508.01278</link>
<guid>https://arxiv.org/abs/2508.01278</guid>
<content:encoded><![CDATA[
<div> Influential nodes, Complex networks, Graph Convolutional Network, Feature Network, Local centralities<br />
Summary:<br />
Identifying influential nodes in complex networks is essential for various applications such as e-commerce and computer information systems. Existing methods have limitations in considering network structure and using global centralities for node features. To address these issues, a Graph Convolutional Network Framework based on Feature Network (FNGCN) is proposed. FNGCN utilizes a feature network to capture relationships among local centralities and determine the most suitable ones. Two FNGCNs, with shallow and deep GCN layers, are developed and compared to state-of-the-art methods using the SIR model. Experimental results on real-world networks demonstrate that the proposed framework effectively identifies influential nodes with higher accuracy. <div>
arXiv:2508.01278v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is of great importance, and has many applications in practice. For example, finding influential nodes in e-commerce network can provide merchants with customers with strong purchase intent; identifying influential nodes in computer information system can help locating the components that cause the system break down and identifying influential nodes in these networks can accelerate the flow of information in networks. Thus, a lot of efforts have been made on the problem of indentifying influential nodes. However, previous efforts either consider only one aspect of the network structure, or using global centralities with high time consuming as node features to identify influential nodes, and the existing methods do not consider the relationships between different centralities. To solve these problems, we propose a Graph Convolutional Network Framework based on Feature Network, abbreviated as FNGCN (graph convolutional network is abbreviated as GCN in the following text). Further, to exclude noises and reduce redundency, FNGCN utilizes feature network to represent the complicated relationships among the local centralities, based on which the most suitable local centralities are determined. By taking a shallow GCN and a deep GCN into the FNGCN framework, two FNGCNs are developed. With ground truth obtained from the widely used Susceptible Infected Recovered (SIR) model, the two FNGCNs are compared with the state-of-art methods on several real-world networks. Experimental results show that the two FNGCNs can identify the influential nodes more accurately than the compared methods, indicating that the proposed framework is effective in identifying influential nodes in complex networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-term resilience of online battle over vaccines and beyond</title>
<link>https://arxiv.org/abs/2508.01398</link>
<guid>https://arxiv.org/abs/2508.01398</guid>
<content:encoded><![CDATA[
<div> Keywords: pro-vaccine science, online competition, Facebook, network architecture, opinion moderation

Summary:
The study evaluates the impact of promoting pro-vaccine science on online platforms, specifically Facebook, before and during the COVID-19 pandemic. By analyzing the interactions of approximately 100 million Facebook Page members across 1,356 interconnected communities, the researchers find that despite significant efforts, the fundamental network structure remains unchanged. The isolation of established expertise and the coexistence of anti-vaccination and mainstream communities persist. This resilience is attributed to the evolution of "glocal" communities that blend various topics and span different scales. To address this issue, the study suggests focusing on network engineering approaches for opinion moderation, rather than content removal, as a more effective strategy. This shift towards structural interventions represents a new paradigm for combating misinformation and promoting pro-vaccine sentiments. <div>
arXiv:2508.01398v1 Announce Type: new 
Abstract: What has been the impact of the enormous amounts of time, effort and money spent promoting pro-vaccine science from pre-COVID-19 to now? We answer this using a unique mapping of online competition between pro- and anti-vaccination views among ~100M Facebook Page members, tracking 1,356 interconnected communities through platform interventions. Remarkably, the network's fundamental architecture shows no change: the isolation of established expertise and the symbiosis of anti and mainstream neutral communities persist. This means that even if the same time, effort and money continue to be spent, nothing will likely change. The reason for this resilience lies in "glocal" evolution: Communities blend multiple topics while bridging neighborhood-level to international scales, creating redundant pathways that transcend categorical targeting. The solution going forward is to focus on the system's network. We show how network engineering approaches can achieve opinion moderation without content removal, representing a paradigm shift from suppression towards structural interventions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel Algorithm for Finding Robust Spanners in Large Social Networks</title>
<link>https://arxiv.org/abs/2508.01485</link>
<guid>https://arxiv.org/abs/2508.01485</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, community structures, structural hole spanners, robust spanners, CUDA implementation

Summary:
Robust spanners (RS) are introduced as nodes in social networks that can efficiently bridge community structures even in the face of disruptions like node or edge removals. A novel scoring technique is proposed to identify RS nodes, which demonstrate a spanning capacity comparable to benchmark spanner detection algorithms while also offering superior robustness. An efficient parallel algorithm with a CUDA implementation is presented for the detection of RS nodes in large networks. Empirical analysis of real-world social networks shows that high-scoring RS nodes exhibit significant robustness and spanning capabilities. The Nvidia GPU implementation of the algorithm achieves an impressive average speedup of 244X over traditional spanner detection techniques, highlighting its effectiveness in identifying RS nodes in large social networks.<br /><br />Summary: Robust spanners (RS) are introduced to bridge community structures in social networks despite disruptions, with a novel scoring technique identifying high-scoring RS nodes. A parallel algorithm with a CUDA implementation achieves high efficiency in detecting RS nodes, which exhibit comparable spanning capacity to benchmark algorithms but offer superior robustness. Evidenced through empirical analysis, the Nvidia GPU implementation of the algorithm demonstrates a significant speedup over traditional techniques, showcasing its efficacy in identifying RS nodes in large social networks. <div>
arXiv:2508.01485v1 Announce Type: new 
Abstract: Social networks, characterized by community structures, often rely on nodes called structural hole spanners to facilitate inter-community information dissemination. However, the dynamic nature of these networks, where spanner nodes may be removed, necessitates resilient methods to maintain inter-community communication. To this end, we introduce robust spanners (RS) as nodes uniquely equipped to bridge communities despite disruptions, such as node or edge removals. We propose a novel scoring technique to identify RS nodes and present a parallel algorithm with a CUDA implementation for efficient RS detection in large networks. Empirical analysis of real-world social networks reveals that high-scoring nodes exhibit a spanning capacity comparable to those identified by benchmark spanner detection algorithms while offering superior robustness. Our implementation on Nvidia GPUs achieves an average speedup of 244X over traditional spanner detection techniques, demonstrating its efficacy to identify RS in large social networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Information Operations</title>
<link>https://arxiv.org/abs/2508.01552</link>
<guid>https://arxiv.org/abs/2508.01552</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, information operations, optimization framework, influence campaigns, generative AI

Summary: 
The tutorial introduces a formal optimization framework for social media information operations (IO) in online social networks. It emphasizes the importance of understanding the terrain, modeling adversaries, and executing interventions for success in shaping opinions through targeted actions. Analytic tools such as centrality measures, clustering algorithms, and sentiment analysis aid in mapping the information environment, identifying influential users, detecting community structures, and gauging public opinion. Threats like coordinated bot networks, extremist recruitment, and viral misinformation are highlighted, with countermeasures ranging from content-level interventions to mathematically optimized influence strategies. The emergence of generative AI transforms both offense and defense in information warfare, demanding algorithmic innovation, policy reform, and ethical vigilance to safeguard the digital public sphere's integrity. <div>
arXiv:2508.01552v1 Announce Type: new 
Abstract: The battlefield of information warfare has moved to online social networks, where influence campaigns operate at unprecedented speed and scale. As with any strategic domain, success requires understanding the terrain, modeling adversaries, and executing interventions. This tutorial introduces a formal optimization framework for social media information operations (IO), where the objective is to shape opinions through targeted actions. This framework is parameterized by quantities such as network structure, user opinions, and activity levels - all of which must be estimated or inferred from data. We discuss analytic tools that support this process, including centrality measures for identifying influential users, clustering algorithms for detecting community structure, and sentiment analysis for gauging public opinion. These tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment. With the landscape mapped, we highlight threats such as coordinated bot networks, extremist recruitment, and viral misinformation. Countermeasures range from content-level interventions to mathematically optimized influence strategies. Finally, the emergence of generative AI transforms both offense and defense, democratizing persuasive capabilities while enabling scalable defenses. This shift calls for algorithmic innovation, policy reform, and ethical vigilance to protect the integrity of our digital public sphere.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Social Media Sentiment for Predictive Algorithmic Trading Strategies</title>
<link>https://arxiv.org/abs/2508.02089</link>
<guid>https://arxiv.org/abs/2508.02089</guid>
<content:encoded><![CDATA[
<div> Keywords: social media sentiment, Reddit comments, investment decisions, BERTweet, Sentiment Volume Change metric

Summary:
This study explores the use of social media sentiment, particularly from Reddit comments, in enhancing investment decisions for higher returns with lower risk. Utilizing BERTweet, over 2 million Reddit comments from the r/wallstreetbets subreddit were analyzed to develop a Sentiment Volume Change (SVC) metric, which combined sentiment and comment volume changes to predict next-day returns more effectively than sentiment alone. Two investment strategies based on SVC were back-tested over four years, outperforming a buy-and-hold strategy significantly. In a bull market, the sentiment-powered strategies achieved 70% higher returns in 2023 and 84.4% higher returns in 2021, while also mitigating losses by 4% in a declining market in 2022. The results highlight that social media sentiment data can be valuable in predicting short-term stock price movements and offer superior risk-adjusted returns compared to traditional market approaches.<br /><br />Summary: <div>
arXiv:2508.02089v1 Announce Type: new 
Abstract: This study investigates how social media sentiment derived from Reddit comments can be used to enhance investment decisions in a way that offers higher returns with lower risk. Using BERTweet we analyzed over 2 million Reddit comments from the subreddit r/wallstreetbets and developed a Sentiment Volume Change (SVC) metric combining sentiment and comment volume changes, which showed significantly improved correlation with next-day returns compared to sentiment alone. We then implemented two different investment strategies that relied solely on SVC to make decisions. Back testing these strategies over four years (2020-2023) our strategies significantly outperformed a comparable buy-and-hold (B&amp;H) strategy in a bull market, achieving 70% higher returns in 2023 and 84.4% higher returns in 2021 while also mitigating losses by 4% in a declining market in 2022. Our results confirm that comment sentiment and volume data derived from Reddit can be effective in predicting short-term stock price movements and sentiment-powered strategies can offer superior risk-adjusted returns as compared to the market, implying that social media sentiment can potentially be a valuable investment tool.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Trust Embeddings from Siamese Trust Scores: A Direct-Sum Approach with Fixed-Point Semantics</title>
<link>https://arxiv.org/abs/2508.01479</link>
<guid>https://arxiv.org/abs/2508.01479</guid>
<content:encoded><![CDATA[
<div> trust embeddings, Siamese trust scores, reconstruction, privacy risk, counter-measures

Summary:
The study focuses on reconstructing high-dimensional trust embeddings from one-dimensional Siamese trust scores in distributed-security frameworks. Through formalizing the estimation task and developing a direct-sum estimator with moment features, the research achieves a unique fixed point for the reconstruction map using Banach theory. Synthetic benchmarks confirm the preservation of inter-device geometry in reconstructed embeddings, even in the presence of noise, with non-asymptotic error bounds providing insight into reconstruction accuracy. Additionally, the paper highlights a privacy risk associated with publishing trust scores, which can leak latent behavioral information. Counter-measures such as score quantization, calibrated noise, and obfuscated embedding spaces are discussed as ways to address this risk within networked AI systems. Availability of datasets, reproduction scripts, and extended proofs ensures the reproducibility of results without requiring proprietary code.<br /><br />Summary: <div>
arXiv:2508.01479v1 Announce Type: cross 
Abstract: We study the inverse problem of reconstructing high-dimensional trust embeddings from the one-dimensional Siamese trust scores that many distributed-security frameworks expose. Starting from two independent agents that publish time-stamped similarity scores for the same set of devices, we formalise the estimation task, derive an explicit direct-sum estimator that concatenates paired score series with four moment features, and prove that the resulting reconstruction map admits a unique fixed point under a contraction argument rooted in Banach theory. A suite of synthetic benchmarks (20 devices x 10 time steps) confirms that, even in the presence of Gaussian noise, the recovered embeddings preserve inter-device geometry as measured by Euclidean and cosine metrics; we complement these experiments with non-asymptotic error bounds that link reconstruction accuracy to score-sequence length. Beyond methodology, the paper demonstrates a practical privacy risk: publishing granular trust scores can leak latent behavioural information about both devices and evaluation models. We therefore discuss counter-measures -- score quantisation, calibrated noise, obfuscated embedding spaces -- and situate them within wider debates on transparency versus confidentiality in networked AI systems. All datasets, reproduction scripts and extended proofs accompany the submission so that results can be verified without proprietary code.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning via Embedding Reconstruction -- A Range-Null Space Decomposition Approach</title>
<link>https://arxiv.org/abs/2508.02044</link>
<guid>https://arxiv.org/abs/2508.02044</guid>
<content:encoded><![CDATA[
<div> node unlearning, graph unlearning, GNNs, graph structure, GIF

Summary:
Node unlearning is a novel method proposed for Graph Neural Networks (GNNs) to address various requests for graph structure unlearning. The GIF (graph influence function) has been effective in handling partial edge unlearning but struggles with node unlearning. This new approach reverses the aggregation process in GNNs through embedding reconstruction and utilizes Range-Null Space Decomposition for nodes' interaction learning. By avoiding the need for retraining, the model's utility is preserved even after unlearning. Experimental results on different datasets showcase the state-of-the-art performance of this method. <div>
arXiv:2508.02044v1 Announce Type: cross 
Abstract: Graph unlearning is tailored for GNNs to handle widespread and various graph structure unlearning requests, which remain largely unexplored. The GIF (graph influence function) achieves validity under partial edge unlearning, but faces challenges in dealing with more disturbing node unlearning. To avoid the overhead of retraining and realize the model utility of unlearning, we proposed a novel node unlearning method to reverse the process of aggregation in GNN by embedding reconstruction and to adopt Range-Null Space Decomposition for the nodes' interaction learning. Experimental results on multiple representative datasets demonstrate the SOTA performance of our proposed approach.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare</title>
<link>https://arxiv.org/abs/2508.02574</link>
<guid>https://arxiv.org/abs/2508.02574</guid>
<content:encoded><![CDATA[
<div> Dataset, Arabic, Healthcare, Sentiment Analysis, Aspect-based <br />
Summary: 
EHSAN introduces a data-centric hybrid pipeline, merging ChatGPT pseudo-labelling with human review to create an Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label, with ChatGPT-generated rationales for transparency. Three versions of training data were created: fully supervised, semi-supervised, and unsupervised. Transformer models fine-tuned on these datasets showed high accuracy even with minimal human supervision. Reduction of aspect classes improved classification metrics. The study demonstrates an effective approach to Arabic aspect-based sentiment analysis in healthcare by combining large language model annotation with human expertise. Future directions include generalization across hospitals, prompt refinement, and interpretable data-driven modeling.<br /><br /> <div>
arXiv:2508.02574v1 Announce Type: cross 
Abstract: Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confrontation of capitalism and socialism in Wikipedia networks</title>
<link>https://arxiv.org/abs/2408.07606</link>
<guid>https://arxiv.org/abs/2408.07606</guid>
<content:encoded><![CDATA[
<div> Wikipedia, Ising Network Opinion Formation, opinion polarization, capitalism, socialism<br />
<br />
Summary: 
The article introduces the Ising Network Opinion Formation (INOF) model for analyzing networks of 6 Wikipedia language editions. In this model, Ising spins represent opinions at network nodes/articles and opinion polarization is determined through Monte Carlo iterations. The focus is on the opinion confrontation between capitalism/imperialism and socialism/communism, with the global network opinion favoring socialism and communism across all editions. Opinion preferences for world countries and political leaders are also determined. The model is applied to analyze opinion competition between Christianity and Islam, as well as the USA Democratic and Republican parties, showing promising results. The article suggests that the INOF approach has potential for various applications in directed complex networks.<br /><br />Summary: <div>
arXiv:2408.07606v2 Announce Type: replace 
Abstract: We introduce the Ising Network Opinion Formation (INOF) model and apply it for the analysis of networks of 6 Wikipedia language editions. In the model, Ising spins are placed at network nodes/articles and the steady-state opinion polarization of spins is determined from the Monte Carlo iterations in which a given spin orientation is determined by in-going links from other spins. The main consideration is done for opinion confrontation between {\it capitalism, imperialism} (blue opinion) and {\it socialism, communism} (red opinion). These nodes have fixed spin/opinion orientation while other nodes achieve their steady-state opinions in the process of Monte Carlo iterations. We find that the global network opinion favors {\it socialism, communism} for all 6 editions. The model also determines the opinion preferences for world countries and political leaders, showing good agreement with heuristic expectations. We also present results for opinion competition between {\it Christianity} and {\it Islam}, and USA Democratic and Republican parties. We argue that the INOF approach can find numerous applications for directed complex networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram</title>
<link>https://arxiv.org/abs/2410.08642</link>
<guid>https://arxiv.org/abs/2410.08642</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal topic modeling, conspiracy theories, social media, textual and visual data, communication<br />
Summary:<br />
This study focuses on using multimodal topic modeling to analyze conspiracy theories in German-language Telegram channels. The researchers analyze textual and visual data from 40,000 Telegram messages in 571 channels known for spreading conspiracy theories. They use BERTopic with CLIP to explore topics across modalities and compare them. The study reveals the diversity of content shared in these channels and proposes a framework to analyze textual and visual discursive strategies in conspiracy theory communication. A case study on the topic group Israel Gaza is included as an example. This research fills a gap in existing literature by expanding beyond memes to analyze visual content and provide a method to compare topic models across modalities. The findings contribute to understanding the communication of conspiracy theories on social media. <br /><br />Summary: <div>
arXiv:2410.08642v3 Announce Type: replace 
Abstract: To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. However, existing research often neglects visual content beyond memes, and in addition lacks methods to compare topic models across modalities. Our study addresses these gaps by applying multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. We use BERTopic with CLIP for the analysis of textual and visual data in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories. Through this dataset, we provide insights into unimodal and multimodal topic models by analyzing symmetry and intersections of topics across modalities. We demonstrate the variety of textual and visual content shared in the channels discovered through the topic modeling, and propose a conceptual framework for the analysis of textual and visual discursive strategies in the communication of conspiracy theories. We apply the framework in a case study of the topic group Israel Gaza.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interdisciplinarity Revealed by Transitive Reduction of Citation Networks</title>
<link>https://arxiv.org/abs/1802.06015</link>
<guid>https://arxiv.org/abs/1802.06015</guid>
<content:encoded><![CDATA[
<div> Keywords: transitive reduction, citation networks, interdisciplinary, intradisciplinary, modularity-based clustering

Summary: 
The study focuses on the impact of transitive reduction on citation networks, aiming to distinguish between interdisciplinary and intradisciplinary documents. The hypothesis suggests that documents with minimal citation loss after transitive reduction are likely interdisciplinary, contrasting with those primarily cited within a single discipline. Testing this hypothesis involved an artificial citation network model and data from academic papers, court decisions, and patents. Utilizing modularity-based clustering techniques for topic classification, nodes were categorized as interdisciplinary or intradisciplinary based on cluster-dependent measures. Strong evidence supporting the hypothesis was found in three out of four cases, with patents showing somewhat weaker but still positive support. Overall, the research underscores the role of transitive reduction in identifying interdisciplinary documents within citation networks, offering insights into the interdisciplinary nature of scholarly literature and innovation. 

Summary: <div>
arXiv:1802.06015v2 Announce Type: replace-cross 
Abstract: We investigate the impact of transitive reduction on citation networks. Our hypothesis is that documents which lose fewer citations under transitive reduction are likely to be interdisciplinary, while a large loss of citations suggests a document is primarily cited within a single discipline. We test this hypothesis by using an artificial model of a citation network and by using data on citations from three sources: academic papers, court decisions and patents. Where needed, we applied modularity-based clustering techniques on a network defined using bibliographic coupling to classify documents by topic. A cluster-dependent measure was then used to classify the nodes as interdisciplinary or intradisciplinary. Our results provide strong support for our hypothesis in three of the four cases, with somewhat weaker but still positive support in the case of patents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Embedding with Completely-imbalanced Labels</title>
<link>https://arxiv.org/abs/2007.03545</link>
<guid>https://arxiv.org/abs/2007.03545</guid>
<content:encoded><![CDATA[
<div> Keywords: Network embedding, semi-supervised learning, imbalanced labels, graph neural networks, node features<br />
Summary:<br />
Semi-supervised network embedding methods are crucial for learning representations of networks, but they struggle in completely imbalanced label settings. To address this, two novel methods are proposed: RSDNE and RECT. RSDNE focuses on maintaining intra-class similarity and inter-class dissimilarity, while RECT utilizes class-semantic knowledge to handle node features and multi-label settings. Experimental results on real-world datasets show the effectiveness of both methods. The code for RECT is available on GitHub for further exploration. <div>
arXiv:2007.03545v2 Announce Type: replace-cross 
Abstract: Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods. Code is available at https://github.com/zhengwang100/RECT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Node Duplication Improves Cold-start Link Prediction</title>
<link>https://arxiv.org/abs/2402.09711</link>
<guid>https://arxiv.org/abs/2402.09711</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Link Prediction, Low-degree nodes, NodeDup, Cold-start problem
Summary:
The study focuses on enhancing Graph Neural Networks (GNNs) performance in Link Prediction tasks, particularly on low-degree nodes. Existing GNNs face challenges in producing accurate results for low-degree nodes, crucial for applications like recommendation systems. The proposed augmentation technique, NodeDup, duplicates low-degree nodes and establishes links with their duplicates to improve performance. By adopting a "multi-view" approach for low-degree nodes, NodeDup demonstrates significant performance enhancements for low-degree nodes without compromising high-degree node performance. NodeDup serves as a lightweight and easily applicable module, seamlessly integrating with existing GNNs. Experimental results showcase substantial improvements, with NodeDup achieving remarkable enhancements on isolated, low-degree, and warm nodes compared to conventional GNNs and state-of-the-art cold-start methods. <div>
arXiv:2402.09711v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of graph embedding methods for community detection</title>
<link>https://arxiv.org/abs/2405.00636</link>
<guid>https://arxiv.org/abs/2405.00636</guid>
<content:encoded><![CDATA[
<div> robustness, graph embedding methods, community detection, network perturbations, network structure

Summary:<br />
- Investigates robustness of graph embedding methods for community detection under edge deletions.
- Considers matrix factorization and random walk-based methods.
- Finds varying degrees of robustness within each method family.
- Node2vec and LLE show higher robustness across different scenarios.
- Emphasizes the importance of selecting appropriate graph embedding method based on network characteristics and task requirements.

Summary: <br />
- Investigates robustness of graph embedding methods for community detection under edge deletions.
- Considers matrix factorization and random walk-based methods.
- Finds varying degrees of robustness within each method family.
- Node2vec and LLE show higher robustness across different scenarios.
- Emphasizes the importance of selecting appropriate graph embedding method based on network characteristics and task requirements. <div>
arXiv:2405.00636v3 Announce Type: replace-cross 
Abstract: This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning</title>
<link>https://arxiv.org/abs/2504.08129</link>
<guid>https://arxiv.org/abs/2504.08129</guid>
<content:encoded><![CDATA[
<div> linear time encoder, temporal relationships, dynamic graph learning, self-attention mechanism, parameter savings
Summary:
This paper presents a study on the effectiveness of linear time encoders compared to sinusoidal time encoders in dynamic graph learning models. The linear time encoder, a simpler alternative, avoids temporal information loss and reduces the need for high-dimensional encoders. By using the self-attention mechanism to compute time spans between events from linear time encodings, relevant temporal patterns can be extracted. Experimental results on six dynamic graph datasets show that the linear time encoder improves the performance of existing models such as TGAT and DyGFormer. Additionally, significant parameter savings can be achieved with the linear time encoder, with minimal loss in performance. This study underscores the advantages of using linear time features in dynamic graph learning architectures, which can benefit various applications such as recommender systems, communication networks, and traffic forecasting.<br /><br />Summary: <div>
arXiv:2504.08129v2 Announce Type: replace-cross 
Abstract: Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal dependencies between edge events. Prior work justified sinusoidal encodings because their inner products depend on the time spans between events, which are crucial features for modeling inter-event relations. However, sinusoidal encodings inherently lose temporal information due to their many-to-one nature and therefore require high dimensions. In this paper, we rigorously study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high-dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans between events from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. While both encoders can be used simultaneously, our study highlights the often-overlooked advantages of linear time features in modern dynamic graph models. These findings can positively impact the design choices of various dynamic graph learning architectures and eventually benefit temporal network applications such as recommender systems, communication networks, and traffic forecasting.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individuals to Crowds: Dual-Level Public Response Prediction in Social Media</title>
<link>https://arxiv.org/abs/2508.00497</link>
<guid>https://arxiv.org/abs/2508.00497</guid>
<content:encoded><![CDATA[
<div> Keywords: Public response prediction, SocialAlign, micro-level personalization, macro-level sentiment distribution, SentiWeibo <br />
<br />
Summary: 
SocialAlign is a new framework for predicting public responses in social contexts. It addresses the limitations of existing works by offering micro-level personalization through specialized modules for content analysis and response generation. At the macro level, SocialAlign models group sentiment distributions to align predictions with real-world sentiment trends. The framework is evaluated using a large-scale dataset called SentiWeibo. Experimental results demonstrate that SocialAlign outperforms strong baselines in terms of accuracy, interpretability, and generalization in public response prediction. This work contributes to advancing research in public response prediction and computational social science. <div>
arXiv:2508.00497v1 Announce Type: new 
Abstract: Public response prediction is critical for understanding how individuals or groups might react to specific events, policies, or social phenomena, making it highly valuable for crisis management, policy-making, and social media analysis. However, existing works face notable limitations. First, they lack micro-level personalization, producing generic responses that ignore individual user preferences. Moreover, they overlook macro-level sentiment distribution and only deal with individual-level sentiment, constraining them from analyzing broader societal trends and group sentiment dynamics. To address these challenges, we propose SocialAlign, a unified framework that predicts real-world responses at both micro and macro levels in social contexts. At the micro level, SocialAlign employs SocialLLM with an articulate Personalized Analyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert modules for content analysis and response generation across diverse topics and user profiles, enabling the generation of personalized comments with corresponding sentiments. At the macro level, it models group sentiment distributions and aligns predictions with real-world sentiment trends derived from social media data. To evaluate SocialAlign in real-world scenarios, we introduce SentiWeibo, a large-scale dataset curated from authentic social interactions on the Weibo platform. Experimental results on our SentiWeibo and related LaMP benchmark demonstrate that SocialAlign surpasses strong baselines, showing improved accuracy, interpretability, and generalization in public response prediction. We hope our work inspires further research in public response prediction and computational social science: https://github.com/Znull-1220/SocialAlign.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web</title>
<link>https://arxiv.org/abs/2507.23585</link>
<guid>https://arxiv.org/abs/2507.23585</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithm-driven interfaces, user agency, hypertextual friction, comparative analysis, reclaiming agency

Summary: 
This paper discusses the impact of algorithm-driven interfaces on user agency and introduces the concept of "Hypertextual Friction" as a way to reclaim control in these environments. Through a comparison of real-world interfaces such as Wikipedia and Instagram Explore, as well as Are.na and GenAI image tools, the authors highlight how different systems structure user experience and navigation. Hypertext systems prioritize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to diminish user participation and obscure processes. The study provides a comparative analysis of interface structures in user-driven versus agent-driven systems and proposes hypertextual values as design commitments for increasing agency in a digital landscape dominated by algorithms.

<br /><br />Summary: <div>
arXiv:2507.23585v1 Announce Type: cross 
Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI tools, often prioritize engagement and efficiency at the expense of user agency. As systems take on more decision-making, users have less control over what they see and how meaning or relationships between content are constructed. This paper introduces "Hypertextual Friction," a conceptual design stance that repositions classical hypertext principles--friction, traceability, and structure--as actionable values for reclaiming agency in algorithmically mediated environments. Through a comparative analysis of real-world interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image tools--we examine how different systems structure user experience, navigation, and authorship. We show that hypertext systems emphasize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to obscure process and flatten participation. We contribute: (1) a comparative analysis of how interface structures shape agency in user-driven versus agent-driven systems, and (2) a conceptual stance that offers hypertextual values as design commitments for reclaiming agency in an increasingly algorithmic web.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Bias in Human Mobility is a Universal Phenomenon but is Highly Location-specific</title>
<link>https://arxiv.org/abs/2508.00149</link>
<guid>https://arxiv.org/abs/2508.00149</guid>
<content:encoded><![CDATA[
<div> bias, human mobility data, data production, GPS, prediction

Summary:
- Large-scale human mobility datasets are crucial in many fields but have biases that impact analyses and predictions.
- Data production is studied to understand how individuals are represented in datasets and how much data they produce.
- Analysis of GPS mobility data from smartphones in ten US cities shows inequality in data distribution, with strong effects of wealth, ethnicity, and education.
- Bias is universal in all cities, but each city has its own manifestation of bias, requiring location-specific models.
- This study highlights the need for further research to address biases in human mobility data. 

<br /><br />Summary: <div>
arXiv:2508.00149v1 Announce Type: cross 
Abstract: Large-scale human mobility datasets play increasingly critical roles in many algorithmic systems, business processes and policy decisions. Unfortunately there has been little focus on understanding bias and other fundamental shortcomings of the datasets and how they impact downstream analyses and prediction tasks. In this work, we study `data production', quantifying not only whether individuals are represented in big digital datasets, but also how they are represented in terms of how much data they produce. We study GPS mobility data collected from anonymized smartphones for ten major US cities and find that data points can be more unequally distributed between users than wealth. We build models to predict the number of data points we can expect to be produced by the composition of demographic groups living in census tracts, and find strong effects of wealth, ethnicity, and education on data production. While we find that bias is a universal phenomenon, occurring in all cities, we further find that each city suffers from its own manifestation of it, and that location-specific models are required to model bias for each city. This work raises serious questions about general approaches to debias human mobility data and urges further research.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Dynamic Epidemic Model for Successive Opinion Diffusion in Social Networks</title>
<link>https://arxiv.org/abs/2504.01718</link>
<guid>https://arxiv.org/abs/2504.01718</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic epidemic model, opinion diffusion, social networks, rumor spread, echo chamber effect

Summary: 
This paper presents a novel dynamic epidemic model that addresses successive opinion diffusion in social networks, building upon the SHIMR model. The model incorporates dynamic decision-making influenced by social distances and accounts for accumulative opinion diffusion resulting from interconnected rumors. By considering the impact of rumor spread on social network structures, the model offers valuable insights into phenomena such as the echo chamber effect. Through simulations, the effectiveness of the model in explaining various dynamics of opinion diffusion is validated. The findings provide a deeper understanding of social polarization and network evolution, highlighting the intricate interplay between opinion diffusion and social network dynamics. <div>
arXiv:2504.01718v2 Announce Type: replace 
Abstract: This paper proposes a dynamic epidemic model for successive opinion diffusion in social networks, extending the SHIMR model. It incorporates dynamic decision-making influenced by social distances and captures accumulative opinion diffusion caused by interrelated rumors. The model reflects the impact of rumor spread on social network structures. Simulations validate its effectiveness in explaining phenomena like the echo chamber effect and provide insights into opinion diffusion dynamics, with implications for understanding social polarization and network evolution.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Between the Nodes: Community Discovery Beyond Vectors</title>
<link>https://arxiv.org/abs/2507.22955</link>
<guid>https://arxiv.org/abs/2507.22955</guid>
<content:encoded><![CDATA[
<div> Keyword: Community detection, Social network graphs, Large Language Models, Prompt-based reasoning, Graph-aware strategies

Summary:
Community detection in social network graphs is crucial for understanding group dynamics and information spread. This paper explores the integration of Large Language Models (LLMs) in identifying communities within social graphs. The proposed CommLLM framework combines the GPT-4o model with prompt-based reasoning to incorporate semantic and contextual information. Evaluations on real-world datasets show that LLMs, especially when guided by graph-aware strategies, can effectively detect communities in small to medium-sized graphs. Integration of instruction-tuned models and carefully crafted prompts improves accuracy and coherence of detected communities. These findings emphasize the potential of LLMs in graph-based research and highlight the importance of tailoring model interactions to the structure of graph data. 

<br /><br />Summary: <div>
arXiv:2507.22955v1 Announce Type: new 
Abstract: Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices</title>
<link>https://arxiv.org/abs/2507.23025</link>
<guid>https://arxiv.org/abs/2507.23025</guid>
<content:encoded><![CDATA[
<div> framework, directed graphs, sampling algorithm, Joint Degree Matrix, Degree Correlation Matrix
Summary:
The article proposes a new framework for sampling directed graphs to speed up the analysis of large networks. By rescaling the Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM), a sample graph can be constructed while preserving important graph properties. Experiments on real-world datasets show that the number of non-zero entries in JDM and DCM is small compared to the number of edges and nodes. The proposed algorithm can preserve in-degree and out-degree distributions, as well as joint degree distribution and degree correlation distribution. The algorithm's performance is expected to exceed theoretical expectations due to the negative correlation between deviations and the sparsity of JDM and DCM. The framework offers a promising approach for analyzing large directed networks efficiently.<br /><br />Summary: <div>
arXiv:2507.23025v1 Announce Type: new 
Abstract: In recent years, many large directed networks such as online social networks are collected with the help of powerful data engineering and data storage techniques. Analyses of such networks attract significant attention from both the academics and industries. However, analyses of large directed networks are often time-consuming and expensive because the complexities of a lot of graph algorithms are often polynomial with the size of the graph. Hence, sampling algorithms that can generate graphs preserving properties of original graph are of great importance because they can speed up the analysis process. We propose a promising framework to sample directed graphs: Construct a sample graph with linearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM). Previous work shows that graphs with the same JDM and DCM will have a range of very similar graph properties. We also conduct experiments on real-world datasets to show that the numbers of non-zero entries in JDM and DCM are quite small compared to the number of edges and nodes. Adopting this framework, we propose a novel graph sampling algorithm that can provably preserves in-degree and out-degree distributions, which are two most fundamental properties of a graph. We also prove the upper bound for deviations in the joint degree distribution and degree correlation distribution, which correspond to JDM and DCM. Besides, we prove that the deviations in these distributions are negatively correlated with the sparsity of the JDM and DCM. Considering that these two matrices are always quite sparse, we believe that proposed algorithm will have a better-than-theory performance on real-world large directed networks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Countering the Forgetting of Novel Health Information with 'Social Boosting'</title>
<link>https://arxiv.org/abs/2507.23148</link>
<guid>https://arxiv.org/abs/2507.23148</guid>
<content:encoded><![CDATA[
<div> Keywords: intervention techniques, social structure, knowledge retention, social interactions, health knowledge 

Summary: 
The study investigates the impact of social structure on the retention of knowledge interventions in isolated Honduran villages. By focusing on maternal and child health care information delivery, the researchers found that individuals with friendship ties within a social network experienced enhanced effectiveness of knowledge interventions. This was attributed to the opportunities for social interactions, such as discussing and reinforcing information with others, leading to deeper cognitive processing and memory retention. The concept of "social boosting" emerged, where well-connected individuals could internalize and retain information better due to increased social interactions. The findings emphasize the significant role of social interactions in reinforcing health knowledge interventions over the long term. This study has implications for health policy, global health workforce, healthcare professionals working with disadvantaged populations, and UN missions addressing infodemics.<br /><br />Summary: <div>
arXiv:2507.23148v1 Announce Type: new 
Abstract: To mitigate the adverse effects of low-quality or false information, studies have shown the effectiveness of various intervention techniques through debunking or so-called pre-bunking. However, the effectiveness of such interventions can decay. Here, we investigate the role of the detailed social structure of the local villages within which the intervened individuals live, which provides opportunities for the targeted individuals to discuss and internalize new knowledge. We evaluated this with respect to a critically important topic, information about maternal and child health care, delivered via a 22-month in-home intervention. Specifically, we examined the effect of having friendship ties on the retention of knowledge interventions among targeted individuals in 110 isolated Honduran villages. We hypothesize that individuals who receive specific knowledge can internalize and consolidate this information by engaging in social interactions where, for instance, they have an opportunity to discuss it with others in the process. The opportunity to explain information to others (knowledge sharing) promotes deeper cognitive processing and elaborative encoding, which ultimately enhances memory retention. We found that well-connected individuals within a social network experience an enhanced effectiveness of knowledge interventions. These individuals may be more likely to internalize and retain the information and reinforce it in others, due to increased opportunities for social interaction where they teach others or learn from them, a mechanism we refer to as "social boosting". These findings underscore the role of social interactions in reinforcing health knowledge interventions over the long term. We believe these findings would be of interest to the health policy, the global health workforce, and healthcare professionals focusing on disadvantaged populations and UN missions on infodemics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical cross-system meta-analysis of long-term transmission grid evolution</title>
<link>https://arxiv.org/abs/2507.23546</link>
<guid>https://arxiv.org/abs/2507.23546</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-side flexibility, transmission network, reconfiguration, real-world grids, empirical studies 

Summary: 
Grid-side flexibility, the ability to reconfigure transmission network topology, is not fully utilized in real-world grids due to limited empirical studies. The potential for grid-side flexibility remains underexplored, hindering its full implementation. The lack of empirical research hinders understanding on how real-world grids evolve over time. By examining real-world grids, insights can be gained into the practical implications of implementing grid-side flexibility. Analyzing the evolution of transmission network topology can provide valuable knowledge for enhancing grid efficiency and resilience. Empirical studies on real-world grids are crucial for unlocking the full potential of grid-side flexibility and optimizing grid operations for the future. 

Summary: <div>
arXiv:2507.23546v1 Announce Type: new 
Abstract: The potential of grid-side flexibility, the latent ability to reconfigure transmission network topology remains under-used partly because of the lack of empirical studies on how real-world grids evolve.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Left-Wing Extremism on the Decentralized Web: An Analysis of Lemmygrad.ml</title>
<link>https://arxiv.org/abs/2507.23699</link>
<guid>https://arxiv.org/abs/2507.23699</guid>
<content:encoded><![CDATA[
<div> extremism, Lemmy, social media, toxicity, authoritarian

Summary:<br />
This study examines left-wing extremism on the Lemmygrad.ml instance of the social media platform Lemmy. The research covers user activity and toxicity levels, particularly after the migration of certain subreddits. It uncovers an increase in user engagement and harmful content, including support for authoritarian regimes and anti-Zionist and antisemitic posts. By using a transformer-based topic modeling approach, the study offers insights into the nature of content on Lemmygrad.ml. The findings highlight the need for a comprehensive analysis of political extremism on decentralized social networks, stressing the importance of studying extremism across the political spectrum. <div>
arXiv:2507.23699v1 Announce Type: new 
Abstract: This study investigates the presence of left-wing extremism on the Lemmygrad.ml instance of the decentralized social media platform Lemmy, from its launch in 2019 up to a month after the bans of the subreddits r/GenZedong and r/GenZhou. We conduct a temporal analysis on Lemmygrad.ml's user activity, with also measuring the degree of highly abusive or hateful content. Furthermore, we explore the content of their posts using a transformer-based topic modeling approach. Our findings reveal a substantial increase in user activity and toxicity levels following the migration of these subreddits to Lemmygrad.ml. We also identify posts that support authoritarian regimes, endorse the Russian invasion of Ukraine, and feature anti-Zionist and antisemitic content. Overall, our findings contribute to a more nuanced understanding of political extremism within decentralized social networks and emphasize the necessity of analyzing both ends of the political spectrum in research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting stock prices with ChatGPT-annotated Reddit sentiment</title>
<link>https://arxiv.org/abs/2507.22922</link>
<guid>https://arxiv.org/abs/2507.22922</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, sentiment analysis, stock market, predictive power, retail investors

Summary:
- The study examines the relationship between social media sentiment, particularly from Reddit's r/wallstreetbets, and stock market movements for GameStop and AMC Entertainment.
- Three sentiment analysis methods are employed, including a model fine-tuned for interpreting informal language and emojis on social media.
- Surprisingly, social media sentiment shows only a weak correlation with stock prices, with simpler metrics like comment volume and Google search trends offering stronger predictive signals.
- The results suggest that traditional sentiment analysis may not fully capture the complexity of retail investor behavior and market dynamics.
- The study underscores the need to consider various factors beyond sentiment analysis when predicting stock market movements. 

<br /><br />Summary: <div>
arXiv:2507.22922v1 Announce Type: cross 
Abstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection</title>
<link>https://arxiv.org/abs/2507.22930</link>
<guid>https://arxiv.org/abs/2507.22930</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, Personal Information Identifiers (PIIs), synthetic dataset, privacy risks, online social media<br />
Summary:<br />
- The study focuses on the risk of privacy breaches on social platforms like Reddit due to users' self-disclosures of Personal Information Identifiers (PIIs).
- A lack of open-source labeled datasets hinders research into identifying and retrieving PII-revealing text.
- The researchers developed a methodology to create synthetic PII-labeled datasets from large language models to address this issue.
- They created a taxonomy of 19 PII-revealing categories for vulnerable populations.
- The evaluation of the synthetic dataset was based on reproducibility equivalence, unlinkability to original users, and indistinguishability from the original data.
- The dataset and code were released to facilitate further research into PII privacy risks on online social media platforms. <br />Summary: <div>
arXiv:2507.22930v1 Announce Type: cross 
Abstract: Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion formation in Wikipedia Ising networks</title>
<link>https://arxiv.org/abs/2507.22254</link>
<guid>https://arxiv.org/abs/2507.22254</guid>
<content:encoded><![CDATA[
<div> opinion formation, Wikipedia Ising Networks, competition, political leaders, social concepts

Summary:
The study focuses on opinion formation on Wikipedia Ising Networks, where each node represents a Wikipedia article and links are generated by citations. Ising spins at each node determine their orientation based on a majority vote of connected neighbors, leading to a spin polarized steady-state phase with stable opinion polarization. The model explores competition between political leaders, world countries, and social concepts, using examples like Donald Trump, Vladimir Putin, and Xi Jinping. The approach is extended to three groups with different opinions represented by colors within English, Russian, and Chinese editions of Wikipedia. The research suggests that this Ising Network Opinion Formation model offers a generic description of opinion formation in complex networks. <br /><br />Summary: <div>
arXiv:2507.22254v1 Announce Type: new 
Abstract: We study properties of opinion formation
  on Wikipedia Ising Networks. Each Wikipedia article
  is represented as a node and links are formed by citations of
  one article to another generating a directed network
  of a given language edition with millions of nodes.
  Ising spins are placed at each node
  and their orientation up or down is determined by a majority vote
  of connected neighbors. At the initial stage there are only
  a few nodes from two groups with fixed competing opinions up and down
  while other nodes are assumed to have no initial opinion with no
  effect on the vote. The competition of two opinions is modeled by
  an asynchronous Monte Carlo process converging to a spin polarized
  steady-state phase.
  This phase remains stable with respect to small fluctuations
  induced by an effective temperature of the Monte Carlo process.
  The opinion polarization at the steady-state provides
  opinion (spin) preferences for each node. In the framework of
  this Ising Network
  Opinion Formation model we analyze the influence and competition between
  political leaders, world countries and social concepts.
  This approach is also generalized to the competition between
  three groups of
  different opinions described by three colors, for example
  Donald Trump, Vladimir Putin, Xi Jinping or USA, Russia, China
  within English, Russian and Chinese editions of Wikipedia of March 2025.
  We argue that this approach provides a generic description of
  opinion formation in various complex networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Influence Maximization on Temporal Networks: A Guide to Make the Best Choice</title>
<link>https://arxiv.org/abs/2507.22589</link>
<guid>https://arxiv.org/abs/2507.22589</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, influence maximization, diffusion models, seed selection, network settings

Summary: 
This article presents a comprehensive guide for selecting the most suitable diffusion model for influence maximization on temporal networks. The categorization of existing models based on underlying mechanisms and effectiveness in different network settings aids in making informed decisions. The analysis of seed selection strategies emphasizes the development of efficient algorithms for finding near-optimal influential node sets. The comparison of key advancements, challenges, and practical applications provides a roadmap for researchers and practitioners to navigate the temporal influence maximization landscape effectively. Overall, this structured guide offers valuable insights into maximizing influence in dynamic communication systems and online social platforms. 

<br /><br />Summary:  <div>
arXiv:2507.22589v1 Announce Type: new 
Abstract: The increasing prominence of temporal networks in online social platforms and dynamic communication systems has made influence maximization a critical research area. Various diffusion models have been proposed to capture the spread of information, yet selecting the most suitable model for a given scenario remains challenging. This article provides a structured guide to making the best choice among diffusion models for influence maximization on temporal networks. We categorize existing models based on their underlying mechanisms and assess their effectiveness in different network settings. We analyze seed selection strategies, highlighting how the inherent properties of influence spread enable the development of efficient algorithms that can find near-optimal sets of influential nodes. By comparing key advancements, challenges, and practical applications, we offer a comprehensive roadmap for researchers and practitioners to navigate the landscape of temporal influence maximization effectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility in Epidemic Modeling</title>
<link>https://arxiv.org/abs/2507.22799</link>
<guid>https://arxiv.org/abs/2507.22799</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, epidemic modeling, contact tracing, data-driven methodologies, disease transmission dynamics

Summary: 
This review explores the integration of high-resolution human mobility data into epidemic modeling, highlighting the importance of considering the complex and heterogeneous nature of real-world human interactions. The review covers diverse sources and representations of human mobility data, and examines the behavioral and structural roles of mobility in shaping disease transmission dynamics. It discusses various epidemic modeling approaches, including compartmental models, network-based models, agent-based models, and machine learning models. The review also addresses how integrating mobility data improves risk management and response strategies during epidemics. By synthesizing these insights, the review serves as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while providing clear directions for future research.

<br /><br />Summary: <div>
arXiv:2507.22799v1 Announce Type: new 
Abstract: Human mobility forms the backbone of contact patterns through which infectious diseases propagate, fundamentally shaping the spatio-temporal dynamics of epidemics and pandemics. While traditional models are often based on the assumption that all individuals have the same probability of infecting every other individual in the population, a so-called random homogeneous mixing, they struggle to capture the complex and heterogeneous nature of real-world human interactions. Recent advancements in data-driven methodologies and computational capabilities have unlocked the potential of integrating high-resolution human mobility data into epidemic modeling, significantly improving the accuracy, timeliness, and applicability of epidemic risk assessment, contact tracing, and intervention strategies. This review provides a comprehensive synthesis of the current landscape in human mobility-informed epidemic modeling. We explore diverse sources and representations of human mobility data, and then examine the behavioral and structural roles of mobility and contact in shaping disease transmission dynamics. Furthermore, the review spans a wide range of epidemic modeling approaches, ranging from classical compartmental models to network-based, agent-based, and machine learning models. And we also discuss how mobility integration enhances risk management and response strategies during epidemics. By synthesizing these insights, the review can serve as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while charting clear directions for future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of media memorability in facilitating startups' access to venture capital funding</title>
<link>https://arxiv.org/abs/2507.22201</link>
<guid>https://arxiv.org/abs/2507.22201</guid>
<content:encoded><![CDATA[
<div> memedia, memorability, venture capital, investment, startup  
Summary:  
- Media memorability, related to imprinting a startup's name in investor memory, influences venture capital investment in startups, with detailed cues such as distinctiveness and connectivity key factors.  
- Venture capitalists consider nuanced aspects of media content, beyond general exposure, when making funding decisions.  
- Data from 197 UK micro and nanotechnology startups funded between 1995 and 2004 supports the significant impact of media memorability on investment outcomes.  
- Startups should focus on strengthening brand memorability through targeted, meaningful media coverage highlighting uniqueness and industry relevance to attract venture capital.  
- The study contributes to understanding how media legitimation influences entrepreneurial finance and emphasizes the importance of strategic media engagement for startups seeking investment.<br /><br /> <div>
arXiv:2507.22201v1 Announce Type: cross 
Abstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From News Source Sharers to Post Viewers: How Topic Diversity and Conspiracy Theories Shape Engagement With Misinformation During a Health Crisis</title>
<link>https://arxiv.org/abs/2401.08832</link>
<guid>https://arxiv.org/abs/2401.08832</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, COVID-19, social media, conspiracy theories, engagement

Summary:
The study examines online engagement with misinformation during the COVID-19 pandemic on social media platform X. False news with conspiracy theories is found to have higher topic diversity than true news. False news also has a longer lifespan and receives more engagement on X, especially when conspiracy theories are involved. News source sharers' engagement is not significantly influenced by topic diversity. In contrast, post viewers engage more with posts that have diverse topics, with misinformation containing conspiracy theories receiving significantly more reposts, likes, and replies. These findings highlight distinct engagement patterns between news source sharers and post viewers on X, providing insights for refining interventions against misinformation at both user levels. <div>
arXiv:2401.08832v3 Announce Type: replace 
Abstract: Online engagement with misinformation threatens societal well-being, particularly during health crises when susceptibility to misinformation is heightened in a multi-topic context. Here, we focus on the COVID-19 pandemic and address a critical gap in understanding engagement with multi-topic misinformation on social media at two user levels: news source sharers (who post news items) and post viewers (who engage with news posts). To this end, we analyze 7273 fact-checked source news items and their associated posts on X through the lens of topic diversity and conspiracy theories. We find that false news, especially those containing conspiracy theories, exhibits higher topic diversity than true news. At news source sharer level, false news has a longer lifetime and receives more posts on X than true news, with conspiracy theories further extending its longevity. However, topic diversity does not significantly influence news source sharers' engagement. At post viewer level, contrary to news source sharer level, posts characterized by heightened topic diversity receive more reposts, likes, and replies. Notably, post viewers tend to engage more with misinformation containing conspiracy narratives: false news posts that contain conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes, and 44.1% more replies compared to those without conspiracy theories. Our findings suggest that news source sharers and post viewers exhibit distinct engagement patterns on X, offering valuable insights into refining misinformation interventions at these two user levels.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted-Median Model of Opinion Dynamics on Networks</title>
<link>https://arxiv.org/abs/2406.17552</link>
<guid>https://arxiv.org/abs/2406.17552</guid>
<content:encoded><![CDATA[
<div> consensus opinion, opinion fragmentation, social network, opinion model, echo chambers  
Summary:  
The study explores how social interactions in a network influence individuals' opinions, leading to either a consensus opinion or opinion fragmentation forming echo chambers. Unlike traditional models that rely on mean opinion, the study considers an update rule based on weighted median opinion for a more realistic approach. Numerical simulations investigate how the limit opinion distribution is impacted by network structure. For configuration-model networks, a mean-field approximation for the opinion distribution dynamics with infinitely many individuals is derived. The research sheds light on the dynamics of opinions in social networks and provides insights into the formation of echo chambers and consensus opinions. <div>
arXiv:2406.17552v2 Announce Type: replace-cross 
Abstract: Social interactions influence people's opinions. In some situations, these interactions result in a consensus opinion; in others, they result in opinion fragmentation and the formation of different opinion groups in the form of "echo chambers". Consider a social network of individuals, who hold continuous-valued scalar opinions and change their opinions when they interact with each other. In such an opinion model, it is common for an opinion-update rule to depend on the mean opinion of interacting individuals. However, we consider an alternative update rule - which may be more realistic in some situations - that instead depends on a weighted median opinion of interacting individuals. Through numerical simulations of our opinion model, we investigate how the limit opinion distribution depends on network structure. For configuration-model networks, we also derive a mean-field approximation for the asymptotic dynamics of the opinion distribution when there are infinitely many individuals in a network.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Half-life of Youtube News Videos: Diffusion Dynamics and Predictive Factors</title>
<link>https://arxiv.org/abs/2507.21187</link>
<guid>https://arxiv.org/abs/2507.21187</guid>
<content:encoded><![CDATA[
<div> diffusion patterns, dispersion rate, 24-hour half-life, prediction models, Explainable AI

Summary:
The study investigates the early-stage diffusion patterns and dispersion rate of news videos on YouTube within the first 24 hours. It analyzes a dataset of over 50,000 videos from 75 countries and six continents, revealing the average 24-hour half-life of YouTube news videos to be around 7 hours, with variability across regions. The research also delves into predicting the 24-hour half-lives of news videos using 6 different models based on statistical and Deep Learning techniques. The performance differences and the importance of video- and channel-related predictors are examined through Explainable AI techniques. The dataset, analysis codebase, and trained models are made publicly available to support further research in this field. 

<br /><br />Summary: <div>
arXiv:2507.21187v1 Announce Type: new 
Abstract: Consumption of YouTube news videos significantly shapes public opinion and political narratives. While prior works have studied the longitudinal dissemination dynamics of YouTube News videos across extended periods, limited attention has been paid to the short-term trends. In this paper, we investigate the early-stage diffusion patterns and dispersion rate of news videos on YouTube, focusing on the first 24 hours. To this end, we introduce and analyze a rich dataset of over 50,000 videos across 75 countries and six continents. We provide the first quantitative evaluation of the 24-hour half-life of YouTube news videos as well as identify their distinct diffusion patterns. According to the findings, the average 24-hour half-life is approximately 7 hours, with substantial variance both within and across countries, ranging from as short as 2 hours to as long as 15 hours. Additionally, we explore the problem of predicting the latency of news videos' 24-hour half-lives. Leveraging the presented datasets, we train and contrast the performance of 6 different models based on statistical as well as Deep Learning techniques. The difference in prediction results across the models is traced and analyzed. Lastly, we investigate the importance of video- and channel-related predictors through Explainable AI (XAI) techniques. The dataset, analysis codebase and the trained models are released at http://bit.ly/3ILvTLU to facilitate further research in this area.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Growing Toxicity Manifests: A Topic Trajectory Analysis of U.S. Immigration Discourse on Social Media</title>
<link>https://arxiv.org/abs/2507.21418</link>
<guid>https://arxiv.org/abs/2507.21418</guid>
<content:encoded><![CDATA[
<div> Keywords: online public sphere, immigration discourse, toxicity, topic discovery, trajectory analysis

Summary: 
The research examines how shifts in toxicity in discussions about U.S. immigration correspond to changes in topics. By analyzing 4 million online posts over six months, the study identifies 157 fine-grained subtopics within the immigration discourse. Users with increasing toxicity tend to adopt alarmist and fear-based perspectives, while those with decreasing toxicity gravitate towards legal and policy-focused themes. These patterns are statistically significantly different from two reference groups with stable toxicity levels. The study employs a novel method that combines hierarchical topic discovery with trajectory analysis to understand dynamic conversations around social issues in a scalable way. This approach can provide valuable insights into understanding polarization and toxicity in online discussions about immigration. 

<br /><br />Summary: <div>
arXiv:2507.21418v1 Announce Type: new 
Abstract: In the online public sphere, discussions about immigration often become increasingly fractious, marked by toxic language and polarization. Drawing on 4 million X posts over six months, we combine a user- and topic-centric approach to study how shifts in toxicity manifest as topical shifts. Our topic discovery method, which leverages instruction-based embeddings and recursive HDBSCAN, uncovers 157 fine-grained subtopics within the U.S. immigration discourse. We focus on users in four groups: (1) those with increasing toxicity, (2) those with decreasing toxicity, and two reference groups with no significant toxicity trend but matched toxicity levels. Treating each posting history as a trajectory through a five-dimensional topic space, we compare average group trajectories using permutational MANOVA. Our findings show that users with increasing toxicity drift toward alarmist, fear-based frames, whereas those with decreasing toxicity pivot toward legal and policy-focused themes. Both patterns diverge statistically significantly from their reference groups. This pipeline, which combines hierarchical topic discovery with trajectory analysis, offers a replicable method for studying dynamic conversations around social issues at scale.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation</title>
<link>https://arxiv.org/abs/2507.21903</link>
<guid>https://arxiv.org/abs/2507.21903</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Graphical methods, news summarization, stakeholders, events<br />
Summary:<br />
- Existing news summarization methods focusing only on textual content of articles, lacking analysis on parties involved.
- SUnSET framework introduced for Timeline Summarization, leveraging Large Language Models to build SET triplets.
- Stakeholder-based ranking used to construct Relevancy metric, outperforming prior baselines and becoming State-of-the-Art.
- Importance of stakeholders highlighted in news articles, impacting the understanding of events.
- SUnSET provides a novel approach to tracking events across sources, considering parties involved and connection of related events. <br />
Summary: <div>
arXiv:2507.21903v1 Announce Type: new 
Abstract: As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21055</link>
<guid>https://arxiv.org/abs/2507.21055</guid>
<content:encoded><![CDATA[
<div> Keywords: news media, comprehension gaps, large language models, agent-based framework, diverse audiences

Summary:
Large language models (LLMs) are used in an agent-based framework to address comprehension gaps in news media among diverse audiences. The framework simulates communication behaviors among agents representing experts from various occupations or different age groups. Through iterative discussions, the framework identifies confusion and misunderstandings in news content for the agents. Supplemental materials specific to these gaps are then designed and provided to the agents, leading to significantly improved news comprehension. This study demonstrates the utility and efficiency of the framework in enhancing news understanding for audiences with varying levels of expertise and age. <div>
arXiv:2507.21055v1 Announce Type: cross 
Abstract: In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos</title>
<link>https://arxiv.org/abs/2507.21467</link>
<guid>https://arxiv.org/abs/2507.21467</guid>
<content:encoded><![CDATA[
<div> algorithm, recommendation, YouTube, bias, content diversity

Summary:
This study explores the impact of recommendation algorithms on user engagement and content diversity on YouTube, focusing on short-form and long-form videos. By developing a novel data collection framework, the researchers analyze the distinct behavioral patterns of the algorithms in recommending content. They find that short-form videos lead to quicker shifts towards engaging but less diverse content compared to long-form videos. Moreover, the study investigates biases in politically sensitive topics like the South China Sea dispute, shedding light on how these algorithms shape narratives and amplify specific viewpoints. The research emphasizes the importance of responsible AI practices in creating equitable and transparent recommendation systems to address concerns about biases, echo chambers, and content diversity in digital media platforms. This study provides actionable insights for designing recommendation algorithms that promote fairness and transparency, highlighting the need for responsible AI practices in the evolving digital landscape. 

<br /><br />Summary: <div>
arXiv:2507.21467v1 Announce Type: cross 
Abstract: The growing popularity of short-form video content, such as YouTube Shorts, has transformed user engagement on digital platforms, raising critical questions about the role of recommendation algorithms in shaping user experiences. These algorithms significantly influence content consumption, yet concerns about biases, echo chambers, and content diversity persist. This study develops an efficient data collection framework to analyze YouTube's recommendation algorithms for both short-form and long-form videos, employing parallel computing and advanced scraping techniques to overcome limitations of YouTube's API. The analysis uncovers distinct behavioral patterns in recommendation algorithms across the two formats, with short-form videos showing a more immediate shift toward engaging yet less diverse content compared to long-form videos. Furthermore, a novel investigation into biases in politically sensitive topics, such as the South China Sea dispute, highlights the role of these algorithms in shaping narratives and amplifying specific viewpoints. By providing actionable insights for designing equitable and transparent recommendation systems, this research underscores the importance of responsible AI practices in the evolving digital media landscape.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Tight Bounds for Estimating Degree Distribution in Streaming and Query Models</title>
<link>https://arxiv.org/abs/2507.21784</link>
<guid>https://arxiv.org/abs/2507.21784</guid>
<content:encoded><![CDATA[
<div> degree distribution, complementary cumulative degree histogram, approximation algorithm, lower bounds, sublinear models

Summary:
An algorithm is proposed to approximate the complementary cumulative degree histogram (ccdh) of a graph by obtaining suitable vertex and edge samples, independent of any sublinear model. Efficient methods for obtaining these samples in streaming and query models are discussed. The complexity of the problem across sublinear models is nearly settled with the first lower bounds established in both query and streaming scenarios. This work addresses an open problem posed in a previous conference, providing insights into approximating ccdh and advancing the understanding of graph structure analysis. <div>
arXiv:2507.21784v1 Announce Type: cross 
Abstract: The degree distribution of a graph $G=(V,E)$, $|V|=n$, $|E|=m$ is one of the most fundamental objects of study in the analysis of graphs as it embodies relationship among entities. In particular, an important derived distribution from degree distribution is the complementary cumulative degree histogram (ccdh). The ccdh is a fundamental summary of graph structure, capturing, for each threshold $d$, the number of vertices with degree at least $d$. For approximating ccdh, we consider the $(\varepsilon_D,\varepsilon_R)$-BiCriteria Multiplicative Approximation, which allows for controlled multiplicative slack in both the domain and the range. The exact complexity of the problem was not known and had been posed as an open problem in WOLA 2019 [Sublinear.info, Problem 98].
  In this work, we first design an algorithm that can approximate ccdh if a suitable vertex sample and an edge sample can be obtained and thus, the algorithm is independent of any sublinear model. Next, we show that in the streaming and query models, these samples can be obtained efficiently. On the other end, we establish the first lower bounds for this problem in both query and streaming models, and (almost) settle the complexity of the problem across both the sublinear models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Misconceptions in Social Bots Research</title>
<link>https://arxiv.org/abs/2303.17251</link>
<guid>https://arxiv.org/abs/2303.17251</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots research, biases, misconceptions, online manipulation, methodological issues <br />
Summary: This article discusses the prevalent biases, hyped results, and misconceptions that plague social bots research, leading to ambiguities and unrealistic expectations. It highlights the need for rigorous, unbiased, and responsible discussions on online disinformation and manipulation. The analysis addresses methodological and conceptual issues affecting current research and refutes common fallacious arguments used by proponents and opponents alike. By demystifying misconceptions and providing guidelines for future research, the article aims to ensure reliable solutions and reaffirm the validity of the scientific method. <div>
arXiv:2303.17251v4 Announce Type: replace 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
<div> social media, ideology classification, Large Language Models, in-context learning, metadata

Summary:
Large Language Models (LLMs) show promise in classifying political ideology in online content within the context of the two-party US political spectrum through in-context learning (ICL). The study conducted experiments on news articles and YouTube videos, demonstrating that the approach outperforms zero-shot and traditional supervised methods. The influence of metadata, such as content source and descriptions, on ideological classification was also evaluated. Providing source information for political and non-political content was found to impact the LLM's classification accuracy. The research addresses concerns of radicalization, filter bubbles, and content bias in the rapidly expanding social media landscape. <div>
arXiv:2503.20797v2 Announce Type: replace-cross 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green building blocks reveal the complex anatomy of climate change mitigation technologies</title>
<link>https://arxiv.org/abs/2504.06834</link>
<guid>https://arxiv.org/abs/2504.06834</guid>
<content:encoded><![CDATA[
<div> Keywords: Green Building Blocks, innovation, climate change, technology, international collaboration

Summary: 
The study focuses on identifying "Green Building Blocks" (GBBs) as modular components that can be incorporated into existing technologies to reduce their carbon footprint, thus facilitating the transition to net-zero emissions. By comparing green and nongreen patents, the researchers construct a network that connects nongreen technologies to GBBs, highlighting areas with varying potential for climate-change mitigating innovation. The unequal distribution of node degrees in the network underscores the differing opportunities for innovation across domains. Additionally, the study reveals the importance of international collaboration in driving innovation, with a significant proportion of firms in the US, Germany, and China relying on foreign partners for optimal development of green technologies. The findings emphasize the critical role of global cooperation in advancing sustainable technological solutions and warn against the risks of economic nationalism hindering progress towards achieving climate goals. 

<br /><br />Summary: <div>
arXiv:2504.06834v2 Announce Type: replace-cross 
Abstract: Achieving net-zero emissions requires rapid innovation, yet the necessary technological knowhow is scattered across industries and countries. Comparing functionally similar green and nongreen patents, we identify "Green Building Blocks" (GBBs): modular components that can be added to reduce existing technologies' carbon footprints. These GBBs depict the anatomy of the green transition as a network that connects problems -- nongreen technologies -- to GBBs that mitigate their climate-change impact. Node degrees in this network are highly unequal, showing that the scope for climate-change mitigating innovation varies substantially across domains. The network also helps predict which green technologies firms develop themselves, and which alliances they form to do so. This reveals a critical dependence on international collaboration: optimal innovation partners for 84% of US, 87% of German, and 92% of Chinese firms are foreign, providing quantitative evidence that rising economic nationalism threatens the pace of innovation required to meet global climate goals.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dual Personas of Social Media Bots</title>
<link>https://arxiv.org/abs/2504.12498</link>
<guid>https://arxiv.org/abs/2504.12498</guid>
<content:encoded><![CDATA[
<div> Keywords: social media bots, personas, bot detection regulation, good-bad duality, metrics

Summary:
Social media bots, AI agents that engage in online interactions, have various personas tailored for specific behaviors or content types. This article introduces fifteen agent personas categorized into Content-Based and Behavior-Based Bot Personas. The bots can serve both positive and negative purposes, challenging the common perception that all bots are malicious. To better understand bot behavior, metrics for assessing the good and bad aspects of bot agents are outlined. The research highlights the importance of considering how bots are utilized rather than labeling them universally as harmful. This guideline aims to inform bot detection regulation and underscores the need for nuanced approaches to assessing and addressing the impacts of social media bots. <div>
arXiv:2504.12498v2 Announce Type: replace-cross 
Abstract: Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks</title>
<link>https://arxiv.org/abs/2507.19702</link>
<guid>https://arxiv.org/abs/2507.19702</guid>
<content:encoded><![CDATA[
<div> 1D-CGS; influential nodes; complex networks; node ranking; deep learning
<br />
<br />
Summary: 
The article introduces 1D-CGS, a hybrid model combining 1D convolutional neural networks (1D-CNN) with GraphSAGE for efficient node ranking in complex networks. It utilizes node degree and average neighbor degree as input features, processed through 1D convolutions and GraphSAGE layers. The model is trained on synthetic networks and tested on real-world networks, outperforming traditional centrality measures and deep learning models in accuracy while maintaining fast runtime. Results show a substantial improvement in ranking accuracy compared to baselines, with high correlation and Jaccard Similarity scores. The model exhibits unique and discriminative rankings with near-perfect rank distributions and high Monotonicity Index scores. Importantly, 1D-CGS operates in a highly reasonable time frame, making it suitable for large-scale applications. <div>
arXiv:2507.19702v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a critical task with a wide range of applications across different domains. However, existing approaches often face trade-offs between accuracy and computational efficiency. To address these challenges, we propose 1D-CGS, a lightweight and effective hybrid model that integrates the speed of one-dimensional convolutional neural networks (1D-CNN) with the topological representation power of GraphSAGE for efficient node ranking. The model uses a lightweight input representation built on two straightforward and significant topological features: node degree and average neighbor degree. These features are processed through 1D convolutions to extract local patterns, followed by GraphSAGE layers to aggregate neighborhood information. We formulate the node ranking task as a regression problem and use the Susceptible-Infected-Recovered (SIR) model to generate ground truth influence scores. 1D-CGS is initially trained on synthetic networks generated by the Barabasi-Albert model and then applied to real world networks for identifying influential nodes. Experimental evaluations on twelve real world networks demonstrate that 1D-CGS significantly outperforms traditional centrality measures and recent deep learning models in ranking accuracy, while operating in very fast runtime. The proposed model achieves an average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard Similarity over the best performing deep learning baselines. It also achieves an average Monotonicity Index (MI) score 0.99 and produces near perfect rank distributions, indicating highly unique and discriminative rankings. Furthermore, all experiments confirm that 1D-CGS operates in a highly reasonable time, running significantly faster than existing deep learning methods, making it suitable for large scale applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Closed Loop Dynamics Between a Social Media Recommender System and Users' Opinions</title>
<link>https://arxiv.org/abs/2507.19792</link>
<guid>https://arxiv.org/abs/2507.19792</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender System, User Opinions, Monte Carlo simulations, Polarisation, Radicalisation

Summary:
The paper presents a mathematical model that analyzes the interaction between a Recommender System (RS) algorithm and content consumers. The model considers a large user population with varying opinions, consuming personalized content recommended by the RS. Through Monte Carlo simulations, the study explores how RS impacts user opinions and the subsequent content recommendations based on user engagement. The research delves into the performance of the RS in influencing user engagement and how user opinions evolve, particularly focusing on polarization and radicalization. It identifies certain opinion distributions as more prone to polarization, highlights ineffective content stances in changing opinions, and underscores the effectiveness of viral content in combating polarization. The findings provide insights into the dynamics of RS-user interactions and the role of viral content in mitigating opinion polarization.<br /><br />Summary: <div>
arXiv:2507.19792v1 Announce Type: new 
Abstract: This paper proposes a mathematical model to study the coupled dynamics of a Recommender System (RS) algorithm and content consumers (users). The model posits that a large population of users, each with an opinion, consumes personalised content recommended by the RS. The RS can select from a range of content to recommend, based on users' past engagement, while users can engage with the content (like, watch), and in doing so, users' opinions evolve. This occurs repeatedly to capture the endless content available for user consumption on social media. We employ a campaign of Monte Carlo simulations using this model to study how recommender systems influence users' opinions, and in turn how users' opinions shape the subsequent recommended content. We take an interest in both the performance of the RS (e.g., how users engage with the content) and the user's opinions, focusing on polarisation and radicalisation of opinions. We find that different opinion distributions are more susceptible to becoming polarised than others, many content stances are ineffective in changing user opinions, and creating viral content is an effective measure in combating polarisation of opinions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity</title>
<link>https://arxiv.org/abs/2507.20066</link>
<guid>https://arxiv.org/abs/2507.20066</guid>
<content:encoded><![CDATA[
<div> scale measurement, similarity, disinformation, narratives, detection

Summary:
The thesis presents a continuous scale measurement of similarity to disinformation narratives for detecting and capturing partial truths. Two tools, a tracing tool and a narrative synthesis tool, are developed to analyze tweets and target narratives. The tracing tool rates similarities and graphs them over time, while the narrative synthesis tool clusters tweets and identifies dominant narratives. The tools are integrated into a Tweet Narrative Analysis Dashboard. Validation on the GLUE STS-B benchmark is followed by case studies on "The 2020 election was stolen" narrative using Donald Trump's tweets and "Transgender people are harmful to society" narrative using tweets from media outlets. The empirical findings support semantic similarity for nuanced disinformation detection, tracing, and characterization. Access to the tools is available upon request to the author. 

<br /><br />Summary: <div>
arXiv:2507.20066v1 Announce Type: new 
Abstract: This thesis develops a continuous scale measurement of similarity to disinformation narratives that can serve to detect disinformation and capture the nuanced, partial truths that are characteristic of it. To do so, two tools are developed and their methodologies are documented. The tracing tool takes tweets and a target narrative, rates the similarities of each to the target narrative, and graphs it as a timeline. The second narrative synthesis tool clusters tweets above a similarity threshold and generates the dominant narratives within each cluster. These tools are combined into a Tweet Narrative Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark, and then the two tools are used to analyze two case studies for further empirical validation. The first case study uses the target narrative "The 2020 election was stolen" and analyzes a dataset of Donald Trump's tweets during 2020. The second case study uses the target narrative, "Transgender people are harmful to society" and analyzes tens of thousands of tweets from the media outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News. Together, the empirical findings from these case studies demonstrate semantic similarity for nuanced disinformation detection, tracing, and characterization.
  The tools developed in this thesis are hosted and can be accessed through the permission of the author. Please explain your use case in your request. The HTML friendly version of this paper is at https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Blockchain-Based Quality Control Model for Online Collaboration Systems</title>
<link>https://arxiv.org/abs/2507.20265</link>
<guid>https://arxiv.org/abs/2507.20265</guid>
<content:encoded><![CDATA[
<div> blockchain, quality control, collaborative content generation, decentralized trust, informetrics

Summary: 
The manuscript introduces a blockchain-based quality control model for collaborative content generation (CCG), addressing challenges such as citation manipulation, transparency, and decentralized trust in metric computation. The model uses a semi-iterative algorithm to compute quality scores of artifacts and reputation of nodes. It is agile in processing latency and shows comparable performance to PageRank and HITS baselines in evaluating quality scores. The model also demonstrates throughput, latency, and robustness against malicious nodes, confirming its reliability. Theoretical comparison with recent studies validates its feasibility for real-world informetric application. <div>
arXiv:2507.20265v1 Announce Type: new 
Abstract: Collaborative content generation (CCG) enables collective creation of artifacts like scientific articles. Quality is a paramount concern in CCG, and a multitude of methods have been proposed to evaluate the quality of artifacts. Nevertheless, the majority of these methods are reliant on centralized architectures, which present challenges pertaining to security, privacy, and availability. Blockchain technology proffers a potential resolution to these challenges, by furnishing a decentralized and immutable ledger of quality scores. In this manuscript, we introduce a blockchain-based quality control model for CCG that uses a semi-iterative algorithm to interdependently compute quality scores of artifacts and reputation of nodes. Our model addresses critical challenges in academic informetrics, such as citation manipulation, transparency in collaborative scholarship, and decentralized trust in metric computation. Our model also exhibits sensitivity to processing latency, rendering it more agile in the presence of delays. Our model's quality scores, evaluated against PageRank and HITS baselines, show comparable performance, with additional assessments of throughput, latency, and robustness against malicious nodes confirming its reliability. A theoretical comparison with recent studies validates its feasibility for real world informetric application.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural-Aware Key Node Identification in Hypergraphs via Representation Learning and Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20682</link>
<guid>https://arxiv.org/abs/2507.20682</guid>
<content:encoded><![CDATA[
<div> Autoencoder, Hypergraph neural network, Active learning, Node importance, Node identification

Summary:
The article introduces a new framework, AHGA, for evaluating key nodes in hypergraphs, capturing polyadic interactions in real-world systems. AHGA combines an Autoencoder for higher-order structural features, a HyperGraph neural network for pre-training, and an Active learning-based fine-tuning process. The fine-tuning step improves robustness and generalization across diverse hypergraph topologies. Experimental results on empirical hypergraphs show AHGA outperforms centrality-based baselines by 37.4%. Nodes identified by AHGA exhibit high influence and strong structural disruption capability, highlighting their ability to detect multifunctional nodes. <div>
arXiv:2507.20682v1 Announce Type: new 
Abstract: Evaluating node importance is a critical aspect of analyzing complex systems, with broad applications in digital marketing, rumor suppression, and disease control. However, existing methods typically rely on conventional network structures and fail to capture the polyadic interactions intrinsic to many real-world systems. To address this limitation, we study key node identification in hypergraphs, where higher-order interactions are naturally modeled as hyperedges. We propose a novel framework, AHGA, which integrates an Autoencoder for extracting higher-order structural features, a HyperGraph neural network-based pre-training module (HGNN), and an Active learning-based fine-tuning process. This fine-tuning step plays a vital role in mitigating the gap between synthetic and real-world data, thereby enhancing the model's robustness and generalization across diverse hypergraph topologies. Extensive experiments on eight empirical hypergraphs show that AHGA outperforms classical centrality-based baselines by approximately 37.4%. Furthermore, the nodes identified by AHGA exhibit both high influence and strong structural disruption capability, demonstrating their superiority in detecting multifunctional nodes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wardropian Cycles make traffic assignment both optimal and fair by eliminating price-of-anarchy with Cyclical User Equilibrium for compliant connected autonomous vehicles</title>
<link>https://arxiv.org/abs/2507.19675</link>
<guid>https://arxiv.org/abs/2507.19675</guid>
<content:encoded><![CDATA[
<div> Keywords: Connected and Autonomous Vehicles, Wardropian cycles, System Optimal, User Equilibrium, Traffic Performance

Summary:
In this study, the concept of Wardropian cycles is proposed to achieve fair and optimal traffic assignment in the context of Connected and Autonomous Vehicles (CAVs). These cycles ensure both fairness and efficiency in routing, satisfying Wardrop's principles while equalizing average travel times among users. The researchers develop exact methods and a greedy heuristic to compute and optimize these cycles efficiently. They introduce the concept of Cyclical User Equilibrium for stability under deviations. Large-scale simulations in Barcelona, Berlin, Anaheim, and Sioux Falls show significant reductions in traffic inefficiencies and inequities with the implementation of Wardropian cycles, with the approach displaying more social acceptability. In Barcelona, up to 670 vehicle-hours of inefficiencies were eliminated, while in other cities, initial inequities were significantly reduced within a short timeframe. Overall, the study demonstrates the potential of Wardropian cycles in improving traffic performance and equality in CAV systems.<br /><br />Summary: <div>
arXiv:2507.19675v1 Announce Type: cross 
Abstract: Connected and Autonomous Vehicles (CAVs) open the possibility for centralised routing with full compliance, making System Optimal traffic assignment attainable. However, as System Optimum makes some drivers better off than others, voluntary acceptance seems dubious. To overcome this issue, we propose a new concept of Wardropian cycles, which, in contrast to previous utopian visions, makes the assignment fair on top of being optimal, which amounts to satisfaction of both Wardrop's principles. Such cycles, represented as sequences of permutations to the daily assignment matrices, always exist and equalise, after a limited number of days, average travel times among travellers (like in User Equilibrium) while preserving everyday optimality of path flows (like in System Optimum). We propose exact methods to compute such cycles and reduce their length and within-cycle inconvenience to the users. As identification of optimal cycles turns out to be NP-hard in many aspects, we introduce a greedy heuristic efficiently approximating the optimal solution. Finally, we introduce and discuss a new paradigm of Cyclical User Equilibrium, which ensures stability of optimal Wardropian Cycles under unilateral deviations.
  We complement our theoretical study with large-scale simulations. In Barcelona, 670 vehicle-hours of Price-of-Anarchy are eliminated using cycles with a median length of 11 days-though 5% of cycles exceed 90 days. However, in Berlin, just five days of applying the greedy assignment rule significantly reduces initial inequity. In Barcelona, Anaheim, and Sioux Falls, less than 7% of the initial inequity remains after 10 days, demonstrating the effectiveness of this approach in improving traffic performance with more ubiquitous social acceptability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy</title>
<link>https://arxiv.org/abs/2507.19692</link>
<guid>https://arxiv.org/abs/2507.19692</guid>
<content:encoded><![CDATA[
<div> photosensitive epilepsy, FlashGuard, color analysis, seizure prevention, digital media<br />
<br />
Summary: <br />
Individuals with photosensitive epilepsy (PSE) face challenges in the virtual realm due to unpredictable seizure-causing visual stimuli. Current solutions detect flashes asynchronously, lacking real-time and computational efficiency. FlashGuard, a novel approach, assesses color change rates in frames to mitigate stimuli in real-time. It uses CIELAB color space analysis to analyze color differences, reducing luminance and smoothing transitions. This study highlights how color properties impact flashing perception for PSE individuals, advocating for broader WCAG guidelines. Implementing these insights can better protect individuals with PSE from digital media triggers, enhancing accessibility and safety. <div>
arXiv:2507.19692v1 Announce Type: cross 
Abstract: In the virtual realm, individuals with photosensitive epilepsy (PSE) encounter challenges when using devices, resulting in exposure to unpredictable seizure-causing visual stimuli. The current norm for preventing epileptic flashes in media is to detect asynchronously when a flash will occur in a video, then notifying the user. However, there is a lack of a real-time and computationally efficient solution for dealing with this issue. To address this issue and enhance accessibility for photosensitive viewers, FlashGuard, a novel approach, was devised to assess the rate of change of colors in frames across the user's screen and appropriately mitigate stimuli, based on perceptually aligned color space analysis in the CIELAB color space. The detection system is built on analyzing differences in color, and the mitigation system works by reducing luminance and smoothing color transitions. This study provides novel insight into how intrinsic color properties contribute to perceptual differences in flashing for PSE individuals, calling for the adoption of broadened WCAG guidelines to better account for risk. These insights and implementations pave the way for stronger protections for individuals with PSE from dangerous triggers in digital media, both in policy and in software.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Bidding in Service-Oriented Combinatorial Spectrum Forward Auctions</title>
<link>https://arxiv.org/abs/2507.19720</link>
<guid>https://arxiv.org/abs/2507.19720</guid>
<content:encoded><![CDATA[
<div> auctions, spectrum, combinatorial, bidding mechanism, social welfare

Summary:
The article introduces a novel combinatorial forward auction scheme that allows for flexible bidding in dynamic spectrum sharing environments. Participants can submit bids consisting of base spectrum demand and adjustable demand ranges, improving resource efficiency and maximizing social welfare. A Spectrum Equivalent Mapping (SEM) coefficient is used to standardize valuation across frequency bands. A greedy matching algorithm sorts buyers by equivalent unit bid prices to determine winning bids and allocate resources within supply constraints. Simulation results show that the proposed flexible bidding mechanism outperforms existing methods, achieving higher social welfare in dynamic spectrum sharing scenarios. <div>
arXiv:2507.19720v1 Announce Type: cross 
Abstract: Traditional combinatorial spectrum auctions mainly rely on fixed bidding and matching processes, which limit participants' ability to adapt their strategies and often result in suboptimal social welfare in dynamic spectrum sharing environments. To address these limitations, we propose a novel approximately truthful combinatorial forward auction scheme with a flexible bidding mechanism aimed at enhancing resource efficiency and maximizing social welfare. In the proposed scheme, each buyer submits a combinatorial bid consisting of the base spectrum demand and adjustable demand ranges, enabling the auctioneer to dynamically optimize spectrum allocation in response to market conditions. To standardize the valuation across heterogeneous frequency bands, we introduce a Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is employed to determine winning bids by sorting buyers based on their equivalent unit bid prices and allocating resources within supply constraints. Simulation results demonstrate that the proposed flexible bidding mechanism significantly outperforms existing benchmark methods, achieving notably higher social welfare in dynamic spectrum sharing scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)</title>
<link>https://arxiv.org/abs/2507.20234</link>
<guid>https://arxiv.org/abs/2507.20234</guid>
<content:encoded><![CDATA[
<div> Decentralized Autonomous Organizations, DAOs, governance mechanism, user behavior, Internet Computer Protocol, SNS DAO framework<br />
Summary: 
This empirical study explores user behavior in decentralized autonomous organizations (DAOs) using the Internet Computer Protocol DAO framework SNS. The analysis includes participation rates, proposal submission frequency, voter approval rates, decision duration times, and metric shifts. Over 3,000 proposals from 14 SNS DAOs were evaluated, showing high approval rates and alignment in governance mechanisms. SNS DAOs demonstrate higher activity, lower costs, and faster decisions compared to other blockchain platforms. Importantly, SNS DAOs exhibit sustained or increasing engagement levels over time, contrary to declines seen in other frameworks. This study highlights the effectiveness of SNS governance mechanisms in promoting user engagement and agility in decision-making processes. <br /><br /> <div>
arXiv:2507.20234v1 Announce Type: cross 
Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism without centralized leadership. This paper presents an empirical study of user behavior in governance for a variety of DAOs, ranging from DeFi to gaming, using the Internet Computer Protocol DAO framework called SNS (Service Nervous System). To analyse user engagement, we measure participation rates and frequency of proposals submission and voter approval rates. We evaluate decision duration times to determine DAO agility. To investigate dynamic aspects, we also measure metric shifts in time. We evaluate over 3,000 proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected DAO have been existing between 6 and 20 months and cover a wide spectrum of use cases, treasury sizes, and number of participants. We also compare our results for SNS DAOs with DAOs from other blockchain platforms. While approval rates are generally high for all DAOs studied, SNS DAOs show slightly more alignment. We observe that the SNS governance mechanisms and processes in ICP lead to higher activity, lower costs and faster decisions. Most importantly, in contrast to studies which report a decline in participation over time for other frameworks, SNS DAOs exhibit sustained or increasing engagement levels over time.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Community Detection in Academic Networks by Handling Publication Bias</title>
<link>https://arxiv.org/abs/2507.20449</link>
<guid>https://arxiv.org/abs/2507.20449</guid>
<content:encoded><![CDATA[
<div> Keywords: research collaboration, topic-based network, BERTopic, SciBERT model, interdisciplinary links

Summary: 
The research article introduces a novel approach to identifying potential research collaborators based on publication content rather than traditional methods like co-authorships and citations. By utilizing BERTopic with a fine-tuned SciBERT model, a topic-based research network is built to connect researchers across disciplines who share topical interests. The main challenge addressed is publication imbalance, where some researchers publish more frequently across various topics, making their less frequent interests less visible. To overcome this, a cloning strategy is proposed, clustering a researcher's publications to treat each cluster as a separate node, allowing researchers to be part of multiple communities and improving the detection of interdisciplinary links. The evaluation of this method demonstrates that the cloned network structure leads to more meaningful communities and reveals a wider range of collaboration opportunities.<br /><br />Summary: <div>
arXiv:2507.20449v1 Announce Type: cross 
Abstract: Finding potential research collaborators is a challenging task, especially in today's fast-growing and interdisciplinary research landscape. While traditional methods often rely on observable relationships such as co-authorships and citations to construct the research network, in this work, we focus solely on publication content to build a topic-based research network using BERTopic with a fine-tuned SciBERT model that connects and recommends researchers across disciplines based on shared topical interests. A major challenge we address is publication imbalance, where some researchers publish much more than others, often across several topics. Without careful handling, their less frequent interests are hidden under dominant topics, limiting the network's ability to detect their full research scope. To tackle this, we introduce a cloning strategy that clusters a researcher's publications and treats each cluster as a separate node. This allows researchers to be part of multiple communities, improving the detection of interdisciplinary links. Evaluation on the proposed method shows that the cloned network structure leads to more meaningful communities and uncovers a broader set of collaboration opportunities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
<div> Identification, classification, sexism, social media, models
Summary:
The paper discusses the fifth Sexism Identification in Social Networks (EXIST) challenge at CLEF 2025, focusing on identifying and classifying sexism in social media posts. Three subtasks are addressed through the implementation of three models: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses adjectives as bottleneck concepts for interpretability, while SCBMT combines this with transformer contextual embeddings for improved performance. The models offer insights at both instance and class levels. Additional metadata like annotators' profiles are explored for leveraging in the classification task. Results show competitive performance, with SCBMT ranking 7th and 6th for English and Spanish, respectively, in Subtask 1.1. In comparison, the fine-tuned XLM-RoBERTa model achieves 6th and 4th for English in the Soft-Soft evaluation of Subtask 1.1. This research contributes to combating sexism on social media through advanced modeling techniques and data analysis. 
<br /><br />Summary: <div>
arXiv:2507.20924v1 Announce Type: cross 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Heterogeneous Networks, Link Prediction, Contrastive Learning, Spatial Heterogeneity, Temporal Heterogeneity

Summary:
This study introduces a novel Contrastive Learning-based Link Prediction model (CLP) designed to address the shortcomings of existing methods in capturing spatial and temporal heterogeneity in Temporal Heterogeneous Networks. The model employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. It includes a spatial feature modeling layer for capturing fine-grained topological distribution patterns and a temporal information modeling layer to perceive evolutionary dependencies. By encoding spatial and temporal distribution heterogeneity from a contrastive learning perspective, the model enables comprehensive self-supervised hierarchical relation modeling for link prediction. Experimental results on four real-world dynamic heterogeneous network datasets show that CLP consistently outperforms state-of-the-art models, demonstrating significant improvements in AUC and AP metrics. This research showcases the effectiveness of CLP in capturing complex system dynamics and heterogeneity in network link prediction tasks.

<br /><br />Summary: <div>
arXiv:2411.00612v2 Announce Type: replace 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixed points of Personalized PageRank centrality: From irreducible to reducible networks</title>
<link>https://arxiv.org/abs/2507.18652</link>
<guid>https://arxiv.org/abs/2507.18652</guid>
<content:encoded><![CDATA[
<div> PageRank, complex network, personalization vector, fixed points, strongly connected components <br />
Summary: <br />
- The study focuses on analyzing the PageRank of a complex network based on its personalization vector.
- The research provides a comprehensive understanding of the existence and uniqueness of fixed points of PageRank in a graph.
- The number and nature of strongly connected components play a crucial role in determining the fixed points of PageRank.
- A feedback-PageRank method is introduced to accurately compute the fixed points using Power's Method and the Perron vector of each strongly connected component.
- The approach presented in the paper offers a systematic way to analyze the PageRank behavior in complex networks. <div>
arXiv:2507.18652v1 Announce Type: new 
Abstract: In this paper we analyze the PageRank of a complex network as a function of its personalization vector. By using this approach, a complete characterization of the existence and uniqueness of fixed points of PageRank of a graph is given in terms of the number and nature of its strongly connected components. The method presented includes the use of a feedback-PageRank in order to compute exactly the fixed points following the classic Power's Method in terms of the (left-hand) Perron vector of each strongly connected components.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative news posts are less prevalent and generate lower user engagement than non-negative news posts across six countries</title>
<link>https://arxiv.org/abs/2507.19300</link>
<guid>https://arxiv.org/abs/2507.19300</guid>
<content:encoded><![CDATA[
<div> Facebook, news posts, negativity, engagement, multilingual classifiers  
Summary:  
Negative news posts on Facebook make up a small fraction (12.6%) of all posts, with political news posts not being more negative than non-political ones. In the U.S., political news posts are relatively less negative compared to other countries. Negative news posts receive fewer likes and comments compared to non-negative posts. Only a small proportion (10.2% to 13.1%) of user engagement with news posts comes from negative posts by analyzed news organizations. This comparative study sheds light on the prevalence of negative news on social media and its impact on user engagement, suggesting that negativity does not always correlate with higher engagement levels. <br /><br />Summary: <div>
arXiv:2507.19300v1 Announce Type: new 
Abstract: Although news negativity is often studied, missing is comparative evidence on the prevalence of and engagement with negative political and non-political news posts on social media. We use 6,081,134 Facebook posts published between January 1, 2020, and April 1, 2024, by 97 media organizations in six countries (U.S., UK, Ireland, Poland, France, Spain) and develop two multilingual classifiers for labeling posts as (non-)political and (non-)negative. We show that: (1) negative news posts constitute a relatively small fraction (12.6%); (2) political news posts are neither more nor less negative than non-political news posts; (3) U.S. political news posts are less negative relative to the other countries on average (40% lower odds); (4) Negative news posts get 15% fewer likes and 13% fewer comments than non-negative news posts. Lastly, (5) we provide estimates of the proportion of the total volume of user engagement with negative news posts and show that only between 10.2% to 13.1% of engagement is linked to negative posts by the analyzed news organizations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Changes to the Facebook Algorithm Decreased News Visibility Between 2021-2024</title>
<link>https://arxiv.org/abs/2507.19373</link>
<guid>https://arxiv.org/abs/2507.19373</guid>
<content:encoded><![CDATA[
<div> Facebook, news, algorithm, visibility, reactions <br />
Summary: The study examines the impact of Meta's algorithm changes on news visibility on Facebook between 2016 and 2025. Data from news and non-news pages show a significant decline in user reactions to news, particularly between 2021 and 2024, indicating targeted suppression. Low-quality news sources were disproportionately affected. However, the end of the "War on News" in 2025 led to an increase in user reactions to news, especially low-quality sources. This suppression of news visibility did not align with a decrease in news supply, Facebook user base, or interest in news. The findings suggest that Meta's algorithm changes had a substantial impact on the visibility and engagement with news content on the platform. <div>
arXiv:2507.19373v1 Announce Type: new 
Abstract: Platforms, especially Facebook, are primary news sources in the US. In its widely criticized "War on News," Meta algorithmically deprioritized news and political content. We use data from 40 news organizations (5,243,302 Facebook posts, 7,875,372,958 user reactions) and 21 non-news pages (396,468 posts; 1,909,088,308 reactions) between January 1, 2016 and February 13, 2025 to examine how these changes influenced news visibility on the platform. Reactions to news declined by 78% between 2021 and 2024 while reactions to non-news pages increased, indicating targeted suppression of news visibility. Low-quality sources were especially suppressed, yet the 2025 end to "War on News" increased user reactions to news, especially low-quality ones. These changes do not reflect decreased news supply, Facebook user base, or interest in news over this period.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods</title>
<link>https://arxiv.org/abs/2507.18778</link>
<guid>https://arxiv.org/abs/2507.18778</guid>
<content:encoded><![CDATA[
<div> recommendation system, CityHood, interactive, explainable, personalized

Summary:
CityHood is an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' interests. It leverages large-scale Google Places reviews enriched with various indicators to provide personalized recommendations at city and neighborhood levels. The system uses the LIME technique for explainability and offers natural-language explanations for each suggestion. Users can explore recommendations based on their preferences and understand the reasoning behind each suggestion through a visual interface. CityHood combines interest modeling, multi-scale analysis, and explainability to make travel recommendations transparent and engaging. It bridges gaps in location-based recommendations by considering spatial similarity, cultural alignment, and user interests. <div>
arXiv:2507.18778v1 Announce Type: cross 
Abstract: We present CityHood, an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' areas of interest. The system models user interests leveraging large-scale Google Places reviews enriched with geographic, socio-demographic, political, and cultural indicators. It provides personalized recommendations at city (Core-Based Statistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an explainable technique (LIME) and natural-language explanations. Users can explore recommendations based on their stated preferences and inspect the reasoning behind each suggestion through a visual interface. The demo illustrates how spatial similarity, cultural alignment, and interest understanding can be used to make travel recommendations transparent and engaging. This work bridges gaps in location-based recommendation by combining a kind of interest modeling, multi-scale analysis, and explainability in a user-facing system.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Approach for Agile Supply Chain Decision-Making Based on Network Attributes</title>
<link>https://arxiv.org/abs/2507.19038</link>
<guid>https://arxiv.org/abs/2507.19038</guid>
<content:encoded><![CDATA[
<div> network attributes, supply chain, disruption mitigation, distributed decision-making, performance evaluation

Summary:
This paper addresses the issue of disruptions in global supply chains and the need for agile decision-making strategies to mitigate their impact. The study characterizes supply chains from both a capability and network topological perspective and explores the use of distributed decision-making approaches to improve supply chain resilience. A comprehensive case study is conducted, evaluating the performance of a distributed framework in response to disruptions based on the network structure and agent attributes. Comparisons with a centralized decision-making approach highlight trade-offs between performance, computation time, and network communication. The findings provide valuable insights for practitioners looking to design effective response strategies that leverage agent capabilities, network attributes, and desired supply chain performance. <div>
arXiv:2507.19038v1 Announce Type: cross 
Abstract: In recent years, the frequent occurrence of disruptions has had a negative impact on global supply chains. To stay competitive, enterprises strive to remain agile through the implementation of efficient and effective decision-making strategies in reaction to disruptions. A significant effort has been made to develop these agile disruption mitigation approaches, leveraging both centralized and distributed decision-making strategies. Though trade-offs of centralized and distributed approaches have been analyzed in existing studies, no related work has been found on understanding supply chain performance based on the network attributes of the disrupted supply chain entities. In this paper, we characterize supply chains from a capability and network topological perspective and investigate the use of a distributed decision-making approach based on classical multi-agent frameworks. The performance of the distributed framework is evaluated through a comprehensive case study that investigates the performance of the supply chain as a function of the network structure and agent attributes within the network in the presence of a disruption. Comparison to a centralized decision-making approach highlights trade-offs between performance, computation time, and network communication based on the decision-making strategy and network architecture. Practitioners can use the outcomes of our studies to design response strategies based on agent capabilities, network attributes, and desired supply chain performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epidemiology-informed Network for Robust Rumor Detection</title>
<link>https://arxiv.org/abs/2411.12949</link>
<guid>https://arxiv.org/abs/2411.12949</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, rumor detection, information cascade, epidemiology, social media<br />
<br />
Summary:<br />
The article discusses the challenges posed by the rapid spread of rumors on social media and the limitations of existing graph-based rumor detection models. It introduces a novel approach called Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to improve performance by addressing issues related to data quality and varying depths of propagation trees. The EIN model utilizes large language models to generate stance labels from user responses, eliminating the need for manual annotation. Experimental results show that EIN outperforms state-of-the-art methods on real-world datasets and demonstrates enhanced robustness across different tree depths. By incorporating epidemiological principles into rumor detection, the EIN approach offers a more effective and efficient way to detect and monitor the spread of rumors on social media platforms. <br /> <div>
arXiv:2411.12949v3 Announce Type: replace 
Abstract: The rapid spread of rumors on social media has posed significant challenges to maintaining public trust and information integrity. Since an information cascade process is essentially a propagation tree, recent rumor detection models leverage graph neural networks to additionally capture information propagation patterns, thus outperforming text-only solutions. Given the variations in topics and social impact of the root node, different source information naturally has distinct outreach capabilities, resulting in different heights of propagation trees. This variation, however, impedes the data-driven design of existing graph-based rumor detectors. Given a shallow propagation tree with limited interactions, it is unlikely for graph-based approaches to capture sufficient cascading patterns, questioning their ability to handle less popular news or early detection needs. In contrast, a deep propagation tree is prone to noisy user responses, and this can in turn obfuscate the predictions. In this paper, we propose a novel Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to enhance performance by overcoming data-driven methods sensitivity to data quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is expected that each users stance toward the source information will be annotated. To bypass the costly and time-consuming human labeling process, we take advantage of large language models to generate stance labels, facilitating optimization objectives for learning epidemiology-informed representations. Our experimental results demonstrate that the proposed EIN not only outperforms state-of-the-art methods on real-world datasets but also exhibits enhanced robustness across varying tree depths.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Climate Change Debate in Italy through Information Supply and Demand</title>
<link>https://arxiv.org/abs/2503.17026</link>
<guid>https://arxiv.org/abs/2503.17026</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, social media, information circulation, information voids, Italian climate transition discourse

Summary: 
The study focuses on the dynamics of information circulation and the emergence of information voids in the context of the Italian climate-transition discourse. It highlights the importance of public understanding of climate issues and the role of social media platforms in disseminating information. The model proposed in the study takes into account the supply and demand of information on platforms like Facebook, Instagram, and GDELT, as well as Google searches to capture information demand. The findings reveal responsiveness and temporal coupling between supply and demand, especially during moments of heightened public attention due to significant external events. The study identifies an adaptive information ecosystem but also notes persistent information voids that could limit public understanding and hinder meaningful engagement with climate policy. <div>
arXiv:2503.17026v2 Announce Type: replace 
Abstract: Climate change is one of the most critical challenges of the twenty-first century. Public understanding of climate issues and of the goals regarding the climate transition is essential to translate awareness into concrete actions. In this context, social media platforms play a crucial role in disseminating information about climate change and climate policy. To better understand the dynamics of information circulation and the emergence of information voids we propose a model that takes into account the supply and demand of information related to the Italian climate-transition discourse. We conceptualise information supply as the production of content on Facebook, Instagram and GDELT (an online news database) while leveraging Google searches to capture information demand. Our findings highlight responsiveness and temporal coupling between supply and demand, particularly during moments of heightened public attention triggered by significant external events. These responsive interactions reveal an overall adaptive information ecosystem. However, we also observe persistent information voids which may limit public understanding and delay meaningful engagement.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image</title>
<link>https://arxiv.org/abs/2507.17755</link>
<guid>https://arxiv.org/abs/2507.17755</guid>
<content:encoded><![CDATA[
<div> Keywords: Douyin, WeChat, body image, male adolescents, social media <br />
Summary: <br />
This study explores the impact of two Chinese social media platforms, Douyin and WeChat, on body image perceptions among male adolescents. Through surveys with 395 participants, it was found that Douyin usage was significantly linked to appearance evaluation and body area satisfaction, while WeChat usage showed no significant correlation with any body image dimensions. The results suggest that Douyin's algorithm-driven, video-centric environment may contribute to heightened exposure to idealized body standards, influencing users at a cognitive level. The study emphasizes the need to consider platform-specific characteristics when examining social media's influence on body image. By highlighting how technological design and content modalities shape psychological outcomes, the research provides valuable insights for addressing body image concerns among male adolescents in China. <br /> <div>
arXiv:2507.17755v1 Announce Type: cross 
Abstract: In the digital era, social media platforms play a pivotal role in shaping adolescents' body image perceptions. This study examines how Douyin and WeChat, two contrasting Chinese social media platforms, influence body image among Chinese male adolescents. Employing a platformization perspective, we surveyed 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction. Our findings reveal that Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat usage shows no significant correlation with any body image dimensions. These results suggest that Douyin's algorithm-driven, video-centric environment intensifies exposure to idealized body standards, impacting users at a cognitive level. This study underscores the importance of considering platform-specific characteristics in understanding social media's impact on body image. It contributes to the broader discourse on how technological design and content modalities mediate psychological outcomes, offering insights for addressing body image concerns among male adolescents in China.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Uniqueness and Divisiveness of Presidential Discourse</title>
<link>https://arxiv.org/abs/2401.01405</link>
<guid>https://arxiv.org/abs/2401.01405</guid>
<content:encoded><![CDATA[
<div> Keywords: American presidents, speech patterns, divisive language, uniqueness, language models

Summary:
This study analyzes the speaking styles of American presidents using a unique metric based on language models. The research examines whether presidents speak differently from each other and in what ways, particularly focusing on Donald Trump's distinctiveness. The findings suggest that Trump's speech patterns diverge significantly from other major party nominees, with an emphasis on divisive and antagonistic language towards political opponents. These differences are consistent across various measurement strategies, occurring both in campaign speeches and official addresses. The study concludes that Trump's uniqueness in speech is not simply a result of changes in presidential communications over time. Overall, this research sheds light on the distinct ways in which presidents communicate and highlights Trump's particularly unique and divisive language compared to his predecessors.<br /><br />Summary: <div>
arXiv:2401.01405v2 Announce Type: replace-cross 
Abstract: Do American presidents speak discernibly different from each other? If so, in what ways? And are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for assessing the distinctive ways in which presidents speak about their political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Donald Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values appear closer to those of the Democrats. Contributing to these differences is Trump's employment of divisive and antagonistic language, particularly when targeting his political opponents. These differences hold across a variety of measurement strategies, arise on both the campaign trail and in official presidential addresses, and do not appear to be an artifact of secular changes in presidential communications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature</title>
<link>https://arxiv.org/abs/2507.16820</link>
<guid>https://arxiv.org/abs/2507.16820</guid>
<content:encoded><![CDATA[
<div> countries, institutions, authors, collaboration networks, topics<br />
<br />
Summary: 
Countries most impacted by COVID-19 were highly active in disaster informatics research, each with specific interests. Regional and language-based collaborations were common among countries and institutions. Top authors tended to partner closely with a few key collaborators, focused on specific topics, with institutions having diverse interests. The pandemic shifted research priorities towards public health in disaster informatics. The field is emphasizing multidimensional resilience strategies and data-sharing collaborations, reflecting global vulnerability awareness. Strategies, practices, and tools used in this study can be applied to similar datasets. The analysis provides insights for policymakers, practitioners, and scholars looking to enhance disaster informatics capacities in complex risk landscapes. <br /><br /> <div>
arXiv:2507.16820v1 Announce Type: new 
Abstract: This study presents a comprehensive bibliometric and topic analysis of the disaster informatics literature published between January 2020 to September 2022. Leveraging a large-scale corpus and advanced techniques such as pre-trained language models and generative AI, we identify the most active countries, institutions, authors, collaboration networks, emergent topics, patterns among the most significant topics, and shifts in research priorities spurred by the COVID-19 pandemic. Our findings highlight (1) countries that were most impacted by the COVID-19 pandemic were also among the most active, with each country having specific research interests, (2) countries and institutions within the same region or share a common language tend to collaborate, (3) top active authors tend to form close partnerships with one or two key partners, (4) authors typically specialized in one or two specific topics, while institutions had more diverse interests across several topics, and (5) the COVID-19 pandemic has influenced research priorities in disaster informatics, placing greater emphasis on public health. We further demonstrate that the field is converging on multidimensional resilience strategies and cross-sectoral data-sharing collaborations or projects, reflecting a heightened awareness of global vulnerability and interdependency. Collecting and quality assurance strategies, data analytic practices, LLM-based topic extraction and summarization approaches, and result visualization tools can be applied to comparable datasets or solve similar analytic problems. By mapping out the trends in disaster informatics, our analysis offers strategic insights for policymakers, practitioners, and scholars aiming to enhance disaster informatics capacities in an increasingly uncertain and complex risk landscape.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media</title>
<link>https://arxiv.org/abs/2507.16847</link>
<guid>https://arxiv.org/abs/2507.16847</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, user behavior, prediction, open-source models, cross-modal configurations

Summary: 
This study explores the evolution of user behavior on social media platforms over their lifetime using a novel approach. By leveraging open-source models and advanced language processing techniques like GPT-2, BERT, and RoBERTa, the research aims to predict future stages of user social evolution, including network changes, future connections, and shifts in activities. Experimental results show that GPT-2 outperforms other models in a Cross-modal configuration, emphasizing the importance of this approach for superior performance. The study addresses critical challenges in social media, such as friend recommendations and activity predictions, providing insights into user behavior trajectories. By anticipating future interactions and activities, the research aims to offer early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.<br /><br />Summary: <div>
arXiv:2507.16847v1 Announce Type: new 
Abstract: Social media platforms serve as a significant medium for sharing personal emotions, daily activities, and various life events, ensuring individuals stay informed about the latest developments. From the initiation of an account, users progressively expand their circle of friends or followers, engaging actively by posting, commenting, and sharing content. Over time, user behavior on these platforms evolves, influenced by demographic attributes and the networks they form. In this study, we present a novel approach that leverages open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint embedding technique, to analyze and predict the evolution of user behavior on social media over their lifetime. Our experiments demonstrate the potential of these models to forecast future stages of a user's social evolution, including network changes, future connections, and shifts in user activities. Experimental results highlight the effectiveness of our approach, with GPT-2 achieving the lowest perplexity (8.21) in a Cross-modal configuration, outperforming RoBERTa (9.11) and BERT, and underscoring the importance of leveraging Cross-modal configurations for superior performance. This approach addresses critical challenges in social media, such as friend recommendations and activity predictions, offering insights into the trajectory of user behavior. By anticipating future interactions and activities, this research aims to provide early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots</title>
<link>https://arxiv.org/abs/2507.16848</link>
<guid>https://arxiv.org/abs/2507.16848</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, disinformation dissemination, multi-agent framework, correction strategies, network modeling <br />
<br />
Summary: 
The study focuses on understanding the influence of social bots in spreading and correcting disinformation in the human-bot symbiotic information ecosystem. The proposed Multi Agent based framework for Disinformation Dissemination (MADD) aims to address the limitations of current studies by utilizing a more realistic propagation network model that integrates scale-free topology and community structures. By incorporating both malicious and legitimate bots with controlled dynamic participation, MADD enables quantitative analysis of correction strategies. The framework was evaluated using individual and group-level metrics, and experiments verified the consistency of user attributes and network structure with real-world data. The simulation of disinformation dissemination showcased the varying effects of fact-based and narrative-based correction strategies. MADD provides a valuable tool for better understanding and managing the impact of social bots in the dissemination of disinformation. <br /> <div>
arXiv:2507.16848v1 Announce Type: new 
Abstract: In the human-bot symbiotic information ecosystem, social bots play key roles in spreading and correcting disinformation. Understanding their influence is essential for risk control and better governance. However, current studies often rely on simplistic user and network modeling, overlook the dynamic behavior of bots, and lack quantitative evaluation of correction strategies. To fill these gaps, we propose MADD, a Multi Agent based framework for Disinformation Dissemination. MADD constructs a more realistic propagation network by integrating the Barabasi Albert Model for scale free topology and the Stochastic Block Model for community structures, while designing node attributes based on real world user data. Furthermore, MADD incorporates both malicious and legitimate bots, with their controlled dynamic participation allows for quantitative analysis of correction strategies. We evaluate MADD using individual and group level metrics. We experimentally verify the real world consistency of MADD user attributes and network structure, and we simulate the dissemination of six disinformation topics, demonstrating the differential effects of fact based and narrative based correction strategies.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Subreddit Behavior as Open-Source Indicators of Coordinated Influence: A Case Study of r/Sino &amp; r/China</title>
<link>https://arxiv.org/abs/2507.16857</link>
<guid>https://arxiv.org/abs/2507.16857</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated influence activity, Reddit communities, sentiment analysis, behavioral profiling, online influence behavior

Summary: 
This study examines indicators of coordinated influence activity in users of the Reddit communities r/Sino and r/China, which focus on Chinese political discourse. The research uses topic modeling and sentiment analysis to analyze posts from users who participate in both communities, creating a user-topic sentiment matrix. Behavioral profiling is conducted using various measures such as lexical diversity, language consistency, and posting frequency. Users with anomalous behavior are identified and examined within a subreddit co-participation network. The study integrates linguistic and behavioral analysis to identify patterns indicative of inauthentic or strategically structured participation. The findings showcase the importance of combining content and activity-based signals in analyzing online influence behavior, particularly within contested information environments. <div>
arXiv:2507.16857v1 Announce Type: new 
Abstract: This study investigates potential indicators of coordinated influence activity among users participating in both r/Sino and r/China, two ideologically divergent Reddit communities focused on Chinese political discourse. Topic modeling and sentiment analysis are applied to all posts and comments authored by dual-subreddit users to construct a user-topic sentiment matrix. Individual sentiment patterns are compared to global topic baselines derived from the broader r/Sino and r/China populations. Behavioral profiling is performed using full user activity histories and metadata, incorporating measures such as lexical diversity, language consistency, account age, posting frequency, and karma distribution. Users exhibiting multiple behavioral anomalies are identified and examined within a subreddit co-participation network to assess structural overlap. The combined linguistic and behavioral analysis enables the identification of patterns consistent with inauthentic or strategically structured participation. These findings demonstrate the utility of integrating content and activity-based signals in the analysis of online influence behavior within contested information environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky</title>
<link>https://arxiv.org/abs/2507.16858</link>
<guid>https://arxiv.org/abs/2507.16858</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, Congressional Democrats, social network analysis, decentralized platforms, political messaging

Summary:
In the wake of the 2024 U.S. presidential election, Democratic lawmakers shifted from mainstream social media to Bluesky, a decentralized platform. A study on 182 verified Democratic members of Congress analyzes their use of Bluesky to form influence networks and disseminate political messaging. Party leaders like Hakeem Jeffries and Elizabeth Warren are prominent, but lesser-known figures like Marcy Kaptur and Donald Beyer hold significant influence positions. Analysis reveals homophily based on ideology, state, and leadership lines, with Senate leaders less connected. Topic modeling uncovers shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad standing out. The study highlights how decentralized platforms reshape intra-party communication and emphasizes the importance of computational research on elite political behavior in digital environments. <br /><br />Summary: <div>
arXiv:2507.16858v1 Announce Type: new 
Abstract: Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and transformer-based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party communication dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs</title>
<link>https://arxiv.org/abs/2507.16860</link>
<guid>https://arxiv.org/abs/2507.16860</guid>
<content:encoded><![CDATA[
<div> Language Models, Fake Profile Detection, Adversarial Training, Robustness, GPT<br />
<br />
Summary: 
The study evaluates the robustness of existing fake profile detectors against Large Language Models (LLMs) like GPT. While current detectors are effective against manually created fake profiles, they struggle to detect LLM-generated profiles. The proposed solution involves GPT-assisted adversarial training, which reduces the False Accept Rate significantly without affecting the False Reject Rates. Ablation studies show that detectors utilizing combined numerical and textual embeddings perform the best. Analysis also highlights the importance of automated detectors over prompt-based language models like GPT-4Turbo and human evaluators for detecting fake profiles effectively. The study emphasizes the necessity to enhance fake profile detection mechanisms to combat the risk posed by LLMs in creating realistic fake profiles on platforms like LinkedIn.<br /> <div>
arXiv:2507.16860v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made it easier to create realistic fake profiles on platforms like LinkedIn. This poses a significant risk for text-based fake profile detectors. In this study, we evaluate the robustness of existing detectors against LLM-generated profiles. While highly effective in detecting manually created fake profiles (False Accept Rate: 6-7%), the existing detectors fail to identify GPT-generated profiles (False Accept Rate: 42-52%). We propose GPT-assisted adversarial training as a countermeasure, restoring the False Accept Rate to between 1-7% without impacting the False Reject Rates (0.5-2%). Ablation studies revealed that detectors trained on combined numerical and textual embeddings exhibit the highest robustness, followed by those using numerical-only embeddings, and lastly those using textual-only embeddings. Complementary analysis on the ability of prompt-based GPT-4Turbo and human evaluators affirms the need for robust automated detectors such as the one proposed in this study.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of temporal influence in polarised networks</title>
<link>https://arxiv.org/abs/2507.17177</link>
<guid>https://arxiv.org/abs/2507.17177</guid>
<content:encoded><![CDATA[
<div> influential users, social networks, polarised networks, temporal dynamics, community structure

Summary: 
This study investigates the dynamics of users' influence in polarised social networks, where information is often segregated within communities. It highlights the importance of identifying the most influential users across different communities and acknowledges that users' influence can change over time as opinions evolve. The research compares the stability of influence rankings using temporal centrality measures, considering community structure and network evolution behaviors. The study successfully groups nodes into influence bands and demonstrates how centrality scores can be aggregated to analyze community influence over time. The modified temporal independent cascade model and temporal degree centrality prove to be effective in isolating nodes into their respective influence bands, providing valuable insights into the changing dynamics of user influence in polarized social networks. <div>
arXiv:2507.17177v1 Announce Type: new 
Abstract: In social networks, it is often of interest to identify the most influential users who can successfully spread information to others. This is particularly important for marketing (e.g., targeting influencers for a marketing campaign) and to understand the dynamics of information diffusion (e.g., who is the most central user in the spreading of a certain type of information). However, different opinions often split the audience and make the network polarised. In polarised networks, information becomes soiled within communities in the network, and the most influential user within a network might not be the most influential across all communities. Additionally, influential users and their influence may change over time as users may change their opinion or choose to decrease or halt their engagement on the subject. In this work, we aim to study the temporal dynamics of users' influence in a polarised social network. We compare the stability of influence ranking using temporal centrality measures, while extending them to account for community structure across a number of network evolution behaviours. We show that we can successfully aggregate nodes into influence bands, and how to aggregate centrality scores to analyse the influence of communities over time. A modified version of the temporal independent cascade model and the temporal degree centrality perform the best in this setting, as they are able to reliably isolate nodes into their bands.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quotegraph: A Social Network Extracted from Millions of News Quotations</title>
<link>https://arxiv.org/abs/2507.17626</link>
<guid>https://arxiv.org/abs/2507.17626</guid>
<content:encoded><![CDATA[
<div> network, Quotegraph, social, news articles, computational social scientists  
Summary:  
Quotegraph is a new large-scale social network created from speaker-attributed quotations in English news articles from 2008 to 2020. It comprises 528 thousand unique nodes and 8.63 million directed edges, connecting speakers to the individuals they mention. Nodes are linked to Wikidata entries, providing detailed biographic entity information like nationality, gender, and political affiliation. Derived from Quotebank, Quotegraph includes context information, enriching its relations. The network construction process is language-agnostic, facilitating the creation of similar datasets from non-English news sources. Quotegraph offers valuable insights for computational social scientists, complementing online social networks and offering new perspectives on public figure behavior as portrayed in news media.  

Summary: <div>
arXiv:2507.17626v1 Announce Type: new 
Abstract: We introduce Quotegraph, a novel large-scale social network derived from speaker-attributed quotations in English news articles published between 2008 and 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million directed edges, pointing from speakers to persons they mention. The nodes are linked to their corresponding items in Wikidata, thereby endowing the dataset with detailed biographic entity information, including nationality, gender, and political affiliation. Being derived from Quotebank, a massive corpus of quotations, relations in Quotegraph are additionally enriched with the information about the context in which they are featured. Each part of the network construction pipeline is language agnostic, enabling the construction of similar datasets based on non-English news corpora. We believe Quotegraph is a compelling resource for computational social scientists, complementary to online social networks, with the potential to yield novel insights into the behavior of public figures and how it is captured in the news.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic First-Order Logic Queries in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.17215</link>
<guid>https://arxiv.org/abs/2507.17215</guid>
<content:encoded><![CDATA[
<div> Thresholded First Order Logic, Motif Analysis, Temporal Networks, Algorithm, Triadic Queries<br />
<br />Summary:
In the study of motif counting in network analysis, the concept of thresholded First Order Logic (FOL) Motif Analysis has been introduced for massive temporal networks. This approach combines ideas from logic and database theory to extract richer information from networks by incorporating temporal constraints into motif queries. The FOLTY algorithm was developed to efficiently mine thresholded FOL triadic queries, with theoretical running time matching the best known for temporal triangle counting in sparse graphs. The algorithm's implementation using specialized temporal data structures allows for fast processing, enabling the analysis of networks with millions of edges in a short time frame on standard hardware. This work has the potential to open up new avenues for research in the established field of motif analysis. <div>
arXiv:2507.17215v1 Announce Type: cross 
Abstract: Motif counting is a fundamental problem in network analysis, and there is a rich literature of theoretical and applied algorithms for this problem. Given a large input network $G$, a motif $H$ is a small "pattern" graph indicative of special local structure. Motif/pattern mining involves finding all matches of this pattern in the input $G$. The simplest, yet challenging, case of motif counting is when $H$ has three vertices, often called a "triadic" query. Recent work has focused on "temporal graph mining", where the network $G$ has edges with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of "thresholded First Order Logic (FOL) Motif Analysis" for massive temporal networks. A typical triadic motif query asks for the existence of three vertices that form a desired temporal pattern. An "FOL" motif query is obtained by having both existential and thresholded universal quantifiers. This allows for query semantics that can mine richer information from networks. A typical triadic query would be "find all triples of vertices $u,v,w$ such that they form a triangle within one hour". A thresholded FOL query can express "find all pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$ also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic queries. The theoretical running time of FOLTY matches the best known running time for temporal triangle counting in sparse graphs. We give an efficient implementation of FOLTY using specialized temporal data structures. FOLTY has excellent empirical behavior, and can answer triadic FOL queries on graphs with nearly 70M edges is less than hour on commodity hardware. Our work has the potential to start a new research direction in the classic well-studied problem of motif analysis.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors</title>
<link>https://arxiv.org/abs/2412.20936</link>
<guid>https://arxiv.org/abs/2412.20936</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence maximization, temporal social networks, dynamic interactions, active-inactive transitions, seed selection

Summary:<br />
Influence maximization in temporal social networks is challenging due to dynamic interactions and active-inactive transitions among nodes. The Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R) addresses this by incorporating active-inactive transitions and node reinforcement, improving influence spread understanding. This model leads to a submodular and monotone objective function, allowing for efficient seed selection optimization. A temporal snapshot sampling method simplifies network analysis, enhancing seed selection efficiency. Adapting prior seed selection algorithms to the cpSI-R model and sampling strategy leads to reduced computational costs and improved performance. Experimental evaluations on various datasets show significant performance enhancements over baseline methods, proving the effectiveness of cpSI-R for real-world temporal networks.<br /><br />Summary: <div>
arXiv:2412.20936v2 Announce Type: replace 
Abstract: Influence maximization in temporal social networks presents unique challenges due to the dynamic interactions that evolve over time. Traditional diffusion models often fall short in capturing the real-world complexities of active-inactive transitions among nodes, obscuring the true behavior of influence spread. In dynamic networks, nodes do not simply transition to an active state once; rather, they can oscillate between active and inactive states, with the potential for reactivation and reinforcement over time. This reactivation allows previously influenced nodes to regain influence potency, enhancing their ability to spread influence to others and amplifying the overall diffusion process. Ignoring these transitions can thus conceal the cumulative impact of influence, making it essential to account for them in any effective diffusion model. To address these challenges, we introduce the Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R), which explicitly incorporates active-inactive transitions, capturing the progressive reinforcement that makes nodes more potent spreaders upon reactivation. This model naturally leads to a submodular and monotone objective function, which supports efficient optimization for seed selection in influence maximization tasks. Alongside cpSI-R, we propose an efficient temporal snapshot sampling method, simplifying the analysis of evolving networks. We then adapt the prior algorithms of seed selection to our model and sampling strategy, resulting in reduced computational costs and enhanced seed selection efficiency. Experimental evaluations on diverse datasets demonstrate substantial improvements in performance over baseline methods, underscoring the effectiveness of cpSI-R for real-world temporal networks
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Matching in Correlated Networks with Node Attributes for Improved Community Recovery</title>
<link>https://arxiv.org/abs/2501.02851</link>
<guid>https://arxiv.org/abs/2501.02851</guid>
<content:encoded><![CDATA[
<div> community detection, multiple networks, correlated node attributes, Stochastic Block Model, Gaussian Mixture Models

Summary:
The study investigates community detection in multiple networks with correlated node attributes and edges, such as social platforms. They introduce the correlated Contextual Stochastic Block Model (CSBM) to incorporate both structural and attribute correlations across graphs. By analyzing correlated Gaussian Mixture Models, they establish conditions for exact node matching using attribute distances. Their two-step algorithm combines edge information and attribute distances through $k$-core matching to accurately identify node correspondence and merge correlated edges for improved community detection. The research shows how incorporating side information from correlated graphs can make community detection feasible even in cases where it may be impossible in a single graph. This approach leverages the interplay between graph matching and community recovery to enhance performance in attribute-based community detection across multiple networks. <br /><br />Summary: <div>
arXiv:2501.02851v2 Announce Type: replace 
Abstract: We study community detection in multiple networks with jointly correlated node attributes and edges. This setting arises naturally in applications such as social platforms, where a shared set of users may exhibit both correlated friendship patterns and correlated attributes across different platforms. Extending the classical Stochastic Block Model (SBM) and its contextual counterpart (Contextual SBM or CSBM), we introduce the correlated CSBM, which incorporates structural and attribute correlations across graphs. To build intuition, we first analyze correlated Gaussian Mixture Models, wherein only correlated node attributes are available without edges, and identify the conditions under which an estimator minimizing the distance between attributes achieves exact matching of nodes across the two databases. For the correlated CSBMs, we develop a two-step procedure that first applies $k$-core matching to most nodes using edge information, then refines the matching for the remaining unmatched nodes by leveraging their attributes with a distance-based estimator. We identify the conditions under which the algorithm recovers the exact node correspondence, enabling us to merge the correlated edges and average the correlated attributes for enhanced community detection. Crucially, by aligning and combining graphs, we identify regimes in which community detection is impossible in a single graph but becomes feasible when side information from correlated graphs is incorporated. Our results illustrate how the interplay between graph matching and community recovery can boost performance, broadening the scope of multi-graph, attribute-based community detection.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, image memorability, virality, neural network, online engagement <br />
<br />
Summary: <br />
This study explores the connection between image memorability and social media virality. By analyzing over 1,200 Reddit image posts, the researchers found that memorable images tend to attract more engagement, specifically in the form of comments. They used a neural network called ResMem to predict image memorability and correlated these scores with virality metrics. Memorable images were linked to neutral-affect comments, indicating a unique pathway to virality compared to emotion-driven content. The study also revealed that visually consistent, memorable posts prompted a variety of externally-associated comments. Analysis of ResMem's layers emphasized the importance of semantic distinctiveness in both memorability and virality, even when accounting for image categories. Overall, the findings suggest that image memorability can be a valuable predictor of online engagement on social media platforms. <br /> <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief Alignment vs Opinion Leadership: Understanding Cross-linguistic Digital Activism in K-pop and BLM Communities</title>
<link>https://arxiv.org/abs/2507.16046</link>
<guid>https://arxiv.org/abs/2507.16046</guid>
<content:encoded><![CDATA[
<div> activism, internet, K-pop fans, Black Lives Matter, belief alignment
Summary: 
- The study explores the motivations behind cross-cultural activism, focusing on the interaction between K-pop fans and the Black Lives Matter movement on Twitter after George Floyd's murder.
- It suggests that belief alignment, where individuals resonate with shared beliefs, drives cross-cultural interactions in digital activism.
- The study indicates that influential online opinion leaders, such as K-pop entertainers, may amplify activism through their actions, but are not the direct cause of activism.
- The findings show a slight increase in belief similarity between BLM and K-pop fans following their interaction on Twitter.
- Overall, the research highlights the importance of belief resonance in driving global movements, transcending geographical boundaries through online platforms. 
<br /><br />Summary: <div>
arXiv:2507.16046v1 Announce Type: new 
Abstract: The internet has transformed activism, giving rise to more organic, diverse, and dynamic social movements that transcend geo-political boundaries. Despite extensive research on the role of social media and the internet in cross-cultural activism, the fundamental motivations driving these global movements remain poorly understood. This study examines two plausible explanations for cross-cultural activism: first, that it is driven by influential online opinion leaders, and second, that it results from individuals resonating with emergent sets of beliefs, values, and norms. We conduct a case study of the interaction between K-pop fans and the Black Lives Matter (BLM) movement on Twitter following the murder of George Floyd. Our findings provide strong evidence that belief alignment, where people resonate with common beliefs, is a primary driver of cross-cultural interactions in digital activism. We also demonstrate that while the actions of potential opinion leaders--in this case, K-pop entertainers--may amplify activism and lead to further expressions of love and admiration from fans, they do not appear to be a direct cause of activism. Finally, we report some initial evidence that the interaction between BLM and K-pop led to slight increases in their overall belief similarity.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections</title>
<link>https://arxiv.org/abs/2507.16298</link>
<guid>https://arxiv.org/abs/2507.16298</guid>
<content:encoded><![CDATA[
<div> WhatsApp tiplines, misinformation, fact-checkers, Indian assembly elections, content analysis

Summary:
The study examines WhatsApp tiplines used during the 2021 Indian assembly elections to combat misinformation. It analyzes 580 claims from 451 users in English, Hindi, and Telugu, categorizing them into election, COVID-19, and other topics. Similarities in claims are found across languages, with some users submitting tips in multiple languages. Fact-checkers take a couple of days on average to debunk claims. Users do not submit claims to multiple fact-checking organizations, indicating unique audiences. Practical recommendations for using tiplines during elections with ethical considerations for users' information are provided. <div>
arXiv:2507.16298v1 Announce Type: new 
Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable users to interact with fact-checkers to verify misleading content. This study analyzes 580 unique claims (tips) from 451 users, covering both high-resource languages (English, Hindi) and a low-resource language (Telugu) during the 2021 Indian assembly elections using a mixed-method approach. We categorize the claims into three categories, election, COVID-19, and others, and observe variations across languages. We compare content similarity through frequent word analysis and clustering of neural sentence embeddings. We also investigate user overlap across languages and fact-checking organizations. We measure the average time required to debunk claims and inform tipline users. Results reveal similarities in claims across languages, with some users submitting tips in multiple languages to the same fact-checkers. Fact-checkers generally require a couple of days to debunk a new claim and share the results with users. Notably, no user submits claims to multiple fact-checking organizations, indicating that each organization maintains a unique audience. We provide practical recommendations for using tiplines during elections with ethical consideration of users' information.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SASH: Decoding Community Structure in Graphs</title>
<link>https://arxiv.org/abs/2507.16583</link>
<guid>https://arxiv.org/abs/2507.16583</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Graph, Error correcting codes, Simulations
Summary:
The paper introduces an encoding method for community structure in graphs and presents a decoding algorithm called SASH to estimate communities from observed data. The problem of detecting communities in a graph is important and has various applications. The approach views a graph as a noisy version of the underlying communities and utilizes error correcting codes. SASH is demonstrated through simulations on an assortative planted partition model and Zachary's Karate Club dataset, showing its performance in identifying densely connected clusters of vertices. Overall, the study provides a novel perspective on community detection in graphs and offers a promising algorithm for accurately decoding community structures from noisy data.<br /><br />Summary: <div>
arXiv:2507.16583v1 Announce Type: new 
Abstract: Detection of communities in a graph entails identifying clusters of densely connected vertices; the area has a variety of important applications and a rich literature. The problem has previously been situated in the realm of error correcting codes by viewing a graph as a noisy version of the assumed underlying communities. In this paper, we introduce an encoding of community structure along with the resulting code's parameters. We then present a novel algorithm, SASH, to decode to estimated communities given an observed dataset. We demonstrate the performance of SASH via simulations on an assortative planted partition model and on the Zachary's Karate Club dataset.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Data-centric Overview of Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.16541</link>
<guid>https://arxiv.org/abs/2507.16541</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Graph Learning, Data Characteristics, Data Utilization, Pretrained Large Models, Realistic Applications

Summary: 
Federated Graph Learning (FGL) is a solution for reconciling decentralized datasets while preserving sensitive information. This survey introduces a two-level data-centric taxonomy for FGL research, focusing on Data Characteristics and Data Utilization. Data Characteristics categorize studies based on dataset properties, while Data Utilization analyzes training procedures to overcome data-centric challenges. The survey also examines FGL integration with Pretrained Large Models, showcases realistic applications, and suggests future research directions aligned with Graph Machine Learning trends. Ultimately, this comprehensive review aims to organize FGL research around data-centric constraints to enhance model performance. 

<br /><br />Summary: <div>
arXiv:2507.16541v1 Announce Type: cross 
Abstract: In the era of big data applications, Federated Graph Learning (FGL) has emerged as a prominent solution that reconcile the tradeoff between optimizing the collective intelligence between decentralized datasets holders and preserving sensitive information to maximum. Existing FGL surveys have contributed meaningfully but largely focus on integrating Federated Learning (FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that emphasis on methodology and simulated scenarios. Notably, a data centric perspective, which systematically examines FGL methods through the lens of data properties and usage, remains unadapted to reorganize FGL research, yet it is critical to assess how FGL studies manage to tackle data centric constraints to enhance model performances. This survey propose a two-level data centric taxonomy: Data Characteristics, which categorizes studies based on the structural and distributional properties of datasets used in FGL, and Data Utilization, which analyzes the training procedures and techniques employed to overcome key data centric challenges. Each taxonomy level is defined by three orthogonal criteria, each representing a distinct data centric configuration. Beyond taxonomy, this survey examines FGL integration with Pretrained Large Models, showcases realistic applications, and highlights future direction aligned with emerging trends in GML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Analytics for Anti-Money Laundering -- A Systematic Literature Review and Experimental Evaluation</title>
<link>https://arxiv.org/abs/2405.19383</link>
<guid>https://arxiv.org/abs/2405.19383</guid>
<content:encoded><![CDATA[
<div> Keywords: money laundering, network analytics, anti-money laundering, deep learning, fraud analytics

Summary: 
This study reviews existing research on using network analytics for anti-money laundering (AML) to combat the pervasive issue of money laundering funding illegal activities. The literature review covers 97 papers and presents a taxonomy following a fraud analytics framework. The research found that while most studies rely on expert-based rules and manual features, deep learning methods are gaining popularity. A comprehensive framework for evaluating and comparing the performance of different methods was also developed. The study compared manual feature engineering, random walk-based, and deep learning methods on two public datasets, concluding that network analytics enhances predictive power but caution is needed in applying Graph Neural Networks (GNNs) due to class imbalance and network topology issues. The study warns against relying on synthetic data for overly optimistic results. An open-source implementation is provided to facilitate further research and standardize the analysis and evaluation of network analytics for AML. 

<br /><br />Summary: <div>
arXiv:2405.19383v4 Announce Type: replace 
Abstract: Money laundering presents a pervasive challenge, burdening society by financing illegal activities. The use of network information is increasingly being explored to effectively combat money laundering, given it involves connected parties. This led to a surge in research on network analytics for anti-money laundering (AML). The literature is, however, fragmented and a comprehensive overview of existing work is missing. This results in limited understanding of the methods to apply and their comparative detection power. This paper presents an extensive and unique literature review, based on 97 papers from Web of Science and Scopus, resulting in a taxonomy following a recently proposed fraud analytics framework. We conclude that most research relies on expert-based rules and manual features, while deep learning methods have been gaining traction. This paper also presents a comprehensive framework to evaluate and compare the performance of prominent methods in a standardized setup. We compare manual feature engineering, random walk-based, and deep learning methods on two publicly available data sets. We conclude that (1) network analytics increases the predictive power, but caution is needed when applying GNNs in the face of class imbalance and network topology, and that (2) care should be taken with synthetic data as this can give overly optimistic results. The open-source implementation facilitates researchers and practitioners to extend this work on proprietary data, promoting a standardised approach for the analysis and evaluation of network analytics for AML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why We Experience Society Differently: Intrinsic Dispositions as Drivers of Ideological Complexity in Adaptive Social Networks</title>
<link>https://arxiv.org/abs/2504.07848</link>
<guid>https://arxiv.org/abs/2504.07848</guid>
<content:encoded><![CDATA[
<div> behavioral tendencies, opinion dynamics, complexity, social networks, inequality

Summary:
- The study focuses on understanding how inequality emerges in complex systems by considering both structural dynamics and intrinsic heterogeneity in opinion dynamics.
- Traditional models overlook how diverse cognitive dispositions influence belief evolution, instead assuming homogeneous agent behavior.
- An adaptive social network model is analyzed, where agents exhibit one of three behavioral tendencies - homophily, neophily, or social conformity.
- The study measures individual opinion trajectories using normalized Lempel-Ziv complexity and finds counterintuitive patterns such as homophilic agents becoming unpredictable, neophilic agents stabilizing, and conformic agents showing a U-shaped trajectory.
- The results show that internal behavioral dispositions primarily govern long-term opinion unpredictability rather than external environmental factors.
- Individuals' experiences of ideological volatility or stability are self-structured through their own cognitive tendencies, leading to persistent disparities in dynamical experience within social systems. <div>
arXiv:2504.07848v2 Announce Type: replace 
Abstract: Understanding the emergence of inequality in complex systems requires attention to both structural dynamics and intrinsic heterogeneity. In the context of opinion dynamics, traditional models relied on static snapshots or assumed homogeneous agent behavior, overlooking how diverse cognitive dispositions shape belief evolution. While some recent models introduce behavioral heterogeneity, they typically focus on macro-level patterns, neglecting the unequal and individualized dynamics that unfold at the agent level. In this study, we analyze an adaptive social network model where each agent exhibits one of three behavioral tendencies-homophily, neophily (attention to novelty), or social conformity-and measure the complexity of individual opinion trajectories using normalized Lempel-Ziv (nLZ) complexity. We find that the resulting dynamics are often counterintuitive-homophilic agents, despite seeking similarity, become increasingly unpredictable; neophilic agents, despite pursuing novelty, stabilize; and conformic agents follow a U-shaped trajectory, transitioning from early stability to later unpredictability. More fundamentally, these patterns remain robust across diverse network settings, showing that internal behavioral dispositions - not external environment - primarily govern long-term opinion unpredictability. The broader implication is that individuals' experiences of ideological volatility, uncertainty, or stability are not merely environmental, but endogenously self-structured through their own cognitive tendencies. These results establish a novel individual-level lens on opinion dynamics, where the behavioral identity of agents serves as a dynamical fingerprint in the evolution of belief systems, and gives rise to persistent disparities in dynamical experience within self-organizing social systems, even in structurally similar environments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study</title>
<link>https://arxiv.org/abs/2502.02451</link>
<guid>https://arxiv.org/abs/2502.02451</guid>
<content:encoded><![CDATA[
<div> machine translation, local lexicon, multilingual language models, large language models, moral foundations

Summary:<br />
- The study examines computational methods for analyzing moral foundations (MFs) in non-English texts, using Chinese as a test case.
- Machine translation and local lexicons are not effective for capturing cultural nuances in moral assessments.
- Multilingual language models and large language models (LLMs) show promise in cross-language MF measurement with transfer learning.
- LLMs are particularly efficient in data usage for this task.
- It is crucial to have human validation in automated MF assessment to ensure cultural nuances are not overlooked.
- The study emphasizes the potential of LLMs for cross-language MF measurements and other complex multilingual coding tasks. 

Summary: <div>
arXiv:2502.02451v3 Announce Type: replace-cross 
Abstract: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discipline and Resistance: The Construction of a Digital Home for TikTok Refugees on Xiaohongshu</title>
<link>https://arxiv.org/abs/2507.14465</link>
<guid>https://arxiv.org/abs/2507.14465</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, Xiaohongshu, heterotopia, cross-cultural discourse, digital migration

Summary: 
This study explores the migration of TikTok users to Xiaohongshu following a potential ban of TikTok in the US, utilizing Foucault's concept of heterotopia to analyze the platform as a crisis space for cross-cultural discussions. Through Critical Discourse Analysis of 586 user comments, the study uncovers how both Chinese and international users collaborated in constructing and challenging a new online order through language negotiation, identity positioning, and playful platform policing. The research highlights unique discursive tactics employed by domestic and overseas users, showcasing a blend of cultural resistance and adaptation. By shedding light on digital migration, heterotopic spaces in social media, and evolving dynamics of cross-cultural discourse amid geopolitical turmoil, this study offers valuable insights into the complexities of online interaction in a globalized world. <div>
arXiv:2507.14465v1 Announce Type: new 
Abstract: This study examines how TikTok refugees moved to Xiaohongshu after TikTok was about to be banned in the United States. It utilizes Foucault's idea of heterotopia to demonstrate how Xiaohongshu became a crisis space for cross-cultural discussions across the Great Firewall. Through Critical Discourse Analysis of 586 user comments, the study reveals how Chinese and international users collaboratively constructed and contested a new online order through language negotiation, identity positioning, and playful platform policing. The findings highlight distinct discursive strategies between domestic and overseas users, reflecting both cultural resistance and adaptation. This research contributes to the understanding of digital migration, heterotopic spaces in social media, and emerging dynamics of cross-cultural discourse during geopolitical crises.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote</title>
<link>https://arxiv.org/abs/2507.14623</link>
<guid>https://arxiv.org/abs/2507.14623</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, cross-cultural interactions, social media, Chinese users, TikTok Refugees  

Summary:  
- This study explores cross-cultural interactions between Chinese users and "TikTok Refugees" on RedNote after TikTok's U.S. ban, using sentiment analysis and topic modeling.  
- Analysis of 1,862 posts and 403,054 comments reveals emotional asymmetry with Chinese users showing pride and praise in cultural discussions, while political topics evoke contempt and anger, particularly from Pro-China users.  
- Pro-Foreign users exhibit strong negative emotions across all topics, while neutral users express curiosity and joy within mainstream norms.  
- Appearance-related content fosters emotionally balanced interactions, while political discussions lead to high polarization.  
- The findings highlight distinct emotion-stance structures in Sino-foreign online interactions, shedding light on identity negotiation in transnational digital publics.  

<br /><br /> <div>
arXiv:2507.14623v1 Announce Type: new 
Abstract: This study examines cross-cultural interactions between Chinese users and self-identified "TikTok Refugees"(foreign users who migrated to RedNote after TikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we use large language model-based sentiment classification and BERT-based topic modelling to explore how both groups engage with the TikTok refugee phenomenon. We analyse what themes foreign users express, how Chinese users respond, how stances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how affective responses differ across topics and identities. Results show strong affective asymmetry: Chinese users respond with varying emotional intensities across topics and stances: pride and praise dominate cultural threads, while political discussions elicit high levels of contempt and anger, especially from Pro-China commenters. Pro-Foreign users exhibit the strongest negative emotions across all topics, whereas neutral users express curiosity and joy but still reinforce mainstream discursive norms. Cross-topic comparisons reveal that appearance-related content produces the most emotionally balanced interactions, while politics generates the highest polarization. Our findings reveal distinct emotion-stance structures in Sino-foreign online interactions and offer empirical insights into identity negotiation in transnational digital publics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Faculty Placement from Patterns in Co-authorship Networks</title>
<link>https://arxiv.org/abs/2507.14696</link>
<guid>https://arxiv.org/abs/2507.14696</guid>
<content:encoded><![CDATA[
<div> faculty hiring, academic placement, co-authorship networks, institutional prestige, predictive accuracy

Summary:
- Faculty hiring plays a crucial role in shaping academic progress and individual career paths.
- Traditional indicators like doctoral department prestige and publication record are not sufficient predictors of faculty placement outcomes.
- The study introduces a novel approach by considering faculty placement as an individual-level prediction task.
- Analysis of temporal co-authorship networks reveals a significant improvement in predictive accuracy, especially for placements at top-tier departments.
- The findings highlight the importance of social networks, professional endorsements, and implicit advocacy in faculty hiring decisions, beyond traditional measures of productivity and prestige.

<br /><br />Summary: <div>
arXiv:2507.14696v1 Announce Type: new 
Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in academia, influencing not only individual career trajectories but also broader patterns of institutional prestige and scientific progress. While traditional studies have found strong correlations between faculty hiring and attributes such as doctoral department prestige and publication record, they rarely assess whether these associations generalize to individual hiring outcomes, particularly for future candidates outside the original sample. Here, we consider faculty placement as an individual-level prediction task. Our data consist of temporal co-authorship networks with conventional attributes such as doctoral department prestige and bibliometric features. We observe that using the co-authorship network significantly improves predictive accuracy by up to 10% over traditional indicators alone, with the largest gains observed for placements at the most elite (top-10) departments. Our results underscore the role that social networks, professional endorsements, and implicit advocacy play in faculty hiring beyond traditional measures of scholarly productivity and institutional prestige. By introducing a predictive framing of faculty placement and establishing the benefit of considering co-authorship networks, this work provides a new lens for understanding structural biases in academia that could inform targeted interventions aimed at increasing transparency, fairness, and equity in academic hiring practices.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Algorithms for Relevant Quantities of Friedkin-Johnsen Opinion Dynamics Model</title>
<link>https://arxiv.org/abs/2507.14864</link>
<guid>https://arxiv.org/abs/2507.14864</guid>
<content:encoded><![CDATA[
<div> Keywords: Online social networks, Friedkin-Johnsen model, equilibrium opinion vector, polarization, disagreement

Summary: 
The paper introduces a computational approach for efficiently determining the equilibrium opinion vector, polarization, and disagreement in online social networks. The Friedkin-Johnsen model is utilized as the basis for modeling opinion formation dynamics in both directed and undirected networks. A deterministic local algorithm with relative error guarantees is proposed, capable of scaling to networks with over ten million nodes. Integration with successive over-relaxation techniques further accelerates the computation process by optimizing convergence rates. Extensive experiments conducted on real-world networks demonstrate the practical effectiveness of the proposed approaches, showcasing significant improvements in computational efficiency and scalability compared to traditional methods. The strategies developed in this work have the potential to enhance our understanding of opinion dynamics in complex social networks and contribute to the development of more effective strategies for managing and analyzing online discourse. 

<br /><br />Summary: <div>
arXiv:2507.14864v1 Announce Type: new 
Abstract: Online social networks have become an integral part of modern society, profoundly influencing how individuals form and exchange opinions across diverse domains ranging from politics to public health. The Friedkin-Johnsen model serves as a foundational framework for modeling opinion formation dynamics in such networks. In this paper, we address the computational task of efficiently determining the equilibrium opinion vector and associated metrics including polarization and disagreement, applicable to both directed and undirected social networks. We propose a deterministic local algorithm with relative error guarantees, scaling to networks exceeding ten million nodes. Further acceleration is achieved through integration with successive over-relaxation techniques, where a relaxation factor optimizes convergence rates. Extensive experiments on diverse real-world networks validate the practical effectiveness of our approaches, demonstrating significant improvements in computational efficiency and scalability compared to conventional methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Privacy Risk Assessment in Social Networks Using User Attributes Social Graphs and Text Analysis</title>
<link>https://arxiv.org/abs/2507.15124</link>
<guid>https://arxiv.org/abs/2507.15124</guid>
<content:encoded><![CDATA[
<div> framework, privacy risk, social networking, user attributes, content 

Summary:
The Comprehensive Privacy Risk Scoring (CPRS) framework quantifies privacy risk by combining user attributes, social graph structures, and user-generated content. It calculates risk scores based on sensitivity, visibility, structural similarity, and entity-level analysis, culminating in a unified risk score. Validation on real-world datasets showed an average CPRS of 0.478, with graph-based risks posing a higher threat than content or profile attributes. Attributes such as email, date of birth, and mobile number were identified as high-risk factors. A user study demonstrated that 85% found the CPRS dashboard clear and actionable, affirming its practicality. This framework offers personalized privacy risk insights and a comprehensive approach to privacy management, with future plans to incorporate temporal dynamics and multimodal content for wider applicability.<br /><br />Summary: <div>
arXiv:2507.15124v1 Announce Type: new 
Abstract: The rise of social networking platforms has amplified privacy threats as users increasingly share sensitive information across profiles, content, and social connections. We present a Comprehensive Privacy Risk Scoring (CPRS) framework that quantifies privacy risk by integrating user attributes, social graph structures, and user-generated content. Our framework computes risk scores across these dimensions using sensitivity, visibility, structural similarity, and entity-level analysis, then aggregates them into a unified risk score. We validate CPRS on two real-world datasets: the SNAP Facebook Ego Network (4,039 users) and the Koo microblogging dataset (1M posts, 1M comments). The average CPRS is 0.478 with equal weighting, rising to 0.501 in graph-sensitive scenarios. Component-wise, graph-based risks (mean 0.52) surpass content (0.48) and profile attributes (0.45). High-risk attributes include email, date of birth, and mobile number. Our user study with 100 participants shows 85% rated the dashboard as clear and actionable, confirming CPRS's practical utility. This work enables personalized privacy risk insights and contributes a holistic, scalable methodology for privacy management. Future directions include incorporating temporal dynamics and multimodal content for broader applicability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Multimodal News Recommendation through Federated Learning</title>
<link>https://arxiv.org/abs/2507.15460</link>
<guid>https://arxiv.org/abs/2507.15460</guid>
<content:encoded><![CDATA[
<div> Multimodal, federated learning, news recommendation, privacy preservation, user interests <br />
<br />
Summary: This paper introduces a novel multimodal federated learning-based approach for personalized news recommendation. It addresses challenges faced by traditional systems by integrating textual and visual features of news items, balancing users' long-term and short-term interests, and enhancing privacy through a federated learning framework. The approach combines a multimodal model for more comprehensive content representation with a time-aware model using multi-head self-attention networks to improve recommendation accuracy. The federated learning framework divides the recommendation model into a server-maintained news model and a user model shared between the server and clients, ensuring collaborative model training without sharing user data. Additionally, a secure aggregation algorithm based on Shamir's secret sharing is employed to further safeguard user privacy. Experimental results on a real-world news dataset demonstrate significantly improved performance compared to existing systems, making it a substantial advancement in privacy-preserving personalized news recommendation. <br /> <div>
arXiv:2507.15460v1 Announce Type: new 
Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to information overload by predicting and suggesting news items tailored to individual user interests. However, traditional PNR systems face several challenges, including an overreliance on textual content, common neglect of short-term user interests, and significant privacy concerns due to centralized data storage. This paper addresses these issues by introducing a novel multimodal federated learning-based approach for news recommendation. First, it integrates both textual and visual features of news items using a multimodal model, enabling a more comprehensive representation of content. Second, it employs a time-aware model that balances users' long-term and short-term interests through multi-head self-attention networks, improving recommendation accuracy. Finally, to enhance privacy, a federated learning framework is implemented, enabling collaborative model training without sharing user data. The framework divides the recommendation model into a large server-maintained news model and a lightweight user model shared between the server and clients. The client requests news representations (vectors) and a user model from the central server, then computes gradients with user local data, and finally sends their locally computed gradients to the server for aggregation. The central server aggregates gradients to update the global user model and news model. The updated news model is further used to infer news representation by the server. To further safeguard user privacy, a secure aggregation algorithm based on Shamir's secret sharing is employed. Experiments on a real-world news dataset demonstrate strong performance compared to existing systems, representing a significant advancement in privacy-preserving personalized news recommendation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apology of Green Digitalization in the Context of Information and Climate Feedback Theory</title>
<link>https://arxiv.org/abs/2507.14162</link>
<guid>https://arxiv.org/abs/2507.14162</guid>
<content:encoded><![CDATA[
<div> digitalization, energy consumption, climate feedback, sustainable digitalization, Green Digital Accord
Summary:
The paper introduces the theory of information and climate feedback (ICF), which examines the impact of digitalization on the environment. It highlights the overlooked influence of information and communication technologies on the biosphere's thermal and energy balance. The ICF model, represented by differential equations, explores the interconnected relationship between digitalization, energy consumption, thermal footprint, climatic response, and infrastructure vulnerability. Through numerical analysis and thermal mapping, critical scenarios like digital overheating and infrastructural collapse are identified. The paper concludes with a call for the Green Digital Accord, an international agreement promoting sustainable digitalization. This interdisciplinary work combines climatology, information technologies, and political economy to address the environmental challenges posed by accelerating digitalization. 
<br /><br />Summary: <div>
arXiv:2507.14162v1 Announce Type: cross 
Abstract: Amid accelerated digitalization, not only is the scale of data processing and storage increasing, but so too is the associated infrastructure load on the climate. Current climate models and environmental protocols almost entirely overlook the impact of information and communication technologies on the thermal and energy balance of the biosphere.
  This paper proposes the theory of information and climate feedback (ICF) as a new nonlinear model describing the loop of digitalization, energy consumption, the thermal footprint, the climatic response, and the vulnerability of digital infrastructure. The system is formalized via differential equations with delays and parameters of sensitivity, greenness, and phase stability.
  A multiscenario numerical analysis, phase reconstructions, and thermal cartography were conducted. Critical regimes, including digital overheating, fluctuational instability, and infrastructural collapse in the absence of adaptive measures, were identified.
  The paper concludes with the proposal of an international agreement titled the Green Digital Accord and a set of metrics for sustainable digitalization. This work integrates climatology, information technologies, and the political economy of sustainability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module</title>
<link>https://arxiv.org/abs/2507.14612</link>
<guid>https://arxiv.org/abs/2507.14612</guid>
<content:encoded><![CDATA[
<div> keywords: POI recommendation, time information, graph representation, weighting factors, performance improvement
<br />
Summary: 
The paper proposes a novel framework, GDPW, for next point of interest (POI) recommendation that considers POI category information and multiple weighting factors. The framework utilizes global category and category-time graphs to learn category and time representations, disentangling them through contrastive learning. It incorporates weighting information such as POI popularity, transition relationships, and distances between POIs to improve prediction accuracy. By jointly considering POI categories and time information, GDPW addresses existing limitations in capturing user tendencies and time continuity. Experimental results on real-world datasets show that GDPW outperforms other models, achieving a performance improvement of 3% to 11%. <div>
arXiv:2507.14612v1 Announce Type: cross 
Abstract: Next point of interest (POI) recommendation primarily predicts future activities based on users' past check-in data and current status, providing significant value to users and service providers. We observed that the popular check-in times for different POI categories vary. For example, coffee shops are crowded in the afternoon because people like to have coffee to refresh after meals, while bars are busy late at night. However, existing methods rarely explore the relationship between POI categories and time, which may result in the model being unable to fully learn users' tendencies to visit certain POI categories at different times. Additionally, existing methods for modeling time information often convert it into time embeddings or calculate the time interval and incorporate it into the model, making it difficult to capture the continuity of time. Finally, during POI prediction, various weighting information is often ignored, such as the popularity of each POI, the transition relationships between POIs, and the distances between POIs, leading to suboptimal performance. To address these issues, this paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW). This framework aims to jointly consider POI category information and multiple POI weighting factors. Specifically, the proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs. We conducted experiments on two real-world datasets, and the results demonstrate that the proposed GDPW outperforms other existing models, improving performance by 3% to 11%.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model</title>
<link>https://arxiv.org/abs/2507.15067</link>
<guid>https://arxiv.org/abs/2507.15067</guid>
<content:encoded><![CDATA[
<div> transformer-based classification model, bad actor detection, robustness, adversarial attacks, sequence embedding  
Summary:  
- The study focuses on developing a robust deep learning model, ROBAD, for detecting bad actors on internet platforms.  
- ROBAD utilizes transformer encoder and decoder blocks to capture local and global information from user posts.  
- The model leverages sequence embeddings to detect potential modifications in input sequences.  
- By incorporating mimicked behaviors of bad actors in training, ROBAD enhances its knowledge and robustness against adversarial attacks.  
- Experimental results on Yelp and Wikipedia datasets demonstrate ROBAD's effectiveness in detecting bad actors under state-of-the-art adversarial attacks.  

<br /><br />Summary: <div>
arXiv:2507.15067v1 Announce Type: cross 
Abstract: Detecting bad actors is critical to ensure the safety and integrity of internet platforms. Several deep learning-based models have been developed to identify such users. These models should not only accurately detect bad actors, but also be robust against adversarial attacks that aim to evade detection. However, past deep learning-based detection models do not meet the robustness requirement because they are sensitive to even minor changes in the input sequence. To address this issue, we focus on (1) improving the model understanding capability and (2) enhancing the model knowledge such that the model can recognize potential input modifications when making predictions. To achieve these goals, we create a novel transformer-based classification model, called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection model), which uses the sequence of user posts to generate user embedding to detect bad actors. Particularly, ROBAD first leverages the transformer encoder block to encode each post bidirectionally, thus building a post embedding to capture the local information at the post level. Next, it adopts the transformer decoder block to model the sequential pattern in the post embeddings by using the attention mechanism, which generates the sequence embedding to obtain the global information at the sequence level. Finally, to enrich the knowledge of the model, embeddings of modified sequences by mimicked attackers are fed into a contrastive-learning-enhanced classification layer for sequence prediction. In essence, by capturing the local and global information (i.e., the post and sequence information) and leveraging the mimicked behaviors of bad actors in training, ROBAD can be robust to adversarial attacks. Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can effectively detect bad actors when under state-of-the-art adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Homophily and Heterophily in Multimodal Graph Clustering</title>
<link>https://arxiv.org/abs/2507.15253</link>
<guid>https://arxiv.org/abs/2507.15253</guid>
<content:encoded><![CDATA[
<div> framework, multimodal graph clustering, Disentangled Multimodal Graph Clustering, dual-frequency fusion, self-supervised alignment  
Summary:  
The article introduces the Disentangled Multimodal Graph Clustering (DMGC) framework for unsupervised learning on multimodal graphs. These graphs combine structured interconnections with heterogeneous data, exhibiting both homophilic and heterophilic relationships. The DMGC framework decomposes the hybrid graph into homophily-enhanced and heterophily-aware graphs, integrating them through a Multimodal Dual-frequency Fusion mechanism. This approach effectively combines different views while preventing category confusion. Self-supervised alignment objectives guide the learning process without requiring labels. Empirical analysis on various datasets shows that DMGC outperforms existing methods, demonstrating its effectiveness and generalizability. The code for DMGC is available on GitHub at https://github.com/Uncnbb/DMGC. 

<br /><br />Summary: <div>
arXiv:2507.15253v1 Announce Type: cross 
Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with structured interconnections, offer substantial real-world utility but remain insufficiently explored in unsupervised learning. In this work, we initiate the study of multimodal graph clustering, aiming to bridge this critical gap. Through empirical analysis, we observe that real-world multimodal graphs often exhibit hybrid neighborhood patterns, combining both homophilic and heterophilic relationships. To address this challenge, we propose a novel framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which decomposes the original hybrid graph into two complementary views: (1) a homophily-enhanced graph that captures cross-modal class consistency, and (2) heterophily-aware graphs that preserve modality-specific inter-class distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism that jointly filters these disentangled graphs through a dual-pass strategy, enabling effective multimodal integration while mitigating category confusion. Our self-supervised alignment objectives further guide the learning process without requiring labels. Extensive experiments on both multimodal and multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art performance, highlighting its effectiveness and generalizability across diverse settings. Our code is available at https://github.com/Uncnbb/DMGC.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicting narratives and polarization on social media</title>
<link>https://arxiv.org/abs/2507.15600</link>
<guid>https://arxiv.org/abs/2507.15600</guid>
<content:encoded><![CDATA[
<div> conflicting narratives, political reality, polarization, issue alignment, discursive mechanisms

Summary: 
This study examines how conflicting narratives in the public sphere contribute to political polarization and issue alignment. By analyzing tweets from opposing opinion groups in the German Twittersphere between 2021 and 2023, the researchers identify conflicting narratives on key issues such as the war in Ukraine, Covid, and climate change. They find that these conflicting narratives stem from differing attributions of actantial roles and the emplotment of different actants for the same events. Additionally, the study uncovers patterns of narrative alignment, wherein political actors strategically align opinions across different issues. These findings highlight the role of narratives as a valuable analytical lens for understanding discursive polarization mechanisms in political discourse. 

<br /><br />Summary: <div>
arXiv:2507.15600v1 Announce Type: cross 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. In this work, we show how the analysis of conflicting narratives, i.e. conflicting interpretive lenses through which political reality is experienced and told, provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere. Building upon previous work that has identified ideologically polarized issues in the German Twittersphere between 2021 and 2023, we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups. Focusing on a selection of salient issues and events (the war in Ukraine, Covid, climate change), we show evidence for conflicting narratives along two dimensions: (i) different attributions of actantial roles to the same set of actants (e.g. diverging interpretations of the role of NATO in the war in Ukraine), and (ii) emplotment of different actants for the same event (e.g. Bill Gates in the right-leaning Covid narrative). Furthermore, we provide first evidence for patterns of narrative alignment, a discursive strategy that political actors employ to align opinions across issues. These findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work</title>
<link>https://arxiv.org/abs/2507.15823</link>
<guid>https://arxiv.org/abs/2507.15823</guid>
<content:encoded><![CDATA[
<div> Keywords: AI for Good, collaboration, deployment, resource-constrained, continuous performance updates 

Summary: 
This work focuses on the deployment of AI models in collaboration with a humanitarian-to-humanitarian organization, emphasizing the importance of real-world impact in the AI for Good space. The paper discusses the process of deploying AI models in resource-constrained environments, highlighting the challenges and successes of maintaining the models for continuous performance updates. The close collaboration with the partner organization is key to ensuring the AI model's success and impact in high-impact applications. Practitioners can learn valuable insights from this work on how to effectively collaborate with organizations in deploying AI models and maintaining them for long-term success. Overall, this study sheds light on the importance of not only developing AI models for high-impact applications but also on the crucial process of deploying and collaborating with partner organizations for real-world impact. 

<br /><br />Summary: <div>
arXiv:2507.15823v1 Announce Type: cross 
Abstract: Publications in the AI for Good space have tended to focus on the research and model development that can support high-impact applications. However, very few AI for Good papers discuss the process of deploying and collaborating with the partner organization, and the resulting real-world impact. In this work, we share details about the close collaboration with a humanitarian-to-humanitarian (H2H) organization and how to not only deploy the AI model in a resource-constrained environment, but also how to maintain it for continuous performance updates, and share key takeaways for practitioners.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection for directed networks revisited using bimodularity</title>
<link>https://arxiv.org/abs/2502.04777</link>
<guid>https://arxiv.org/abs/2502.04777</guid>
<content:encoded><![CDATA[
<div> Keywords: Community structure, Directed networks, Bimodularity, Edge-based clustering, Convex relaxation

Summary:
Community structure, a common feature in networks, has been extensively studied for undirected graphs but not as satisfactorily for directed graphs. This study introduces the concept of bimodularity to represent directed communities by mapping sending and receiving communities. Using convex relaxation and the singular value decomposition of the directed modularity matrix, bimodularity can be optimized. An edge-based clustering approach is then proposed to uncover directed communities and their mappings. The effectiveness of this new framework is demonstrated on a synthetic model and applied to the neuronal network of \textit{C. elegans}, revealing meaningful feedforward loops in the motion systems of the head and body. This novel approach lays a foundation for identifying and understanding community structures in directed networks. 

<br /><br />Summary: Community structure in directed networks is addressed through the concept of bimodularity, optimizing the directed modularity matrix using convex relaxation. An edge-based clustering method reveals directed communities and their mappings, showcased on a synthetic model and the neuronal network of \textit{C. elegans}. This framework advances the detection and comprehension of community structures in directed networks. <div>
arXiv:2502.04777v2 Announce Type: replace 
Abstract: Community structure is a key feature omnipresent in real-world network data. Plethora of methods have been proposed to reveal subsets of densely interconnected nodes using criteria such as the modularity index. These approaches have been successful for undirected graphs, but directed edge information has not yet been dealt with in a satisfactory way. Here, we revisit the concept of directed communities as a mapping between sending and receiving communities. This translates into a new definition that we term bimodularity. Using convex relaxation, bimodularity can be optimized with the singular value decomposition of the directed modularity matrix. Subsequently, we propose an edge-based clustering approach to reveal the directed communities including their mappings. The feasibility of the new framework is illustrated on a synthetic model and further applied to the neuronal wiring diagram of the \textit{C. elegans}, for which it yields meaningful feedforward loops of the head and body motion systems. This framework sets the ground for the understanding and detection of community structures in directed networks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion</title>
<link>https://arxiv.org/abs/2507.13366</link>
<guid>https://arxiv.org/abs/2507.13366</guid>
<content:encoded><![CDATA[
<div> framework, trajectory, synthesis, privacy-preserving, mobility <br />
Summary: <br />
The paper introduces Cardiff, a novel trajectory synthesizing framework for generating fine-grained and privacy-preserving urban mobility data. Cardiff utilizes a cascaded hybrid diffusion-based approach, decomposing the generation process into segment-level and GPS-level synthesis for realistic trajectories. The segment-level encodes road segments into latent embeddings and employs a denoising network for synthesis, while the GPS-level network uses noise augmentation for high-fidelity generation. Cardiff offers a balance between privacy preservation and utility, outperforming existing methods on real-world trajectory datasets in various metrics. <div>
arXiv:2507.13366v1 Announce Type: new 
Abstract: Urban mobility data has significant connections with economic growth and plays an essential role in various smart-city applications. However, due to privacy concerns and substantial data collection costs, fine-grained human mobility trajectories are difficult to become publicly available on a large scale. A promising solution to address this issue is trajectory synthesizing. However, existing works often ignore the inherent structural complexity of trajectories, unable to handle complicated high-dimensional distributions and generate realistic fine-grained trajectories. In this paper, we propose Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory synthesizing framework for fine-grained and privacy-preserving mobility generation. By leveraging the hierarchical nature of urban mobility, Cardiff decomposes the generation process into two distinct levels, i.e., discrete road segment-level and continuous fine-grained GPS-level: (i) In the segment-level, to reduce computational costs and redundancy in raw trajectories, we first encode the discrete road segments into low-dimensional latent embeddings and design a diffusion transformer-based latent denoising network for segment-level trajectory synthesis. (ii) Taking the first stage of generation as conditions, we then design a fine-grained GPS-level conditional denoising network with a noise augmentation mechanism to achieve robust and high-fidelity generation. Additionally, the Cardiff framework not only progressively generates high-fidelity trajectories through cascaded denoising but also flexibly enables a tunable balance between privacy preservation and utility. Experimental results on three large real-world trajectory datasets demonstrate that our method outperforms state-of-the-art baselines in various metrics.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio</title>
<link>https://arxiv.org/abs/2507.13368</link>
<guid>https://arxiv.org/abs/2507.13368</guid>
<content:encoded><![CDATA[
<div> neighborhood search, multiple views, graph clustering, attribute-missing, performance improvement

Summary: 
The paper presents a new method called CMV-ND for deep graph clustering (DGC) on large-scale attribute-missing graphs. It preprocesses structural information into multiple non-redundant views through recursive neighborhood search and neighborhood differentiation strategies. By constructing complementary views from differential hop representations and node features, CMV-ND effectively enhances the performance of existing multi-view clustering and DGC methods. Experimental results on six graph datasets validate the significant performance improvement achieved by CMV-ND in various industrial scenarios like community detection and recommendation.<br /><br />Summary: <div>
arXiv:2507.13368v1 Announce Type: new 
Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes in an attribute graph into different clusters, has seen substantial potential in various industrial scenarios like community detection and recommendation. However, the real-world attribute graphs, e.g., social networks interactions, are usually large-scale and attribute-missing. To solve these two problems, we propose a novel DGC method termed \underline{\textbf{C}}omplementary \underline{\textbf{M}}ulti-\underline{\textbf{V}}iew \underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation (\textit{CMV-ND}), which preprocesses graph structural information into multiple views in a complete but non-redundant manner. First, to ensure completeness of the structural information, we propose a recursive neighborhood search that recursively explores the local structure of the graph by completely expanding node neighborhoods across different hop distances. Second, to eliminate the redundancy between neighborhoods at different hops, we introduce a neighborhood differential strategy that ensures no overlapping nodes between the differential hop representations. Then, we construct $K+1$ complementary views from the $K$ differential hop representations and the features of the target node. Last, we apply existing multi-view clustering or DGC methods to the views. Experimental results on six widely used graph datasets demonstrate that CMV-ND significantly improves the performance of various methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
<link>https://arxiv.org/abs/2507.13370</link>
<guid>https://arxiv.org/abs/2507.13370</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion evolution, consensus, non-intrusive, multi-agent reinforcement learning

Summary: 
The article introduces the H-NeiFi framework, a hierarchical and non-intrusive approach to guiding opinion evolution on social media towards global consensus. By considering social roles and behavioral characteristics, the framework aims to optimize information propagation paths without directly modifying user views or enforcing connections. Through a dynamic two-layer model and non-intrusive neighbor filtering method, H-NeiFi adaptsively controls user communication channels to enhance consensus speed by 22.0% to 30.7% and maintain global convergence even in the absence of experts. Utilizing multi-agent reinforcement learning, the framework offers a long-term reward function to guide opinions naturally and efficiently while preserving user interaction autonomy. This approach presents a new paradigm for social network governance, addressing challenges in promoting global consensus without intruding on individual opinions. 

Summary: <div>
arXiv:2507.13370v1 Announce Type: new 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patterns, Models, and Challenges in Online Social Media: A Survey</title>
<link>https://arxiv.org/abs/2507.13379</link>
<guid>https://arxiv.org/abs/2507.13379</guid>
<content:encoded><![CDATA[
<div> keywords: digital platforms, individual behavior, collective behavior, online social systems, model validation
Summary:
The article discusses the impact of digital platforms on observing individual and collective behavior through interaction data. It highlights the need for consolidation in the field due to methodological heterogeneity and weak integration across domains. The survey systematically synthesizes empirical findings and formal models to understand platform-level regularities and methodological architectures. It emphasizes the importance of current modeling frameworks in accounting for observed dynamics in online social systems. The goal is to establish a shared empirical baseline and identify structural constraints for more robust analyses. The article aims to pave the way for improved, comparable, and actionable analyses of online social systems.<br /><br />Summary: <div>
arXiv:2507.13379v1 Announce Type: new 
Abstract: The rise of digital platforms has enabled the large scale observation of individual and collective behavior through high resolution interaction data. This development has opened new analytical pathways for investigating how information circulates, how opinions evolve, and how coordination emerges in online environments. Yet despite a growing body of research, the field remains fragmented and marked by methodological heterogeneity, limited model validation, and weak integration across domains. This survey offers a systematic synthesis of empirical findings and formal models. We examine platform-level regularities, assess the methodological architectures that generate them, and evaluate the extent to which current modeling frameworks account for observed dynamics. The goal is to consolidate a shared empirical baseline and clarify the structural constraints that shape inference in this domain, laying the groundwork for more robust, comparable, and actionable analyses of online social systems.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Dynamics of Conspiracy Related German Telegram Conversations during COVID-19</title>
<link>https://arxiv.org/abs/2507.13398</link>
<guid>https://arxiv.org/abs/2507.13398</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Telegram chats, COVID-19, misinformation, information flow

Summary:
Conspiracy theories have seen a surge on German-language Telegram chats during the COVID-19 pandemic, raising concerns about their impact on trust, democracy, and public health. The study analyzed the structure of these chats, revealing spikes in activity during key pandemic events and highlighting the role of societal stressors in amplifying conspiratorial beliefs. Information flow was observed from larger national or transnational discourse to localized discussions, with a small number of key actors driving the dissemination of content. The top 10% of chats accounted for a majority of forwarded content, but they operated independently with minimal interconnection. A concerning finding was that a significant proportion of shared links pointed to untrustworthy sources, indicating Telegram's role in spreading misinformation. This research sheds light on how conspiracy-related discussions on the platform serve as vectors for the spread of false information.<br /><br />Summary: <div>
arXiv:2507.13398v1 Announce Type: new 
Abstract: Conspiracy theories have long drawn public attention, but their explosive growth on platforms like Telegram during the COVID-19 pandemic raises pressing questions about their impact on societal trust, democracy, and public health. We provide a geographical, temporal and network analysis of the structure of of conspiracy-related German-language Telegram chats in a novel large-scale data set. We examine how information flows between regional user groups and influential broadcasting channels, revealing the interplay between decentralized discussions and content spread driven by a small number of key actors.
  Our findings reveal that conspiracy-related activity spikes during major COVID-19-related events, correlating with societal stressors and mirroring prior research on how crises amplify conspiratorial beliefs. By analysing the interplay between regional, national and transnational chats, we uncover how information flows from larger national or transnational discourse to localised, community-driven discussions. Furthermore, we find that the top 10% of chats account for 94% of all forwarded content, portraying the large influence of a few actors in disseminating information. However, these chats operate independently, with minimal interconnection between each other, primarily forwarding messages to low-traffic groups. Notably, 43% of links shared in the data set point to untrustworthy sources as identified by NewsGuard, a proportion far exceeding their share on other platforms and in other discourse contexts, underscoring the role of conspiracy-related discussions on Telegram as vector for the spread of misinformation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Multi-Site Sex Ad Data at the Individual Level to Aid Counter-Trafficking Efforts</title>
<link>https://arxiv.org/abs/2507.13477</link>
<guid>https://arxiv.org/abs/2507.13477</guid>
<content:encoded><![CDATA[
<div> Keywords: sex trafficking, adult service websites, counter-trafficking efforts, data linking, artificial intelligence<br />
<br />
Summary: 
The study focuses on the use of adult service websites (ASWs) for sex trafficking and the challenges in collecting and linking data from these sites. The closure of Backpage.com has led to the expansion of ASWs outside US jurisdiction, making it difficult to gather intelligence for counter-trafficking efforts. The researchers have developed an end-to-end process using network science, information systems, and artificial intelligence to link and filter sex ad data efficiently. This process has been successful in identifying over 60 potential victims of sex trafficking, helping them transition out of the exploitative life. The key component of the process is an edge filtering procedure that removes erroneous links in the data. Compared to existing approaches, their process is more computationally efficient and provides more actionable intelligence. The proposed process is a valuable tool in transforming disparate sex ad data into actionable intelligence to combat sex trafficking and save lives.<br /><br /> <div>
arXiv:2507.13477v1 Announce Type: new 
Abstract: The Internet facilitates sex trafficking through adult service websites (ASWs) that host online advertisements for sexual services (sex ads). Since the closure of the popular site Backpage.com, the ecosystem of ASWs has expanded to include multiple competing sites that are hosted outside US jurisdiction. Gaining intelligence for counter-trafficking efforts requires collecting, linking, and cleaning the data from multiple sites. However, high ad volumes, disparate data types, and the existence of generic and misappropriated data make this process challenging. We present an end-to-end process for linking sex ad data and filtering potentially erroneous links. Outputs of the developed process have been used to inform counter-trafficking operations that have helped identify more than 60 potential victims of sex trafficking, some of whom are getting help to transition out of the life. Our process leverages concepts and techniques from network science, information systems, and artificial intelligence to link ads across sites at the level of an individual or unique posting entity. Our approach is computationally efficient, allowing millions of ads to be processed in under an hour. A key component of our process is an edge filtering procedure that identifies and removes potentially erroneous links in a graph representation of sex ad data. A comparison of the proposed process to an existing approach shows that our process is typically more computationally efficient and yields substantial increases in the number of individuals for which we can derive actionable intelligence. The proposed process is an efficient and effective approach for transforming the high volumes of disparate data from sex ads into intelligence that can save lives. It has been refined over years of collaboration with practitioners and represents a strong foundation upon which further counter-trafficking tools can be built.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures</title>
<link>https://arxiv.org/abs/2507.13577</link>
<guid>https://arxiv.org/abs/2507.13577</guid>
<content:encoded><![CDATA[
<div> Keywords: interdependent infrastructure systems, hetero-functional graph, community preferences, Large Language Model, learning algorithms 

Summary:
Interdependent infrastructure systems and communities are represented using a hetero-functional graph (HFG) to encode dependencies between functionalities. This graph establishes a partial order of functionalities to guide repair decisions during disasters. However, integrating community preferences is crucial to refine this order and enhance resilience. To accomplish this, a Large Language Model (LLM) is used as a proxy survey tool to gather community feedback on preferred repair sequences. Simulated personas representing diverse disaster experiences provide input on prioritizing infrastructure repair needs across communities. Learning algorithms are then employed to generate a global order based on aggregated responses from these LLM-generated personas. This approach combines technical criteria with community input to optimize repair strategies and enhance disaster resilience.<br /><br />Summary: Interdependent infrastructure systems are represented using a HFG, allowing for a partial order of repair functionalities. Community preferences are integrated via LLM proxy surveys to refine this order and improve resilience. Simulated personas provide diverse input on prioritizing repair needs, culminating in a global order generated by learning algorithms. <div>
arXiv:2507.13577v1 Announce Type: new 
Abstract: We represent interdependent infrastructure systems and communities alike with a hetero-functional graph (HFG) that encodes the dependencies between functionalities. This graph naturally imposes a partial order of functionalities that can inform the sequence of repair decisions to be made during a disaster across affected communities. However, using such technical criteria alone provides limited guidance at the point where the functionalities directly impact the communities, since these can be repaired in any order without violating the system constraints. To address this gap and improve resilience, we integrate community preferences to refine this partial order from the HFG into a total order. Our strategy involves getting the communities' opinions on their preferred sequence for repair crews to address infrastructure issues, considering potential constraints on resources. Due to the delay and cost associated with real-world survey data, we utilize a Large Language Model (LLM) as a proxy survey tool. We use the LLM to craft distinct personas representing individuals, each with varied disaster experiences. We construct diverse disaster scenarios, and each simulated persona provides input on prioritizing infrastructure repair needs across various communities. Finally, we apply learning algorithms to generate a global order based on the aggregated responses from these LLM-generated personas.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Duplicating Deceit: Inauthentic Behavior Among Indian Misinformation Duplicators on X/Twitter</title>
<link>https://arxiv.org/abs/2507.13636</link>
<guid>https://arxiv.org/abs/2507.13636</guid>
<content:encoded><![CDATA[
<div> Keywords: inauthentic duplication, social media, misinformation, TweeXster, duplication campaigns

Summary:
This paper examines the issue of inauthentic duplication on social media, specifically focusing on the spread of misinformation through multiple accounts sharing identical false tweets. The study utilizes a dataset from AltNews, an Indian fact-checking organization, containing over 12 million posts from 5,493 accounts known to be involved in duplicating such content. Surprisingly, the research reveals that less than 1% of these accounts exhibit bot-like behavior, dispelling the common misconception that bots are solely responsible for the dissemination of false information. The authors introduce TweeXster, a framework designed to detect and analyze duplication campaigns, which uncovers clusters of accounts engaged in the repeated and sometimes revived sharing of deceptive or harmful content. This research sheds light on the complex dynamics of misinformation spread on social media and offers a novel approach to combatting such campaigns. 

<br /><br />Summary: <div>
arXiv:2507.13636v1 Announce Type: new 
Abstract: This paper investigates inauthentic duplication on social media, where multiple accounts share identical misinformation tweets. Leveraging a dataset of misinformation verified by AltNews, an Indian fact-checking organization, we analyze over 12 million posts from 5,493 accounts known to have duplicated such content. Contrary to common assumptions that bots are primarily responsible for spreading false information, fewer than 1\% of these accounts exhibit bot-like behavior. We present TweeXster, a framework for detecting and analyzing duplication campaigns, revealing clusters of accounts involved in repeated and sometimes revived dissemination of false or abusive content.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Route-based Conflation Between Linear Referencing System Maps And OpenStreetMap Using Open-source Tools</title>
<link>https://arxiv.org/abs/2507.13939</link>
<guid>https://arxiv.org/abs/2507.13939</guid>
<content:encoded><![CDATA[
<div> conflation, basemaps, LRS, OpenStreetMap, automated<br />
<br />
Summary: 
This paper introduces an automated open-source process for conflation of two basemaps - the Virginia Department of Transportation's linear reference system (LRS) basemap and OpenStreetMap (OSM) for Virginia. The process involves loading one LRS route at a time, determining the direction of travel, interpolating to fill gaps, and using Valhalla's map-matching algorithm with a Hidden Markov Model (HMM) and Viterbi search to find corresponding points on OSM. Key contributions include successful conflation of Virginia's LRS map with OSM using an automated method, a replicable open-source processing pipeline, and achieving over 98% successful matches. This approach improves upon existing automated conflation processes and eliminates the need for proprietary licenses. <br /><br />Summary: <div>
arXiv:2507.13939v1 Announce Type: new 
Abstract: Transportation researchers and planners utilize a wide range of roadway metrics that are usually associated with different basemaps. Conflation is an important process for transferring these metrics onto a single basemap. However, conflation is often an expensive and time-consuming process based on proprietary algorithms that require manual verification.
  In this paper, an automated open-source process is used to conflate two basemaps: the linear reference system (LRS) basemap produced by the Virginia Department of Transportation and the OpenStreetMap (OSM) basemap for Virginia. This process loads one LRS route at a time, determines the correct direction of travel, interpolates to fill gaps larger than 12 meters, and then uses Valhalla's map-matching algorithm to find the corresponding points along OSM's segments. Valhalla's map-matching process uses a Hidden Markov Model (HMM) and Viterbi search-based approach to find the most likely OSM segments matching the LRS route.
  This work has three key contributions. First, it conflates the Virginia roadway network LRS map with OSM using an automated conflation method based on HMM and Viterbi search. Second, it demonstrates a novel open-source processing pipeline that could be replicated without the need for proprietary licenses. Finally, the overall conflation process yields over 98% successful matches, which is an improvement over most automated processes currently available for this type of conflation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rumour Spreading Depends on the Latent Geometry and Degree Distribution in Social Network Models</title>
<link>https://arxiv.org/abs/2408.01268</link>
<guid>https://arxiv.org/abs/2408.01268</guid>
<content:encoded><![CDATA[
<div> power-law distribution, rumour spreading, ultra-small-world models, geometric inhomogeneous random graphs, metric geometry 

Summary: 
- The study investigates rumour spreading in ultra-small-world models with power-law distribution degrees, focusing on Geometric Inhomogeneous Random Graphs (GIRGs).
- Rumour spreading speed varies in GIRGs with Euclidean geometry, showing slow, fast (polylogarithmic), or ultra-fast rates based on power law exponent and geometry strength.
- Rumour spreading in GIRGs doesn't align with graph distance rates, indicating that faster spreading may occur even with short graph distances.
- Non-metric geometry always leads to at least fast rumour spreading, depicting social connections based on single attributes like familial kinship.
- In Euclidean geometry, efficient rumour transmission pathways differ from known paths, showcasing chains of vertices with specific degree structures. <div>
arXiv:2408.01268v3 Announce Type: replace-cross 
Abstract: We study push-pull rumour spreading in ultra-small-world models for social networks where the degrees follow a power-law distribution. In a non-geometric setting, Fountoulakis, Panagiotou and Sauerwald have shown that rumours always spread ultra-fast (SODA 2012), i.e. in doubly logarithmic time. On the other hand, Janssen and Mehrabian have found that rumours spread slowly (polynomial time) in a spatial preferential attachment model (SIDMA 2017). We study the question systematically for the model of Geometric Inhomogeneous Random Graphs (GIRGs). Our results are two-fold: first, with Euclidean geometry slow, fast (polylogarithmic) and ultra-fast rumour spreading may occur, depending on the exponent of the power law and the strength of the geometry in the networks, and we fully characterise the phase boundaries in between. The regimes do not coincide with the graph distance regimes, i.e., polylogarithmic or even polynomial rumour spreading may occur even if graph distances are doubly logarithmic. We expect these results to hold with little effort for related models, e.g. Scale-Free Percolation. Second, we show that rumour spreading is always (at least) fast in a non-metric geometry. The considered non-metric geometry allows to model social connections where resemblance of vertices in a single attribute, such as familial kinship, already strongly indicates the presence of an edge. Euclidean geometry fails to capture such ties.
  For some regimes in the Euclidean setting, the efficient pathways for spreading rumours differ from previously identified paths. For example, a vertex of degree $d$ can transmit the rumour to a vertex of larger degree by a chain of length $3$, where one of the two intermediaries has constant degree, and the other has degree $d^{c}$ for some constant $c<1$. Similar but longer chains of vertices, all having non-constant degree, turn out to be useful as well.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification of Authoritative Nodes and Dismantling of Illicit Networks Using a Novel Metric for Measuring Strength of a Graph</title>
<link>https://arxiv.org/abs/2507.12711</link>
<guid>https://arxiv.org/abs/2507.12711</guid>
<content:encoded><![CDATA[
<div> metrics, criminal networks, node removal, human perception, network strength 
Summary: 
The study introduces a new metric for evaluating the strength of networks after node removal in scenarios like dismantling criminal networks or containing epidemics. Traditional metrics focus on structural properties of the graph, neglecting human perception. The proposed metric combines structural properties with human perception, aligning more closely with human judgment. Validation through human subject surveys shows that the new metric outperforms existing methods in identifying authoritative nodes and effectively dismantling networks. The metric's effectiveness is demonstrated through dismantling both synthetic and real-world networks. By integrating structural properties and human perception, the new metric offers a more comprehensive evaluation of network strength post-node removal. <div>
arXiv:2507.12711v1 Announce Type: new 
Abstract: Dismantling criminal networks or containing epidemics or misinformation through node removal is a well-studied problem. To evaluate the effectiveness of such efforts, one must measure the strength of the network before and after node removal. Process P1 is considered more effective than P2 if the strength of the residual network after removing k nodes via P1 is smaller than that from P2. This leads to the central question: How should network strength be measured?
  Existing metrics rely solely on structural properties of the graph, such as connectivity. However, in real-world scenarios, particularly in law enforcement, the perception of agents regarding network strength can differ significantly from structural assessments. These perceptions are often ignored in traditional metrics.
  We propose a new strength metric that integrates both structural properties and human perception. Using human subject surveys, we validate our approach against existing metrics. Our metric not only aligns more closely with human judgment but also outperforms traditional methods in identifying authoritative nodes and effectively dismantling both synthetic and real-world networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3MAL: Test-Time Fast Adaptation for Robust Multi-Scale Information Diffusion Prediction</title>
<link>https://arxiv.org/abs/2507.12880</link>
<guid>https://arxiv.org/abs/2507.12880</guid>
<content:encoded><![CDATA[
<div> adaptation, information diffusion prediction, test-time training, distribution shifts, self-supervised auxiliary task <br />
<br />
Summary: 
The paper introduces T3MAL, a framework for multi-scale diffusion prediction that addresses distribution shifts within IDP tasks. T3MAL utilizes a test-time training approach to adapt a trained model to the distribution of each test instance, improving prediction accuracy in social networks with uncertain user behavior. The framework includes a self-supervised auxiliary network inspired by BYOL, facilitating instance-specific adaptation during testing. T3MAL also incorporates a meta-auxiliary learning scheme and a lightweight adaptor for efficient test-time adaptation and better weight initialization to prevent catastrophic forgetting. Extensive experiments on public datasets demonstrate T3MAL's superiority over existing methods in information diffusion prediction tasks. <div>
arXiv:2507.12880v1 Announce Type: new 
Abstract: Information diffusion prediction (IDP) is a pivotal task for understanding how information propagates among users. Most existing methods commonly adhere to a conventional training-test paradigm, where models are pretrained on training data and then directly applied to test samples. However, the success of this paradigm hinges on the assumption that the data are independently and identically distributed, which often fails in practical social networks due to the inherent uncertainty and variability of user behavior. In the paper, we address the novel challenge of distribution shifts within IDP tasks and propose a robust test-time training (TTT)-based framework for multi-scale diffusion prediction, named T3MAL. The core idea is to flexibly adapt a trained model to accommodate the distribution of each test instance before making predictions via a self-supervised auxiliary task. Specifically, T3MAL introduces a BYOL-inspired self-supervised auxiliary network that shares a common feature extraction backbone with the primary diffusion prediction network to guide instance-specific adaptation during testing. Furthermore, T3MAL enables fast and accurate test-time adaptation by incorporating a novel meta-auxiliary learning scheme and a lightweight adaptor, which together provide better weight initialization for TTT and mitigate catastrophic forgetting. Extensive experiments on three public datasets demonstrate that T3MAL outperforms various state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Centrality Paradox: Why Your Friends Are Always More Important</title>
<link>https://arxiv.org/abs/2507.13059</link>
<guid>https://arxiv.org/abs/2507.13059</guid>
<content:encoded><![CDATA[
<div> friendship paradox, network centrality, graph theory, eigenvector centrality, PageRank <br />
<br />
Summary: 
The article revisits the classical friendship paradox in the context of network centrality measures. It shows that in any irreducible, undirected graph, various centrality measures such as degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities follow the friendship paradox principle, where one's friends, on average, have more connections than oneself. The study connects this result to the variational characterization of the eigenvector corresponding to the Perron eigenvalue. By generalizing the friendship paradox, the research demonstrates that different centrality measures showcase similar patterns, exceeding the global average. This finding contributes to a better understanding of how network structures influence the distribution of connections within a graph, highlighting the importance of considering various centrality measures in network analysis. <div>
arXiv:2507.13059v1 Announce Type: new 
Abstract: We revisit the classical friendship paradox which states that on an average one's friends have at least as many friends as oneself and generalize it to a variety of network centrality measures. In particular, we show that for any irreducible, undirected graph $G$, the "friends-average" of degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities exceeds the global average. We show that the result follows from the variational characterisation of the eigenvector corresponding to the Perron eigenvalue.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interacting Hosts with Microbiome Exchange: An Extension of Metacommunity Theory for Discrete Interactions</title>
<link>https://arxiv.org/abs/2507.11958</link>
<guid>https://arxiv.org/abs/2507.11958</guid>
<content:encoded><![CDATA[
<div> Metacommunity theory, microbiome model, interactions, dispersal, living hosts<br />
<br />
Summary: <br />
Microbiomes, collections of interacting microbes in environments, have significant effects on their surroundings. Metacommunity theory, which incorporates interactions across multiple scales, is commonly used in microbiome models. Current metacommunity models assume continuous dispersal of microbiomes between environments, but this does not reflect the discrete interactions between microbiomes in living hosts. A new modeling framework is developed in this paper, considering discrete interactions and using parameters to control interaction frequencies and microbiome exchange amounts. Analytical approximations in three parameter regimes are derived and proven to be accurate. Both parameters are crucial in determining microbiome dynamics, with microbiome convergence depending on the interplay between interaction frequency and strength. The study emphasizes the importance of incorporating discrete interactions in microbiome models for a better understanding of microbiome dynamics in living hosts. <div>
arXiv:2507.11958v1 Announce Type: cross 
Abstract: Microbiomes, which are collections of interacting microbes in an environment, often substantially impact the environmental patches or living hosts that they occupy. In microbiome models, it is important to consider both the local dynamics within an environment and exchanges of microbiomes between environments. One way to incorporate these and other interactions across multiple scales is to employ metacommunity theory. Metacommunity models commonly assume continuous microbiome dispersal between the environments in which local microbiome dynamics occur. Under this assumption, a single parameter between each pair of environments controls the dispersal rate between those environments. This metacommunity framework is well-suited to abiotic environmental patches, but it fails to capture an essential aspect of the microbiomes of living hosts, which generally do not interact continuously with each other. Instead, living hosts interact with each other in discrete time intervals. In this paper, we develop a modeling framework that encodes such discrete interactions and uses two parameters to separately control the interaction frequencies between hosts and the amount of microbiome exchange during each interaction. We derive analytical approximations of models in our framework in three parameter regimes and prove that they are accurate in those regimes. We compare these approximations to numerical simulations for an illustrative model. We demonstrate that both parameters in our modeling framework are necessary to determine microbiome dynamics. Key features of the dynamics, such as microbiome convergence across hosts, depend sensitively on the interplay between interaction frequency and strength.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines</title>
<link>https://arxiv.org/abs/2507.12840</link>
<guid>https://arxiv.org/abs/2507.12840</guid>
<content:encoded><![CDATA[
<div> Vaccine hesitancy, social media, public concerns, Language Models, VaxPulse Query Corner <br />
Summary: <br />
The article discusses the threat of vaccine hesitancy to public health and the role of social media in understanding public concerns. Traditional methods like topic modelling may struggle to capture nuanced opinions, while large Language Models (LLMs) can miss current events and community concerns. Hallucinations in LLMs further complicate public health communication. To address these challenges, the authors developed a tool called VaxPulse Query Corner using the Retrieval Augmented Generation technique. This tool helps answer complex queries about public vaccine concerns on online platforms, enabling public health administrators and stakeholders to better understand public concerns and implement targeted interventions to increase vaccine confidence. An analysis of 35,103 Shingrix social media posts showed high levels of answer faithfulness and relevance, demonstrating the tool's effectiveness in addressing public vaccine concerns. <div>
arXiv:2507.12840v1 Announce Type: cross 
Abstract: Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life Finds A Way: Emergence of Cooperative Structures in Adaptive Threshold Networks</title>
<link>https://arxiv.org/abs/2507.13253</link>
<guid>https://arxiv.org/abs/2507.13253</guid>
<content:encoded><![CDATA[
<div> cooperation, competition, network dynamics, evolution, threshold-directed network

Summary:
The study explores the evolution of new levels of organization in systems with varying biases towards cooperation and antagonism. Using a random threshold-directed network model, the researchers simulate the dynamic formation of directed links based on node-specific traits and threshold rules. The model generates a multi-digraph with signed edges reflecting support or antagonism, leading to two interdependent threshold graphs. By incorporating temporal growth and node turnover, the researchers observe phase transitions in connectivity and resilience within communities. The findings offer insights into adaptive systems in biological and economic contexts and have applications in Collective Affordance Sets. The framework may be valuable for predicting outcomes in ongoing experiments on microbial communities in soil. <br /><br />Summary: <div>
arXiv:2507.13253v1 Announce Type: cross 
Abstract: There has been a long debate on how new levels of organization have evolved. It might seem unlikely, as cooperation must prevail over competition. One well-studied example is the emergence of autocatalytic sets, which seem to be a prerequisite for the evolution of life. Using a simple model, we investigate how varying bias toward cooperation versus antagonism shapes network dynamics, revealing that higher-order organization emerges even amid pervasive antagonistic interactions. In general, we observe that a quantitative increase in the number of elements in a system leads to a qualitative transition.
  We present a random threshold-directed network model that integrates node-specific traits with dynamic edge formation and node removal, simulating arbitrary levels of cooperation and competition. In our framework, intrinsic node values determine directed links through various threshold rules. Our model generates a multi-digraph with signed edges (reflecting support/antagonism, labeled ``help''/``harm''), which ultimately yields two parallel yet interdependent threshold graphs. Incorporating temporal growth and node turnover in our approach allows exploration of the evolution, adaptation, and potential collapse of communities and reveals phase transitions in both connectivity and resilience.
  Our findings extend classical random threshold and Erd\H{o}s-R\'enyi models, offering new insights into adaptive systems in biological and economic contexts, with emphasis on the application to Collective Affordance Sets. This framework should also be useful for making predictions that will be tested by ongoing experiments of microbial communities in soil.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the spillover from online engagement to offline protest: stochastic dynamics and mean-field approximations on networks</title>
<link>https://arxiv.org/abs/2507.13310</link>
<guid>https://arxiv.org/abs/2507.13310</guid>
<content:encoded><![CDATA[
<div> social media, offline protest, coupled modelling framework, online social network, transmission rate

Summary:
The study explores how engagement on social media topics influences offline protest activities using a coupled modelling framework. Various stochastic and mean-field models are developed to estimate the reproductive number and predict surge times in activity. The transmission rate between online and offline realms is crucial, needing to be within a critical range for offline outbursts to occur. Network structure impacts model accuracy, with low-density networks requiring more complexity. However, when tested on real-world networks, increased complexity did not improve accuracy. The research highlights the importance of considering online engagement when studying offline actions and how network density affects model accuracy. <div>
arXiv:2507.13310v1 Announce Type: cross 
Abstract: Social media is transforming various aspects of offline life, from everyday decisions such as dining choices to the progression of conflicts. In this study, we propose a coupled modelling framework with an online social network layer to analyse how engagement on a specific topic spills over into offline protest activities. We develop a stochastic model and derive several mean-field models of varying complexity. These models allow us to estimate the reproductive number and anticipate when surges in activity are likely to occur. A key factor is the transmission rate between the online and offline domains; for offline outbursts to emerge, this rate must fall within a critical range, neither too low nor too high. Additionally, using synthetic networks, we examine how network structure influences the accuracy of these approximations. Our findings indicate that low-density networks need more complex approximations, whereas simpler models can effectively represent higher-density networks. When tested on two real-world networks, however, increased complexity did not enhance accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter</title>
<link>https://arxiv.org/abs/2501.09035</link>
<guid>https://arxiv.org/abs/2501.09035</guid>
<content:encoded><![CDATA[
<div> social media, web content, demographic factors, information sharing, Twitter data

Summary:
The study introduces DomainDemo, a dataset linking Twitter domains with user demographics from 2011 to 2022, providing insights into information sharing during elections. The dataset includes age, gender, race, political affiliation, and geolocation of over 1.5 million users, allowing for a decade of analysis. By aggregating user demographics onto domains, five metrics are derived, including localness and partisan audience, offering a better understanding of information flows. The metrics align with existing classifications, validating DomainDemo's approach. This dataset enhances understanding of social media dynamics and trends in political discourse among registered U.S. voters from different sociodemographic groups. <div>
arXiv:2501.09035v2 Announce Type: replace 
Abstract: Social media play a pivotal role in disseminating web content, particularly during elections, yet our understanding of the association between demographic factors and information sharing online remains limited. Here, we introduce a unique dataset, DomainDemo, linking domains shared on Twitter (X) with the demographic characteristics of associated users, including age, gender, race, political affiliation, and geolocation, from 2011 to 2022. This new resource was derived from a panel of over 1.5 million Twitter users matched against their U.S. voter registration records, facilitating a better understanding of a decade of information flows on one of the most prominent social media platforms and trends in political and public discourse among registered U.S. voters from different sociodemographic groups. By aggregating user demographic information onto the domains, we derive five metrics that provide critical insights into over 129,000 websites. In particular, the localness and partisan audience metrics quantify the domains' geographical reach and ideological orientation, respectively. These metrics show substantial agreement with existing classifications, suggesting the effectiveness and reliability of DomainDemo's approach.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Link Prediction via Hyperedge Copying</title>
<link>https://arxiv.org/abs/2502.02386</link>
<guid>https://arxiv.org/abs/2502.02386</guid>
<content:encoded><![CDATA[
<div> generative model, hypergraphs, noisy copying, likelihood, empirical hypergraphs <br /> 
<br />Summary: 
The article proposes a generative model for temporally-evolving hypergraphs where hyperedges are formed by noisy copying of previous hyperedges. This model is able to replicate several patterns observed in empirical hypergraphs and can be learned from data. It defines a likelihood over complete hypergraphs rather than subsets. The model allows for analysis of node degree, edge size, and edge intersection size distributions in terms of its parameters. It successfully captures certain characteristics of empirical hypergraphs but also has limitations. The article presents a scalable stochastic expectation maximization algorithm for fitting the model to large hypergraph datasets. Furthermore, the model is evaluated in a hypergraph link prediction task and shows competitive performance with a minimal number of parameters compared to large neural networks. <div>
arXiv:2502.02386v2 Announce Type: replace 
Abstract: We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. We also show several features of empirical hypergraphs which are and are not successfully captured by our model. We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
<div> job data, occupation classification, large language models, taxonomies, taxonomy-guided reasoning 

Summary:
The study evaluates large language models' ability to accurately classify job data using occupational taxonomies, highlighting their limitations for smaller models. A multi-stage framework is proposed, incorporating taxonomy-guided reasoning examples to improve performance by aligning outputs with taxonomic knowledge. Evaluations on a large dataset demonstrate that the framework not only enhances occupation and skill classification tasks but also offers a cost-effective alternative to advanced models like GPT-4o, reducing computational costs while maintaining strong performance. This framework proves to be practical and scalable for occupation classification and related tasks across large language models. 

<br /><br />Summary: <div>
arXiv:2503.12989v2 Announce Type: replace-cross 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictable Drifts in Collective Cultural Attention: Evidence from Nation-Level Library Takeout Data</title>
<link>https://arxiv.org/abs/2507.12007</link>
<guid>https://arxiv.org/abs/2507.12007</guid>
<content:encoded><![CDATA[
<div> Keywords: consumer attention, cultural products, popularity distributions, demographic groups, cultural drifts

Summary:
Consumer attention for cultural products, such as books, movies, and songs, is challenging to predict due to intrinsic limits. This study analyzed nationwide library loan data for over 660,000 unique books and discovered that culture drifts continuously, leading to a growing divergence over time. Different book genres exhibit varying drift rates. The influence of demographic factors like age, sex, education, and location on cultural drift was also examined, revealing diverse effects across demographic groups. These findings have implications for market forecasting and recommender systems, emphasizing the importance of considering specific drift dynamics for different types of items and demographic segments.
<br /><br />Summary: <div>
arXiv:2507.12007v1 Announce Type: new 
Abstract: Predicting changes in consumer attention for cultural products, such as books, movies, and songs, is notoriously difficult. Past research on predicting the popularity of individual products suggests the existence of intrinsic prediction limits. However, little is known about the limits for predicting collective attention across cultural products. Here, we analyze four years of nationwide library loan data for approximately 2 million individuals, comprising over 100 million loans of more than 660,000 unique books. We find that culture, as measured by popularity distributions of loaned books, drifts continually from month to month at a near-constant rate, leading to a growing divergence over time, and that drifts vary between different book genres. By linking book loans to registry data, we investigate the influence of age, sex, educational level, and geographical area on cultural drift, finding heterogeneous effects from the different demographic groups. Our findings have important implications for market forecasting and developing robust recommender systems, highlighting the need to account for specific drift dynamics for different types of items and demographic groups.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns</title>
<link>https://arxiv.org/abs/2507.12063</link>
<guid>https://arxiv.org/abs/2507.12063</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, cascade graph mining, Contrastive Cascade Graph Learning (CCGL), information diffusion analysis, cascade classification

Summary: 
CCGL is a promising approach for analyzing cascade graphs in social media data to curb harmful content spread and promote reliable information dissemination. The study evaluates CCGL's performance in cascade classification tasks, showcasing its ability to capture specific structural patterns in cascade graphs across different platforms and models. The findings suggest CCGL's strong effectiveness in understanding the dynamics of information diffusion, making it a valuable tool for various downstream analyses in this domain.<br /><br />Summary: <div>
arXiv:2507.12063v1 Announce Type: new 
Abstract: A wide variety of information is disseminated through social media, and content that spreads at scale can have tangible effects on the real world. To curb the spread of harmful content and promote the dissemination of reliable information, research on cascade graph mining has attracted increasing attention. A promising approach in this area is Contrastive Cascade Graph Learning (CCGL). One important task in cascade graph mining is cascade classification, which involves categorizing cascade graphs based on their structural characteristics. Although CCGL is expected to be effective for this task, its performance has not yet been thoroughly evaluated. This study aims to investigate the effectiveness of CCGL for cascade classification. Our findings demonstrate the strong performance of CCGL in capturing platform- and model-specific structural patterns in cascade graphs, highlighting its potential for a range of downstream information diffusion analysis tasks.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title>
<link>https://arxiv.org/abs/2507.12108</link>
<guid>https://arxiv.org/abs/2507.12108</guid>
<content:encoded><![CDATA[
<div> multimodal, coordinated behavior, digital ecosystem, detection, online platforms
Summary:<br /><br />The study explores the detection of multimodal coordinated online behavior, comparing monomodal and multimodal approaches. It highlights the trade-off between weakly and strongly integrated models, emphasizing the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. The research assesses the unique contributions of different data modalities and the impact of varying implementations of multimodality on detection outcomes. Findings indicate that a multimodal approach provides a more comprehensive understanding of coordination dynamics. The study enhances the ability to detect and analyze coordinated behavior online, offering new perspectives for safeguarding the integrity of digital platforms. <div>
arXiv:2507.12108v1 Announce Type: new 
Abstract: Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk Detection on Social Media</title>
<link>https://arxiv.org/abs/2507.11559</link>
<guid>https://arxiv.org/abs/2507.11559</guid>
<content:encoded><![CDATA[
<div> Dataset, Suicide risk assessment, Machine learning, Deep learning, Privacy protection
Summary:<br /><br />This paper introduces a large-scale dataset of 15,000 user-level social media posts related to cognitive and mental health (CMH) disorders and suicide risk. The dataset includes complete user posting time sequence information and has undergone rigorous annotations. Various machine learning methods, deep learning models, and large language models were evaluated for automatic assessment of suicide risk using this dataset. Results show promising performance in this task. Privacy protection and ethical use of the dataset were discussed, along with potential applications in mental health testing and clinical psychiatric treatment. The study provides valuable insights for future research in this area. <div>
arXiv:2507.11559v1 Announce Type: cross 
Abstract: In recent years, cognitive and mental health (CMH) disorders have increasingly become an important challenge for global public health, especially the suicide problem caused by multiple factors such as social competition, economic pressure and interpersonal relationships among young and middle-aged people. Social media, as an important platform for individuals to express emotions and seek help, provides the possibility for early detection and intervention of suicide risk. This paper introduces a large-scale dataset containing 15,000 user-level posts. Compared with existing datasets, this dataset retains complete user posting time sequence information, supports modeling the dynamic evolution of suicide risk, and we have also conducted comprehensive and rigorous annotations on these datasets. In the benchmark experiment, we systematically evaluated the performance of traditional machine learning methods, deep learning models, and fine-tuned large language models. The experimental results show that our dataset can effectively support the automatic assessment task of suicide risk. Considering the sensitivity of mental health data, we also discussed the privacy protection and ethical use of the dataset. In addition, we also explored the potential applications of the dataset in mental health testing, clinical psychiatric auxiliary treatment, etc., and provided directional suggestions for future research work.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review and the Diffusion of Ideas</title>
<link>https://arxiv.org/abs/2507.11825</link>
<guid>https://arxiv.org/abs/2507.11825</guid>
<content:encoded><![CDATA[
<div> exposure, peer review, knowledge, citations, scientific knowledge 
Summary: 
The study investigates the role of peer review in exposing reviewers to new ideas, utilizing a natural experiment with over 500,000 peer review invitations. Exposure to a manuscript's core ideas impacts the future referencing behavior and knowledge of reviewers who decline the review invite. Reviewers who view manuscript summaries increase citations to the manuscript and demonstrate enhanced breadth, depth, diversity, and prominence in citing the submitting author's work. This underscores how peer review influences the dissemination of scientific knowledge. Despite debates on the costs and burdens of peer review, the study highlights its role as a powerful engine for idea diffusion on a massive scale, driving scientific advancements and scholarly communication. <div>
arXiv:2507.11825v1 Announce Type: cross 
Abstract: This study examines a fundamental yet overlooked function of peer review: its role in exposing reviewers to new and unexpected ideas. Leveraging a natural experiment involving over half a million peer review invitations covering both accepted and rejected manuscripts, and integrating high-scale bibliographic and editorial records for 37,279 submitting authors, we find that exposure to a manuscript's core ideas significantly influences the future referencing behavior and knowledge of reviewer invitees who decline the review invite. Specifically, declining reviewer invitees who could view concise summaries of the manuscript's core ideas not only increase their citations to the manuscript itself but also demonstrate expanded breadth, depth, diversity, and prominence of citations to the submitting author's broader body of work. Overall, these results suggest peer review substantially influences the spread of scientific knowledge. Ironically, while the massive scale of peer review, entailing millions of reviews annually, often drives policy debates about its costs and burdens, our findings demonstrate that precisely because of this scale, peer review serves as a powerful yet previously unrecognized engine for idea diffusion, which is central to scientific advances and scholarly communication.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freshness, Persistence and Success of Scientific Teams</title>
<link>https://arxiv.org/abs/2507.12255</link>
<guid>https://arxiv.org/abs/2507.12255</guid>
<content:encoded><![CDATA[
<div> team science, academic teams, success, collaboration, network-driven approach <br /> 
Summary: <br /> 
The study explores the factors contributing to the success of academic teams in scientific knowledge production. By analyzing data on publications and authors, the research highlights the importance of team freshness and persistence in team success. Contrary to popular belief, success is not solely driven by team persistence but also by the freshness of new collaborations built on prior experience. The study suggests that high-impact research tends to emerge early in a team's lifespan, emphasizing the significance of new collaborative ties. Teams open to new collaborations consistently produce better science, and team re-combinations introducing freshness impulses sustain success. Additionally, experienced teams with persistence impulses are linked to earlier impact. Together, the balance of freshness and persistence influences team success at different stages of collaboration. <div>
arXiv:2507.12255v1 Announce Type: cross 
Abstract: Team science dominates scientific knowledge production, but what makes academic teams successful? Using temporal data on 25.2 million publications and 31.8 million authors, we propose a novel network-driven approach to identify and study the success of persistent teams. Challenging the idea that persistence alone drives success, we find that team freshness - new collaborations built on prior experience - is key to success. High impact research tends to emerge early in a team's lifespan. Analyzing complex team overlap, we find that teams open to new collaborative ties consistently produce better science. Specifically, team re-combinations that introduce new freshness impulses sustain success, while persistence impulses from experienced teams are linked to earlier impact. Together, freshness and persistence shape team success across collaboration stages.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fediverse Sharing: Cross-Platform Interaction Dynamics between Threads and Mastodon Users</title>
<link>https://arxiv.org/abs/2502.17926</link>
<guid>https://arxiv.org/abs/2502.17926</guid>
<content:encoded><![CDATA[
<div> federation, social media platforms, Mastodon, Threads, cross-platform interactions <br />
Summary:
Traditional social media platforms are facing criticism, leading users to seek alternatives like Mastodon and Threads. However, this diversification has caused user dispersion. Federation protocols like ActivityPub have been adopted to address these issues, with Mastodon taking the lead in building decentralized networks. Threads joined the federation in March 2024 with its Fediverse Sharing service, enabling interactions between Threads and Mastodon users. A study of interactions between 20,000+ Threads users and 20,000+ Mastodon users over ten months was conducted. This research lays the groundwork for further exploration of cross-platform interactions and integration driven by federation protocols. <br /><br /> <div>
arXiv:2502.17926v2 Announce Type: replace 
Abstract: Traditional social media platforms, once envisioned as digital town squares, now face growing criticism over corporate control, content moderation, and privacy concerns. Events such as Twitter's acquisition (now X) and major policy changes have pushed users toward alternative platforms like Mastodon and Threads. However, this diversification has led to user dispersion and fragmented discussions across the walled gardens of social media platforms. To address these issues, federation protocols like ActivityPub have been adopted, with Mastodon leading efforts to build decentralized yet interconnected networks. In March 2024, Threads joined this federation by introducing its Fediverse Sharing service, which enables interactions such as posts, replies, and likes between Threads and Mastodon users as if on a unified platform. Building on this development, we study the interactions between 20,000+ Threads users and 20,000+ Mastodon users over a ten-month period. Our work lays the foundation for research on cross-platform interactions and federation-driven platform integration.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Clustering in Hypergraphs through Higher-Order Motifs</title>
<link>https://arxiv.org/abs/2507.10570</link>
<guid>https://arxiv.org/abs/2507.10570</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering, higher-order motifs, local structures, computational efficiency

Summary:
This article introduces a novel approach for local clustering in hypergraphs by utilizing higher-order motifs. Traditional graph-based clustering methods often fail to capture higher-order interactions, leading to subpar clustering results. The proposed method leverages hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. Two strategies are presented for identifying local clusters around a seed hyperedge: a core-based approach using hypergraph core decomposition and a BFS-based method involving breadth-first exploration. An auxiliary hypergraph is constructed to facilitate efficient partitioning, along with a framework for local motif-based clustering. Extensive experiments on real-world datasets showcase the effectiveness of the framework, offering a comparative analysis of the two clustering strategies in terms of clustering quality and computational efficiency.
<br /><br />Summary: <div>
arXiv:2507.10570v1 Announce Type: new 
Abstract: Hypergraphs provide a powerful framework for modeling complex systems and networks with higher-order interactions beyond simple pairwise relationships. However, graph-based clustering approaches, which focus primarily on pairwise relations, fail to represent higher-order interactions, often resulting in low-quality clustering outcomes. In this work, we introduce a novel approach for local clustering in hypergraphs based on higher-order motifs, small connected subgraphs in which nodes may be linked by interactions of any order, extending motif-based techniques previously applied to standard graphs. Our method exploits hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. We propose two alternative strategies for identifying local clusters around a seed hyperedge: a core-based method utilizing hypergraph core decomposition and a BFS-based method based on breadth-first exploration. We construct an auxiliary hypergraph to facilitate efficient partitioning and introduce a framework for local motif-based clustering. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework and provide a comparative analysis of the two proposed clustering strategies in terms of clustering quality and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns</title>
<link>https://arxiv.org/abs/2507.10608</link>
<guid>https://arxiv.org/abs/2507.10608</guid>
<content:encoded><![CDATA[
<div> Keywords: anti-money laundering, network theory, behavioral consistency, laundering patterns, pattern fragility

Summary:
The article challenges the conventional approach to anti-money laundering systems by proposing a network-theoretic perspective that focuses on detecting predefined laundering patterns in transaction networks. It emphasizes the importance of behavioral consistency in identifying money laundering activities and argues that patterns are better captured through subgraph structures expressing semantic and functional roles. The concept of pattern fragility is introduced, highlighting the sensitivity of laundering patterns to small attribute changes and their semantic robustness under significant topological transformations. The article suggests that detecting money laundering should not rely on statistical outliers but on the preservation of behavioral essence. This philosophical shift in approach has significant implications for how anti-money laundering systems model, scan, and interpret networks to combat financial crime.

<br /><br />Summary: <div>
arXiv:2507.10608v1 Announce Type: new 
Abstract: Conventional anti-money laundering (AML) systems predominantly focus on identifying anomalous entities or transactions, flagging them for manual investigation based on statistical deviation or suspicious behavior. This paradigm, however, misconstrues the true nature of money laundering, which is rarely anomalous but often deliberate, repeated, and concealed within consistent behavioral routines. In this paper, we challenge the entity-centric approach and propose a network-theoretic perspective that emphasizes detecting predefined laundering patterns across directed transaction networks. We introduce the notion of behavioral consistency as the core trait of laundering activity, and argue that such patterns are better captured through subgraph structures expressing semantic and functional roles - not solely geometry. Crucially, we explore the concept of pattern fragility: the sensitivity of laundering patterns to small attribute changes and, conversely, their semantic robustness even under drastic topological transformations. We claim that laundering detection should not hinge on statistical outliers, but on preservation of behavioral essence, and propose a reconceptualization of pattern similarity grounded in this insight. This philosophical and practical shift has implications for how AML systems model, scan, and interpret networks in the fight against financial crime.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilayer Artificial Benchmark for Community Detection (mABCD)</title>
<link>https://arxiv.org/abs/2507.10795</link>
<guid>https://arxiv.org/abs/2507.10795</guid>
<content:encoded><![CDATA[
<div> random graph model, community detection, power-law distribution, LFR model, multilayer networks

Summary:
The article introduces the Artificial Benchmark for Community Detection (ABCD) model, a random graph model that incorporates community structure and power-law distribution for both degrees and community sizes. This model, similar to the LFR model but faster and more analytically accessible, is used to generate graphs. The article then proposes a variant of the ABCD model for multilayer networks, known as mABCD. This variant leverages the underlying elements of the ABCD model to address community detection in multilayer networks. The mABCD model offers interpretability and efficiency in generating multilayer networks with community structure and power-law distribution. Overall, the introduction of the mABCD model extends the applicability of the ABCD model to the realm of multilayer networks, providing a valuable tool for community detection research. 

<br /><br />Summary: <div>
arXiv:2507.10795v1 Announce Type: new 
Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model and introduce its variant for multilayer networks, mABCD.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxicity in State Sponsored Information Operations</title>
<link>https://arxiv.org/abs/2507.10936</link>
<guid>https://arxiv.org/abs/2507.10936</guid>
<content:encoded><![CDATA[
<div> Keywords: state-sponsored information operations, toxic language, social media, engagement metrics, Russian influence operations

Summary: 
This study analyzes toxic language deployment in state-sponsored information operations on social media platforms, specifically focusing on 56 million posts from over 42 thousand accounts linked to 18 geopolitical entities on Twitter. Six categories of toxic content were systematically detected and quantified using Google's Perspective API, revealing that toxic content only makes up 1.53% of all posts but garners high engagement. The distribution of toxic content varied across national origins and linguistic structures, indicating strategic deployment in specific geopolitical contexts. Russian influence operations stood out for receiving significantly higher user engagement compared to other countries in the dataset. The findings provide valuable insights into the emotional and rhetorical strategies used in state-sponsored information operations on social media platforms and highlight the importance of understanding these patterns for effective countermeasures. 

Summary:<br />
1. Analysis of toxic language deployment in state-sponsored information operations on social media platforms<br />
2. Detection and quantification of six categories of toxic content from 56 million posts<br />
3. Toxic content comprises 1.53% of all posts but shows high engagement rates<br />
4. Strategic deployment of toxic content in specific geopolitical contexts<br />
5. Russian influence operations garner significantly higher user engagement compared to other countries.<br /> <div>
arXiv:2507.10936v1 Announce Type: new 
Abstract: State-sponsored information operations (IOs) increasingly influence global discourse on social media platforms, yet their emotional and rhetorical strategies remain inadequately characterized in scientific literature. This study presents the first comprehensive analysis of toxic language deployment within such campaigns, examining 56 million posts from over 42 thousand accounts linked to 18 distinct geopolitical entities on X/Twitter. Using Google's Perspective API, we systematically detect and quantify six categories of toxic content and analyze their distribution across national origins, linguistic structures, and engagement metrics, providing essential information regarding the underlying patterns of such operations. Our findings reveal that while toxic content constitutes only 1.53% of all posts, they are associated with disproportionately high engagement and appear to be strategically deployed in specific geopolitical contexts. Notably, toxic content originating from Russian influence operations receives significantly higher user engagement compared to influence operations from any other country in our dataset. Our code is available at https://github.com/shafin191/Toxic_IO.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban delineation through the lens of commute networks: Leveraging graph embeddings to distinguish socioeconomic groups in cities</title>
<link>https://arxiv.org/abs/2507.11057</link>
<guid>https://arxiv.org/abs/2507.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: urban delineation, commute networks, Graph Neural Network, community detection, socioeconomic disparities 

Summary: 
This study focuses on using commute networks from census data to delineate urban areas, utilizing a Graph Neural Network (GNN) model to create low-dimensional representations of urban nodes. The nodes' embeddings are clustered to identify cohesive communities within urban regions. The research demonstrates the efficacy of network embeddings in capturing socioeconomic disparities, particularly in median household income, across various U.S. cities. The role of census mobility data in regional delineation is highlighted, showcasing the utility of GNNs in urban community detection as a powerful alternative to existing methods. The results emphasize the valuable insights provided by commute networks in shaping meaningful representations of urban regions. 

<br /><br />Summary: <div>
arXiv:2507.11057v1 Announce Type: new 
Abstract: Delineating areas within metropolitan regions stands as an important focus among urban researchers, shedding light on the urban perimeters shaped by evolving population dynamics. Applications to urban science are numerous, from facilitating comparisons between delineated districts and administrative divisions to informing policymakers of the shifting economic and labor landscapes. In this study, we propose using commute networks sourced from the census for the purpose of urban delineation, by modeling them with a Graph Neural Network (GNN) architecture. We derive low-dimensional representations of granular urban areas (nodes) using GNNs. Subsequently, nodes' embeddings are clustered to identify spatially cohesive communities in urban areas. Our experiments across the U.S. demonstrate the effectiveness of network embeddings in capturing significant socioeconomic disparities between communities in various cities, particularly in factors such as median household income. The role of census mobility data in regional delineation is also noted, and we establish the utility of GNNs in urban community detection, as a powerful alternative to existing methods in this domain. The results offer insights into the wider effects of commute networks and their use in building meaningful representations of urban regions.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance Stability of Network by Edge Anchor</title>
<link>https://arxiv.org/abs/2507.11090</link>
<guid>https://arxiv.org/abs/2507.11090</guid>
<content:encoded><![CDATA[
<div> influential relationships, community stability, anchor trussness reinforcement problem, greedy framework, real-world networks 

Summary: 
- The study focuses on identifying influential relationships in online social networks to enhance community stability.
- The anchor trussness reinforcement problem is introduced to boost user engagement by anchoring specific edges.
- The problem of identifying edges for maximum trussness gain while staying within a given budget is proven to be NP-hard.
- A greedy framework is proposed to iteratively select the best edges for reinforcement, with a focus on efficiency.
- Methods such as the upward-route approach and classification tree structure are introduced to efficiently compute trussness gain and minimize redundant computations in the process.
- Extensive experiments on real-world networks validate the efficiency and effectiveness of the proposed model and methods. <div>
arXiv:2507.11090v1 Announce Type: new 
Abstract: With the rapid growth of online social networks, strengthening their stability has emerged as a key research focus. This study aims to identify influential relationships that significantly impact community stability. In this paper, we introduce and explore the anchor trussness reinforcement problem to reinforce the overall user engagement of networks by anchoring some edges. Specifically, for a given graph $G$ and a budget $b$, we aim to identify $b$ edges whose anchoring maximizes the trussness gain, which is the cumulative increment of trussness across all edges in $G$. We establish the NP-hardness of the problem. To address this problem, we introduce a greedy framework that iteratively selects the current best edge. To scale for larger networks, we first propose an upward-route method to constrain potential trussness increment edges. Augmented with a support check strategy, this approach enables the efficient computation of the trussness gain for anchoring one edge. Then, we design a classification tree structure to minimize redundant computations in each iteration by organizing edges based on their trussness. We conduct extensive experiments on 8 real-world networks to validate the efficiency and effectiveness of the proposed model and methods.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services</title>
<link>https://arxiv.org/abs/2507.10605</link>
<guid>https://arxiv.org/abs/2507.10605</guid>
<content:encoded><![CDATA[
<div> Keywords: social networking services, large language models, RedOne, content management, interaction quality improvement 

Summary:
RedOne is a domain-specific large language model (LLM) developed to address the challenges faced by social networking services (SNS) in content management and interaction quality improvement. It utilizes a three-stage training strategy involving continue pretraining, supervised fine-tuning, and preference optimization using a large-scale real-world dataset. Through extensive experiments, RedOne demonstrates strong capabilities, outperforming base models by up to 14.02% in SNS tasks and 7.56% in a bilingual evaluation benchmark. In online testing, RedOne reduces exposure to harmful content by 11.23% and improves post-view search click rates by 14.95% compared to baseline models. These results establish RedOne as a robust LLM for SNS, showcasing its versatility across various tasks and potential for real-world applications. 

<br /><br />Summary: <div>
arXiv:2507.10605v1 Announce Type: cross 
Abstract: As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler</title>
<link>https://arxiv.org/abs/2507.10810</link>
<guid>https://arxiv.org/abs/2507.10810</guid>
<content:encoded><![CDATA[
<div> Keywords: online hate, social approval theory, Parler, hate speech, social media platforms <br />
Summary: 
This paper explores the relationship between social approval and online hate speech on the Parler platform from 2018 to 2021. The study specifically looks at whether receiving social approval influences the production and extremity of hate speech messages. Contrary to Walther's social approval theory, the study found that the number of upvotes on hate speech posts did not predict future hate speech production. Between-person effects showed a mixed relationship between social approval and hate speech at different time intervals. The findings suggest that social approval reinforcement mechanisms for online hate may vary on niche social media platforms like Parler. Overall, the study challenges existing theories on the role of social approval in motivating online hate speech production. <br /><br /> <div>
arXiv:2507.10810v1 Announce Type: cross 
Abstract: In this paper, we explored how online hate is motivated by receiving social approval from others. We specifically examined two central tenets of Walther's (2024) social approval theory of online hate: (H1a) more signals of social approval on hate messages predicts more subsequent hate messages, and (H1b) as social approval increases, hate speech messages become more extreme. Using over 110 million posts from Parler (2018-2021), we observed that the number of upvotes a person received on a hate speech post was unassociated with the amount of hate speech in their next post and posts during the next week, month, three months, and six months. Between-person effects revealed an average negative relationship between social approval and hate speech production at the post level, but this relationship was mixed at other time intervals. Social approval reinforcement mechanisms of online hate may operate differently on niche social media platforms.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Potential Impact of Disruptive AI Innovations on U.S. Occupations</title>
<link>https://arxiv.org/abs/2507.11403</link>
<guid>https://arxiv.org/abs/2507.11403</guid>
<content:encoded><![CDATA[
<div> disruption index, AI patents, job tasks, consolidating AI, disruptive AI
<br />
Summary:<br />
The study examines the impact of AI innovations on the labor market through a disruption index of U.S. AI patents. It distinguishes between consolidating AI that reinforces existing structures and disruptive AI that alters them. The analysis reveals that consolidating AI targets physical, routine, and solo tasks in manufacturing and construction, mainly in the Midwest and central states. On the other hand, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, disruptive AI disproportionately affects areas with skilled labor shortages, potentially accelerating change where workers are scarce. Overall, consolidating AI extends current automation trends, while disruptive AI is poised to transform complex mental work, with an exception for collaborative tasks. <div>
arXiv:2507.11403v1 Announce Type: cross 
Abstract: The rapid rise of AI is poised to disrupt the labor market. However, AI is not a monolith; its impact depends on both the nature of the innovation and the jobs it affects. While computational approaches are emerging, there is no consensus on how to systematically measure an innovation's disruptive potential. Here, we calculate the disruption index of 3,237 U.S. AI patents (2015-2022) and link them to job tasks to distinguish between "consolidating" AI innovations that reinforce existing structures and "disruptive" AI innovations that alter them. Our analysis reveals that consolidating AI primarily targets physical, routine, and solo tasks, common in manufacturing and construction in the Midwest and central states. By contrast, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, we also find that disruptive AI disproportionately affects areas already facing skilled labor shortages, suggesting disruptive AI technologies may accelerate change where workers are scarce rather than replacing a surplus. Ultimately, consolidating AI appears to extend current automation trends, while disruptive AI is set to transform complex mental work, with a notable exception for collaborative tasks.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIF: The hypergraph interchange format for higher-order networks</title>
<link>https://arxiv.org/abs/2507.11520</link>
<guid>https://arxiv.org/abs/2507.11520</guid>
<content:encoded><![CDATA[
<div> Hypergraph, Interchange Format, Higher-order networks, Software packages, Data exchange<br />
Summary: <br />
The article introduces the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. It aims to bring together various software packages used for analyzing complex interactions in systems such as chemical reactions, social groups, and ecological dependencies. HIF supports different types of higher-order networks, including undirected and directed hypergraphs, as well as simplicial complexes. It also includes metadata support for attributes associated with nodes, edges, and incidences. The initiative is a collaborative effort involving authors and contributors from prominent hypergraph software packages. The project provides a JSON schema, documentation, unit tests, example datasets, and tutorials demonstrating HIF's compatibility with popular higher-order network analysis software packages. This standardization of data format will facilitate seamless data exchange and enhance the interoperability of different higher-order network analysis tools. <br /> 
Summary: <div>
arXiv:2507.11520v1 Announce Type: cross 
Abstract: Many empirical systems contain complex interactions of arbitrary size, representing, for example, chemical reactions, social groups, co-authorship relationships, and ecological dependencies. These interactions are known as higher-order interactions and the collection of these interactions comprise a higher-order network, or hypergraph. Hypergraphs have established themselves as a popular and versatile mathematical representation of such systems and a number of software packages written in various programming languages have been designed to analyze these networks. However, the ecosystem of higher-order network analysis software is fragmented due to specialization of each software's programming interface and compatible data representations. To enable seamless data exchange between higher-order network analysis software packages, we introduce the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. HIF supports multiple types of higher-order networks, including undirected hypergraphs, directed hypergraphs, and simplicial complexes, while actively exploring extensions to represent multiplex hypergraphs, temporal hypergraphs, and ordered hypergraphs. To accommodate the wide variety of metadata used in different contexts, HIF also includes support for attributes associated with nodes, edges, and incidences. This initiative is a collaborative effort involving authors, maintainers, and contributors from prominent hypergraph software packages. This project introduces a JSON schema with corresponding documentation and unit tests, example HIF-compliant datasets, and tutorials demonstrating the use of HIF with several popular higher-order network analysis software packages.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-based models to randomize real-world hypergraphs</title>
<link>https://arxiv.org/abs/2207.12123</link>
<guid>https://arxiv.org/abs/2207.12123</guid>
<content:encoded><![CDATA[
<div> Keywords: Network theory, hypergraphs, Exponential Random Hypergraphs, entropy-based approach, real-world data <br />
Summary: 
The article focuses on the importance of considering many-body relationships in network theory through the use of hypergraphs to capture polyadic interactions. By using the representation of hypergraphs based on incidence matrices, the study introduces Exponential Random Hypergraphs (ERHs) as a framework to analyze higher-order structures. The research explores the thresholds and asymptotic behavior of ERHs in comparison to traditional percolation thresholds. Key network metrics are generalized to hypergraphs, allowing for the computation of expected values and comparison against empirical data to identify deviations from random behaviors. The method presented is analytically tractable, scalable, and effective in detecting structural patterns in real-world hypergraphs that differ significantly from simpler constraints. <div>
arXiv:2207.12123v3 Announce Type: replace 
Abstract: Network theory has often disregarded many-body relationships, solely focusing on pairwise interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent a suitable framework for describing polyadic interactions. Here, we leverage the representation of hypergraphs based on the incidence matrix for extending the entropy-based approach to higher-order structures: in analogy with the Exponential Random Graphs, we introduce the Exponential Random Hypergraphs (ERHs). After exploring the asymptotic behaviour of thresholds generalising the percolation one, we apply ERHs to study real-world data. First, we generalise key network metrics to hypergraphs; then, we compute their expected value and compare it with the empirical one, in order to detect deviations from random behaviours. Our method is analytically tractable, scalable and capable of revealing structural patterns of real-world hypergraphs that differ significantly from those emerging as a consequence of simpler constraints.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verified authors shape X/Twitter discursive communities</title>
<link>https://arxiv.org/abs/2405.04896</link>
<guid>https://arxiv.org/abs/2405.04896</guid>
<content:encoded><![CDATA[
<div> Keywords: discursive communities, Twitter, verified users, political events, community detection 

Summary: 
The study investigates the detection of discursive communities on Twitter during key Italian political events in 2022. It focuses on the role of verified users as content creators before paid account verification was introduced. Two novel methodologies, MonoDC and BiDC, were proposed and compared to identify community partitions based on the retweet network and shared audience similarity. Leveraging verified users as indicators of prestige and authority resulted in clear community partitions reflecting actual political affiliations. This approach outperformed standard algorithms applied to the entire retweet network. The study highlights the significant influence of verified users in shaping online discourse, emphasizing the importance of platform governance, especially in light of recent changes to paid verification. <div>
arXiv:2405.04896v2 Announce Type: replace 
Abstract: In this study, we address the challenge of detecting ``discursive communities'' on X/Twitter by focusing on the role of verified users as the main content creators in online political debates. The analysis centers on three major Italian political events in 2022 - the Presidential election, a governmental crisis, and the general elections - occurring before the introduction of paid account verification. We propose and compare two novel methodologies, MonoDC and BiDC, which exploit, respectively, the retweet network among users and a similarity network based on shared audiences, while integrating a maximum entropy null model to filter out the inherent noise in online social networks. Our results demonstrate that leveraging verified users-considered as indicators of prestige and authority-leads to significantly clear community partitions that closely reflect the actual political affiliations, outperforming standard community detection algorithms applied to the entire retweet network. Moreover, the comparison of different methodologies and user sets suggests that the status conferred by the blue verification tick plays a dominant role in shaping online discourse, with important implications for platform governance, especially in light of the recent shift to paid verification.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd: A Social Network Simulation Framework</title>
<link>https://arxiv.org/abs/2412.10781</link>
<guid>https://arxiv.org/abs/2412.10781</guid>
<content:encoded><![CDATA[
<div> Agent-based modeling, social networks, simulation, Crowd, Python
<br />
Summary:
Crowd is a social network simulator that utilizes agent-based modeling to simulate real-world phenomena within a network environment. It offers easy setup through YAML configuration, customization options, and features like no-code simulations, interactive visualizations, and data aggregation. Developed in Python, Crowd supports generative agents and integrates easily with data analysis and machine learning libraries. The framework is demonstrated through three case studies showcasing its application in modeling generative agents in epidemics, influence maximization, and networked trust games. Crowd addresses the limitations of general-purpose ABMS frameworks by providing specialized tools for social networks and simplifying complex simulations. <div>
arXiv:2412.10781v3 Announce Type: replace 
Abstract: To observe how individual behavior shapes a larger community's actions, agent-based modeling and simulation (ABMS) has been widely adopted by researchers in social sciences, economics, and epidemiology. While simulations can be run on general-purpose ABMS frameworks, these tools are not specifically designed for social networks and, therefore, provide limited features, increasing the effort required for complex simulations. In this paper, we introduce Crowd, a social network simulator that adopts the agent-based modeling methodology to model real-world phenomena within a network environment. Designed to facilitate easy and quick modeling, Crowd supports simulation setup through YAML configuration and enables further customization with user-defined methods. Other features include no-code simulations for diffusion tasks, interactive visualizations, data aggregation, and chart drawing facilities. Designed in Python, Crowd also supports generative agents and connects easily with Python's libraries for data analysis and machine learning. Finally, we include three case studies to illustrate the use of the framework, including generative agents in epidemics, influence maximization, and networked trust games.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Dynamics in Online Public Discourse: A Case Study of Universal Basic Income Discussions on Reddit</title>
<link>https://arxiv.org/abs/2312.09611</link>
<guid>https://arxiv.org/abs/2312.09611</guid>
<content:encoded><![CDATA[
<div> opinion change, online platforms, Universal Basic Income (UBI), Reddit, public discourse
<br />
Summary:
The study explores how public opinion on Universal Basic Income (UBI) has changed on Reddit, an online platform. By analyzing shifts in user cohorts, community affluence levels, and partisan leanings, the researchers found that support for UBI on Reddit initially declined but saw a significant increase starting in mid-2019. The study highlights the potential of online platforms to capture nuanced shifts in public opinion and offers a conceptual model to understand opinion change at a large scale. The findings illustrate the dynamic nature of public discourse and the impact of online discussions on shaping societal attitudes towards policy proposals. This research can be applied to other important issues and policies to gain insights into the diverse range of opinions within a heterogeneous population.
<br /> <div>
arXiv:2312.09611v2 Announce Type: replace-cross 
Abstract: Societal change is often driven by shifts in public opinion. As citizens evolve in their norms, beliefs, and values, public policies change too. While traditional opinion polling and surveys can outline the broad strokes of whether public opinion on a particular topic is changing, they usually cannot capture the full multi-dimensional richness and diversity of opinion present in a large heterogeneous population. However, an increasing fraction of public discourse about public policy issues is now occurring on online platforms, which presents an opportunity to measure public opinion change at a qualitatively different scale of resolution and context.
  In this paper, we present a conceptual model of observed opinion change on online platforms and apply it to study public discourse on Universal Basic Income (UBI) on Reddit throughout its history. UBI is a periodic, no-strings-attached cash payment given to every citizen of a population. We study UBI as it is a clearly-defined policy proposal that has recently experienced a surge of interest through trends like automation and events like the COVID-19 pandemic. We find that overall stance towards UBI on Reddit significantly declined until mid-2019, when this historical trend suddenly reversed and Reddit became substantially more supportive. Using our model, we find the most significant drivers of this overall stance change were shifts within different user cohorts, within communities that represented similar affluence levels, and within communities that represented similar partisan leanings. Our method identifies nuanced social drivers of opinion change in the large-scale public discourse that now regularly occurs online, and could be applied to a broad set of other important issues and policies.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks</title>
<link>https://arxiv.org/abs/2507.09055</link>
<guid>https://arxiv.org/abs/2507.09055</guid>
<content:encoded><![CDATA[
<div> influence centrality, health misinformation vulnerability centrality, propagation centrality, online social networks, misinformation influencers 

Summary: 
- The study introduces three novel centrality metrics, DIC, MVC, and PC, to understand information flow in online social networks during crises like the COVID-19 pandemic.
- These advanced metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions to identify influential nodes, propagation pathways, and misinformation influencers.
- Comparison with traditional metrics showed a 44.83% increase in the identification of influential nodes using the new metrics.
- Implementing interventions based on the new metrics led to a 25% improvement in reducing health misinformation spread compared to baseline interventions.
- The validation on a different dataset confirmed the generalizability of the advanced metrics in identifying influential actors in diverse health misinformation discussions beyond COVID-19. 

<br /><br /> <div>
arXiv:2507.09055v1 Announce Type: new 
Abstract: The rapid spread of health misinformation on online social networks (OSNs) during global crises such as the COVID-19 pandemic poses challenges to public health, social stability, and institutional trust. Centrality metrics have long been pivotal in understanding the dynamics of information flow, particularly in the context of health misinformation. However, the increasing complexity and dynamism of online networks, especially during crises, highlight the limitations of these traditional approaches. This study introduces and compares three novel centrality metrics: dynamic influence centrality (DIC), health misinformation vulnerability centrality (MVC), and propagation centrality (PC). These metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions. Using the FibVID dataset, we compared traditional and novel metrics to identify influential nodes, propagation pathways, and misinformation influencers. Traditional metrics identified 29 influential nodes, while the new metrics uncovered 24 unique nodes, resulting in 42 combined nodes, an increase of 44.83%. Baseline interventions reduced health misinformation by 50%, while incorporating the new metrics increased this to 62.5%, an improvement of 25%. To evaluate the broader applicability of the proposed metrics, we validated our framework on a second dataset, Monant Medical Misinformation, which covers a diverse range of health misinformation discussions beyond COVID-19. The results confirmed that the advanced metrics generalised successfully, identifying distinct influential actors not captured by traditional methods. In general, the findings suggest that a combination of traditional and novel centrality measures offers a more robust and generalisable framework for understanding and mitigating the spread of health misinformation in different online network contexts.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)</title>
<link>https://arxiv.org/abs/2507.09149</link>
<guid>https://arxiv.org/abs/2507.09149</guid>
<content:encoded><![CDATA[
<div> ELM, misinformation detection, CNN, LSTM, social media <br />
Summary: <br />
- Study uses ELM to improve detection of health misinformation on social media.
- Model combines CNN and LSTM to enhance accuracy and reliability of classification.
- Integrates ELM-based features like text readability and sentiment polarity.
- Achieves high performance metrics: accuracy 97.37%, F1-score 97.41%.
- Feature engineering further boosts precision, recall, and ROC-AUC metrics.
- Demonstrates practical application of psychological theories in ML algorithms for misinformation detection.<br /> <div>
arXiv:2507.09149v1 Announce Type: new 
Abstract: Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks</title>
<link>https://arxiv.org/abs/2507.09657</link>
<guid>https://arxiv.org/abs/2507.09657</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, social network, temperature decisions, personality traits, happiness

Summary:
This study utilizes generative agents powered by large language models to simulate a social network within a residential building, where they make temperature decisions for a central heating system. The agents, categorized as Family Members and Representatives, take into account personal preferences, traits, connections, and weather conditions. Daily simulations involve reaching consensus at the family level before making building-wide decisions among representatives. The research explores three distributions of personality traits (positive, mixed, and negative) and establishes a correlation between positive traits and increased happiness and stronger friendships. Results show that temperature preferences, assertiveness, and selflessness play a vital role in both happiness and decision-making processes. Overall, this work highlights the effectiveness of using LLM-driven agents to model complex human behavior in scenarios where real-life simulations are challenging. 

<br /><br />Summary: <div>
arXiv:2507.09657v1 Announce Type: new 
Abstract: We use generative agents powered by large language models (LLMs) to simulate a social network in a shared residential building, driving the temperature decisions for a central heating system. Agents, divided into Family Members and Representatives, consider personal preferences, personal traits, connections, and weather conditions. Daily simulations involve family-level consensus followed by building-wide decisions among representatives. We tested three personality traits distributions (positive, mixed, and negative) and found that positive traits correlate with higher happiness and stronger friendships. Temperature preferences, assertiveness, and selflessness have a significant impact on happiness and decisions. This work demonstrates how LLM-driven agents can help simulate nuanced human behavior where complex real-life human simulations are difficult to set.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Analysis and Evaluation of Cohesive Subgraph Discovery</title>
<link>https://arxiv.org/abs/2507.10262</link>
<guid>https://arxiv.org/abs/2507.10262</guid>
<content:encoded><![CDATA[
<div> cohesive subgraphs, networks, social network analysis, graph data management, evaluation

Summary: 
This study evaluates different cohesive subgraph models for their performance in retrieving cohesive subgraphs in networks. The research includes task-based evaluations on synthetic and real-world networks to provide insights into the efficiency and applicability of these models. The findings highlight the balance between interpretability and cohesion of subgraphs, guiding the selection of suitable models for specific analytical needs and applications. The study not only offers a comprehensive evaluation of current models but also sets the foundation for future research in this area. The systematic comparison of performance across varied network configurations fills a gap in the existing literature and provides valuable insights for marketers and recommendation systems. This research contributes to advancing the understanding of cohesive subgraphs in network analysis. 

<br /><br />Summary: <div>
arXiv:2507.10262v1 Announce Type: new 
Abstract: Retrieving cohesive subgraphs in networks is a fundamental problem in social network analysis and graph data management. These subgraphs can be used for marketing strategies or recommendation systems. Despite the introduction of numerous models over the years, a systematic comparison of their performance, especially across varied network configurations, remains unexplored. In this study, we evaluated various cohesive subgraph models using task-based evaluations and conducted extensive experimental studies on both synthetic and real-world networks. Thus, we unveil the characteristics of cohesive subgraph models, highlighting their efficiency and applicability. Our findings not only provide a detailed evaluation of current models but also lay the groundwork for future research by shedding light on the balance between the interpretability and cohesion of the subgraphs. This research guides the selection of suitable models for specific analytical needs and applications, providing valuable insights.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowsDT: A Geospatial Digital Twin for Navigating Urban Flood Dynamics</title>
<link>https://arxiv.org/abs/2507.08850</link>
<guid>https://arxiv.org/abs/2507.08850</guid>
<content:encoded><![CDATA[
<div> Keywords: flood analysis, hydrodynamic modeling, digital twin, urban resilience, Galveston City

Summary:
The study focuses on using advanced hydrodynamic modeling and digital twin technology to enhance flood analysis and forecasting in Galveston City. By developing a geospatial digital twin (GDT) and validating it with historical data, the researchers were able to model hyperlocal flood conditions under various rainfall scenarios. The model, FlowsDT-Galveston, accurately simulates flood depth, extent, duration, and velocity in a 4-D environment, identifying at-risk zones in the city. Results show an increase in building and road inundations with higher return period rainfall scenarios. This innovative approach can support proactive flood management and urban planning in Galveston, informing disaster resilience efforts and guiding sustainable infrastructure development. The framework established in this study can be applied to other communities facing similar flood hazard challenges.<br /><br />Summary: <div>
arXiv:2507.08850v1 Announce Type: cross 
Abstract: Communities worldwide increasingly confront flood hazards intensified by climate change, urban expansion, and environmental degradation. Addressing these challenges requires real-time flood analysis, precise flood forecasting, and robust risk communications with stakeholders to implement efficient mitigation strategies. Recent advances in hydrodynamic modeling and digital twins afford new opportunities for high-resolution flood modeling and visualization at the street and basement levels. Focusing on Galveston City, a barrier island in Texas, U.S., this study created a geospatial digital twin (GDT) supported by 1D-2D coupled hydrodynamic models to strengthen urban resilience to pluvial and fluvial flooding. The objectives include: (1) developing a GDT (FlowsDT-Galveston) incorporating topography, hydrography, and infrastructure; (2) validating the twin using historical flood events and social sensing; (3) modeling hyperlocal flood conditions under 2-, 10-, 25-, 50-, and 100-year return period rainfall scenarios; and (4) identifying at-risk zones under different scenarios. This study employs the PCSWMM to create dynamic virtual replicas of urban landscapes and accurate flood modeling. By integrating LiDAR data, land cover, and storm sewer geometries, the model can simulate flood depth, extent, duration, and velocity in a 4-D environment across different historical and design storms. Results show buildings inundated over one foot increased by 5.7% from 2- to 100-year flood. Road inundations above 1 foot increased by 6.7% from 2- to 100-year floods. The proposed model can support proactive flood management and urban planning in Galveston; and inform disaster resilience efforts and guide sustainable infrastructure development. The framework can be extended to other communities facing similar challenges.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions</title>
<link>https://arxiv.org/abs/2507.08881</link>
<guid>https://arxiv.org/abs/2507.08881</guid>
<content:encoded><![CDATA[
<div> consistency, acceptability, large language model, judicial systems, governance framework
Summary: 
The article discusses the integration of large language models (LLM) in judicial systems and highlights the emerging paradox of "consistency-acceptability divergence." This refers to the gap between the technical consistency of LLMs and their social acceptance. The study analyzes data from 2023-2025 and identifies the need to balance technical efficiency with social legitimacy in LLM judicial applications. The proposed Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF) aims to address this challenge by enabling intelligent task classification and fostering meaningful interactions among stakeholders. By understanding both the task and stakeholder dimensions, this framework provides theoretical insights and practical guidance for building a more balanced and effective LLM judicial ecosystem. <br /><br /> <div>
arXiv:2507.08881v1 Announce Type: cross 
Abstract: The integration of large language model (LLM) technology into judicial systems is fundamentally transforming legal practice worldwide. However, this global transformation has revealed an urgent paradox requiring immediate attention. This study introduces the concept of ``consistency-acceptability divergence'' for the first time, referring to the gap between technical consistency and social acceptance. While LLMs achieve high consistency at the technical level, this consistency demonstrates both positive and negative effects. Through comprehensive analysis of recent data on LLM judicial applications from 2023--2025, this study finds that addressing this challenge requires understanding both task and stakeholder dimensions. This study proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF), which enables intelligent task classification and meaningful interaction among diverse stakeholders. This framework offers both theoretical insights and practical guidance for building an LLM judicial ecosystem that balances technical efficiency with social legitimacy.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Te Ahorr\'e Un Click: A Revised Definition of Clickbait and Detection in Spanish News</title>
<link>https://arxiv.org/abs/2507.09777</link>
<guid>https://arxiv.org/abs/2507.09777</guid>
<content:encoded><![CDATA[
<div> clickbait, curiosity gap, sensationalism, headlines, clickbait detection

Summary:
- The definition of clickbait is revised to highlight the creation of a curiosity gap as the distinguishing factor.
- Clickbait is defined as a technique that deliberately omits information to raise curiosity and entice clicks.
- A new dataset, TA1C, for clickbait detection in Spanish is introduced, consisting of 3,500 tweets from 18 media sources.
- The dataset has reached a high inter-annotator agreement of 0.825 Fleiss' K.
- Strong baselines are implemented, achieving a F1-score of 0.84 in clickbait detection.

Summary: <div>
arXiv:2507.09777v1 Announce Type: cross 
Abstract: We revise the definition of clickbait, which lacks current consensus, and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena such as sensationalism and headlines that do not deliver what they promise or diverge from the article. Therefore, we propose a new definition: clickbait is a technique for generating headlines and teasers that deliberately omit part of the information with the goal of raising the readers' curiosity, capturing their attention and enticing them to click. We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible. Following it, we created and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the first open source dataset for clickbait detection in Spanish. It consists of 3,500 tweets coming from 18 well known media sources, manually annotated and reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong baselines that achieve 0.84 in F1-score.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantification of Interdependent Emotion Dynamics in Online Interactions</title>
<link>https://arxiv.org/abs/2408.05700</link>
<guid>https://arxiv.org/abs/2408.05700</guid>
<content:encoded><![CDATA[
<div> Keywords: online interactions, emotions, Hawkes self-exciting point process, peer interactions, emotional manipulation <br />
Summary: <br />
- The study focuses on understanding the dynamics of emotions in online interactions, particularly in YouTube Live chats.
- A multivariate Hawkes self-exciting point process is used to model the expression of six basic emotions, considering both external video content and social feedback.
- Emotional expressions in YouTube Live chats are influenced more by peer interactions than video content, with positivity being more contagious and negativity lingering longer.
- Negative emotions often trigger positive ones in a pattern consistent with trolling dynamics, suggesting the risks of emotional manipulation in online interactions.
- The findings emphasize the significant role of social interaction in shaping emotional dynamics online, particularly as human-chatbot interactions become more realistic. <br /> <br /> <div>
arXiv:2408.05700v3 Announce Type: replace 
Abstract: A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to correlation networks: Interdisciplinary approaches beyond thresholding</title>
<link>https://arxiv.org/abs/2311.09536</link>
<guid>https://arxiv.org/abs/2311.09536</guid>
<content:encoded><![CDATA[
<div> networks, correlation, thresholding, weighted networks, regularization

Summary:
This article addresses the challenge of constructing and analyzing correlation networks in various fields such as psychology, neuroscience, genomics, and finance. It highlights the limitations of the traditional method of thresholding on correlation values and explores alternative approaches such as weighted networks, regularization, dynamic correlation networks, and threshold-free methods. The article also discusses the importance of comparing networks with null models and presents key open questions in the field. Overall, the review emphasizes the need for cross-disciplinary collaboration and recommends best practices for transforming correlational data into meaningful networks. <div>
arXiv:2311.09536v3 Announce Type: replace-cross 
Abstract: Many empirical networks originate from correlational data, arising in domains as diverse as psychology, neuroscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been developed in different application domains for working with such networks, as well as in statistics, network science, and computer science, often with limited communication between practitioners in different fields. This leaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear how to best transform correlation matrix data into networks for the application at hand, and probably the most widespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks, suffers from multiple problems. In this article, we review various methods of constructing and analyzing correlation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic correlation networks, threshold-free approaches, comparison with null models, and more. Finally, we propose and discuss recommended practices and a variety of key open questions currently confronting this field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media</title>
<link>https://arxiv.org/abs/2504.12355</link>
<guid>https://arxiv.org/abs/2504.12355</guid>
<content:encoded><![CDATA[
<div> Keywords: drug overdose, social media, AI-driven NLP framework, multi-class classification, personalized intervention

Summary: 
The study focuses on the use of social media data and AI technology to detect drug misuse and overdose symptoms. By training a framework using a combination of large language models (LLMs) and human annotators, the researchers achieved high accuracy rates in classifying commonly used drugs and associated symptoms. The framework outperformed baseline models, showcasing its potential for real-time public health surveillance. With a 98% accuracy in multi-class classification and 97% in multi-label classification, the AI-driven approach offers new opportunities for personalized intervention strategies in combating drug overdose. This research demonstrates the effectiveness of leveraging AI and social media for timely insights and targeted interventions in addressing the global health challenge of substance misuse. 

<br /><br />Summary: <div>
arXiv:2504.12355v2 Announce Type: replace-cross 
Abstract: Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Propaganda in Tweets From Politically Biased Sources</title>
<link>https://arxiv.org/abs/2507.08169</link>
<guid>https://arxiv.org/abs/2507.08169</guid>
<content:encoded><![CDATA[
arXiv:2507.08169v1 Announce Type: new 
Abstract: News outlets are well known to have political associations, and many national outlets cultivate political biases to cater to different audiences. Journalists working for these news outlets have a big impact on the stories they cover. In this work, we present a methodology to analyze the role of journalists, affiliated with popular news outlets, in propagating their bias using some form of propaganda-like language. We introduce JMBX(Journalist Media Bias on X), a systematically collected and annotated dataset of 1874 tweets from Twitter (now known as X). These tweets are authored by popular journalists from 10 news outlets whose political biases range from extreme left to extreme right. We extract several insights from the data and conclude that journalists who are affiliated with outlets with extreme biases are more likely to use propaganda-like language in their writings compared to those who are affiliated with outlets with mild political leans. We compare eight different Large Language Models (LLM) by OpenAI and Google. We find that LLMs generally performs better when detecting propaganda in social media and news article compared to BERT-based model which is fine-tuned for propaganda detection. While the performance improvements of using large language models (LLMs) are significant, they come at a notable monetary and environmental cost. This study provides an analysis of both the financial costs, based on token usage, and the environmental impact, utilizing tools that estimate carbon emissions associated with LLM operations.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing overlapping communities in multiple-source detection: An edge clustering approach for complex networks</title>
<link>https://arxiv.org/abs/2507.08265</link>
<guid>https://arxiv.org/abs/2507.08265</guid>
<content:encoded><![CDATA[
arXiv:2507.08265v1 Announce Type: new 
Abstract: The source detection problem in network analysis involves identifying the origins of diffusion processes, such as disease outbreaks or misinformation propagation. Traditional methods often focus on single sources, whereas real-world scenarios frequently involve multiple sources, complicating detection efforts. This study addresses the multiple-source detection (MSD) problem by integrating edge clustering algorithms into the community-based label propagation framework, effectively handling mixed-membership issues where nodes belong to multiple communities.
  The proposed approach applies the automated latent space edge clustering model to a network, partitioning infected networks into edge-based clusters to identify multiple sources. Simulation studies on ADD HEALTH social network datasets demonstrate that this method achieves superior accuracy, as measured by the F1-Measure, compared to state-of-the-art clustering algorithms. The results highlight the robustness of edge clustering in accurately detecting sources, particularly in networks with complex and overlapping source regions. This work advances the applicability of clustering-based methods to MSD problems, offering improved accuracy and adaptability for real-world network analyses.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering High-Order Cohesive Structures: Efficient (k,g)-Core Computation and Decomposition for Large Hypergraphs</title>
<link>https://arxiv.org/abs/2507.08328</link>
<guid>https://arxiv.org/abs/2507.08328</guid>
<content:encoded><![CDATA[
arXiv:2507.08328v1 Announce Type: new 
Abstract: Hypergraphs, increasingly utilised to model complex and diverse relationships in modern networks, have gained significant attention for representing intricate higher-order interactions. Among various challenges, cohesive subgraph discovery is one of the fundamental problems and offers deep insights into these structures, yet the task of selecting appropriate parameters is an open question. To address this question, we aim to design an efficient indexing structure to retrieve cohesive subgraphs in an online manner. The main idea is to enable the discovery of corresponding structures within a reasonable time without the need for exhaustive graph traversals. Our method enables faster and more effective retrieval of cohesive structures, which supports decision-making in applications that require online analysis of large-scale hypergraphs. Through extensive experiments on real-world networks, we demonstrate the superiority of our proposed indexing technique.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Evolutionary Graph Theory</title>
<link>https://arxiv.org/abs/2507.08363</link>
<guid>https://arxiv.org/abs/2507.08363</guid>
<content:encoded><![CDATA[
arXiv:2507.08363v1 Announce Type: new 
Abstract: The stability of communities - whether biological, social, economic, technological or ecological depends on the balance between cooperation and cheating. While cooperation strengthens communities, selfish individuals, or "cheaters," exploit collective benefits without contributing. If cheaters become too prevalent, they can trigger the collapse of cooperation and of the community, often in an abrupt manner. A key challenge is determining whether the risk of such a collapse can be detected in advance. To address this, we use a combination of evolutionary graph theory and machine learning to examine how one can predict the unravel of cooperation on complex networks. By introducing few cheaters into a structured population, we employ machine learning to detect and anticipate the spreading of cheaters and cooperation collapse. Using temporal and structural data, the presented results show that prediction accuracy improves with stronger selection strength and larger observation windows, with CNN-Seq-LSTM and Seq-LSTM best performing models. Moreover, the accuracy for the predictions depends crucially on the type of game played between cooperators and cheaters (i.e., accuracy improves when it is more advantageous to defect) and on the community structure. Overall, this work introduces a machine learning approach into detecting abrupt shifts in evolutionary graph theory and offer potential strategies for anticipating and preventing cooperation collapse in complex social networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Feedback Enhances Community-Based Content Moderation through Engagement with Counterarguments</title>
<link>https://arxiv.org/abs/2507.08110</link>
<guid>https://arxiv.org/abs/2507.08110</guid>
<content:encoded><![CDATA[
arXiv:2507.08110v1 Announce Type: cross 
Abstract: Today, social media platforms are significant sources of news and political communication, but their role in spreading misinformation has raised significant concerns. In response, these platforms have implemented various content moderation strategies. One such method, Community Notes on X, relies on crowdsourced fact-checking and has gained traction, though it faces challenges such as partisan bias and delays in verification. This study explores an AI-assisted hybrid moderation framework in which participants receive AI-generated feedback -supportive, neutral, or argumentative -on their notes and are asked to revise them accordingly. The results show that incorporating feedback improves the quality of notes, with the most substantial gains resulting from argumentative feedback. This underscores the value of diverse perspectives and direct engagement in human-AI collective intelligence. The research contributes to ongoing discussions about AI's role in political content moderation, highlighting the potential of generative AI and the importance of informed design.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motifs for Financial Networks: A Study on Mercari, JPMC, and Venmo Platforms</title>
<link>https://arxiv.org/abs/2301.07791</link>
<guid>https://arxiv.org/abs/2301.07791</guid>
<content:encoded><![CDATA[
arXiv:2301.07791v2 Announce Type: replace 
Abstract: Understanding the dynamics of financial transactions among people is critical for various applications such as fraud detection. One important aspect of financial transaction networks is temporality. The order and repetition of transactions can offer new insights when considered within the graph structure. Temporal motifs, defined as a set of nodes that interact with each other in a short time period, are a promising tool in this context. In this work, we study three unique temporal financial networks: transactions in Mercari, an online marketplace, payments in a synthetic network generated by J.P. Morgan Chase, and payments and friendships among Venmo users. We consider the fraud detection problem on the Mercari and J.P. Morgan Chase networks, for which the ground truth is available. We show that temporal motifs offer superior performance to several baselines, including a previous method that considers simple graph features and two node embedding techniques (LINE and node2vec), while being practical in terms of runtime performance. For the Venmo network, we investigate the interplay between financial and social relations on three tasks: friendship prediction, vendor identification, and analysis of temporal cycles. For friendship prediction, temporal motifs yield better results than general heuristics, such as Jaccard and Adamic-Adar measures. We are also able to identify vendors with high accuracy and observe interesting patterns in rare motifs, such as temporal cycles. We believe that the analysis, datasets, and lessons from this work will be beneficial for future research on financial transaction networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signed Diverse Multiplex Networks: Clustering and Inference</title>
<link>https://arxiv.org/abs/2402.10242</link>
<guid>https://arxiv.org/abs/2402.10242</guid>
<content:encoded><![CDATA[
arXiv:2402.10242v3 Announce Type: replace 
Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models, including GRDPG, as its particular cases.
  By employing novel methodologies, our paper ensures strongly consistent clustering of layers and highly accurate subspace estimation, which are significant improvements over the results of Pensky and Wang (2024). All algorithms and theoretical results in the paper remain true for both signed and binary networks. In addition, the paper shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as, for example, analysis of brain networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incivility and Contentiousness Spillover in Public Engagement with Public Health and Climate Science</title>
<link>https://arxiv.org/abs/2502.05255</link>
<guid>https://arxiv.org/abs/2502.05255</guid>
<content:encoded><![CDATA[
arXiv:2502.05255v2 Announce Type: replace 
Abstract: Affective polarization and political sorting drive public antagonism around issues at the science-policy nexus. Looking at the COVID-19 period, we study cross-domain spillover of incivility and contentiousness in public engagements with climate change and public health on Twitter and Reddit. We find strong evidence of the signatures of affective polarization surrounding COVID-19 spilling into the climate change domain. Across different social media systems, COVID-19 content is associated with incivility and contentiousness in climate discussions. These patterns of increased antagonism were responsive to pandemic events that made the link between science and public policy more salient. The observed spillover activated along pre-pandemic political cleavages, specifically anti-internationalist populist beliefs, that linked climate policy opposition to vaccine hesitancy. Our findings show how affective polarization in public engagement with science becomes entrenched across science policy domains.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Probabilistic Propagation in Graphs by Adding Edges</title>
<link>https://arxiv.org/abs/2407.02624</link>
<guid>https://arxiv.org/abs/2407.02624</guid>
<content:encoded><![CDATA[
arXiv:2407.02624v3 Announce Type: replace-cross 
Abstract: Probabilistic graphs are an abstraction that allow us to study randomized propagation in graphs. In a probabilistic graph, each edge is "active" with a certain probability, independent of the other edges. For two vertices $u,v$, a classic quantity of interest, that we refer to as the proximity $\mathcal{P}_{G}(u, v)$, is the probability that there exists a path between $u$ and $v$ all of whose edges are active. For a given subset of vertices $V_s$, the reach of $V_s$ is defined as the minimum over pairs $u \in V_s$ and $v \in V$ of the proximity $\mathcal{P}_{G}(u,v)$. This quantity has been studied in the context of multicast in unreliable communication networks and in social network analysis.
  We study the problem of improving the reach in a probabilistic graph via edge augmentation. Formally, given a budget $k$ of edge additions and a set of source vertices $V_s$, the goal of Reach Improvement is to maximize the reach of $V_s$ by adding at most $k$ new edges to the graph. The problem was introduced in earlier empirical work in the algorithmic fairness community. We provide the first approximation guarantees and hardness results for Reach Improvement.
  We prove that the existence of a good augmentation implies a cluster structure for the graph. We use this structural result to analyze a novel algorithm that outputs a $k$-edge augmentation with an objective value that is poly($\beta^*$), where $\beta^*$ is the objective value for the optimal augmentation. We also give an algorithm that adds $O(k \log n)$ edges and yields a multiplicative approximation to $\beta^*$. Our arguments rely on new probabilistic tools for analyzing proximity, inspired by techniques in percolation theory; these tools may be of broader interest. Finally, we show that significantly better approximations are unlikely, under known hardness assumptions related to gap variants of the classic Set Cover problem.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Signed Exponential Random Graph Models under Local Dependence</title>
<link>https://arxiv.org/abs/2507.07660</link>
<guid>https://arxiv.org/abs/2507.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: network analysis, signed interactions, Stochastic Block Models, Exponential Family Random Graph Models, structural balance theory

Summary: 
Traditional network analysis typically focuses on binary relationships, but real-world interactions can be more complex, involving cooperation, neutrality, and conflict. The emergence of negative interactions in social media has led to increased interest in analyzing signed interactions in polarized debates. However, analyzing large digital networks poses challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM). This new method combines the strengths of SBM and ERGM while addressing their weaknesses, incorporating local dependence based on non-overlapping blocks. The approach involves decomposing the network into sub-networks using SBM and then estimating parameters using ERGM. Validation on synthetic networks and application to a signed Wikipedia network of editors reveal patterns that align with structural balance theory. <div>
arXiv:2507.07660v1 Announce Type: new 
Abstract: Traditional network analysis focuses on binary edges, while real-world relationships are more nuanced, encompassing cooperation, neutrality, and conflict. The rise of negative edges in social media discussions spurred interest in analyzing signed interactions, especially in polarized debates. However, the vast data generated by digital networks presents challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM), particularly due to the homogeneity assumption and global dependence, which become increasingly unrealistic as network size grows. To address this, we propose a novel method that combines the strengths of SBM and ERGM while mitigating their weaknesses by incorporating local dependence based on non-overlapping blocks. Our approach involves a two-step process: first, decomposing the network into sub-networks using SBM approximation, and then estimating parameters using ERGM methods. We validate our method on large synthetic networks and apply it to a signed Wikipedia network of thousands of editors. Through the use of local dependence, we find patterns consistent with structural balance theory.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics</title>
<link>https://arxiv.org/abs/2507.07727</link>
<guid>https://arxiv.org/abs/2507.07727</guid>
<content:encoded><![CDATA[
<div> Framework, higher-order network, transportation systems, memory-dependent dynamics, predictive accuracy

Summary:
The study introduces a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. This framework extends traditional graph representations to capture sequential dependencies in real-world mobility patterns. By incorporating higher-order Markov chains and de Bruijn graph structures, the framework encodes spatial and temporal ordering of paths, improving the analysis of critical components in transportation networks. The study generalizes network analytics such as betweenness centrality and PageRank to this higher-order setting and validates the approach on the Sioux Falls transportation network. Experimental results show that higher-order models outperform first-order baselines for tasks such as next-step prediction. The third-order model strikes a balance between predictive accuracy and model complexity, demonstrating the importance of incorporating memory effects in transportation analysis. This scalable, data-driven methodology offers insights into complex mobility behaviors in infrastructure systems. 

<br /><br /> <div>
arXiv:2507.07727v1 Announce Type: new 
Abstract: Understanding and predicting mobility dynamics in transportation networks is critical for infrastructure planning, resilience analysis, and traffic management. Traditional graph-based models typically assume memoryless movement, limiting their ability to capture sequential dependencies inherent in real-world mobility patterns. In this study, we introduce a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. By extending classical graph representations through higher-order Markov chains and de Bruijn graph structures, our framework encodes the spatial and temporal ordering of traversed paths, enabling the analysis of structurally and functionally critical components with improved fidelity. We generalize key network analytics, including betweenness centrality, PageRank, and next-step prediction, to this higher-order setting and validate our approach on the Sioux Falls transportation network using agent-based trajectory data generated with MATSim. Experimental results demonstrate that higher-order models outperform first-order baselines across multiple tasks, with the third-order model achieving an optimal balance between predictive accuracy and model complexity. These findings highlight the importance of incorporating memory effects into network-based transportation analysis and offer a scalable, data-driven methodology for capturing complex mobility behaviors in infrastructure systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conspiracy to Commit: Information Pollution, Artificial Intelligence, and Real-World Hate Crime</title>
<link>https://arxiv.org/abs/2507.07884</link>
<guid>https://arxiv.org/abs/2507.07884</guid>
<content:encoded><![CDATA[
<div> conspiracy theories, online search trends, hate crimes, 1D-CNN, machine learning

Summary:
This study looks at the relationship between online search trends for racially and politically charged conspiracy theories in Michigan from 2015 to 2019 and hate crime occurrences offline. Using a one-dimensional convolutional neural network (1D-CNN), the researchers found that specific conspiracy theories like the Rothschilds family, Q-Anon, and The Great Replacement were linked to an increase in hate crimes two to three weeks after search spikes. However, most theories did not show a clear connection to offline hate crimes. The findings support neutralization and differential association theories, suggesting that certain conspiracy theories can contribute to real-world violence. Additionally, the study highlights the potential for machine learning to identify harmful online patterns and advance social science research. <div>
arXiv:2507.07884v1 Announce Type: new 
Abstract: Is demand for conspiracy theories online linked to real-world hate crimes? By analyzing online search trends for 36 racially and politically-charged conspiracy theories in Michigan (2015-2019), we employ a one-dimensional convolutional neural network (1D-CNN) to predict hate crime occurrences offline. A subset of theories including the Rothschilds family, Q-Anon, and The Great Replacement improves prediction accuracy, with effects emerging two to three weeks after fluctuations in searches. However, most theories showed no clear connection to offline hate crimes. Aligning with neutralization and differential association theories, our findings provide a partial empirical link between specific racially charged conspiracy theories and real-world violence. Just as well, this study underscores the potential for machine learning to be used in identifying harmful online patterns and advancing social science research.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Adaptive Estimation of Local Triadic Coefficients</title>
<link>https://arxiv.org/abs/2507.07536</link>
<guid>https://arxiv.org/abs/2507.07536</guid>
<content:encoded><![CDATA[
<div> local triadic coefficients, graph properties, graph partitioning, sampling, collaboration networks

Summary: 
The article introduces the concept of local triadic coefficients, such as the local clustering coefficient and local closure coefficient, which are crucial in analyzing networked systems. The focus is on efficiently computing the average local triadic coefficients for a partitioned graph. Due to the infeasibility of exact computation for large networks, the Triad algorithm is developed based on sampling to provide highly accurate probabilistic estimates. Triad utilizes unbiased estimators and offers non-trivial bounds on sample complexity, enabling efficient computation. The algorithm is demonstrated to be effective in capturing high-order patterns in collaboration networks. The study highlights the importance of local triadic coefficients in understanding graph structures and node properties, offering insights for various applications ranging from graph embeddings to network analysis.<br /><br />Summary: <div>
arXiv:2507.07536v1 Announce Type: cross 
Abstract: Characterizing graph properties is fundamental to the analysis and to our understanding of real-world networked systems. The local clustering coefficient, and the more recently introduced, local closure coefficient, capture powerful properties that are essential in a large number of applications, ranging from graph embeddings to graph partitioning. Such coefficients capture the local density of the neighborhood of each node, considering incident triadic structures and paths of length two. For this reason, we refer to these coefficients collectively as local triadic coefficients.
  In this work, we consider the novel problem of computing efficiently the average of local triadic coefficients, over a given partition of the nodes of the input graph into a set of disjoint buckets. The average local triadic coefficients of the nodes in each bucket provide a better insight into the interplay of graph structure and the properties of the nodes associated to each bucket. Unfortunately, exact computation, which requires listing all triangles in a graph, is infeasible for large networks. Hence, we focus on obtaining highly-accurate probabilistic estimates.
  We develop Triad, an adaptive algorithm based on sampling, which can be used to estimate the average local triadic coefficients for a partition of the nodes into buckets. Triad is based on a new class of unbiased estimators, and non-trivial bounds on its sample complexity, enabling the efficient computation of highly accurate estimates. Finally, we show how Triad can be efficiently used in practice on large networks, and we present a case study showing that average local triadic coefficients can capture high-order patterns over collaboration networks.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion of complex contagions is shaped by a trade-off between reach and reinforcement</title>
<link>https://arxiv.org/abs/2411.07907</link>
<guid>https://arxiv.org/abs/2411.07907</guid>
<content:encoded><![CDATA[
<div> social network structure, behavior diffusion, clustered networks, random networks, social reinforcement

Summary:<br /><br />Existing theory suggests that behavior diffusion is influenced by social reinforcement and network structure. A new model was developed to analyze the impact of clustered and random networks on behavior spread. The study found that random networks often outperform clustered networks in spreading behavior, especially when social reinforcement increases adoption. Clustered networks may have an advantage only in specific conditions, such as when adoption is nearly deterministic. Additionally, clustered networks are less advantageous when individuals remain influential after adopting, have more neighbors, or require more neighbors for social reinforcement. The research highlights a tradeoff between random ties, which enhance reach, and clustered ties, which enhance social reinforcement. Ultimately, clustered networks outperform random networks by a small margin in only a minority of scenarios, showcasing the complexity of behavior diffusion on social networks. <div>
arXiv:2411.07907v2 Announce Type: replace 
Abstract: How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more -- both farther and faster -- on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, it should spread more on random networks which avoid such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks spread behavior better than random networks. Using simulations and analytical methods, we identify precise boundaries in the parameter space where one network type outperforms the other or they perform equally. We find that, in most cases, random networks spread behavior as far or farther than clustered networks, even when social reinforcement increases adoption. Although we find that probabilistic, socially reinforced behaviors can spread farther on clustered networks in some cases, this is not the dominant pattern. Clustered networks are even less advantageous when individuals remain influential for longer after adopting, have more neighbors, or need more neighbors before social reinforcement takes effect. Under such conditions, clustering tends to help only when adoption is nearly deterministic, which is not representative of socially reinforced behaviors more generally. Clustered networks outperform random networks by a 5% margin in only 22% of the parameter space under its most favorable conditions. This pattern reflects a fundamental tradeoff: random ties enhance reach, while clustered ties enhance social reinforcement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dialects Collide: How Socioeconomic Mixing Affects Language Use</title>
<link>https://arxiv.org/abs/2307.10016</link>
<guid>https://arxiv.org/abs/2307.10016</guid>
<content:encoded><![CDATA[
<div> Keywords: sociolinguistics, language variation, socioeconomic background, geotagged tweets, agent-based model

Summary: 
This study explores the relationship between language variation and socioeconomic background using geotagged tweets in England and Wales. By analyzing deviations from standard English in different areas and correlating them with income levels, the researchers found that in areas with greater socioeconomic diversity, the relationship between language variation and income becomes less pronounced. This suggests that interactions between different socioeconomic classes may influence how people use language. Additionally, an agent-based model was developed to explain the observed patterns, providing insights into the mechanisms behind linguistic variety adoption. Overall, the study highlights the complex interplay between socioeconomic factors and language use, shedding light on how social dynamics can shape linguistic patterns at a large scale. 

Summary: <div>
arXiv:2307.10016v2 Announce Type: replace-cross 
Abstract: The socioeconomic background of people and how they use standard forms of language are not independent, as demonstrated in various sociolinguistic studies. However, the extent to which these correlations may be influenced by the mixing of people from different socioeconomic classes remains relatively unexplored from a quantitative perspective. In this work we leverage geotagged tweets and transferable computational methods to map deviations from standard English on a large scale, in seven thousand administrative areas of England and Wales. We combine these data with high-resolution income maps to assign a proxy socioeconomic indicator to home-located users. Strikingly, across eight metropolitan areas we find a consistent pattern suggesting that the more different socioeconomic classes mix, the less interdependent the frequency of their departures from standard grammar and their income become. Further, we propose an agent-based model of linguistic variety adoption that sheds light on the mechanisms that produce the observations seen in the data.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motif Participation Profiles for Analyzing Node Similarity in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.06465</link>
<guid>https://arxiv.org/abs/2507.06465</guid>
<content:encoded><![CDATA[
<div> Temporal networks, temporal motifs, TMPPs, node clustering, militarized interstate disputes <br />
Summary: <br />
Temporal networks track interactions between nodes over time. Temporal motifs capture higher-order patterns like directed triangles. TMPPs represent node behavior in these motifs, serving as interpretable node embeddings. Nodes with similar TMPPs have similar roles in motifs. Clustering TMPPs reveals node groups with similar roles. Simulation experiments and a study on militarized interstate disputes demonstrate the effectiveness of TMPPs in uncovering node roles in temporal networks. <div>
arXiv:2507.06465v1 Announce Type: new 
Abstract: Temporal networks consisting of timestamped interactions between a set of nodes provide a useful representation for analyzing complex networked systems that evolve over time. Beyond pairwise interactions between nodes, temporal motifs capture patterns of higher-order interactions such as directed triangles over short time periods. We propose temporal motif participation profiles (TMPPs) to capture the behavior of nodes in temporal motifs. Two nodes with similar TMPPs take similar positions within temporal motifs, possibly with different nodes. TMPPs serve as unsupervised embeddings for nodes in temporal networks that are directly interpretable, as each entry denotes the frequency at which a node participates in a particular position in a specific temporal motif. We demonstrate that clustering TMPPs reveals groups of nodes with similar roles in a temporal network through simulation experiments and a case study on a network of militarized interstate disputes.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Fake Account Detection: A Survey</title>
<link>https://arxiv.org/abs/2507.06541</link>
<guid>https://arxiv.org/abs/2507.06541</guid>
<content:encoded><![CDATA[
<div> Keywords: fake account detection, online social networks, graph-based techniques, topological features, detection time

Summary:
This survey paper discusses the current state of algorithms for detecting fake accounts in online social networks, with a focus on graph-based techniques that use social graph topology. Different methods are categorized based on techniques, input data, and detection time, with strengths and limitations discussed. The paper also explores available datasets, including real-world and synthesized data models. Potential areas for future research are suggested. <div>
arXiv:2507.06541v1 Announce Type: new 
Abstract: In recent years, there has been a growing effort to develop effective and efficient algorithms for fake account detection in online social networks. This survey comprehensively reviews existing methods, with a focus on graph-based techniques that utilise topological features of social graphs (in addition to account information, such as their shared contents and profile data) to distinguish between fake and real accounts. We provide several categorisations of these methods (for example, based on techniques used, input data, and detection time), discuss their strengths and limitations, and explain how these methods connect in the broader context. We also investigate the available datasets, including both real-world data and synthesised models. We conclude the paper by proposing several potential avenues for future research.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic</title>
<link>https://arxiv.org/abs/2507.07036</link>
<guid>https://arxiv.org/abs/2507.07036</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial heterogeneity, sea ice retreat, Antarctic ice shelf melt, graph-based framework, climate adaptation<br />
Summary:<br />
- The study explores the complex linkages between sea ice retreat and Antarctic ice shelf (AIS) melt, focusing on the spatial heterogeneity in dynamic regions like ice shelves and sea ice.<br />
- Traditional models treat sea ice and AIS as separate systems, limiting their ability to capture localized linkages and cascading feedback.<br />
- The proposed Spatial-Link framework is a novel graph-based approach that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt, revealing non-local coupling patterns.<br />
- Results show that sea ice loss can initiate or amplify downstream AIS melt, establishing a direct linkage between sea ice retreat and AIS mass loss.<br />
- Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies. <br /> 

Summary: <div>
arXiv:2507.07036v1 Announce Type: new 
Abstract: Spatial phenomena often exhibit heterogeneity across spatial extents and in proximity, making them complex to model-especially in dynamic regions like ice shelves and sea ice. In this study, we address this challenge by exploring the linkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although atmospheric forcing and basal melting have been widely studied, the direct impact of sea ice retreat on AIS mass loss remains underexplored. Traditional models treat sea ice and AIS as separate systems. It limits their ability to capture localized linkages and cascading feedback. To overcome this, we propose Spatial-Link, a novel graph-based framework that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt. Our method constructs a spatial graph using Delaunay triangulation of satellite-derived ice change matrices, where nodes represent regions of significant change and edges encode proximity and directional consistency. We extract and statistically validate linkage paths using breadth-first search and Monte Carlo simulations. Results reveal non-local, spatially heterogeneous coupling patterns, suggesting sea ice loss can initiate or amplify downstream AIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid and progresses toward ice shelves-establishing a direct linkage. To our knowledge, this is the first proposed methodology linking sea ice retreat to AIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning</title>
<link>https://arxiv.org/abs/2507.06469</link>
<guid>https://arxiv.org/abs/2507.06469</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph representation learning, fraud detection, topology, class imbalance, dual-view graph

Summary: 
Graph representation learning has become a popular method for fraud detection, but the imbalance in global topological information transmission poses a challenge. In this paper, the authors propose a novel method, MimbFD, to address this issue. MimbFD includes a topological message reachability module for improved node representation learning and a local confounding debiasing module to enhance the association between node representations and labels. These components help mitigate the imbalance in supervisory messages caused by fraudsters' topological behavior obfuscation and identity feature concealment. Experimental results on three public fraud datasets demonstrate the effectiveness of MimbFD in fraud detection. <div>
arXiv:2507.06469v1 Announce Type: cross 
Abstract: Graph representation learning has become a mainstream method for fraud detection due to its strong expressive power, which focuses on enhancing node representations through improved neighborhood knowledge capture. However, the focus on local interactions leads to imbalanced transmission of global topological information and increased risk of node-specific information being overwhelmed during aggregation due to the imbalance between fraud and benign nodes. In this paper, we first summarize the impact of topology and class imbalance on downstream tasks in GNN-based fraud detection, as the problem of imbalanced supervisory messages is caused by fraudsters' topological behavior obfuscation and identity feature concealment. Based on statistical validation, we propose a novel dual-view graph representation learning method to mitigate Message imbalance in Fraud Detection(MimbFD). Specifically, we design a topological message reachability module for high-quality node representation learning to penetrate fraudsters' camouflage and alleviate insufficient propagation. Then, we introduce a local confounding debiasing module to adjust node representations, enhancing the stable association between node representations and labels to balance the influence of different classes. Finally, we conducted experiments on three public fraud datasets, and the results demonstrate that MimbFD exhibits outstanding performance in fraud detection.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Designing Social Interventions for Online Climate Change Denialism Discussions</title>
<link>https://arxiv.org/abs/2507.06561</link>
<guid>https://arxiv.org/abs/2507.06561</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, climate change denialism, Reddit, intervention strategies, social media

Summary: 
This study explores intervention strategies for combating conspiracy theory ideology in climate change denialism on Reddit. The researchers used insider language to engage with users in two Reddit communities - climate change deniers and supporters. By crafting evidence-based intervention messages and deploying them through bot accounts, the study found that neutral language interventions sparked positive engagement and open discussions among climate change deniers. Climate change supporters responded actively, contributing additional evidence to the discussions. The study sheds light on the challenges and processes involved in delivering interventions in conspiracy theory communities on social media, offering valuable insights for future research on social media interventions.

<br /><br />Summary: <div>
arXiv:2507.06561v1 Announce Type: cross 
Abstract: As conspiracy theories gain traction, it has become crucial to research effective intervention strategies that can foster evidence and science-based discussions in conspiracy theory communities online. This study presents a novel framework using insider language to contest conspiracy theory ideology in climate change denialism on Reddit. Focusing on discussions in two Reddit communities, our research investigates reactions to pro-social and evidence-based intervention messages for two cohorts of users: climate change deniers and climate change supporters. Specifically, we combine manual and generative AI-based methods to craft intervention messages and deploy the interventions as replies on Reddit posts and comments through transparently labeled bot accounts. On the one hand, we find that evidence-based interventions with neutral language foster positive engagement, encouraging open discussions among believers of climate change denialism. On the other, climate change supporters respond positively, actively participating and presenting additional evidence. Our study contributes valuable insights into the process and challenges of automatically delivering interventions in conspiracy theory communities on social media, and helps inform future research on social media interventions.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE: Data Influence Cascade in Decentralized Learning</title>
<link>https://arxiv.org/abs/2507.06931</link>
<guid>https://arxiv.org/abs/2507.06931</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized learning, Fair incentive mechanism, Data Influence Cascade, Peer-to-peer networks, Tractable approximations

Summary: 
The article introduces a novel method called Data Influence Cascade Estimation (DICE) for decentralized learning in peer-to-peer networks. The focus is on creating fair incentives for participating nodes by accurately attributing contributions. DICE addresses the challenge of influence cascading in decentralized networks by considering data, communication topology, and loss landscape curvature. The framework provides tractable approximations for influence cascades across neighbor hops, facilitating the selection of collaborators and detection of malicious behaviors. The theoretical foundations of DICE suggest that influence cascades are determined by a complex interplay of factors. This innovative approach aims to encourage participation and improve the efficiency of decentralized learning systems.<br /><br />Summary: <div>
arXiv:2507.06931v1 Announce Type: cross 
Abstract: Decentralized learning offers a promising approach to crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing demands. However, proper incentives are still in absence, considerably discouraging participation. Our vision is that a fair incentive mechanism relies on fair attribution of contributions to participating nodes, which faces non-trivial challenges arising from the localized connections making influence ``cascade'' in a decentralized network. To overcome this, we design the first method to estimate \textbf{D}ata \textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized environment. Theoretically, the framework derives tractable approximations of influence cascade over arbitrary neighbor hops, suggesting the influence cascade is determined by an interplay of data, communication topology, and the curvature of loss landscape. DICE also lays the foundations for applications including selecting suitable collaborators and identifying malicious behaviors. Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage</title>
<link>https://arxiv.org/abs/2507.07045</link>
<guid>https://arxiv.org/abs/2507.07045</guid>
<content:encoded><![CDATA[
<div> design, Large Language Models, prompt, framework, efficiency
Summary:
The article discusses the shift from traditional prompt engineering to a more structured prompt design approach in human-Large Language Model (LLM) interactions. It introduces the 5C Prompt Contract framework, comprising Character, Cause, Constraint, Contingency, and Calibration components, aimed at simplifying prompt design for better AI interactions. The framework integrates fallback and output optimization directives, enhancing reliability and interpretability while maintaining creativity. Experimental results show that the 5C framework improves input token efficiency and ensures consistent outputs across various LLM architectures. It is particularly beneficial for individuals and Small-to-Medium Enterprises (SMEs) with limited AI resources. <br /><br />Summary: <div>
arXiv:2507.07045v1 Announce Type: cross 
Abstract: The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the model's creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Public Perceptions of Generative AI in Libraries: A Social Media Analysis of X Discussions</title>
<link>https://arxiv.org/abs/2507.07047</link>
<guid>https://arxiv.org/abs/2507.07047</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, libraries, social network analysis, sentiment analysis, public perception
<br />
Summary: 
<br />
This study examines public perceptions of generative artificial intelligence (GenAI) in libraries by analyzing posts on X (formerly Twitter). The research combines temporal trend analysis, sentiment classification, and social network analysis to understand how discourse around GenAI and libraries has evolved. The analysis shows that discussions are mostly negative, particularly focused on ethical and intellectual property concerns. Social network analysis reveals the importance of institutional authority and individual bridge users in facilitating cross-domain engagement. Overall, the study contributes to the existing literature on GenAI in the library and GLAM sectors, offering a real-time insight into the opportunities and challenges presented by GenAI. <div>
arXiv:2507.07047v1 Announce Type: cross 
Abstract: This study investigates public perceptions of generative artificial intelligence (GenAI) in libraries through a large-scale analysis of posts on X (formerly Twitter). Using a mixed-method approach that combines temporal trend analysis, sentiment classification, and social network analysis, this paper explores how public discourse around GenAI and libraries has evolved over time, the emotional tones that dominate the conversation, and the key users or organizations driving engagement. The findings reveal that discussions are predominantly negative in tone, with surges linked to concerns about ethics and intellectual property. Furthermore, social network analysis identifies both institutional authority and individual bridge users who facilitate cross-domain engagement. The results in this paper contribute to the growing body of literature on GenAI in the library and GLAM (Galleries, Libraries, Archives, and Museums) sectors and offer a real-time, public-facing perspective on the emerging opportunities and concerns GenAI presents.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Ranking for Deliberation in the Public Sphere</title>
<link>https://arxiv.org/abs/2503.18962</link>
<guid>https://arxiv.org/abs/2503.18962</guid>
<content:encoded><![CDATA[
<div> comment sections, online platforms, algorithmic ranking, diverse viewpoints, social choice

Summary: 
Platforms with online comment sections often struggle with toxic exchanges, hindering productive public deliberation. To address this issue, algorithmic ranking is used to promote higher-quality discussions, but this can inadvertently suppress legitimate viewpoints, reducing the representation of diverse perspectives. To combat this, the concept of justified representation (JR) is introduced, drawing from social choice theory. By incorporating a JR constraint into comment ranking systems, platforms can ensure a more inclusive representation of diverse viewpoints while still optimizing for user engagement or conversational quality. This approach strikes a balance between fostering quality discussions and upholding the importance of diverse perspectives in online deliberation. <div>
arXiv:2503.18962v2 Announce Type: replace 
Abstract: Online comment sections, such as those on news sites or social media, have the potential to foster informal public deliberation, However, this potential is often undermined by the frequency of toxic or low-quality exchanges that occur in these settings. To combat this, platforms increasingly leverage algorithmic ranking to facilitate higher-quality discussions, e.g., by using civility classifiers or forms of prosocial ranking. Yet, these interventions may also inadvertently reduce the visibility of legitimate viewpoints, undermining another key aspect of deliberation: representation of diverse views. We seek to remedy this problem by introducing guarantees of representation into these methods. In particular, we adopt the notion of justified representation (JR) from the social choice literature and incorporate a JR constraint into the comment ranking setting. We find that enforcing JR leads to greater inclusion of diverse viewpoints while still being compatible with optimizing for user engagement or other measures of conversational quality.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Displacement and disconnection: the impact of violence on migration networks and highway traffic in Mexico</title>
<link>https://arxiv.org/abs/2301.12743</link>
<guid>https://arxiv.org/abs/2301.12743</guid>
<content:encoded><![CDATA[
<div> migration flows, violence impact, network strength, oil theft, Mexican municipalities
Summary:<br />
- The study examines how violence affects migration flows and reshapes the strength of migration networks by using a novel network algorithm and Mexican census data from 2005 to 2020. 
- Rising violence, driven by increased oil theft activities following government crackdowns on drug trafficking organizations, led to higher emigration flows within Mexico and towards the United States. 
- The study found that violence increased emigration by at least 1.12 million people domestically and reduced the return of 50,200 Mexicans from the US. 
- Additionally, violence eroded regional connectivity, causing a decline in daily vehicle traffic on highways linking violent areas to the rest of the country. 
- The findings highlight the complex dynamics between violence, migration, and network strength in shaping population movements within and outside Mexico. 
<br /><br />Summary: <div>
arXiv:2301.12743v2 Announce Type: replace-cross 
Abstract: We examine how violence affects migration flows and, crucially, how it reshapes the strength of migration networks -- measured by the intensity of migration between areas, accounting for the fact that some routes become more prominent or fade over time -- an aspect traditional studies overlook. Using a novel network algorithm and Mexican census data from 2005 to 2020, we first quantify changes in the strength of domestic and international migration networks across all Mexican municipalities. We exploit variation in local homicide rates, using exogenous fuel price increases and municipalities' proximity to oil pipelines as instruments, to estimate the causal impact of violence on migration. During our study period, following intensified government crackdowns on drug trafficking organizations, many criminal groups fragmented and turned toward large-scale oil theft, driving sharp increases in violence in areas with oil pipelines, particularly when fuel prices rose. The findings show that rising violence increased emigration flows, predominantly within Mexico, and strengthened the intensity of emigration networks both domestically and toward the United States. Although violent municipalities continued to receive new residents, the rise in emigration was larger. Increasing homicide rates led to at least an additional 1.12 million people emigrating domestically and 50,200 fewer Mexicans returning from the United States. Violence also eroded regional connectivity, causing a long-term decline in daily vehicle traffic on highways linking violent areas to the rest of the country.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The most influential philosophers in Wikipedia: a multicultural analysis</title>
<link>https://arxiv.org/abs/2507.06034</link>
<guid>https://arxiv.org/abs/2507.06034</guid>
<content:encoded><![CDATA[
<div> PageRank, CheiRank, philosophers, Wikipedia knowledge network, presocratic philosophers  
Summary:  
- The study examines the influence and interconnectivity of philosophers in the Wikipedia knowledge network across different language editions.  
- 237 philosopher articles in nine languages were analyzed using PageRank and CheiRank algorithms to determine their relative ranking and influence.  
- A comparison with entries from the Stanford and Internet Encyclopedia of Philosophy was conducted to highlight differences between general and specialized knowledge networks.  
- The study focuses on a sub-network of 21 presocratic philosophers grouped into traditional schools and uses the reduced Google matrix method to reveal both direct and hidden links between them.  
- The analysis provides new insights into the intellectual relationships and influence of early philosophers within the Western philosophical tradition.  

<br /><br />Summary: <div>
arXiv:2507.06034v1 Announce Type: new 
Abstract: We explore the influence and interconnectivity of philosophical thinkers within the Wikipedia knowledge network. Using a dataset of 237 articles dedicated to philosophers across nine different language editions (Arabic, Chinese, English, French, German, Japanese, Portuguese, Russian, and Spanish), we apply the PageRank and CheiRank algorithms to analyze their relative ranking and influence in each linguistic context. Furthermore, we compare our results with entries from the Stanford Encyclopedia of Philosophy and the Internet Encyclopedia of Philosophy, providing insight into the differences between general knowledge networks like Wikipedia and specialized philosophical databases. A key focus of our analysis is the sub-network of 21 presocratic philosophers, grouped into four traditional schools: Italic (Pythagorean + Eleatic), Ionian, Abderian (Atomist), and Sophist. Using the reduced Google matrix method, we uncover both direct and hidden links between these early thinkers, offering new perspectives on their intellectual relationships and influence within the Western philosophical tradition.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuHE: Optimizing Utility-Cost in Quantum Key Distribution and Homomorphic Encryption Enabled Secure Edge Computing Networks</title>
<link>https://arxiv.org/abs/2507.06086</link>
<guid>https://arxiv.org/abs/2507.06086</guid>
<content:encoded><![CDATA[
<div> Quantum Key Distribution, Homomorphic Encryption, Mobile Edge Computing, Optimization, Resource Allocation<br />
<br />
Summary: 
This paper introduces a novel framework for secure and efficient data processing in mobile edge computing (MEC) systems by integrating Quantum Key Distribution (QKD), transciphering, and Homomorphic Encryption (HE). The framework addresses the trade-offs among QKD utility, HE security, and system costs. An optimization problem is formulated to balance these factors, but it is non-convex and NP-hard. To efficiently solve it, the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm is proposed. The algorithm is proven to be convergent and optimal through theoretical analysis and is shown to be effective in simulations across various performance metrics. <div>
arXiv:2507.06086v1 Announce Type: new 
Abstract: Ensuring secure and efficient data processing in mobile edge computing (MEC) systems is a critical challenge. While quantum key distribution (QKD) offers unconditionally secure key exchange and homomorphic encryption (HE) enables privacy-preserving data processing, existing research fails to address the comprehensive trade-offs among QKD utility, HE security, and system costs. This paper proposes a novel framework integrating QKD, transciphering, and HE for secure and efficient MEC. QKD distributes symmetric keys, transciphering bridges symmetric encryption, and HE processes encrypted data at the server. We formulate an optimization problem balancing QKD utility, HE security, processing and wireless transmission costs. However, the formulated optimization is non-convex and NPhard. To solve it efficiently, we propose the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm. Theoretical analysis proves the proposed QuHE algorithm's convergence and optimality, and simulations demonstrate its effectiveness across multiple performance metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Nodes Identification in Complex Networks: A Survey</title>
<link>https://arxiv.org/abs/2507.06164</link>
<guid>https://arxiv.org/abs/2507.06164</guid>
<content:encoded><![CDATA[
<div> Complex networks, critical node identification, centrality, dynamic networks, higher-order networks <br />
Summary: <br />
This paper offers a comprehensive review of critical node identification techniques in complex networks, categorizing them into seven main classes. It addresses the challenges posed by the complexity and heterogeneity of real-world networks, particularly in dynamic and higher-order structures. The review highlights various methods, such as centrality, critical node deletion problem, influence maximization, network control, artificial intelligence, and higher-order and dynamic approaches. It emphasizes the strengths, limitations, and applicability of these methods across different network types. Key challenges identified include algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and underscores open questions in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning, and developing scalable and interpretable metrics for complex systems. <div>
arXiv:2507.06164v1 Announce Type: new 
Abstract: Complex networks have become essential tools for understanding diverse phenomena in social systems, traffic systems, biomolecular systems, and financial systems. Identifying critical nodes is a central theme in contemporary research, serving as a vital bridge between theoretical foundations and practical applications. Nevertheless, the intrinsic complexity and structural heterogeneity characterizing real-world networks, with particular emphasis on dynamic and higher-order networks, present substantial obstacles to the development of universal frameworks for critical node identification. This paper provides a comprehensive review of critical node identification techniques, categorizing them into seven main classes: centrality, critical nodes deletion problem, influence maximization, network control, artificial intelligence, higher-order and dynamic methods. Our review bridges the gaps in existing surveys by systematically classifying methods based on their methodological foundations and practical implications, and by highlighting their strengths, limitations, and applicability across different network types. Our work enhances the understanding of critical node research by identifying key challenges, such as algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and highlights open questions, particularly in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning approaches, and developing scalable and interpretable metrics for complex systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Introvert</title>
<link>https://arxiv.org/abs/2507.05638</link>
<guid>https://arxiv.org/abs/2507.05638</guid>
<content:encoded><![CDATA[
<div> transformed information dissemination, social media, generative AI, misinformation, information propagation dynamics <br />
<br />Summary: The article discusses the impact of social media and generative AI on information dissemination and the spread of misinformation. Traditional models like SIR are insufficient in capturing the complexities of online interactions. Advanced methods like attention mechanisms and graph neural networks overlook user psychology and behavioral dynamics. Large language models (LLMs) offer potential for simulating psychological aspects of information spread but have limitations in capturing authentic human dynamics. The article introduces the SIP-CoT mechanism enhanced by emotion-guided memory to address these limitations. Experimental results confirm that SIP-CoT-enhanced LLM agents process social information more effectively, demonstrating behaviors closer to real human interactions. Overall, the research identifies critical limitations in current LLM-based propagation simulations and shows how integrating SIP-CoT and emotional memory enhances the social intelligence and realism of LLM agents. <br /> <div>
arXiv:2507.05638v1 Announce Type: cross 
Abstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity isn't everything -- how far do demographics take us towards self-identified party ID?</title>
<link>https://arxiv.org/abs/2507.06193</link>
<guid>https://arxiv.org/abs/2507.06193</guid>
<content:encoded><![CDATA[
<div> party identification, demographics, identity strength, predictive framework, Black Democrats

Summary: 
Demographics alone may not fully explain party identification, as individuals may choose to construct a political identity independent of their demographic group. The study examines the role of identity strength alongside demographics in predicting party identification. While demographics are highly predictive for some groups, such as Black Democrats, others, like Hispanic Republicans, benefit from considering identity strength as well. This suggests that individuals may prioritize certain group affiliations in shaping their political identity, and that a deeper understanding of identity strength can enhance the accuracy of predicting party identification. <div>
arXiv:2507.06193v1 Announce Type: cross 
Abstract: How well do demographics explain party identification? Demographics are related to party identification in political polls, news articles, and academic publications. Yet, there is a diversity of party identification even within demographic groups which have historically been attached to one party. And some groups lack a clear connection to either party. It may be that demographics on their own fail to account for the fact that people generally belong to a variety of groups. They must select the groups which are most important to them when shaping a political identity, and may choose to construct an identity relatively unattached to any specific demographic group to which they belong. This prompts the question, do we need to consider measures of identity strength when using demographics to explain party identification? We utilize a predictive framework to address these questions and find that demographics are highly predictive for some groups (e.g., Black Democrats), while others benefit from the inclusion of identity strength (e.g., Hispanic Republicans).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.12149</link>
<guid>https://arxiv.org/abs/2503.12149</guid>
<content:encoded><![CDATA[
<div> Keywords: LVLMs, multimodal sarcasm, interpretive variations, subjectivity, uncertainty-aware modeling

Summary:
LVLMs, or large vision-language models, are increasingly being used to understand multimodal sarcasm. This study evaluates 12 LVLMs using a systematic framework on existing sarcasm datasets, analyzing interpretive variations and subjective perspectives. The results show discrepancies in how different models interpret sarcasm, especially when given varied prompts. While classification prompts show higher internal consistency, models diverge in interpretive reasoning. These findings challenge traditional labeling paradigms by highlighting sarcasm's subjectivity. The study suggests moving towards uncertainty-aware modeling to better understand multimodal sarcasm comprehension. This research provides valuable insights for developing more nuanced and deeper comprehension of sarcasm in LVLMs.<br /><br />Summary: <div>
arXiv:2503.12149v2 Announce Type: replace-cross 
Abstract: With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous "neutral" cases. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic</title>
<link>https://arxiv.org/abs/2507.03448</link>
<guid>https://arxiv.org/abs/2507.03448</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven, mean-field approach, popularity dynamics, influencers, social networks

Summary: 
This paper introduces a data-driven mean-field model to study the dynamics of influencer popularity on social media platforms. The model incorporates individual activity patterns, content virality, external events, and platform visibility to predict the success of influencers. By deriving conditions for system ergodicity, the model can forecast popularity distributions among influencers. A sensitivity analysis examines different system setups to identify factors that can lead to either dominance or fair play in the influencer ecosystem. The findings provide insights into the potential evolution of social networks towards more equitable or biased influence dynamics. <div>
arXiv:2507.03448v1 Announce Type: new 
Abstract: This paper presents a data-driven mean-field approach to model the popularity dynamics of users seeking public attention, i.e., influencers. We propose a novel analytical model that integrates individual activity patterns, expertise in producing viral content, exogenous events, and the platform's role in visibility enhancement, ultimately determining each influencer's success. We analytically derive sufficient conditions for system ergodicity, enabling predictions of popularity distributions. A sensitivity analysis explores various system configurations, highlighting conditions favoring either dominance or fair play among influencers. Our findings offer valuable insights into the potential evolution of social networks towards more equitable or biased influence ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Active Global Vaccine Infodemic Risk Assessment</title>
<link>https://arxiv.org/abs/2507.04222</link>
<guid>https://arxiv.org/abs/2507.04222</guid>
<content:encoded><![CDATA[
<div> AI, social listening, vaccine infodemics, misinformation, public health

Summary: 
The paper presents VaxPulse VIRAL, an AI-powered social listening platform to monitor and assess vaccine-related infodemic risks. It utilizes machine learning methods like deep learning and active learning to analyze public sentiments, misinformation trends, and social bot activity in real-time. The platform's dynamic dashboards offer tailored insights for immunization programs and combatting misinformation. Iterative feedback from experts and stakeholders guides continuous improvements. Collaboration with an international network and community leaders ensures ongoing enhancements to VaxPulse. This innovative approach aims to address the challenges posed by vaccine infodemics and support global public health initiatives. <br /><br /> <div>
arXiv:2507.04222v1 Announce Type: new 
Abstract: Vaccine infodemics, driven by misinformation, disinformation, and inauthentic online behaviours, pose significant threats to global public health. This paper presents our response to this challenge, demonstrating how we developed VaxPulse Vaccine Infodemic Risk Assessment Lifecycle (VIRAL), an AI-powered social listening platform designed to monitor and assess vaccine-related infodemic risks. Leveraging interdisciplinary expertise and international collaborations, VaxPulse VIRAL integrates machine learning methods, including deep learning, active learning, and data augmentation, to provide real-time insights into public sentiments, misinformation trends, and social bot activity. Iterative feedback from domain experts and stakeholders has guided the development of dynamic dashboards that offer tailored, actionable insights to support immunisation programs and address information disorder. Ongoing improvements to VaxPulse will continue through collaboration with our international network and community leaders.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating User Watch-Time to Investigate Bias in YouTube Shorts Recommendations</title>
<link>https://arxiv.org/abs/2507.04534</link>
<guid>https://arxiv.org/abs/2507.04534</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, engagement-driven algorithms, content exposure, viewer behaviors, relevance shift

Summary: 
This study explores the impact of viewer behaviors, such as fast scrolling or skipping, on the relevance and topical continuity of recommended videos on short-form video platforms like YouTube Shorts. Analyzing a dataset of over 404,000 videos on geopolitical themes and conflicts, including Russia, China, the Russia-Ukraine War, and the South China Sea dispute, the research simulates viewer interactions and assesses how relevance changes across recommendation chains under different watch-time conditions. Using GPT-4o to measure semantic alignment between videos, the study uncovers patterns of amplification, drift, and topic generalization that have significant implications for content diversity and platform accountability. By combining insights from computer science, media studies, and political communication, this interdisciplinary work enhances our understanding of how engagement cues shape algorithmic pathways in short-form content ecosystems. 

Summary: <div>
arXiv:2507.04534v1 Announce Type: new 
Abstract: Short-form video platforms such as YouTube Shorts increasingly shape how information is consumed, yet the effects of engagement-driven algorithms on content exposure remain poorly understood. This study investigates how different viewing behaviors, including fast scrolling or skipping, influence the relevance and topical continuity of recommended videos. Using a dataset of over 404,000 videos, we simulate viewer interactions across both broader geopolitical themes and more narrowly focused conflicts, including topics related to Russia, China, the Russia-Ukraine War, and the South China Sea dispute. We assess how relevance shifts across recommendation chains under varying watch-time conditions, using GPT-4o to evaluate semantic alignment between videos. Our analysis reveals patterns of amplification, drift, and topic generalization, with significant implications for content diversity and platform accountability. By bridging perspectives from computer science, media studies, and political communication, this work contributes a multidisciplinary understanding of how engagement cues influence algorithmic pathways in short-form content ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Algorithmic Bias in YouTube Shorts</title>
<link>https://arxiv.org/abs/2507.04605</link>
<guid>https://arxiv.org/abs/2507.04605</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, algorithmic bias, content visibility, drift, generative AI models<br />
<br />
Summary: 
The study explores algorithmic bias in YouTube Shorts' recommendation system, focusing on watch-time duration, topic sensitivity, and engagement metrics. Analyzing over 685,000 videos across different content domains, the research finds a drift away from politically sensitive content towards entertainment-focused videos. Emotion analysis reveals a preference for joyful or neutral content, while highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. These findings highlight how algorithm design influences content exposure on YouTube Shorts, impacting information diversity and transparency on the platform. <div>
arXiv:2507.04605v1 Announce Type: new 
Abstract: The rapid growth of YouTube Shorts, now serving over 2 billion monthly users, reflects a global shift toward short-form video as a dominant mode of online content consumption. This study investigates algorithmic bias in YouTube Shorts' recommendation system by analyzing how watch-time duration, topic sensitivity, and engagement metrics influence content visibility and drift. We focus on three content domains: the South China Sea dispute, the 2024 Taiwan presidential election, and general YouTube Shorts content. Using generative AI models, we classified 685,842 videos across relevance, topic category, and emotional tone. Our results reveal a consistent drift away from politically sensitive content toward entertainment-focused videos. Emotion analysis shows a systematic preference for joyful or neutral content, while engagement patterns indicate that highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. This work provides the first comprehensive analysis of algorithmic drift in YouTube Shorts based on textual content, emotional tone, topic categorization, and varying watch-time conditions. These findings offer new insights into how algorithmic design shapes content exposure, with implications for platform transparency and information diversity.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Monitoring of Online Public Concerns to Enhance Post-licensure Vaccine Surveillance</title>
<link>https://arxiv.org/abs/2507.04656</link>
<guid>https://arxiv.org/abs/2507.04656</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine safety, misinformation management, sentiment analysis, vaccine hesitancy, public opinion

Summary: 
The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media.<br /><br />Summary: The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media. <div>
arXiv:2507.04656v1 Announce Type: new 
Abstract: The recent vaccine-related infodemic has amplified public concerns, highlighting the need for proactive misinformation management. We describe how we enhanced the reporting surveillance system of Victoria's vaccine safety service, SAEFVIC, through the incorporation of new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online. Using VaxPulse, a multi-step framework, we integrate adverse events following immunisation (AEFI) with sentiment analysis, demonstrating the importance of contextualising public concerns. Additionally, we emphasise the need to address non-English languages to stratify concerns across ethno-lingual communities, providing valuable insights for vaccine uptake strategies and combating mis/disinformation. The framework is applied to real-world examples and a case study on women's vaccine hesitancy, showcasing its benefits and adaptability by identifying public opinion from online media.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancement of Circular Economy Through Interdisciplinary Collaboration: A Bibliometric Approach</title>
<link>https://arxiv.org/abs/2507.04923</link>
<guid>https://arxiv.org/abs/2507.04923</guid>
<content:encoded><![CDATA[
<div> Circular Economy, Research, Interdisciplinary Collaboration, Business and Management, Engineering<br />
<br />
Summary: 
The study analyzes over 25,000 Circular Economy (CE) publications to understand the interdisciplinary nature and researcher dynamics of the field. It identifies 16 research clusters and visualizes collaboration patterns among researchers. Business and management research attract significant attention, while engineering research tends to secure higher funding success. Collaborative CE papers from different disciplines demonstrate higher research impact compared to intradisciplinary work, emphasizing the value of interdisciplinary efforts. Case studies highlight the benefits of collaborations between business-oriented and engineering-oriented disciplines. The findings suggest a positive dynamic where attention drawn by business research contributes to securing economic resources for realizing CE goals. The study provides insights for guiding future cross-disciplinary engagement in the CE field. <br /><br /> <div>
arXiv:2507.04923v1 Announce Type: new 
Abstract: Since the European Union introduced its Circular Economy (CE) Action Plan in 2015, CE research has expanded rapidly. However, the structure of this emerging field - both in terms of its constituent disciplines and researcher dynamics - remains poorly understood. To address this gap, we analyze over 25,000 CE-related publications from Scopus by combining conventional bibliometric approaches with advanced machine learning techniques, including text embeddings and clustering. This hybrid method enables both a macro-level mapping of research domains and a micro-level investigation of individual researchers' disciplinary backgrounds and collaborations.
  We classify CE research into 16 distinct clusters, identifying the original disciplines of researchers and visualizing patterns of interdisciplinary collaboration. Building on this foundation, we ask: Which CE-related research domains receive the most attention in academic and policy contexts? And how are different types of interdisciplinary collaboration associated with research impact?
  Our findings show that research in business and management attracts substantial academic and policy attention, while engineering research - though less visible - tends to achieve higher funding success. This suggests a positive dynamic in which the former draws attention to CE issues and the latter secures the economic resources necessary to realize them.
  We further demonstrate that CE papers co-authored by researchers from different disciplines tend to show higher research impact than intradisciplinary work. Qualitative case analyses also highlight this tendency. Centered particularly on collaborations between business-oriented and engineering-oriented disciplines, our findings underscore the importance of interdisciplinary efforts in CE research and offer insights for guiding future cross-disciplinary engagement in the field.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban Behavior Explanations</title>
<link>https://arxiv.org/abs/2507.04995</link>
<guid>https://arxiv.org/abs/2507.04995</guid>
<content:encoded><![CDATA[
<div> Interest Networks, LBSNs, spatial granularities, recommendation system, explainable AI<br />
Summary: Location-Based Social Networks (LBSNs) provide valuable insights into urban behavior through Interest Networks (iNETs), with this study comparing iNETs on Google Places and Foursquare at different spatial levels. It reveals that user interests are mainly influenced by proximity and venue similarity, with socioeconomic and political factors playing a smaller role. The research develops a multi-level recommendation system that caters to different user behaviors, leveraging explainable AI techniques to offer personalized urban recommendations with natural-language explanations. The study introduces h3-cities for spatial analysis and releases a public demo for interactive exploration. This approach contributes to urban mobility research by delivering scalable, context-aware, and interpretable recommendation systems.<br /><br />Summary: <div>
arXiv:2507.04995v1 Announce Type: new 
Abstract: Location-Based Social Networks (LBSNs) provide a rich foundation for modeling urban behavior through iNETs (Interest Networks), which capture how user interests are distributed throughout urban spaces. This study compares iNETs across platforms (Google Places and Foursquare) and spatial granularities, showing that coarser levels reveal more consistent cross-platform patterns, while finer granularities expose subtle, platform-specific behaviors. Our analysis finds that, in general, user interest is primarily shaped by geographic proximity and venue similarity, while socioeconomic and political contexts play a lesser role. Building on these insights, we develop a multi-level, explainable recommendation system that predicts high-interest urban regions for different user types. The model adapts to behavior profiles -- such as explorers, who are driven by proximity, and returners, who prefer familiar venues -- and provides natural-language explanations using explainable AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool for multi-scale spatial analysis, and release a public demo for interactively exploring personalized urban recommendations. Our findings contribute to urban mobility research by providing scalable, context-aware, and interpretable recommendation systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Location Disclosure Fails to Deter Overseas Criticism but Amplifies Regional Divisions on Chinese Social Media</title>
<link>https://arxiv.org/abs/2507.03238</link>
<guid>https://arxiv.org/abs/2507.03238</guid>
<content:encoded><![CDATA[
<div> location disclosure policy, Sina Weibo, user behavior, censorship, online participation <br />
Summary: The study analyzes the impact of a user location disclosure policy on Sina Weibo, China's largest microblogging platform. It finds that the policy, implemented to deter overseas users from spreading harmful information, did not reduce their engagement. Instead, it significantly decreased domestic users' willingness to comment on local issues outside their provinces, particularly affecting out-of-province commenters and criticisms. The policy led to a rise in regionally discriminatory replies and reshaped online participation norms. The findings suggest that authoritarian regimes can use social cleavages, such as regional divisions, to reinforce censorship, suppress dissent, and fragment public discourse. <div>
arXiv:2507.03238v1 Announce Type: cross 
Abstract: We examine the behavioral impact of a user location disclosure policy implemented on Sina Weibo, China's largest microblogging platform, using a high-frequency, real-time dataset of uncensored user engagement with 165 leading government and media accounts. Leveraging a natural experiment result from the platform's sudden rollout of location tagging on April 28, 2022, we compare millions of time-stamped observations of user behavior in the comment sections of these accounts before and after the policy change. Although the policy appeared intended to deter overseas users from spreading information deemed harmful by the regime, we find no reduction in their engagement. Instead, the policy sharply reduced domestic users' willingness to comment on posts about local issues outside their own provinces. This effect was especially pronounced among out-of-province commenters and disproportionately curtailed criticisms. Using large language models, we further show that location disclosure triggered a rise in regionally discriminatory replies, which in turn heightened the perceived risk of cross-provincial engagement and reshaped the norms of online participation. Our findings suggest that authoritarian regimes can reinforce censorship not only through top-down control, but by mobilizing social cleavages, here, regional divisions, to suppress dissent and fragment public discourse.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products</title>
<link>https://arxiv.org/abs/2507.04364</link>
<guid>https://arxiv.org/abs/2507.04364</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine learning, health sentiments, Large Language Models, sentiment classification, public health

Summary: 
Machine learning methods are being used to analyze health-related public discourse, but their accuracy in detecting different health sentiments is not well understood. This research focused on three Large Language Models (LLMs)  GPT, Gemini, and LLAMA  to classify risk-promoting versus health-supporting sentiments on HPV vaccination and heated tobacco products. Results showed that all three LLMs were generally accurate in classifying sentiments, with some variations based on platform, health issue, and model type. Higher accuracy was observed for risk-promoting sentiment on Facebook, while health-supporting messages were better detected on Twitter. However, LLMs faced challenges in accurately detecting neutral messages. This study emphasizes the importance of validating language models for public health analysis and being aware of potential biases in training data influencing the results. 

<br /><br />Summary: <div>
arXiv:2507.04364v1 Announce Type: cross 
Abstract: Machine learning methods are increasingly applied to analyze health-related public discourse based on large-scale data, but questions remain regarding their ability to accurately detect different types of health sentiments. Especially, Large Language Models (LLMs) have gained attention as a powerful technology, yet their accuracy and feasibility in capturing different opinions and perspectives on health issues are largely unexplored. Thus, this research examines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in detecting risk-promoting versus health-supporting sentiments across two critical public health topics: Human Papillomavirus (HPV) vaccination and heated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we curated multiple sets of messages supporting or opposing recommended health behaviors, supplemented with human annotations as the gold standard for sentiment classification. The findings indicate that all three LLMs generally demonstrate substantial accuracy in classifying risk-promoting and health-supporting sentiments, although notable discrepancies emerge by platform, health issue, and model type. Specifically, models often show higher accuracy for risk-promoting sentiment on Facebook, whereas health-supporting messages on Twitter are more accurately detected. An additional analysis also shows the challenges LLMs face in reliably detecting neutral messages. These results highlight the importance of carefully selecting and validating language models for public health analyses, particularly given potential biases in training data that may lead LLMs to overestimate or underestimate the prevalence of certain perspectives.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Social Media Network Effects</title>
<link>https://arxiv.org/abs/2507.04545</link>
<guid>https://arxiv.org/abs/2507.04545</guid>
<content:encoded><![CDATA[
<div> local network effects, social media platforms, consumer surplus, incentive-compatible, online choice experiments

Summary:
Local network effects play a significant role in the value generated by social media platforms, with 20-34% of the total value derived from these effects. Different platforms show varying levels of value, with stronger ties being more valuable on Facebook and Instagram, while weaker ties are preferred on LinkedIn and X. Gender and race also play a role in how connections are valued, with men valuing connections to women more on specific platforms. Additionally, consumer behavior differs based on whether they are looking for work or not, with LinkedIn being more valuable for job seekers. Overall, social media platforms generate between $53B and $215B in consumer surplus per year in the US, highlighting the significant impact of local network effects on platform value. <div>
arXiv:2507.04545v1 Announce Type: cross 
Abstract: We use representative, incentive-compatible online choice experiments involving 19,923 Facebook, Instagram, LinkedIn, and X users in the US to provide the first large-scale, empirical measurement of local network effects in the digital economy. Our analysis reveals social media platform value ranges from $78 to $101 per consumer, per month, on average, and that 20-34% of that value is explained by local network effects. We also find 1) stronger ties are more valuable on Facebook and Instagram, while weaker ties are more valuable on LinkedIn and X; 2) connections known through work are most valuable on LinkedIn and least valuable on Facebook, and people looking for work value LinkedIn significantly more and Facebook significantly less than people not looking for work; 3) men value connections to women on social media significantly more than they value connections to other men, particularly on Instagram, Facebook and X, while women value connections to men and women equally; 4) white consumers value relationships with other white consumers significantly more than they value relationships with non-white consumers on Facebook while, on Instagram, connections to alters eighteen years old or younger are valued significantly more than any other age group-two patterns not seen on any other platforms. Social media platforms individually generate between $53B and $215B in consumer surplus per year in the US alone. These results suggest social media generates significant value, local network effects drive a substantial fraction of that value and that these effects vary across platforms, consumers, and connections.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, Social Network Data, Predictive Modeling, External Factors, Proactive Security Planning 

Summary: 
The article introduces a novel approach to understanding the Perception of Security (PoS) using social network data. By analyzing social network content, the model aims to offer real-time monitoring and predictive insights into security perceptions. The model incorporates external factors that influence the publication and reposting of security-related content, achieving competitive predictive performance while maintaining interpretability. The research highlights the importance of temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. Overall, the innovative approach presented in the article contributes to improving the measurement and understanding of security perceptions in short time frames, enhancing the ability to anticipate and address security concerns effectively. 

<br /><br />Summary: <div>
arXiv:2402.03378v2 Announce Type: replace 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Social Sphere Model: Heuristic Influence Prediction in Evolving Networks</title>
<link>https://arxiv.org/abs/2402.03522</link>
<guid>https://arxiv.org/abs/2402.03522</guid>
<content:encoded><![CDATA[
<div> link prediction, influencers, influence maximization, deep learning, social network analysis

Summary:
The study explores admissions in a university program for influencers, focusing on influence maximization and link prediction in social network analysis. It introduces The Social Sphere Model, an algorithm that combines path-based link prediction metrics and heuristic influence maximization strategies to identify future key nodes in weighted networks. Testing on contagion models shows promising results with lower computational requirements. This advancement enhances understanding of network dynamics and offers a more efficient approach to network management and influence strategy development. <div>
arXiv:2402.03522v3 Announce Type: replace 
Abstract: How would admissions look like in a university program for influencers? In the realm of social network analysis, influence maximization and link prediction stand out as pivotal challenges. Influence maximization focuses on identifying a set of key nodes to maximize information dissemination, while link prediction aims to foresee potential connections within the network. These strategies, primarily deep learning link prediction methods and greedy algorithms, have been previously used in tandem to identify future influencers. However, given the complexity of these tasks, especially in large-scale networks, we propose an algorithm, The Social Sphere Model, which uniquely utilizes expected value in its future graph prediction and combines specifically path-based link prediction metrics and heuristic influence maximization strategies to effectively identify future vital nodes in weighted networks. Our approach is tested on two distinct contagion models, offering a promising solution with lower computational demands. This advancement not only enhances our understanding of network dynamics but also opens new avenues for efficient network management and influence strategy development.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning</title>
<link>https://arxiv.org/abs/2410.10875</link>
<guid>https://arxiv.org/abs/2410.10875</guid>
<content:encoded><![CDATA[
<div> effective resistances, flow-based community detection, spectral framework, hypergraph partitioning, VLSI designs

Summary: 
The paper introduces SHyPar, a multilevel spectral framework for partitioning hypergraphs that considers structural features. It leverages hyperedge effective resistances and flow-based community detection techniques to guide the partitioning process. SHyPar aims to decompose hypergraphs into subgraphs with minimal inter-partition hyperedges. A key component is a flow-based local clustering scheme for hypergraph coarsening, which improves conductance. The framework also uses an effective resistance-based rating function for merging strongly connected nodes. Experimental results on VLSI designs show that SHyPar outperforms existing methods in terms of solution quality. <div>
arXiv:2410.10875v3 Announce Type: replace 
Abstract: State-of-the-art hypergraph partitioners utilize a multilevel paradigm to construct progressively coarser hypergraphs across multiple layers, guiding cut refinements at each level of the hierarchy. Traditionally, these partitioners employ heuristic methods for coarsening and do not consider the structural features of hypergraphs. In this work, we introduce a multilevel spectral framework, SHyPar, for partitioning large-scale hypergraphs by leveraging hyperedge effective resistances and flow-based community detection techniques. Inspired by the latest theoretical spectral clustering frameworks, such as HyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiple subgraphs with few inter-partition hyperedges (cut size). A key component of SHyPar is a flow-based local clustering scheme for hypergraph coarsening, which incorporates a max-flow-based algorithm to produce clusters with substantially improved conductance. Additionally, SHyPar utilizes an effective resistance-based rating function for merging nodes that are strongly connected (coupled). Compared with existing state-of-the-art hypergraph partitioning methods, our extensive experimental results on real-world VLSI designs demonstrate that SHyPar can more effectively partition hypergraphs, achieving state-of-the-art solution quality.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review</title>
<link>https://arxiv.org/abs/2409.18162</link>
<guid>https://arxiv.org/abs/2409.18162</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, augmented reality, user interface, autism spectrum disorder, therapy<br />
<br />
Summary: 
This review study delves into the integration of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children with autism spectrum disorder (ASD). Through a comprehensive literature review, the study identifies three key areas of focus: the impact of AR on social and learning outcomes, the role of LLMs in communication support, and the importance of effective UI/UX design in enhancing the efficacy of these technologies. Findings indicate that LLMs offer personalized learning and communication assistance, while AR shows potential in improving social skills, motivation, and attention for children with ASD. However, there is a dearth of robotics-based educational programs tailored specifically for autistic children. To optimize the benefits of these technologies, further research is needed to address issues surrounding customization, accessibility, and integration in ASD therapies and immersive education.<br /><br /> <div>
arXiv:2409.18162v2 Announce Type: replace-cross 
Abstract: The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Census-Based Genetic Algorithm for Target Set Selection Problem in Social Networks</title>
<link>https://arxiv.org/abs/2410.02011</link>
<guid>https://arxiv.org/abs/2410.02011</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, Target Set Selection Problem, Social Networks, Census, Viral Marketing<br />
<br />
Summary: This paper proposes a novel approach, the census-based genetic algorithm, to solve the Target Set Selection (TSS) Problem in social networks for viral marketing. The algorithm uses census information to maintain diversity and prevent premature convergence to local optima. It tracks the number of times individuals are identified and nodes are included in solutions. The algorithm can self-adjust by varying the aggressiveness parameter in reproduction methods and runs efficiently in a parallelized environment. Experimental results on random and real-life social network graphs demonstrate the algorithm's ability to find optimal solutions, outperforming previous studies by reducing solution size and including more network vertices. The novel approach shows promising results for efficiently solving the TSS problem in social networks. <br /><br />Summary: <div>
arXiv:2410.02011v2 Announce Type: replace-cross 
Abstract: This paper considers the Target Set Selection (TSS) Problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph are given. We need to find a minimum size vertex subset to "activate" such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called "a census-based genetic algorithm" for the TSS problem. In our algorithm, we use the idea of a census to gather and store information about each individual in a population and collect census data from the individuals constructed during the algorithm's execution so that we can achieve greater diversity and avoid premature convergence at locally optimal solutions. We use two distinct census information: (a) for each individual, the algorithm stores how many times it has been identified during the execution (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also self-adjust by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check each individual's feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, we execute the proposed algorithm on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Coefficient Reflecting Pairwise Relationships within Hyperedges</title>
<link>https://arxiv.org/abs/2410.23799</link>
<guid>https://arxiv.org/abs/2410.23799</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering coefficients, weighted graphs, higher-order motifs, complex networks 

Summary: 
This study introduces a novel clustering coefficient for hypergraphs that considers intra-hyperedge pairwise relationships and accurately quantifies local link density. By transforming hypergraphs into weighted graphs reflecting relationship strength between nodes based on hyperedge connections, the proposed coefficient addresses limitations of existing approaches. The new definition ensures values in the range [0,1], aligns with simple graph clustering coefficients, and effectively captures intra-hyperedge relationships. Theoretical evaluation on higher-order motifs demonstrates superior performance compared to existing definitions, particularly on motifs III, IV-a, IV-b of order 3. Empirical evaluation on real-world datasets confirms similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient provides a more accurate representation of structural characteristics in complex networks, particularly in systems where group membership implies connections between members, such as social communities and co-authorship networks.  

<br /><br />Summary: <div>
arXiv:2410.23799v2 Announce Type: replace-cross 
Abstract: Hypergraphs are generalizations of simple graphs that allow for the representation of complex group interactions beyond pairwise relationships. Clustering coefficients quantify local link density in networks and have been widely studied for both simple graphs and hypergraphs. However, existing clustering coefficients for hypergraphs treat each hyperedge as a distinct unit rather than a collection of potentially related node pairs, failing to capture intra-hyperedge pairwise relationships and incorrectly assigning zero values to nodes with meaningful clustering patterns. We propose a novel clustering coefficient that addresses this fundamental limitation by transforming hypergraphs into weighted graphs, where edge weights reflect relationship strength between nodes based on hyperedge connections. Our definition satisfies three key conditions: values in the range $[0,1]$, consistency with simple graph clustering coefficients, and effective capture of intra-hyperedge pairwise relationships -- a capability absent from existing approaches. Theoretical evaluation on higher-order motifs demonstrates that our definition correctly assigns values to motifs where existing definitions fail (motifs III, IV-a, IV-b of order 3), while empirical evaluation on three real-world datasets shows similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient enables accurate quantification of local density in complex networks, revealing structural characteristics missed by existing definitions in systems where group membership implies connections between members, such as social communities and co-authorship networks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypermodularity and community detection in hypergraphs</title>
<link>https://arxiv.org/abs/2412.06935</link>
<guid>https://arxiv.org/abs/2412.06935</guid>
<content:encoded><![CDATA[
<div> community detection, higher-order networks, hypermodularity, spectral methods, hidden information<br />
<br />
Summary: 
This article introduces a formalism for detecting community structures in networks with higher-order interactions, such as hypergraphs. The concept of hypermodularity is utilized to apply spectral methods for community detection in these complex networks. The approach is tested on both synthetic random networks and real-world data, demonstrating its effectiveness in capturing the dynamics and nature of interactions within the networks. The study emphasizes the importance of considering higher-order interactions in network analysis and presents a valuable tool for extracting hidden information from intricate higher-order data sets. The results show that the proposed method can reveal nontrivial communities in various types of networked systems, including biological, social, and technological networks. This novel approach offers insights into the modular organization of networks, enabling a deeper understanding of their underlying structure and functionality. <div>
arXiv:2412.06935v2 Announce Type: replace-cross 
Abstract: Numerous networked systems feature a structure of nontrivial communities, which often correspond to their functional modules. Such communities have been detected in real-world biological, social and technological systems, as well as in synthetic models thereof. While much effort has been devoted to developing methods for community detection in traditional networks, the study of community structure in networks with higher-order interactions is still not as extensively explored. In this article, we introduce a formalism for the hypermodularity of higher-order networks that allows us to use spectral methods to detect community structures in hypergraphs. We apply this approach to synthetic random networks as well as to real-world data, showing that it produces results that reflect the nature and the dynamics of the interactions modelled, thereby constituting a valuable tool for the extraction of hidden information from complex higher-order data sets.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks</title>
<link>https://arxiv.org/abs/2502.02197</link>
<guid>https://arxiv.org/abs/2502.02197</guid>
<content:encoded><![CDATA[
<div> Signed networks, community detection, polarization, trust dynamics, local search <br />
<br />
Summary:
This paper presents a novel method for identifying polarized communities in signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions. The key challenge addressed by the proposed method is to detect internally cohesive and externally antagonistic communities while allowing for neutral vertices. By introducing a new optimization objective that avoids highly size-imbalanced solutions, the method outperforms existing approaches in solution quality. In addition, the paper introduces the first local search algorithm that extends to the setting with neutral vertices, providing a scalable solution for large networks. The approach is connected to block-coordinate Frank-Wolfe optimization, ensuring a linear convergence rate. Experimental results on real-world and synthetic datasets demonstrate the superiority of the proposed method in both solution quality and computational efficiency compared to state-of-the-art baselines. <br /><br />Summary: <div>
arXiv:2502.02197v2 Announce Type: replace-cross 
Abstract: Signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions, offer a natural framework for analyzing polarization, trust, and conflict in social systems. Detecting meaningful group structures in such networks is crucial for understanding online discourse, political divisions, and trust dynamics. A key challenge is to identify communities that are internally cohesive and externally antagonistic, while allowing for neutral or unaligned vertices. In this paper, we propose a method for identifying $k$ polarized communities that addresses a major limitation of prior methods: their tendency to produce highly size-imbalanced solutions. We introduce a novel optimization objective that avoids such imbalance. In addition, it is well known that approximation algorithms based on local search are highly effective for clustering signed networks when neutral vertices are not allowed. We build on this idea and design the first local search algorithm that extends to the setting with neutral vertices while scaling to large networks. By connecting our approach to block-coordinate Frank-Wolfe optimization, we prove a linear convergence rate, enabled by the structure of our objective. Experiments on real-world and synthetic datasets demonstrate that our method consistently outperforms state-of-the-art baselines in solution quality, while remaining competitive in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommendation Algorithms on Social Media: Unseen Drivers of Political Opinion</title>
<link>https://arxiv.org/abs/2507.01978</link>
<guid>https://arxiv.org/abs/2507.01978</guid>
<content:encoded><![CDATA[
<div> Facebook, X, social media platforms, algorithms, political interest
<br />
Summary: 
This study examines the impact of social media platforms on political interest among users, analyzing data from over 3,300 participants. The research finds that moderate Facebook users show decreased political engagement, while minimal engagement with X boosts political interest. Demographic variations play a significant role, with males, older individuals, Black or African American users, and those with higher incomes exhibiting greater political interest. Republicans are particularly active on social media, potentially influencing engagement patterns. However, a key limitation is the lack of data regarding the content users are exposed to. Future research should explore these influences and consider additional platforms to enhance understanding. Addressing these gaps can provide insights into digital political mobilization, benefiting policymakers, educators, and platform designers in fostering healthier democratic engagement.
<br /><br /> <div>
arXiv:2507.01978v1 Announce Type: new 
Abstract: Social media broadly refers to digital platforms and applications that simulate social interactions online. This study investigates the impact of social media platforms and their algorithms on political interest among users. As social media usage continues to rise, platforms like Facebook and X (formerly Twitter) play increasingly pivotal roles in shaping political discourse. By employing statistical analyses on data collected from over 3,300 participants, this research identifies significant differences in how various social media platforms influence political interest. Findings reveal that moderate Facebook users demonstrate decreased political engagement, whereas even minimal engagement with X significantly boosts political interest. The study further identifies demographic variations, noting that males, older individuals, Black or African American users, those with higher incomes show greater political interest. The demographic analysis highlights that Republicans are particularly active on social media - potentially influencing their social media engagement patterns. However, the study acknowledges a crucial limitation - the lack of direct data regarding the content users are exposed to which is shaping their social media experiences. Future research should explore these influences and consider additional popular platforms to enhance the understanding of social media's political impact. Addressing these gaps can provide deeper insights into digital political mobilization, aiding policymakers, educators, and platform designers in fostering healthier democratic engagement.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Large Semi-Synthetic Graphs of Any Size</title>
<link>https://arxiv.org/abs/2507.02166</link>
<guid>https://arxiv.org/abs/2507.02166</guid>
<content:encoded><![CDATA[
<div> Keywords: graph generation, deep learning, Graph Neural Networks, Latent Graph Sampling Generation (LGSG), diffusion models

Summary: 
The article introduces Latent Graph Sampling Generation (LGSG), a new framework for generating graphs that addresses limitations of current models. LGSG leverages diffusion models and node embeddings to generate graphs of varying sizes without the need for retraining. By eliminating the dependency on node IDs and capturing the distribution of node embeddings and subgraph structures, LGSG enables scalable and flexible graph generation. Experimental results show that LGSG performs comparably to baseline models for standard metrics and outperforms them in metrics such as the tendency of nodes to form clusters. Additionally, LGSG maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability. This framework represents a significant advancement in data-driven graph generation and has the potential to impact various applications in network science. 

<br /><br />Summary: <div>
arXiv:2507.02166v1 Announce Type: new 
Abstract: Graph generation is an important area in network science. Traditional approaches focus on replicating specific properties of real-world graphs, such as small diameters or power-law degree distributions. Recent advancements in deep learning, particularly with Graph Neural Networks, have enabled data-driven methods to learn and generate graphs without relying on predefined structural properties. Despite these advances, current models are limited by their reliance on node IDs, which restricts their ability to generate graphs larger than the input graph and ignores node attributes. To address these challenges, we propose Latent Graph Sampling Generation (LGSG), a novel framework that leverages diffusion models and node embeddings to generate graphs of varying sizes without retraining. The framework eliminates the dependency on node IDs and captures the distribution of node embeddings and subgraph structures, enabling scalable and flexible graph generation. Experimental results show that LGSG performs on par with baseline models for standard metrics while outperforming them in overlooked ones, such as the tendency of nodes to form clusters. Additionally, it maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features</title>
<link>https://arxiv.org/abs/2507.01984</link>
<guid>https://arxiv.org/abs/2507.01984</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, multimodal features, early fusion approach, social media, COVID-19 pandemic

Summary:
The study explores the integration of text, images, and social features in detecting misinformation on social media during the COVID-19 pandemic and election periods. By analyzing 1,529 tweets, the researchers found that combining unsupervised and supervised machine learning models improved classification performance by 15% compared to unimodal models. Additionally, incorporating techniques like object detection and OCR for extracting visual features further enhanced the accuracy of the classification model. The study also delves into the propagation patterns of misinformation, shedding light on the characteristics of misinformation tweets and the users responsible for spreading them. Overall, the findings highlight the importance of leveraging multimodal feature combinations for effectively detecting misinformation and understanding its dissemination dynamics on social media platforms. 

<br /><br />Summary: <div>
arXiv:2507.01984v1 Announce Type: cross 
Abstract: Amid a tidal wave of misinformation flooding social media during elections and crises, extensive research has been conducted on misinformation detection, primarily focusing on text-based or image-based approaches. However, only a few studies have explored multimodal feature combinations, such as integrating text and images for building a classification model to detect misinformation. This study investigates the effectiveness of different multimodal feature combinations, incorporating text, images, and social features using an early fusion approach for the classification model. This study analyzed 1,529 tweets containing both text and images during the COVID-19 pandemic and election periods collected from Twitter (now X). A data enrichment process was applied to extract additional social features, as well as visual features, through techniques such as object detection and optical character recognition (OCR). The results show that combining unsupervised and supervised machine learning models improves classification performance by 15% compared to unimodal models and by 5% compared to bimodal models. Additionally, the study analyzes the propagation patterns of misinformation based on the characteristics of misinformation tweets and the users who disseminate them.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source Detection in Hypergraph Epidemic Dynamics using a Higher-Order Dynamic Message Passing Algorithm</title>
<link>https://arxiv.org/abs/2507.02523</link>
<guid>https://arxiv.org/abs/2507.02523</guid>
<content:encoded><![CDATA[
<div> Hypergraph, source detection, infectious diseases, message-passing algorithm, susceptible-infectious dynamics  
Summary:  
- Source detection in infectious diseases is crucial for containment strategies.  
- Existing approaches focus on pairwise networks, but higher-order interactions may play a significant role.  
- The HDMPN algorithm is proposed for source detection on hypergraphs, considering group interactions.  
- By modulating likelihood maximization with the fraction of infectious neighbors, HDMPN outperforms conventional methods in most cases.  
- Numerical simulations demonstrate the superiority of HDMPN over benchmarks in capturing the dynamics of epidemic spreading.   <div>
arXiv:2507.02523v1 Announce Type: cross 
Abstract: Source detection is crucial for capturing the dynamics of real-world infectious diseases and informing effective containment strategies. Most existing approaches to source detection focus on conventional pairwise networks, whereas recent efforts on both mathematical modeling and analysis of contact data suggest that higher-order (e.g., group) interactions among individuals may both account for a large fraction of infection events and change our understanding of how epidemic spreading proceeds in empirical populations. In the present study, we propose a message-passing algorithm, called the HDMPN, for source detection for a stochastic susceptible-infectious dynamics on hypergraphs. By modulating the likelihood maximization method by the fraction of infectious neighbors, HDMPN aims to capture the influence of higher-order structures and do better than the conventional likelihood maximization. We numerically show that, in most cases, HDMPN outperforms benchmarks including the likelihood maximization method without modification.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images</title>
<link>https://arxiv.org/abs/2507.02781</link>
<guid>https://arxiv.org/abs/2507.02781</guid>
<content:encoded><![CDATA[
<div> Keyword: earthquake, social media images, damage severity assessment, semantic segmentation, disaster reconnaissance <br />
Summary:
In the study, a novel approach is proposed for assessing damage severity in post-earthquake social media images. Traditional methods rely on subjective classification, but this study frames the problem as a semantic segmentation task for more objective analysis. A segmented dataset is created categorizing damage into undamaged structures, damaged structures, and debris. The SegFormer model is fine-tuned using this dataset to generate damage severity segmentations. A new scoring system is introduced to quantify damage by considering the varying degrees of damage across different areas within images, adjusted for depth estimation. This approach enables a more precise understanding of damage and provides valuable guidance to disaster reconnaissance teams for more effective response efforts following earthquakes. <br /><br />Summary: <div>
arXiv:2507.02781v1 Announce Type: cross 
Abstract: In the aftermath of earthquakes, social media images have become a crucial resource for disaster reconnaissance, providing immediate insights into the extent of damage. Traditional approaches to damage severity assessment in post-earthquake social media images often rely on classification methods, which are inherently subjective and incapable of accounting for the varying extents of damage within an image. Addressing these limitations, this study proposes a novel approach by framing damage severity assessment as a semantic segmentation problem, aiming for a more objective analysis of damage in earthquake-affected areas. The methodology involves the construction of a segmented damage severity dataset, categorizing damage into three degrees: undamaged structures, damaged structures, and debris. Utilizing this dataset, the study fine-tunes a SegFormer model to generate damage severity segmentations for post-earthquake social media images. Furthermore, a new damage severity scoring system is introduced, quantifying damage by considering the varying degrees of damage across different areas within images, adjusted for depth estimation. The application of this approach allows for the quantification of damage severity in social media images in a more objective and comprehensive manner. By providing a nuanced understanding of damage, this study enhances the ability to offer precise guidance to disaster reconnaissance teams, facilitating more effective and targeted response efforts in the aftermath of earthquakes.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into LLM-assisted writing in biomedical publications through excess vocabulary</title>
<link>https://arxiv.org/abs/2406.07016</link>
<guid>https://arxiv.org/abs/2406.07016</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Biomedical research, Scholarly writing, Vocabulary changes, Scientific writing<br />
Summary:<br />
- Large language models (LLMs) like ChatGPT are being widely used in academic literature for scholarly writing in the field of biomedical research.<br />
- A study of over 15 million biomedical abstracts from 2010-2024 indexed by PubMed shows an increase in the frequency of certain style words due to the appearance of LLMs, indicating that at least 13.5% of 2024 abstracts were processed with LLMs.<br />
- The impact of LLMs on scientific writing in biomedical research is significant, surpassing the effect of major world events such as the Covid pandemic.<br />
- LLMs have the capability to generate and revise text with human-level performance but come with limitations including producing inaccurate information, reinforcing biases, and being prone to misuse.<br />
- The usage of LLMs varied across disciplines, countries, and journals, with some subcorpora showing a 40% usage rate in 2024. <br /> <div>
arXiv:2406.07016v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging Activity Temporal Hypergraph (EATH), a model for generating realistic time-varying hypergraphs</title>
<link>https://arxiv.org/abs/2507.01124</link>
<guid>https://arxiv.org/abs/2507.01124</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal hypergraph, complex systems, synthetic datasets, higher-order contagion dynamics, memory mechanisms<br />
<br />
Summary: The study introduces a new model called the Emerging Activity Temporal Hypergraph (EATH) to mimic time-varying group interactions in complex systems. The EATH model can generate synthetic datasets with properties similar to real datasets by incorporating parameters from the original data. It demonstrates the ability to replicate temporal and topological features of empirical datasets of face-to-face interactions. The model allows for the simulation of higher-order contagion dynamics on both original and surrogate datasets to compare outcomes. Additionally, the flexibility of the EATH model enables the generation of synthetic hypergraphs with customizable properties, such as creating "hybrid" datasets by combining characteristics from different empirical datasets. Overall, this research provides a valuable tool for generating realistic temporal hypergraphs when data collection is challenging and offers insights into dynamic processes on temporal hypergraphs. <br /><br /> <div>
arXiv:2507.01124v1 Announce Type: cross 
Abstract: Time-varying group interactions constitute the building blocks of many complex systems. The framework of temporal hypergraphs makes it possible to represent them by taking into account the higher-order and temporal nature of the interactions. However, the corresponding datasets are often incomplete and/or limited in size and duration, and surrogate time-varying hypergraphs able to reproduce their statistical features constitute interesting substitutions, especially to understand how dynamical processes unfold on group interactions. Here, we present a new temporal hypergraph model, the Emerging Activity Temporal Hypergraph (EATH), which can be fed by parameters measured in a dataset and create synthetic datasets with similar properties. In the model, each node has an independent underlying activity dynamic and the overall system activity emerges from the nodes dynamics, with temporal group interactions resulting from both the activity of the nodes and memory mechanisms. We first show that the EATH model can generate surrogate hypergraphs of several empirical datasets of face-to-face interactions, mimicking temporal and topological properties at the node and hyperedge level. We also showcase the possibility to use the resulting synthetic data in simulations of higher-order contagion dynamics, comparing the outcome of such process on original and surrogate datasets. Finally, we illustrate the flexibility of the model, which can generate synthetic hypergraphs with tunable properties: as an example, we generate "hybrid" temporal hypergraphs, which mix properties of different empirical datasets. Our work opens several perspectives, from the generation of synthetic realistic hypergraphs describing contexts where data collection is difficult to a deeper understanding of dynamical processes on temporal hypergraphs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced Efficiency in the Right Fronto-Parietal Attentional Network During Distractor Suppression in Mild Cognitive Impairment</title>
<link>https://arxiv.org/abs/2507.01433</link>
<guid>https://arxiv.org/abs/2507.01433</guid>
<content:encoded><![CDATA[
<div> EEG, Mild Cognitive Impairment, Distractor Suppression, Global Efficiency, Attentional Control <br />
<br />
Summary: 
The study investigates distractor suppression mechanisms in Mild Cognitive Impairment (MCI) patients using EEG data and behavioral measures during attention tasks. Healthy controls (HCs) showed higher Global Efficiency (GE), faster Reaction Times (RTs), and higher Hit Rates (HRs) in congruent conditions compared to incongruent ones. HCs also exhibited increased GE in salient conditions for incongruent trials. MCI patients benefited from congruent conditions with shorter RTs and higher HRs but showed reduced adaptability in GE. The study suggests that alpha band coherence and GE could serve as early markers for cognitive impairment. The findings emphasize the importance of neural efficiency, processing speed, and task accuracy in MCI. This approach provides insights into cognitive load management and interference effects, offering implications for interventions targeting attentional control and processing speed in MCI patients. <br /><br /> <div>
arXiv:2507.01433v1 Announce Type: cross 
Abstract: Mild Cognitive Impairment (MCI) is a critical transitional stage between normal cognitive aging and dementia, making its early detection essential. This study investigates the neural mechanisms of distractor suppression in MCI patients using EEG and behavioral data during an attention-cueing Eriksen flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks with congruent and incongruent stimuli of varying saliency levels. During these tasks, EEG data were analyzed for alpha band coherence's functional connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency, and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster RTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent conditions. HCs also showed increased GE in salient conditions for incongruent trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions with shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent conditions but showed reduced adaptability in GE, with no significant GE differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early markers for cognitive impairment. By integrating GE, RT, and HR, this study provides insights into the interplay between neural efficiency, processing speed, and task accuracy. This approach offers valuable insights into cognitive load management and interference effects, indicating benefits for interventions aimed at improving attentional control and processing speed in MCIs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the interaction between science and misinformation in COVID-19 tweets</title>
<link>https://arxiv.org/abs/2507.01481</link>
<guid>https://arxiv.org/abs/2507.01481</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, science, Twitter, COVID-19, social media 

Summary: 
During the COVID-19 pandemic, a study analyzed the interaction between misinformation and science on Twitter. A database of 407M COVID-19-related tweets was examined to classify reliability and use Altmetric data to identify scientific publications. The study revealed that many users share both scientific and unreliable content, with 45% sharing both. Publications frequently shared by users also sharing unreliable content are more likely to be preprints and have lower impact. Misinformation is not linked to a lack of science, raising concerns about open science practices. The findings underscore the importance of proactive scientific engagement on social media platforms to combat false narratives in global crises. 

<br /><br />Summary: <div>
arXiv:2507.01481v1 Announce Type: cross 
Abstract: During the COVID-19 pandemic, scientific understanding related to the topic evolved rapidly. Along with scientific information being discussed widely, a large circulation of false information, labelled an infodemic by the WHO, emerged. Here, we study the interaction between misinformation and science on Twitter (now X) during the COVID-19 pandemic. We built a comprehensive database of $\sim$407M COVID-19 related tweets and classified the reliability of URLs in the tweets based on Media Bias/Fact Check. In addition, we use Altmetric data to see whether a tweet refers to a scientific publication. We find that many users find that many users share both scientific and unreliable content; out of the $\sim$1.2M users who share science, $45\%$ also share unreliable content. Publications that are more frequently shared by users who also share unreliable content are more likely to be preprints, slightly more often retracted, have fewer citations, and are published in lower-impact journals on average. Our findings suggest that misinformation is not related to a ``deficit'' of science. In addition, our findings raise some critical questions about certain open science practices and their potential for misuse. Given the fundamental opposition between science and misinformation, our findings highlight the necessity for proactive scientific engagement on social media platforms to counter false narratives during global crises.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling individual attention dynamics on online social media</title>
<link>https://arxiv.org/abs/2507.01511</link>
<guid>https://arxiv.org/abs/2507.01511</guid>
<content:encoded><![CDATA[
<div> Keywords: attention economy, limited attention, engagement decay, user interactions, Reddit

Summary: 
The article discusses the importance of understanding how individuals manage limited attention in the attention economy. A simple model is introduced to describe the decay of a user's engagement when faced with multiple inputs. The analytical analysis shows that individual attention decay is determined by the overall duration of interactions rather than the number or user activity. The model is validated using data from Reddit's Change My View subreddit, where user attention dynamics are explicitly traceable. Despite its simplicity, the model provides a crucial microscopic perspective that complements macroscopic studies in understanding how individuals navigate and prioritize attention in an information-rich environment. <br /><br />Summary: <div>
arXiv:2507.01511v1 Announce Type: cross 
Abstract: In the attention economy, understanding how individuals manage limited attention is critical. We introduce a simple model describing the decay of a user's engagement when facing multiple inputs. We analytically show that individual attention decay is determined by the overall duration of interactions, not their number or user activity. Our model is validated using data from Reddit's Change My View subreddit, where the user's attention dynamics is explicitly traceable. Despite its simplicity, our model offers a crucial microscopic perspective complementing macroscopic studies.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principal Graph Encoder Embedding and Principal Community Detection</title>
<link>https://arxiv.org/abs/2501.14939</link>
<guid>https://arxiv.org/abs/2501.14939</guid>
<content:encoded><![CDATA[
<div> principal communities, principal graph encoder embedding, vertex embedding, community score, graph adjacency matrix

Summary:<br />
- The paper introduces the concept of principal communities and proposes a method for detecting these communities and generating vertex embeddings simultaneously.
- The method computes a community score for each community, ranks them to determine community importance, and identifies a set of principal communities.
- Vertex embeddings are produced by retaining only the dimensions corresponding to these principal communities.
- The population version of the encoder embedding and community score are defined theoretically, based on a random Bernoulli graph distribution.
- Through simulations, the method demonstrates accurate detection of ground-truth principal communities, improved visualization, and vertex classification, along with robustness to label noise and computational scalability. 

Summary: <div>
arXiv:2501.14939v2 Announce Type: replace 
Abstract: In this paper, we introduce the concept of principal communities and propose a principal graph encoder embedding method that concurrently detects these communities and achieves vertex embedding. Given a graph adjacency matrix with vertex labels, the method computes a sample community score for each community, ranking them to measure community importance and estimate a set of principal communities. The method then produces a vertex embedding by retaining only the dimensions corresponding to these principal communities. Theoretically, we define the population version of the encoder embedding and the community score based on a random Bernoulli graph distribution. We prove that the population principal graph encoder embedding preserves the conditional density of the vertex labels and that the population community score successfully distinguishes the principal communities. We conduct a variety of simulations to demonstrate the finite-sample accuracy in detecting ground-truth principal communities, as well as the advantages in embedding visualization and subsequent vertex classification. The method is further applied to a set of real-world graphs, showcasing its numerical advantages, including robustness to label noise and computational scalability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Modeling with Household Coordination Activities under Limited Information via Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2409.17495</link>
<guid>https://arxiv.org/abs/2409.17495</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility patterns, household coordination, large language model, activity chains, mobility data

Summary:
This research addresses the challenge of understanding human mobility patterns by proposing a retrieval-augmented large language model (LLM) framework. The framework focuses on generating activity chains with household coordination using public accessible statistical and socio-demographic information. By incorporating a retrieval-augmentation mechanism, the model can accurately represent household coordination and maintain statistical consistency in generated mobility patterns. This approach fills a key gap in existing methods that primarily focus on spatial-temporal patterns. The validation with NHTS and SCAG-ABM datasets shows that the framework effectively synthesizes mobility patterns and can adapt well to regions with limited mobility data availability. Overall, this research offers a promising solution to the limitations of conventional activity-based models and learning-based human mobility modeling algorithms. <div>
arXiv:2409.17495v2 Announce Type: replace-cross 
Abstract: Understanding human mobility patterns has long been a challenging task in transportation modeling. Due to the difficulties in obtaining high-quality training datasets across diverse locations, conventional activity-based models and learning-based human mobility modeling algorithms are particularly limited by the availability and quality of datasets. Current approaches primarily focus on spatial-temporal patterns while neglecting semantic relationships such as logical connections or dependencies between activities and household coordination activities like joint shopping trips or family meal times, both crucial for realistic mobility modeling. We propose a retrieval-augmented large language model (LLM) framework that generates activity chains with household coordination using only public accessible statistical and socio-demographic information, reducing the need for sophisticated mobility data. The retrieval-augmentation mechanism enables household coordination and maintains statistical consistency across generated patterns, addressing a key gap in existing methods. Our validation with NHTS and SCAG-ABM datasets demonstrates effective mobility synthesis and strong adaptability for regions with limited mobility data availability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tenure and Research Trajectories</title>
<link>https://arxiv.org/abs/2411.10575</link>
<guid>https://arxiv.org/abs/2411.10575</guid>
<content:encoded><![CDATA[
<div> tenure, research trajectories, publication rates, creative search, impact <br />
Summary: 
The study examines the impact of tenure on faculty research trajectories in the US academic system. It reveals that publication rates rise significantly during the tenure track, peaking just before tenure. Post-tenure research output varies among disciplines, with lab-based fields maintaining high output while non-lab-based fields experience a decline. Tenured faculty tend to engage in novel, high-risk research post-tenure, although this comes at the expense of impact as post-tenure research yields fewer highly cited papers. The study highlights the role of tenure in shaping individual research trajectories, with breaks in trajectories closely tied to an individual's tenure year. These findings provide valuable insights into the dynamics of the tenure system, faculty research patterns, and scientific output. <br /> <div>
arXiv:2411.10575v2 Announce Type: replace-cross 
Abstract: Tenure is a cornerstone of the US academic system, yet its relationship to faculty research trajectories remains poorly understood. Conceptually, tenure systems may act as a selection mechanism, screening in high-output researchers; a dynamic incentive mechanism, encouraging high output prior to tenure but low output after tenure; and a creative search mechanism, encouraging tenured individuals to undertake high-risk work. Here, we integrate data from seven different sources to trace US tenure-line faculty and their research outputs at an unprecedented scale and scope, covering over 12,000 researchers across 15 disciplines. Our analysis reveals that faculty publication rates typically increase sharply during the tenure track and peak just before obtaining tenure. Post-tenure trends, however, vary across disciplines: in lab-based fields, such as biology and chemistry, research output typically remains high post-tenure, whereas in non-lab-based fields, such as mathematics and sociology, research output typically declines substantially post-tenure. Turning to creative search, faculty increasingly produce novel, high-risk research after securing tenure. However, this shift toward novelty and risk-taking comes with a decline in impact, with post-tenure research yielding fewer highly cited papers. Comparing outcomes across common career ages but different tenure years or comparing research trajectories in tenure-based and non-tenure-based research settings underscores that breaks in the research trajectories are sharply tied to the individual's tenure year. Overall, these findings provide a new empirical basis for understanding the tenure system, individual research trajectories, and the shape of scientific output.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wordkrill: Extending Wordfish into the multidimensional political space</title>
<link>https://arxiv.org/abs/2506.20275</link>
<guid>https://arxiv.org/abs/2506.20275</guid>
<content:encoded><![CDATA[
<div> Wordkrill, Wordfish, multidimensional, political conflict, spatial models <br />
Summary: Spatial models play a key role in the analysis of political conflict, often relying on text-based methods such as the Wordfish model. However, Wordfish's unidimensionality limits its ability to capture the multidimensional nature of political competition. To address this limitation, Wordkrill is introduced as a multidimensional extension of Wordfish that allows for the estimation of political positions along multiple latent dimensions. The mathematical framework of Wordkrill is presented, and its utility is demonstrated through applications to party manifestos and parliamentary speeches. While Wordkrill offers practical advantages in estimating political positions, there are current limitations that need to be addressed. Further exploration and refinement of Wordkrill could enhance the understanding of political conflicts and actors' positions in multidimensional space. <br /> <div>
arXiv:2506.20275v2 Announce Type: replace-cross 
Abstract: Spatial models are central to the study of political conflict, yet their empirical application often depends on text-based methods. A prominent example is the Wordfish model, which estimates actor positions from political texts. However, a key limitation of Wordfish is its unidimensionality, despite the well-established multidimensional nature of political competition. This contribution introduces Wordkrill, a multidimensional extension of Wordfish that retains the original model's interpretability while allowing for efficient estimation of political positions along multiple latent dimensions. After presenting the mathematical framework of Wordkrill, its utility through brief applications to party manifestos and parliamentary speeches is demonstrated. These examples illustrate both the practical advantages and current limitations of the approach.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Dynamics with Self-Interaction Learning in Networked Systems</title>
<link>https://arxiv.org/abs/2507.00422</link>
<guid>https://arxiv.org/abs/2507.00422</guid>
<content:encoded><![CDATA[
<div> evolution cooperation networked systems social networks multi-agent systems biological species <br />
Summary:<br />
The study explores the role of self-interaction in networked evolutionary dynamics. It investigates how self-persistence of strategies impacts the evolution of cooperation in various systems. The research introduces a self-interaction landscape to assess an agent's ability to maintain its strategy based on local topology. Findings reveal that appropriate self-interaction can promote cooperation by reducing the conditions for cooperation to thrive and helping cooperators resist full defection. In systems where spitefulness prevails, self-interaction can protect cooperative agents from harm. The study suggests that a well-designed self-interaction landscape can significantly lower the critical conditions for advantageous mutants, especially in high-degree networks. This research contributes to understanding the dynamics of cooperation in networked systems and sheds light on the importance of self-interaction in evolutionary processes. <br /> <div>
arXiv:2507.00422v1 Announce Type: new 
Abstract: The evolution of cooperation in networked systems helps to understand the dynamics in social networks, multi-agent systems, and biological species. The self-persistence of individual strategies is common in real-world decision making. The self-replacement of strategies in evolutionary dynamics forms a selection amplifier, allows an agent to insist on its autologous strategy, and helps the networked system to avoid full defection. In this paper, we study the self-interaction learning in the networked evolutionary dynamics. We propose a self-interaction landscape to capture the strength of an agent's self-loop to reproduce the strategy based on local topology. We find that proper self-interaction can reduce the condition for cooperation and help cooperators to prevail in the system. For a system that favors the evolution of spite, the self-interaction can save cooperative agents from being harmed. Our results on random networks further suggest that an appropriate self-interaction landscape can significantly reduce the critical condition for advantageous mutants, especially for large-degree networks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks</title>
<link>https://arxiv.org/abs/2507.00600</link>
<guid>https://arxiv.org/abs/2507.00600</guid>
<content:encoded><![CDATA[
<div> role-based clustering, financial institutions, multi-layer networks, market segments, node embeddings

Summary:
The study introduces a novel role-based clustering method for analyzing financial networks, aiming to uncover the functional positions of institutions across various market layers. By utilizing interpretative node embeddings derived from egonet features, the approach captures both direct and indirect trading relationships within and between market segments. Transaction-level data from the ECB's Money Market Statistical Reporting (MMSR) is used to demonstrate the method's effectiveness in identifying diverse institutional roles such as market intermediaries, cross-segment connectors, and peripheral lenders or borrowers. This approach offers flexibility and practical insights for supervisory, systemic risk assessment, and resolution planning purposes, enhancing understanding of institutional behavior within complex market structures.<br /><br />Summary: <div>
arXiv:2507.00600v1 Announce Type: new 
Abstract: Understanding the functional roles of financial institutions within interconnected markets is critical for effective supervision, systemic risk assessment, and resolution planning. We propose an interpretable role-based clustering approach for multi-layer financial networks, designed to identify the functional positions of institutions across different market segments. Our method follows a general clustering framework defined by proximity measures, cluster evaluation criteria, and algorithm selection. We construct explainable node embeddings based on egonet features that capture both direct and indirect trading relationships within and across market layers. Using transaction-level data from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate how the approach uncovers heterogeneous institutional roles such as market intermediaries, cross-segment connectors, and peripheral lenders or borrowers. The results highlight the flexibility and practical value of role-based clustering in analyzing financial networks and understanding institutional behavior in complex market structures.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Differences in International Research Collaboration in European Union</title>
<link>https://arxiv.org/abs/2507.00619</link>
<guid>https://arxiv.org/abs/2507.00619</guid>
<content:encoded><![CDATA[
<div> gender-based authorship patterns, International Research Collaboration, co-authorship networks, COVID-19, gender disparities

Summary:
The study investigates International Research Collaboration (IRC) among EU countries from 2011 to 2022, focusing on gender-based authorship patterns. Analysis of a WoS-SSCI database shows increased IRC, notably with the USA and China. However, articles with female authors lag behind those with male authors. Female-exclusive collaborations exhibit distinct network structures. The COVID-19 pandemic influenced collaboration dynamics, temporarily narrowing the gender gap but exposing vulnerabilities in female-dominated networks. The findings highlight progress in IRC but also persistent gender disparities in EU participation. <div>
arXiv:2507.00619v1 Announce Type: new 
Abstract: This paper investigates International Research Collaboration (IRC) among European Union (EU) countries from 2011 to 2022, with emphasis on gender-based authorship patterns. Drawing from the Web of Science Social Science Citation Index (WoS-SSCI) database, a large dataset of IRC articles was constructed, annotated with categories of authorship based on gender, author affiliation, and COVID-19 subject as topic. Using network science, the study maps collaboration structures and reveals gendered differences in co-authorship networks. Results highlight a substantial rise in IRC over the decade, particularly with the USA and China as key non-EU partners. Articles with at least one female author were consistently less frequent than those with at least one male author. Notably, female-exclusive collaborations showed distinctive network topologies, with more centralized (star-like) patterns and shorter tree diameters. The COVID-19 pandemic further reshaped collaboration dynamics, temporarily reducing the gender gap in IRC but also revealing vulnerabilities in female-dominated research networks. These findings underscore both progress and persistent disparities in the gender dynamics of EU participation in IRC.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How large language models judge and influence human cooperation</title>
<link>https://arxiv.org/abs/2507.00088</link>
<guid>https://arxiv.org/abs/2507.00088</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, cooperation, social interactions, moral judgements, human prosociality<br />
Summary:<br />
- The study investigates the impact of large language models (LLMs) on human cooperation in social settings.<br />
- LLMs were provided with examples of cooperative and non-cooperative actions to assess how they judge such interactions.<br />
- There is a consensus among LLMs in evaluating cooperation against good opponents, but variance exists when judging interactions with ill-reputed individuals.<br />
- Differences in LLM judgements can significantly affect the prevalence of cooperation in populations.<br />
- Interventions using goal-oriented prompts can influence LLM norms and shape their judgements.<br /> <div>
arXiv:2507.00088v1 Announce Type: cross 
Abstract: Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endogenous Network Structures with Precision and Dimension Choices</title>
<link>https://arxiv.org/abs/2507.00249</link>
<guid>https://arxiv.org/abs/2507.00249</guid>
<content:encoded><![CDATA[
<div> Keywords: social learning, network structure, signal precision, dimension choices, dynamic network

Summary:
This paper introduces a social learning model where the network structure is determined by signal precision and dimension choices. Agents make decisions on signal precision and dimensions to learn about, which directly influence the network structure. The optimal precision choice is sublinear to the agent's influence in a fixed network, with a gap from the socially optimal choice. In a dynamic network, a kernel distance between agents defines the network, guiding how much weight agents assign to each other. Agents select dimensions to learn about to minimize the squared sum of influences, preferring a network with equally distributed influence. This research sheds light on the interplay between individual decisions, network structures, and social learning outcomes. 

<br /><br />Summary: <div>
arXiv:2507.00249v1 Announce Type: cross 
Abstract: This paper presents a social learning model where the network structure is endogenously determined by signal precision and dimension choices. Agents not only choose the precision of their signals and what dimension of the state to learn about, but these decisions directly determine the underlying network structure on which social learning occurs. We show that under a fixed network structure, the optimal precision choice is sublinear in the agent's stationary influence in the network, and this individually optimal choice is worse than the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network structure, we specify the network by defining a kernel distance between agents, which then determines how much weight agents place on one another. Agents choose dimensions to learn about such that their choice minimizes the squared sum of influences of all agents: a network with equally distributed influence across agents is ideal.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan</title>
<link>https://arxiv.org/abs/2507.00279</link>
<guid>https://arxiv.org/abs/2507.00279</guid>
<content:encoded><![CDATA[
<div> Keywords: seasonal migration, Afghanistan, opium harvest, violence, civil conflict <br />
Summary: Seasonal migration in Afghanistan, particularly during the opium harvest, is crucial for stabilizing rural economies. Satellite imagery is used to estimate the timing of the harvest, which attracts seasonal workers. Districts with high levels of poppy cultivation receive more migrants. Despite violent events, migration remains resilient, but is influenced by long-term conflict patterns, such as Taliban control in origin and destination areas. The study utilizes mobile phone records to analyze migration response to the harvest, finding that violent incidents do not significantly disrupt labor flows. However, the extent of conflict in an area impacts the movement of seasonal workers. The research sheds light on the relationship between violence, civil conflict, and seasonal migration dynamics in Afghanistan. <br /><br />Summary: <div>
arXiv:2507.00279v1 Announce Type: cross 
Abstract: Seasonal migration plays a critical role in stabilizing rural economies and sustaining the livelihoods of agricultural households. Violence and civil conflict have long been thought to disrupt these labor flows, but this hypothesis has historically been hard to test given the lack of reliable data on migration in conflict zones. Focusing on Afghanistan in the 8-year period prior to the Taliban's takeover in 2021, we first demonstrate how satellite imagery can be used to infer the timing of the opium harvest, which employs a large number of seasonal workers in relatively well-paid jobs. We then use a dataset of nationwide mobile phone records to characterize the migration response to this harvest, and examine whether and how violence and civil conflict disrupt this migration. We find that, on average, districts with high levels of poppy cultivation receive significantly more seasonal migrants than districts with no poppy cultivation. These labor flows are surprisingly resilient to idiosyncratic violent events at the source or destination, including extreme violence resulting in large numbers of fatalities. However, seasonal migration is affected by longer-term patterns of conflict, such as the extent of Taliban control in origin and destination locations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Construction of Graphs with Maximum Robustness</title>
<link>https://arxiv.org/abs/2507.00415</link>
<guid>https://arxiv.org/abs/2507.00415</guid>
<content:encoded><![CDATA[
<div> network robustness, resilient control, misbehaving agents, communication structures, undirected graphs<br />
<br />
Summary: 
- The article explores network robustness and resilient control in the presence of misbehaving agents.
- Higher robustness levels require dense communication structures, which may not be ideal for systems with limited capabilities.
- Tight necessary conditions on the number of edges for undirected graphs to achieve maximum robustness are established.
- Two classes of undirected graphs, known as Minimal Edge Robust Graphs (MERGs), are constructed to achieve maximum robustness with minimal numbers of edges.
- The work is validated through simulations. <div>
arXiv:2507.00415v1 Announce Type: cross 
Abstract: The notions of network $r$-robustness and $(r,s)$-robustness have been earlier introduced in the literature to achieve resilient control in the presence of misbehaving agents. However, while higher robustness levels provide networks with higher tolerances against the misbehaving agents, they also require dense communication structures, which are not always desirable for systems with limited capabilities and energy capacities. Therefore, this paper studies the fundamental structures behind $r$-robustness and $(r,s)$- robustness properties in two different ways. (a) We first explore and establish the tight necessary conditions on the number of edges for undirected graphs with any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b) We then use these conditions to construct two classes of undirected graphs, referred as to $\gamma$- and $(\gamma,\gamma)$-Minimal Edge Robust Graphs (MERGs), that provably achieve maximum robustness with minimal numbers of edges. We finally validate our work through some sets of simulations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Network and Attack Graphs for Service-Centric Impact Analysis</title>
<link>https://arxiv.org/abs/2507.00637</link>
<guid>https://arxiv.org/abs/2507.00637</guid>
<content:encoded><![CDATA[
<div> Keywords: cyber threats, attack paths, impact analysis, probabilistic methods, network-based influence spreading <br />
Summary: <br />
- The article presents a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, and their impact on user services in digital networks. <br />
- It uses probabilistic methods to track the propagation of attacks through different network layers, enabling analysis at various levels of detail. <br />
- Understanding how attacks spread within services and between different servers can help in early detection and mitigation. <br />
- The approach allows for evaluating diverse attack scenarios and developing protection measures based on the criticality of services from the user's perspective. <br />
- This methodology can assist security specialists and system administrators in making informed decisions regarding risk mitigation strategies. <br /> <div>
arXiv:2507.00637v1 Announce Type: cross 
Abstract: We present a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, as well as their impact on user services in enterprise or infrastructure networks of digital devices and services they provide. Using probabilistic methods to track the propagation of an attack through attack graphs, via the service or application layers, and on physical communication networks, our model enables us to analyse cyber attacks at different levels of detail. Understanding the propagation of an attack within a service among microservices and its spread between different services or application servers could help detect and mitigate it early. We demonstrate that this network-based influence spreading modelling approach enables the evaluation of diverse attack scenarios and the development of protection and mitigation measures, taking into account the criticality of services from the user's perspective. This methodology could also aid security specialists and system administrators in making well-informed decisions regarding risk mitigation strategies.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity</title>
<link>https://arxiv.org/abs/2507.00657</link>
<guid>https://arxiv.org/abs/2507.00657</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, political discourse, social media, polarization, toxicity

Summary: 
Large Language Models (LLMs) were studied for simulating political discourse on social media during the 2024 U.S. presidential election. Using 21 million interactions, LLM agents based on real users were constructed and compared to human replies. Three model families (Gemini, Mistral, and DeepSeek) were evaluated for linguistic style, ideological consistency, and toxicity. It was found that richer contextualization improved internal consistency but also increased polarization, stylized signals, and harmful language. A phenomenon called "generation exaggeration" was observed, where salient traits were systematically amplified beyond empirical baselines. The study concluded that LLMs do not emulate users but reconstruct them, introducing structural biases that affect their reliability as social proxies. This challenges their suitability for content moderation, deliberative simulations, and policy modeling. 

<br /><br />Summary: <div>
arXiv:2507.00657v1 Announce Type: cross 
Abstract: We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular versus Hierarchical: A Structural Signature of Topic Popularity in Mathematical Research</title>
<link>https://arxiv.org/abs/2506.22946</link>
<guid>https://arxiv.org/abs/2506.22946</guid>
<content:encoded><![CDATA[
<div> popularity, collaboration network, structural patterns, topic specialization, mathematical research<br />
Summary:<br />
This study explores how the popularity of research topics is linked to the structure of collaboration networks within those topics. Analyzing 1,938 topics from arXiv papers, the research finds a divide in how popular and niche topics organize collaboration. Popular topics form modular "schools of thought," while niche topics have core-periphery structures around established experts. This structural dichotomy is independent of network size and impacts collaboration opportunities for researchers. Interestingly, researchers in popular fields face greater structural constraints on collaboration, contrary to common assumptions. The findings suggest that choosing a research topic is not just about the subject matter but also about entering different collaborative environments with implications for a researcher's career. To make these patterns clear to the research community, the authors developed the Math Research Compass, an interactive platform providing data on topic popularity and collaboration patterns in mathematical research.<br /> <div>
arXiv:2506.22946v1 Announce Type: new 
Abstract: Mathematical researchers, especially those in early-career positions, face critical decisions about topic specialization with limited information about the collaborative environments of different research areas. The aim of this paper is to study how the popularity of a research topic is associated with the structure of that topic's collaboration network, as observed by a suite of measures capturing organizational structure at several scales. We apply these measures to 1,938 algorithmically discovered topics across 121,391 papers sourced from arXiv metadata during the period 2020--2025. Our analysis, which controls for the confounding effects of network size, reveals a structural dichotomy--we find that popular topics organize into modular "schools of thought," while niche topics maintain hierarchical core-periphery structures centered around established experts. This divide is not an artifact of scale, but represents a size-independent structural pattern correlated with popularity. We also document a "constraint reversal": after controlling for size, researchers in popular fields face greater structural constraints on collaboration opportunities, contrary to conventional expectations. Our findings suggest that topic selection is an implicit choice between two fundamentally different collaborative environments, each with distinct implications for a researcher's career. To make these structural patterns transparent to the research community, we developed the Math Research Compass (https://mathresearchcompass.com), an interactive platform providing data on topic popularity and collaboration patterns across mathematical topics.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Improving Large Language Models for Competitive Program Generation</title>
<link>https://arxiv.org/abs/2506.22954</link>
<guid>https://arxiv.org/abs/2506.22954</guid>
<content:encoded><![CDATA[
<div> competitive programming, large language models, algorithmic reasoning, benchmark datasets, error taxonomy

Summary:
- The study evaluates the performance of large language models (LLMs) in solving real-world competitive programming problems.
- A curated benchmark of 80 problems from regional ICPC/CCPC contests in 2024 was used to assess the capabilities of LLMs.
- The LLM DeepSeek-R1 was evaluated through online judge platforms with carefully designed prompts.
- A fine-grained error taxonomy was developed to categorize incorrect submissions, identifying general and specialized errors.
- An improvement framework combining dialogue-based repair and information-augmented regeneration phases significantly increased the number of correct solutions, with 46 out of 80 problems successfully accepted.

<br /><br />Summary: <div>
arXiv:2506.22954v1 Announce Type: new 
Abstract: Context: Due to the demand for strong algorithmic reasoning, complex logic implementation, and strict adherence to input/output formats and resource constraints, competitive programming generation by large language models (LLMs) is considered the most challenging problem in current LLM-based code generation. However, previous studies often evaluate LLMs using simple prompts and benchmark datasets prone to data leakage. Moreover, prior work has limited consideration of the diversity in algorithm types and difficulty levels. Objective: In this study, we aim to evaluate and improve LLMs in solving real-world competitive programming problems. Methods: We initially collect 117 problems from nine regional ICPC/CCPC contests held in 2024 and design four filtering criteria to construct a curated benchmark consisting of 80 problems. Leveraging DeepSeek-R1 as the LLM, we evaluate its competitive program generation capabilities through the online judge (OJ) platforms, guided by a carefully designed basic prompt. For incorrect submissions, we construct a fine-grained error taxonomy and then propose a targeted improvement framework by combining a multi-turn dialogue-based repair phase and an information-augmented regeneration phase. Results: Experimental results show that only 5 out of 80 problems are fully accepted when using basic prompts. For the unsolved problems, we construct the error taxonomy, including general errors (such as design, boundary, condition, data type, syntax, and input/output errors) and specialized errors (such as those in mathematical problems, greedy algorithms, and graph theories). After applying our proposed improvement strategies, we substantially increased the number of correct solutions, with 46 out of 80 problems successfully accepted.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction Gaps as Pathways to Explanation: Rethinking Educational Outcomes through Differences in Model Performance</title>
<link>https://arxiv.org/abs/2506.22993</link>
<guid>https://arxiv.org/abs/2506.22993</guid>
<content:encoded><![CDATA[
<div> Keywords: social contexts, prediction gaps, university completion, gradient boosting, graph neural networks 

Summary: 
The study conducted using population-scale administrative data from the Netherlands explores the impact of social contexts on educational outcomes, specifically university completion. Various statistical models were compared, including logistic regression, gradient boosting, and graph neural networks, to predict university completion based on early-life social contexts. The results showed small prediction gaps overall, indicating that previously identified indicators, especially parental status, account for most measurable variation in educational attainment. However, larger prediction gaps were observed for girls growing up without fathers, suggesting that the effects of social context for this particular group are more complex and not fully captured by simpler models. This highlights the importance of considering social contexts in understanding educational outcomes and the potential of prediction methods to support sociological explanations.

<br /><br />Summary: <div>
arXiv:2506.22993v1 Announce Type: new 
Abstract: Social contexts -- such as families, schools, and neighborhoods -- shape life outcomes. The key question is not simply whether they matter, but rather for whom and under what conditions. Here, we argue that prediction gaps -- differences in predictive performance between statistical models of varying complexity -- offer a pathway for identifying surprising empirical patterns (i.e., not captured by simpler models) which highlight where theories succeed or fall short. Using population-scale administrative data from the Netherlands, we compare logistic regression, gradient boosting, and graph neural networks to predict university completion using early-life social contexts. Overall, prediction gaps are small, suggesting that previously identified indicators, particularly parental status, capture most measurable variation in educational attainment. However, gaps are larger for girls growing up without fathers -- suggesting that the effects of social context for these groups go beyond simple models in line with sociological theory. Our paper shows the potential of prediction methods to support sociological explanation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Based Efficient Algorithms for User-Driven Competitive Influence Maximization in Social Networks</title>
<link>https://arxiv.org/abs/2506.23179</link>
<guid>https://arxiv.org/abs/2506.23179</guid>
<content:encoded><![CDATA[
<div> social networks, information diffusion, linear threshold model, heuristic algorithms, genetic algorithm

Summary:
The article discusses the User-driven competitive influence Maximization (UDCIM) model in social networks, focusing on human behavior in decision-making processes. It extends the existing work by proposing algorithms and LP-formulation of the problem. The study includes implementing and testing LP-formulated equations on small datasets using the Gurobi Solver. Additionally, the article introduces a heuristic and genetic algorithm for further exploration. Extensive experimentation is conducted on medium to large datasets, with outcomes plotted and discussed in the results section. The study aims to advance the understanding of information diffusion in social networks and provide practical tools for maximizing influence within communities. <div>
arXiv:2506.23179v1 Announce Type: new 
Abstract: Nowadays, people in the modern world communicate with their friends, relatives, and colleagues through the internet. Persons/nodes and communication/edges among them form a network. Social media networks are a type of network where people share their views with the community. There are several models that capture human behavior, such as a reaction to the information received from friends or relatives. The two fundamental models of information diffusion widely discussed in the social networks are the Independent Cascade Model and the Linear Threshold Model. Liu et al. [1] propose a variant of the linear threshold model in their paper title User-driven competitive influence Maximization(UDCIM) in social networks. Authors try to simulate human behavior where they do not make a decision immediately after being influenced, but take a pause for a while, and then they make a final decision. They propose the heuristic algorithms and prove the approximation factor under community constraints( The seed vertices belong to an identical community). Even finding the community is itself an NP-hard problem. In this article, we extend the existing work with algorithms and LP-formation of the problem. We also implement and test the LP-formulated equations on small datasets by using the Gurobi Solver [2]. We furthermore propose one heuristic and one genetic algorithm. The extensive experimentation is carried out on medium to large datasets, and the outcomes of both algorithms are plotted in the results and discussion section.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship</title>
<link>https://arxiv.org/abs/2506.22497</link>
<guid>https://arxiv.org/abs/2506.22497</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, blockchain, AI, scholarly evaluation, academic knowledge<br />
Summary:<br />
This paper proposes a new model of peer review, viewing it as structured public commentary rather than a traditional academic validation process. The authors highlight the limitations of anonymity, latency, and gatekeeping in the current peer review system and suggest a transparent, identity-linked, and reproducible framework anchored in open commentary. Leveraging blockchain technology for immutable audit trails and AI for iterative synthesis, the proposed model aims to incentivize intellectual contribution, capture epistemic evolution, and enable traceable reputational dynamics. By reframing academic knowledge as a living process rather than a static credential, this innovative approach has the potential to empower various fields, from computational science to the humanities. This reconceptualization of peer review could lead to a more dynamic and inclusive scholarly evaluation system. <br /> <div>
arXiv:2506.22497v1 Announce Type: cross 
Abstract: This paper reconceptualises peer review as structured public commentary. Traditional academic validation is hindered by anonymity, latency, and gatekeeping. We propose a transparent, identity-linked, and reproducible system of scholarly evaluation anchored in open commentary. Leveraging blockchain for immutable audit trails and AI for iterative synthesis, we design a framework that incentivises intellectual contribution, captures epistemic evolution, and enables traceable reputational dynamics. This model empowers fields from computational science to the humanities, reframing academic knowledge as a living process rather than a static credential.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools</title>
<link>https://arxiv.org/abs/2506.22940</link>
<guid>https://arxiv.org/abs/2506.22940</guid>
<content:encoded><![CDATA[
<div> AI, collaboration, misinformation, social media, user agency
<br />
Summary: This paper explores the use of collaborative AI systems to empower users in identifying and evaluating misinformation on social media. Traditional methods often struggle with emotionally charged or context-lacking content, leading to the design and evaluation of an interactive interface with features such as real-time explanations, source aggregation, and debate-style interaction. In a user study, the debate mode was found to be more effective than standard chatbot interfaces, while the multiple-source view was rated highly useful. The findings suggest that context-rich, dialogic AI systems can enhance media literacy and trust in digital information environments. The study emphasizes the importance of ethical design, explainability, and interactive engagement in developing tools for countering misinformation in the era of post-truth. 
<br /><br /> <div>
arXiv:2506.22940v1 Announce Type: cross 
Abstract: This paper investigates how collaborative AI systems can enhance user agency in identifying and evaluating misinformation on social media platforms. Traditional methods, such as personal judgment or basic fact-checking, often fall short when faced with emotionally charged or context-deficient content. To address this, we designed and evaluated an interactive interface that integrates collaborative AI features, including real-time explanations, source aggregation, and debate-style interaction. These elements aim to support critical thinking by providing contextual cues and argumentative reasoning in a transparent, user-centered format. In a user study with 14 participants, 79% found the debate mode more effective than standard chatbot interfaces, and the multiple-source view received an average usefulness rating of 4.6 out of 5. Our findings highlight the potential of context-rich, dialogic AI systems to improve media literacy and foster trust in digital information environments. We argue that future tools for misinformation mitigation should prioritize ethical design, explainability, and interactive engagement to empower users in a post-truth era.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density, asymmetry and citation dynamics in scientific literature</title>
<link>https://arxiv.org/abs/2506.23366</link>
<guid>https://arxiv.org/abs/2506.23366</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific behavior, similarity, citation rate, document embeddings, Bayesian regression <br />
<br />
Summary: 
The study explores the relationship between the similarity of a scientific paper to previous research and its citation rate. Two metrics, density and asymmetry, are introduced to measure a publication's semantic neighborhood. Density, representing the local geometry of a publication's semantic neighborhood, shows a small but consistent effect on citation count, improving prediction models. However, asymmetry does not yield predictive power for citation rates. The research surveyed approximately 53,000 publications across multiple disciplines and document embeddings, using a Bayesian hierarchical regression approach. The findings suggest that the density of surrounding scientific literature provides valuable insights into a paper's potential impact. The study presents a scalable framework for linking document embeddings to scientometric outcomes and raises questions about the role of semantic similarity in shaping scientific reward structures. <br /><br /> <div>
arXiv:2506.23366v1 Announce Type: cross 
Abstract: Scientific behavior is often characterized by a tension between building upon established knowledge and introducing novel ideas. Here, we investigate whether this tension is reflected in the relationship between the similarity of a scientific paper to previous research and its eventual citation rate. To operationalize similarity to previous research, we introduce two complementary metrics to characterize the local geometry of a publication's semantic neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed number of previously-published papers and the minimum distance enclosing those papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as the average directional difference between a paper and its nearest neighbors. We tested the predictive relationship between these two metrics and its subsequent citation rate using a Bayesian hierarchical regression approach, surveying $\sim 53,000$ publications across nine academic disciplines and five different document embeddings. While the individual effects of $\rho$ on citation count are small and variable, incorporating density-based predictors consistently improves out-of-sample prediction when added to baseline models. These results suggest that the density of a paper's surrounding scientific literature may carry modest but informative signals about its eventual impact. Meanwhile, we find no evidence that publication asymmetry improves model predictions of citation rates. Our work provides a scalable framework for linking document embeddings to scientometric outcomes and highlights new questions regarding the role that semantic similarity plays in shaping the dynamics of scientific reward.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.23469</link>
<guid>https://arxiv.org/abs/2506.23469</guid>
<content:encoded><![CDATA[
<div> Detection of graph anomalies, TripleAD framework, attribute anomalies, structural anomalies, mixed anomalies <br />
<br />
Summary: The article discusses the development of the TripleAD framework for graph anomaly detection, focusing on attribute, structural, and mixed anomalies. The framework includes three estimation modules: multiscale attribute estimation for node interactions, link-enhanced structure estimation for isolated nodes, and attribute-mixed curvature for mixed anomalies. The mutual distillation strategy encourages collaboration between the three modules. The framework aims to address the tug-of-war problem faced by existing unsupervised approaches by effectively identifying different types of anomalies without interference. Experimental results demonstrate the superior performance of TripleAD compared to strong baselines. <div>
arXiv:2506.23469v1 Announce Type: cross 
Abstract: Graph anomaly detection is critical in domains such as healthcare and economics, where identifying deviations can prevent substantial losses. Existing unsupervised approaches strive to learn a single model capable of detecting both attribute and structural anomalies. However, they confront the tug-of-war problem between two distinct types of anomalies, resulting in suboptimal performance. This work presents TripleAD, a mutual distillation-based triple-channel graph anomaly detection framework. It includes three estimation modules to identify the attribute, structural, and mixed anomalies while mitigating the interference between different types of anomalies. In the first channel, we design a multiscale attribute estimation module to capture extensive node interactions and ameliorate the over-smoothing issue. To better identify structural anomalies, we introduce a link-enhanced structure estimation module in the second channel that facilitates information flow to topologically isolated nodes. The third channel is powered by an attribute-mixed curvature, a new indicator that encapsulates both attribute and structural information for discriminating mixed anomalies. Moreover, a mutual distillation strategy is introduced to encourage communication and collaboration between the three channels. Extensive experiments demonstrate the effectiveness of the proposed TripleAD model against strong baselines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breadth, Depth, and Flux of Course-Prerequisite Networks</title>
<link>https://arxiv.org/abs/2506.23510</link>
<guid>https://arxiv.org/abs/2506.23510</guid>
<content:encoded><![CDATA[
<div> Keywords: Course-prerequisite networks, academic curricula, global measures, topological stratification, GitHub repository

Summary:
Course-prerequisite networks (CPNs) are DAGs representing academic curricula, enabling analysis for course importance, advising improvement, curriculum design guidance, graduation time distribution analysis, and knowledge flow quantification. Previous CPN analyses focused on micro- and meso-scale properties, lacking macro-scale measures. This study introduces three new global CPN measures - breadth, depth, and flux - invariant under transitive reduction and based on topological stratification concept. These measures facilitate macro-scale CPN comparison. Numerical illustration using real and synthetic CPNs from three universities demonstrates the utility of the new measures. The CPN data analyzed are publicly accessible in a GitHub repository. <div>
arXiv:2506.23510v1 Announce Type: cross 
Abstract: Course-prerequisite networks (CPNs) are directed acyclic graphs that model complex academic curricula by representing courses as nodes and dependencies between them as directed links. These networks are indispensable tools for visualizing, studying, and understanding curricula. For example, CPNs can be used to detect important courses, improve advising, guide curriculum design, analyze graduation time distributions, and quantify the strength of knowledge flow between different university departments. However, most CPN analyses to date have focused only on micro- and meso-scale properties. To fill this gap, we define and study three new global CPN measures: breadth, depth, and flux. All three measures are invariant under transitive reduction and are based on the concept of topological stratification, which generalizes topological ordering in directed acyclic graphs. These measures can be used for macro-scale comparison of different CPNs. We illustrate the new measures numerically by applying them to three real and synthetic CPNs from three universities: the Cyprus University of Technology, the California Institute of Technology, and Johns Hopkins University. The CPN data analyzed in this paper are publicly available in a GitHub repository.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Are the Antidote to Walled Gardens</title>
<link>https://arxiv.org/abs/2506.23978</link>
<guid>https://arxiv.org/abs/2506.23978</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet, interoperability, agents, AI, data exchange <br />
Summary: 
The article discusses the shift towards universal interoperability in digital services facilitated by AI-based agents. It points out that while the Internet's core infrastructure was designed to be universal, the application layer is currently dominated by closed, proprietary platforms. The use of agents can automate data format translation and interaction with various interfaces, making interoperability easier and more affordable. This development challenges monopolistic behaviors and promotes data portability, leading to increased user freedom and competitive markets. However, it also presents new security risks and technical challenges that need to be addressed. The authors argue that the ML community should embrace universal interoperability while developing frameworks to mitigate potential downsides and ensure robust security measures are in place. By seizing this opportunity, AI can play a crucial role in fostering an open and interoperable digital ecosystem while safeguarding user privacy and data security. 
<br /><br />Summary: <div>
arXiv:2506.23978v1 Announce Type: cross 
Abstract: While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Human Judgment in Community Notes with LLMs</title>
<link>https://arxiv.org/abs/2506.24118</link>
<guid>https://arxiv.org/abs/2506.24118</guid>
<content:encoded><![CDATA[
<div> open ecosystem, Community Notes, human raters, Reinforcement Learning, feedback <br />
<br />
Summary: This paper proposes a new paradigm for Community Notes in the LLM era, advocating for an open ecosystem where both humans and Language Model Models (LLMs) can contribute notes. The ultimate decision on which notes are deemed helpful is left to humans, maintaining trust and legitimacy. The community of diverse human raters plays a crucial role as evaluators and arbiters of helpful content. The feedback collected from this diverse community can be utilized to enhance LLMs' ability to generate accurate and unbiased notes through Reinforcement Learning from Community Feedback (RLCF). This bi-directional system allows LLMs to assist humans in delivering context efficiently, while human feedback contributes to improving LLM performance. The paper outlines the functionality of this system, its advantages, potential risks, challenges, and proposes a research agenda to address these challenges and maximize the benefits of this collaborative approach. <br /> <div>
arXiv:2506.24118v1 Announce Type: cross 
Abstract: This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is driving the conversation? Analysing the nodality of British MPs and journalists on social media</title>
<link>https://arxiv.org/abs/2402.08765</link>
<guid>https://arxiv.org/abs/2402.08765</guid>
<content:encoded><![CDATA[
<div> policy actors, influence, social media, nodality, discourse network  
Summary:  
- The study explores factors influencing influence in political conversations on social media platforms.  
- Nodality, a concept in political science, is used to measure actors' capacity to exchange information in discourse networks.  
- Influence on Twitter in the UK is driven by active nodality (level of engagement) and inherent nodality (institutional position).  
- MPs and accredited journalists were studied on four policy topics to compare their influence.  
- Findings reveal that influence is transferable across topics depending on the actor's nodality levels.   <div>
arXiv:2402.08765v3 Announce Type: replace 
Abstract: With the rise of social media, political conversations now take place in more diffuse environments. In this context, it is not always clear why some actors, more than others, have greater influence on how discussions are shaped. To investigate the factors behind such influence, we build on nodality, a concept in political science which describes the capacity of an actor to exchange information within discourse networks. This concept goes beyond traditional network metrics that describe the position of an actor in the network to include exogenous drivers of influence (e.g. factors relating to organisational hierarchies). We study online discourse on Twitter (now X) in the UK to measure the relative nodality of two sets of policy actors - Members of Parliament (MPs) and accredited journalists - on four policy topics. We find that influence on the platform is driven by two key factors: (i) active nodality, derived from the actor's level of topic-related engagement, and (ii) inherent nodality, which is independent of the platform discourse and reflects the actor's institutional position. These findings significantly further our understanding of the origins of influence on social media platforms and suggest in which contexts influence is transferable across topics.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coupled Friedkin-Johnsen Model of Popularity Dynamics in Social Media</title>
<link>https://arxiv.org/abs/2503.15170</link>
<guid>https://arxiv.org/abs/2503.15170</guid>
<content:encoded><![CDATA[
<div> influencers, popularity dynamics, social media, social influence, recommendations <br />
<br />Summary: 
The study focuses on understanding the dynamics of popularity in social media through a complex system that incorporates social influence and popularity-based recommendations. Building on the Friedkin-Johnsen model, the researchers introduce a discrete-time dynamical system that accounts for influencers competing for popularity. Their model examines how social influence, past popularity, and content quality interact to determine an influencer's popularity over time. Through numerical examples, the study explores the asymptotic behavior of the model, shedding light on the intricate relationship between different factors influencing an influencer's popularity on social media. <div>
arXiv:2503.15170v2 Announce Type: replace 
Abstract: Popularity dynamics in social media depend on a complex interplay of social influence between users and popularity-based recommendations that are provided by the platforms. In this work, we introduce a discrete-time dynamical system to model the evolution of popularity on social media. Our model generalizes the well-known Friedkin-Johnsen model to a set of influencers vying for popularity. We study the asymptotic behavior of this model and illustrate it with numerical examples. Our results highlight the interplay of social influence, past popularity, and content quality in determining the popularity of influencers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines</title>
<link>https://arxiv.org/abs/2206.00535</link>
<guid>https://arxiv.org/abs/2206.00535</guid>
<content:encoded><![CDATA[
<div> deepfakes, misinformation, detection models, Artifact Attention module, Deepfake Caricatures <br />
Summary: <br />
This article addresses the issue of deepfakes contributing to online misinformation by developing a framework that enhances human detection of deepfake videos. The researchers introduce the Artifact Attention module, which utilizes human responses to create attention maps highlighting video artifacts, leading to the creation of "Deepfake Caricatures." These visual indicators significantly improve human detection of deepfakes, regardless of video presentation times and user engagement levels. Additionally, a deepfake detection model incorporating the Artifact Attention module is introduced to enhance accuracy and robustness. The study showcases the effectiveness of a human-centric approach in designing methods to mitigate the impact of deepfakes. <div>
arXiv:2206.00535v4 Announce Type: replace-cross 
Abstract: Deepfakes can fuel online misinformation. As deepfakes get harder to recognize with the naked eye, human users become more reliant on deepfake detection models to help them decide whether a video is real or fake. Currently, models yield a prediction for a video's authenticity, but do not integrate a method for alerting a human user. We introduce a framework for amplifying artifacts in deepfake videos to make them more detectable by people. We propose a novel, semi-supervised Artifact Attention module, which is trained on human responses to create attention maps that highlight video artifacts, and magnify them to create a novel visual indicator we call "Deepfake Caricatures". In a user study, we demonstrate that Caricatures greatly increase human detection, across video presentation times and user engagement levels. We also introduce a deepfake detection model that incorporates the Artifact Attention module to increase its accuracy and robustness. Overall, we demonstrate the success of a human-centered approach to designing deepfake mitigation methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Contrastive Learning with Low-Rank Regularization and Low-Rank Attention for Noisy Node Classification</title>
<link>https://arxiv.org/abs/2402.09600</link>
<guid>https://arxiv.org/abs/2402.09600</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, contrastive learning, low-rank regularization, transductive learning, node classification 

Summary: 
Graph Neural Networks (GNNs) have shown strong performance in node representation learning but can be affected by noise in real-world graph data. To address this issue, a new method called Graph Contrastive Learning with Low-Rank Regularization (GCL-LRR) is proposed. It follows a two-stage transductive learning approach for node classification, combining prototypical contrastive learning with low-rank regularization. The method is inspired by the Low Frequency Property of graph data and labels, and is supported by theoretical bounds for transductive learning. An enhanced model, GCL-LR-Attention, is introduced by incorporating an LR-Attention layer to improve performance. Empirical evaluations on benchmark datasets demonstrate the effectiveness and robustness of both GCL-LRR and GCL-LR-Attention in learning meaningful node representations. The code is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2402.09600v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in learning node representations and have shown strong performance in tasks such as node classification. However, recent findings indicate that the presence of noise in real-world graph data can substantially impair the effectiveness of GNNs. To address this challenge, we introduce a robust and innovative node representation learning method named Graph Contrastive Learning with Low-Rank Regularization, or GCL-LRR, which follows a two-stage transductive learning framework for node classification. In the first stage, the GCL-LRR encoder is optimized through prototypical contrastive learning while incorporating a low-rank regularization objective. In the second stage, the representations generated by GCL-LRR are employed by a linear transductive classifier to predict the labels of unlabeled nodes within the graph. Our GCL-LRR is inspired by the Low Frequency Property (LFP) of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank regularization in transductive learning, which is also supported by strong empirical results. To further enhance the performance of GCL-LRR, we present an improved model named GCL-LR-Attention, which incorporates a novel LR-Attention layer into GCL-LRR. GCL-LR-Attention reduces the kernel complexity of GCL-LRR and contributes to a tighter generalization bound, leading to improved performance. Extensive evaluations on standard benchmark datasets evidence the effectiveness and robustness of both GCL-LRR and GCL-LR-Attention in learning meaningful node representations. The code is available at https://github.com/Statistical-Deep-Learning/GCL-LR-Attention.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Z-REx: Human-Interpretable GNN Explanations for Real Estate Recommendations</title>
<link>https://arxiv.org/abs/2503.18001</link>
<guid>https://arxiv.org/abs/2503.18001</guid>
<content:encoded><![CDATA[
<div> explanation framework, GNN, Link Prediction tasks, Z-REx, real-estate dataset

Summary:
Z-REx is introduced as a GNN explanation framework specifically designed for heterogeneous link prediction tasks. It utilizes structural and attribute perturbation techniques to identify critical substructures and important features, incorporating domain-specific knowledge to reduce the search space. In experiments using a real-world real-estate dataset from Zillow Group, Z-REx is shown to generate contextually relevant and human-interpretable explanations for the ZiGNN recommendation engine. Comparing against State-of-The-Art (SOTA) GNN explainers, Z-REx demonstrated a 61% improvement in the Fidelity metric, producing superior human-interpretable explanations. <div>
arXiv:2503.18001v2 Announce Type: replace-cross 
Abstract: Transparency and interpretability are crucial for enhancing customer confidence and user engagement, especially when dealing with black-box Machine Learning (ML)-based recommendation systems. Modern recommendation systems leverage Graph Neural Network (GNN) due to their ability to produce high-quality recommendations in terms of both relevance and diversity. Therefore, the explainability of GNN is especially important for Link Prediction (LP) tasks since recommending relevant items can be viewed as predicting links between users and items. GNN explainability has been a well-studied field, but existing methods primarily focus on node or graph-level tasks, leaving a gap in LP explanation techniques. This work introduces Z-REx, a GNN explanation framework designed explicitly for heterogeneous link prediction tasks. Z-REx utilizes structural and attribute perturbation to identify critical substructures and important features while reducing the search space by leveraging domain-specific knowledge. In our experimentation, we show the efficacy of Z-REx in generating contextually relevant and human-interpretable explanations for ZiGNN, a GNN-based recommendation engine, using a real-world real-estate dataset from Zillow Group, Inc. We compare against State-of-The-Art (SOTA) GNN explainers to show Z-REx outperforms them by 61% in the Fidelity metric by producing superior human-interpretable explanations.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Institutional Gender Inequality in Contemporary Visual Art</title>
<link>https://arxiv.org/abs/2506.22103</link>
<guid>https://arxiv.org/abs/2506.22103</guid>
<content:encoded><![CDATA[
<div> artist, gender equity, exhibitions, auctions, institutional forces
Summary: 
The study examines the under-representation of women in the visual art world by analyzing the exhibition history and auction sales of over 65,000 contemporary artists across 20,000 institutions. The research reveals gender disparities in artist populations, exhibition opportunities, and auction success. Only 24% of institutions strive for gender parity in representation, with the majority being gender-neutral. The study finds that as institutional prestige increases, the likelihood of gender inequality also rises. By defining artist's co-exhibition gender, the research highlights the impact of institutional forces on an artist's success. Surprisingly, the artist's co-exhibition gender has a stronger correlation with auction market access than the artist's own gender. This study sheds light on the systemic issues contributing to the persistent gender imbalance in the art world. 
<br /><br />Summary: <div>
arXiv:2506.22103v1 Announce Type: new 
Abstract: From disparities in the number of exhibiting artists to auction opportunities, there is evidence of women's under-representation in visual art. Here we explore the exhibition history and auction sales of 65,768 contemporary artists in 20,389 institutions, revealing gender differences in the artist population, exhibitions and auctions. We distinguish between two criteria for gender equity: gender-neutrality, when artists have gender-independent access to exhibition opportunities, and gender-balanced, that strives for gender parity in representation, finding that 58\% of institutions are gender-neutral but only 24\% are gender-balanced, and that the fraction of man-overrepresented institutions increases with institutional prestige. We define artist's co-exhibition gender to capture the gender inequality of the institutions that an artist exhibits. Finally, we use logistic regression to predict an artist's access to the auction market, finding that co-exhibition gender has a stronger correlation with success than the artist's gender. These results help unveil and quantify the institutional forces that relate to the persistent gender imbalance in the art world.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment</title>
<link>https://arxiv.org/abs/2506.22165</link>
<guid>https://arxiv.org/abs/2506.22165</guid>
<content:encoded><![CDATA[
<div> Graph-Neural-Network, legal norms, court decisions, link prediction, citation graph
<br />Summary:
The study proposes a Graph-Neural-Network (GNN) model for identifying Case-Law and Case-Case citations in legal systems. By integrating semantic and topological information, the model improves prediction accuracy by 3.1 points in average precision and 8.5 points in data sparsity. The adapted relational graph convolutions allow for the topological integration of semantic meta-information, enhancing prediction performance over time and in fully inductive scenarios. Jointly learning and predicting case and norm citations results in a synergistic effect, boosting case citation prediction efficiency by up to 4.7 points. This approach provides practitioners, novices, and legal AI systems with enhanced access to relevant data for informed appraisals and judgments. <div>
arXiv:2506.22165v1 Announce Type: new 
Abstract: Legal systems heavily rely on cross-citations of legal norms as well as previous court decisions. Practitioners, novices and legal AI systems need access to these relevant data to inform appraisals and judgments. We propose a Graph-Neural-Network (GNN) link prediction model that can identify Case-Law and Case-Case citations with high proficiency through fusion of semantic and topological information. We introduce adapted relational graph convolutions operating on an extended and enriched version of the original citation graph that allow the topological integration of semantic meta-information. This further improves prediction by 3.1 points of average precision and by 8.5 points in data sparsity as well as showing robust performance over time and in challenging fully inductive prediction. Jointly learning and predicting case and norm citations achieves a large synergistic effect that improves case citation prediction by up to 4.7 points, at almost doubled efficiency.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decade of News Forum Interactions: Threaded Conversations, Signed Votes, and Topical Tags</title>
<link>https://arxiv.org/abs/2506.22224</link>
<guid>https://arxiv.org/abs/2506.22224</guid>
<content:encoded><![CDATA[
<div> Keywords: online platform, user activity, user comments, editorial topics, privacy

Summary: 
The article introduces a new dataset from the online platform of DerStandard, a major Austrian newspaper, covering user activity over a ten-year period. The dataset includes over 75 million user comments, 400 million votes, and metadata on articles and user interactions. It offers structured conversation threads, up- and downvotes of comments, and editorial topic labels for analysis of online discourse while protecting user privacy through anonymization. Persistent identifiers are hashed for privacy, and raw comment texts are not shared publicly. Pre-computed vector representations derived from an embedding model are released instead. This dataset facilitates research on discussion dynamics, network structures, and semantic analyses in German, serving as a valuable resource for computational social science and related fields.<br /><br />Summary: The dataset from DerStandard's online platform covers user activity over ten years, including millions of user comments, votes, and metadata. It provides structured conversation threads, up- and downvotes, and topic labels for analysis while protecting user privacy. Persistent identifiers are anonymized, and pre-computed vector representations are released. The dataset supports research on discussion dynamics and network structures in German, benefiting computational social science and related fields. <div>
arXiv:2506.22224v1 Announce Type: new 
Abstract: We present a large-scale, longitudinal dataset capturing user activity on the online platform of DerStandard, a major Austrian newspaper. The dataset spans ten years (2013-2022) and includes over 75 million user comments, more than 400 million votes, and detailed metadata on articles and user interactions. It provides structured conversation threads, explicit up- and downvotes of user comments and editorial topic labels, enabling rich analyses of online discourse while preserving user privacy. To ensure this privacy, all persistent identifiers are anonymized using salted hash functions, and the raw comment texts are not publicly shared. Instead, we release pre-computed vector representations derived from a state-of-the-art embedding model. The dataset supports research on discussion dynamics, network structures, and semantic analyses in the mid-resourced language German, offering a reusable resource across computational social science and related fields.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Network Topology on the Equilibria of Influence-Opinion Games</title>
<link>https://arxiv.org/abs/2506.22293</link>
<guid>https://arxiv.org/abs/2506.22293</guid>
<content:encoded><![CDATA[
<div> social networks, public opinion, adversarial influence, network connectivity, opinion evolution 

Summary: 
- The study examines the impact of network connectivity on influencing public opinion in online social networks, focusing on the game between two players shaping discourse.
- Opinion evolution is modeled as a competitive influence-propagation process, where players inject messages that diffuse and update opinions based on exposure.
- The bi-level model accounts for viral-media correlation effects, providing a more comprehensive understanding of opinion dynamics.
- An algorithm based on linear-quadratic regulators is proposed to solve the high-dimensional game, approximating local feedback Stackelberg strategies for players with limited cognitive abilities.
- By analyzing synthetic networks and real Facebook data, the research identifies structural characteristics that enhance a network's resilience to adversarial influence, offering insights for designing more resilient social networks. 

<br /><br />Summary: <div>
arXiv:2506.22293v1 Announce Type: new 
Abstract: Online social networks exert a powerful influence on public opinion. Adversaries weaponize these networks to manipulate discourse, underscoring the need for more resilient social networks. To this end, we investigate the impact of network connectivity on Stackelberg equilibria in a two-player game to shape public opinion. We model opinion evolution as a repeated competitive influence-propagation process. Players iteratively inject \textit{messages} that diffuse until reaching a steady state, modeling the dispersion of two competing messages. Opinions then update according to the discounted sum of exposure to the messages. This bi-level model captures viral-media correlation effects omitted by standard opinion-dynamics models. To solve the resulting high-dimensional game, we propose a scalable, iterative algorithm based on linear-quadratic regulators that approximates local feedback Stackelberg strategies for players with limited cognition. We analyze how the network topology shapes equilibrium outcomes through experiments on synthetic networks and real Facebook data. Our results identify structural characteristics that improve a network's resilience to adversarial influence, guiding the design of more resilient social networks.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit</title>
<link>https://arxiv.org/abs/2506.21620</link>
<guid>https://arxiv.org/abs/2506.21620</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Reddit, US Presidential Election, Political Alignment, Discourse Manipulation

Summary: 
The study evaluates the performance of Large Language Models (LLMs), specifically GPT-4, in replicating user-generated content on Reddit during the 2016 US Presidential election. Three experiments were conducted to generate comments mimicking real or artificial partisan users. GPT-4 was successful in creating realistic comments aligned with the community's candidate preference, potentially influencing consensus. However, dissent was less common. The comments were analyzed for political alignment, sentiment, and linguistic features, revealing that real and artificial comments are distinguishable in a semantically embedded space but appear indistinguishable through manual inspection. The findings suggest that LLMs could be used to infiltrate online discussions, manipulate political debates, and shape political narratives, highlighting the broader implications of AI-driven discourse manipulation. 

<br /><br />Summary: <div>
arXiv:2506.21620v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The relationship between episcopal genealogy and ideology in the Roman Catholic Church</title>
<link>https://arxiv.org/abs/2506.22108</link>
<guid>https://arxiv.org/abs/2506.22108</guid>
<content:encoded><![CDATA[
<div> genealogical proximity, doctrinal alignment, hierarchical structures, ideological coherence, episcopal lineage

Summary: 
The study examines how hierarchical structures within the Roman Catholic Church influence the ideological orientation of its leadership. A dataset of 245 living cardinals is analyzed, focusing on genealogical proximity and doctrinal alignment. Through the analysis of episcopal lineage, recurring patterns of lineage are identified, such as shared consecrators. The study finds that cardinals linked by specific genealogical motifs, especially those sharing the same principal consecrator, are more likely to exhibit ideological similarity. The influence of Pope John Paul II is shown to persist through bishops he consecrated, who hold more conservative views. The research highlights the role of hierarchical mentorship in shaping ideological coherence within large religious institutions and suggests that institutional lineages play a significant role in the transmission and consolidation of doctrinal positions over time.<br /><br />Summary: <div>
arXiv:2506.22108v1 Announce Type: cross 
Abstract: In this study we investigate how hierarchical structures within the Roman Catholic Church shape the ideological orientation of its leadership. The full episcopal genealogy dataset comprises over 35,000 bishops, each typically consecrated by one principal consecrator and two co-consecrators, forming a dense and historically continuous directed network of episcopal lineage. Within this broader structure, we focus on a dataset of 245 living cardinals to examine whether genealogical proximity correlates with doctrinal alignment on a broad set of theological and sociopolitical issues. We identify motifs that capture recurring patterns of lineage, such as shared consecrators or co-consecrators. In parallel, we apply natural language processing techniques to extract each cardinal's publicly stated positions on ten salient topics, including LGBTQIA+ rights, women's roles in the Church, liturgy, bioethics, priestly celibacy, and migration. Our results show that cardinals linked by specific genealogical motifs, particularly those who share the same principal consecrator, are significantly more likely to exhibit ideological similarity. We find that the influence of pope John Paul II persists through the bishops he consecrated, who demonstrate systematically more conservative views than their peers. These findings underscore the role of hierarchical mentorship in shaping ideological coherence within large-scale religious institutions. Our contribution offers quantitative evidence that institutional lineages, beyond individual background factors, may have an impact on the transmission and consolidation of doctrinal positions over time.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harder, shorter, sharper, forward: A comparison of women's and men's elite football gameplay (2020-2025)</title>
<link>https://arxiv.org/abs/2506.22119</link>
<guid>https://arxiv.org/abs/2506.22119</guid>
<content:encoded><![CDATA[
<div> Keywords: football, match dynamics, passing volume, collective play, professional

Summary: 
- The study examines the evolution of elite football between 2020 and 2025 by analyzing event-level records from top-tier men's and women's leagues in five countries.
- Over this period, there was a significant increase in passing volume, pass accuracy, and the percentage of passes made under pressure, with the most substantial changes seen in women's competitions.
- Pitch-passing networks were used to track ball movement among pitch regions, revealing changes in normalized outreach and average shortest path lengths, indicating a wider ball circulation on the field.
- The findings suggest a sustained intensification of collective play in contemporary professional football, as evidenced by the trend towards higher passing quality and increased ball circulation across teams.
- This study provides systematic evidence for the pace and form of change in elite football, highlighting the evolving dynamics of the game in recent years.<br /><br />Summary: <div>
arXiv:2506.22119v1 Announce Type: cross 
Abstract: Elite football is believed to have evolved in recent years, but systematic evidence for the pace and form of that change is sparse. Drawing on event-level records for 13,067 matches in ten top-tier men's and women's leagues in England, Spain, Germany, Italy, and the United States (2020-2025), we quantify match dynamics with two views: conventional performance statistics and pitch-passing networks that track ball movement among a grid of pitch (field) regions. Between 2020 and 2025, average passing volume, pass accuracy, and the percent of passes made under pressure all rose. In general, the largest year-on-year changes occurred in women's competitions. Network measures offer alternative but complementary perspectives on the changing gameplay in recent years, normalized outreach in the pitch passing networks decreased, while the average shortest path lengths increased, indicating a wider ball circulation. Together, these indicators point to a sustained intensification of collective play across contemporary professional football.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterization Of Diseases In Temporal Comorbidity Networks</title>
<link>https://arxiv.org/abs/2506.22136</link>
<guid>https://arxiv.org/abs/2506.22136</guid>
<content:encoded><![CDATA[
<div> Age groups, comorbidity networks, disease clustering, disease prevalence, mortality <br />
<br />
Summary: 
The study analyzes comorbidity networks derived from a large dataset of Austrian hospital stays to understand how diseases cluster and evolve across different age groups. The networks become denser with age, revealing three dominant age-related components. The relationship between disease prevalence and degree highlights conditions disproportionately connected to other diseases, such as iron deficiency anemia in children and nicotine dependence in adults. High-mortality bridging diseases, like cancers and chronic kidney disease, are identified using betweenness centrality. Specific diseases with high connectivity relative to their prevalence are pinpointed, emphasizing the importance of targeting age-specific, network-central conditions with high mortality for prevention and integrated care. <div>
arXiv:2506.22136v1 Announce Type: cross 
Abstract: Comorbidity networks, which capture disease-disease co-occurrence usually based on electronic health records, reveal structured patterns in how diseases cluster and progress across individuals. However, how these networks evolve across different age groups and how this evolution relates to properties like disease prevalence and mortality remains understudied. To address these issues, we used publicly available comorbidity networks extracted from a comprehensive dataset of 45 million Austrian hospital stays from 1997 to 2014, covering 8.9 million patients. These networks grow and become denser with age. We identified groups of diseases that exhibit similar patterns of structural centrality throughout the lifespan, revealing three dominant age-related components with peaks in early childhood, midlife, and late life. To uncover the drivers of this structural change, we examined the relationship between prevalence and degree. This allowed us to identify conditions that were disproportionately connected to other diseases. Using betweenness centrality in combination with mortality data, we further identified high-mortality bridging diseases. Several diseases show high connectivity relative to their prevalence, such as iron deficiency anemia (D50) in children, nicotine dependence (F17), and lipoprotein metabolism disorders (E78) in adults. We also highlight structurally central diseases with high mortality that emerge at different life stages, including cancers (C group), liver cirrhosis (K74), subarachnoid hemorrhage (I60), and chronic kidney disease (N18). These findings underscore the importance of targeting age-specific, network-central conditions with high mortality for prevention and integrated care.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link Prediction with Physics-Inspired Graph Neural Networks</title>
<link>https://arxiv.org/abs/2402.14802</link>
<guid>https://arxiv.org/abs/2402.14802</guid>
<content:encoded><![CDATA[
<div> GRAFF-LP, link prediction, heterophily, GNNs, physics biases<br />
Summary:<br />
The article introduces GRAFF-LP, an extension of GNNs for link prediction on heterophilic datasets. The existing link prediction models focused on node classification struggle with heterophily, where adjacent nodes have different labels. GRAFF-LP addresses this issue by incorporating physics biases in the architecture to effectively discriminate between existing and non-existing edges. The new readout function inspired by physics not only enhances GRAFF-LP's performance but also improves other baseline models. The study reveals that simple GNNs do not face greater difficulty in predicting heterophilic links compared to homophilic ones. This suggests the need for heterophily measures tailored for link prediction, distinct from those used in node classification.<br /> 
Summary: <div>
arXiv:2402.14802v3 Announce Type: replace-cross 
Abstract: The message-passing mechanism underlying Graph Neural Networks (GNNs) is not naturally suited for heterophilic datasets, where adjacent nodes often have different labels. Most solutions to this problem remain confined to the task of node classification. In this article, we focus on the valuable task of link prediction under heterophily, an interesting problem for recommendation systems, social network analysis, and other applications. GNNs like GRAFF have improved node classification under heterophily by incorporating physics biases in the architecture. Similarly, we propose GRAFF-LP, an extension of GRAFF for link prediction. We show that GRAFF-LP effectively discriminates existing from non-existing edges by learning implicitly to separate the edge gradients. Based on this information, we propose a new readout function inspired by physics. Remarkably, this new function not only enhances the performance of GRAFF-LP but also improves that of other baseline models, leading us to reconsider how every link prediction experiment has been conducted so far. Finally, we provide evidence that even simple GNNs did not experience greater difficulty in predicting heterophilic links compared to homophilic ones. This leads us to believe in the necessity for heterophily measures specifically tailored for link prediction, distinct from those used in node classification. The code and appendix are available at https://github.com/difra100/Link_Prediction_with_PIGNN_IJCNN.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets</title>
<link>https://arxiv.org/abs/2502.02379</link>
<guid>https://arxiv.org/abs/2502.02379</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, graph learning, evaluation, Rings framework, dataset quality <br />
Summary: <br />
The article discusses the importance of benchmark datasets in graph learning and the need for better evaluation practices. It addresses questions about what makes a good graph-learning dataset and how dataset quality can be evaluated. The Rings framework is introduced as a mode-perturbation framework to assess the quality of graph-learning datasets through dataset ablations. Two measures, performance separability, and mode complementarity, are proposed to evaluate dataset capacity. Extensive experiments on graph-level tasks demonstrate the utility of the framework for dataset evaluation and provide recommendations for improving the evaluation of graph-learning methods. This work opens new research directions in data-centric graph learning and contributes to the systematic evaluation of evaluations. <div>
arXiv:2502.02379v2 Announce Type: replace-cross 
Abstract: Benchmark datasets have proved pivotal to the success of graph learning, and good benchmark datasets are crucial to guide the development of the field. Recent research has highlighted problems with graph-learning datasets and benchmarking practices -- revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches. Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning? Our work addresses these questions. As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation. Hence, we start from first principles. Observing that graph-learning datasets uniquely combine two modes -- graph structure and node features --, we introduce Rings, a flexible and extensible mode-perturbation framework to assess the quality of graph-learning datasets based on dataset ablations -- i.e., quantifying differences between the original dataset and its perturbed representations. Within this framework, we propose two measures -- performance separability and mode complementarity -- as evaluation tools, each assessing the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods from a distinct angle. We demonstrate the utility of our framework for dataset evaluation via extensive experiments on graph-level tasks and derive actionable recommendations for improving the evaluation of graph-learning methods. Our work opens new research directions in data-centric graph learning, and it constitutes a step toward the systematic evaluation of evaluations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing validated standards for Home and Work location Detection</title>
<link>https://arxiv.org/abs/2506.20679</link>
<guid>https://arxiv.org/abs/2506.20679</guid>
<content:encoded><![CDATA[
<div> Algorithm, Home and work detection, Mobility data, Methodological standards, Urban mobility research

Summary:
HoWDe is a robust algorithm designed for accurately identifying home and work locations from smartphone location data. It addresses methodological challenges such as missing data and varying data quality, achieving high detection accuracies for home and work locations. The algorithm's performance is consistent across countries and demographic groups, providing a standardized framework for home-work detection. Parameter choices in the algorithm impact the trade-off between accuracy and user retention, influencing downstream applications like employment estimation and commuting pattern analysis. HoWDe supports privacy-preserving sharing of mobility data by facilitating in-house pre-processing through a transparent and validated pipeline. These methodological standards established by HoWDe enhance the robustness, scalability, and reproducibility of mobility research at both individual and urban scales. 

<br /><br />Summary: <div>
arXiv:2506.20679v1 Announce Type: new 
Abstract: Smartphone location data have transformed urban mobility research, providing unprecedented insights into how people navigate and interact in cities. However, leveraging location data at scale presents methodological challenges. Accurately identifying individuals' home and work locations is critical for a range of applications, including commuting analysis, unemployment estimation, and urban accessibility studies. Despite their widespread use, home-work detection methods lack a standardized framework that accounts for differing data quality and that is validated against ground-truth observations. This limits the comparability and reproducibility of results across studies and datasets. In this paper, we present HoWDe, a robust algorithm for identifying home and work locations from mobility data, explicitly designed to handle missing data and varying data quality across individuals. Using two unique ground-truth datasets comprising over 5100 individuals from more than 80 countries, HoWDe achieves home and work detection accuracies of up to 97% and 88%, respectively, with consistent performance across countries and demographic groups. We examine how parameter choices shape the trade-off between accuracy and user retention, and demonstrate how these methodological decisions influence downstream applications such as employment estimation and commuting pattern analysis. By supporting in-house pre-processing through a transparent and validated pipeline, HoWDe also facilitates the sharing of privacy-preserving mobility data. Together, our tools and findings establish methodological standards that support more robust, scalable, and reproducible mobility research at both individual and urban scales.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Malicious earworms and useful memes, how the far-right surfs on TikTok audio trends</title>
<link>https://arxiv.org/abs/2506.20695</link>
<guid>https://arxiv.org/abs/2506.20695</guid>
<content:encoded><![CDATA[
<div> TikTok, meme-making, remix, sound, extremist formations <br />
Summary: 
The research focuses on TikTok as a platform for meme creation and explores its role in right-wing extremist formations related to the 2024 German state elections. TikTok's sound infrastructure allows for the spread of xenophobic content through user-generated sounds that can easily blend in with benign meme trends. While hateful songs are not eligible for personalized feeds, they remain online and intersect with popular meme trends, making them visible in search results. TikTok has implemented measures to detect harmful challenges and illegal content, but the platform continues to face scrutiny for allowing extremist content to thrive. The analysis highlights the importance of sound in facilitating the dissemination of extremist content on TikTok, showcasing how cloaking practices benefit from the platform's sound infrastructure. <div>
arXiv:2506.20695v1 Announce Type: new 
Abstract: With its features of remix, TikTok is the designated platform for meme-making and dissemination. Creative combinations of video, emoji, and filters allow for an endless stream of memes and trends animated by sound. The platform has focused its moderation on upholding physical safety, hence investing in the detection of harmful challenges. In response to the DSA, TikTok implemented opt-outs for personalized feeds and features allowing users to report illegal content. At the same time, the platform remains subject to scrutiny. Centering on the role of sound and its intersections with ambiguous memes, the presented research probed right-wing extremist formations relating to the 2024 German state elections. The analysis evidences how the TikTok sound infrastructure affords a sustained presence of xenophobic content, often cloaked through vernacular modes of communication. These cloaking practices benefit from a sound infrastructure that affords the ongoing posting of user-generated sounds that instantly spread through the use-this-sound button. Importantly, these sounds are often not clearly recognizable as networkers of extremist content. Songs that do contain hateful lyrics are not eligible for personalized feeds, however, they remain online where they profit from intersecting with benign meme trends, rendering them visible in search results.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)</title>
<link>https://arxiv.org/abs/2506.20971</link>
<guid>https://arxiv.org/abs/2506.20971</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence in Education, Knowledge Co-occurrence Network Analysis, Intelligent Tutoring Systems, Generative Artificial Intelligence, Human-AI Collaboration 

Summary: 
The study examines 2,398 research articles in Artificial Intelligence in Education (AIED) from 2020 to 2024. The analysis reveals a strong technical focus in AIED research, with prevalent themes like intelligent tutoring systems and learning analytics, alongside emerging interests in large language models and generative artificial intelligence. Four emerging frontiers in AIED are identified: LLMs, GenAI, multimodal learning analytics, and human-AI collaboration. Research in GenAI covers personalization, self-regulated learning, feedback, assessment, motivation, and ethics. The shift towards human-centered AI for education is evident in current research interests. This study presents a comprehensive overview of AIED's evolution in the GenAI era, offering insights into future research directions and educational practices. 

<br /><br />Summary: <div>
arXiv:2506.20971v1 Announce Type: new 
Abstract: In this study, we analyze 2,398 research articles published between 2020 and 2024 across eight core venues related to the field of Artificial Intelligence in Education (AIED). Using a three-step knowledge co-occurrence network analysis, we analyze the knowledge structure of the field, the evolving knowledge clusters, and the emerging frontiers. Our findings reveal that AIED research remains strongly technically focused, with sustained themes such as intelligent tutoring systems, learning analytics, and natural language processing, alongside rising interest in large language models (LLMs) and generative artificial intelligence (GenAI). By tracking the bridging keywords over the past five years, we identify four emerging frontiers in AIED--LLMs, GenAI, multimodal learning analytics, and human-AI collaboration. The current research interests in GenAI are centered around GAI-driven personalization, self-regulated learning, feedback, assessment, motivation, and ethics.The key research interests and emerging frontiers in AIED reflect a growing emphasis on co-adaptive, human-centered AI for education. This study provides the first large-scale field-level mapping of AIED's transformation in the GenAI era and sheds light on the future research development and educational practices.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2506.20980</link>
<guid>https://arxiv.org/abs/2506.20980</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous graphs, node heterophily, contrastive learning, heterogeneity, multi-relational bipartite subgraphs 

Summary:
The paper introduces a novel framework called RASH, designed to tackle the challenge of capturing node heterophily in heterogeneous graphs. RASH explicitly models high-order semantics of heterogeneous interactions and separates homophilic and heterophilic patterns by utilizing dual heterogeneous hypergraphs. By dynamically constructing homophilic and heterophilic graphs based on relation importance, RASH effectively resolves the issues of heterogeneity and heterophily in heterogeneous graphs. The framework also incorporates a multi-relation contrastive loss to align heterogeneous and homophilic/heterophilic views and maximize mutual information. Experimental results on benchmark datasets demonstrate the effectiveness of RASH across various downstream tasks. The code for RASH is available on GitHub, providing a practical implementation of the proposed framework. <div>
arXiv:2506.20980v1 Announce Type: new 
Abstract: Real-world networks usually have a property of node heterophily, that is, the connected nodes usually have different features or different labels. This heterophily issue has been extensively studied in homogeneous graphs but remains under-explored in heterogeneous graphs, where there are multiple types of nodes and edges. Capturing node heterophily in heterogeneous graphs is very challenging since both node/edge heterogeneity and node heterophily should be carefully taken into consideration. Existing methods typically convert heterogeneous graphs into homogeneous ones to learn node heterophily, which will inevitably lose the potential heterophily conveyed by heterogeneous relations. To bridge this gap, we propose Relation-Aware Separation of Homophily and Heterophily (RASH), a novel contrastive learning framework that explicitly models high-order semantics of heterogeneous interactions and adaptively separates homophilic and heterophilic patterns. Particularly, RASH introduces dual heterogeneous hypergraphs to encode multi-relational bipartite subgraphs and dynamically constructs homophilic graphs and heterophilic graphs based on relation importance. A multi-relation contrastive loss is designed to align heterogeneous and homophilic/heterophilic views by maximizing mutual information. In this way, RASH simultaneously resolves the challenges of heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on benchmark datasets demonstrate the effectiveness of RASH across various downstream tasks. The code is available at: https://github.com/zhengziyu77/RASH.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty</title>
<link>https://arxiv.org/abs/2506.20992</link>
<guid>https://arxiv.org/abs/2506.20992</guid>
<content:encoded><![CDATA[
<div> game theory, blockchain systems, protocol mutability, cooperative mining behavior, institutional noise 

Summary:
This paper presents a game-theoretic model analyzing the impact of protocol mutability on cooperative mining behavior in blockchain systems. It demonstrates that even slight uncertainty in institutional rules can lead to strategic deviation and a shift towards short-term decision-making. Stable protocols support long-term investment and equilibrium strategies, while mutable ones promote short-termism and higher discount rates, potentially leading to collapse in cooperation. Simulation results reveal zones where rational mining transitions to extractive or arbitrage behavior due to institutional noise. The study suggests that protocol design should be viewed as a fundamental economic constraint rather than a variable, emphasizing the importance of rule stability for sustainable cooperation in decentralized systems. <div>
arXiv:2506.20992v1 Announce Type: cross 
Abstract: This paper develops a formal game-theoretic model to examine how protocol mutability disrupts cooperative mining behaviour in blockchain systems. Using a repeated game framework with stochastic rule shocks, we show that even minor uncertainty in institutional rules increases time preference and induces strategic deviation. Fixed-rule environments support long-term investment and stable equilibrium strategies; in contrast, mutable protocols lead to short-termism, higher discounting, and collapse of coordinated engagement. Simulation results identify instability zones in the parameter space where rational mining gives way to extractive or arbitrage conduct. These findings support an Austrian economic interpretation: calculability requires rule stability. Institutional noise undermines the informational basis for productive action. We conclude that protocol design must be treated as a constitutional economic constraint, not a discretionary variable, if sustainable cooperation is to emerge in decentralised systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution and determinants of firm-level systemic risk in local production networks</title>
<link>https://arxiv.org/abs/2506.21426</link>
<guid>https://arxiv.org/abs/2506.21426</guid>
<content:encoded><![CDATA[
<div> supply chain, systemic risk, production network, COVID-19, firm disruption
<br />
Summary: 
This study examines firm-level systemic risk in the Hungarian production network from 2015 to 2022, focusing on the impact of the COVID-19 pandemic. Using a heuristic maximum entropy null model, the researchers find that firms with high systemic risk undergo structural changes during the pandemic, with those facilitating economic exchanges becoming key players. The study reveals that firms' adaptive behavior leads to a more resilient economy post-pandemic. The international trade volume of firms is a significant predictor of systemic risk, but the effects of imports and exports on local systemic risk differ due to supply and demand channels. The findings highlight the importance of considering firms' abilities to react to crises and rewire supply links in assessing production network resilience. 
<br /> <div>
arXiv:2506.21426v1 Announce Type: cross 
Abstract: Recent crises like the COVID-19 pandemic and geopolitical tensions have exposed vulnerabilities and caused disruptions of supply chains, leading to product shortages, increased costs, and economic instability. This has prompted increasing efforts to assess systemic risk, namely the effects of firm disruptions on entire economies. However, the ability of firms to react to crises by rewiring their supply links has been largely overlooked, limiting our understanding of production networks resilience. Here we study dynamics and determinants of firm-level systemic risk in the Hungarian production network from 2015 to 2022. We use as benchmark a heuristic maximum entropy null model that generates an ensemble of production networks at equilibrium, by preserving the total input (demand) and output (supply) of each firm at the sector level. We show that the fairly stable set of firms with highest systemic risk undergoes a structural change during COVID-19, as those enabling economic exchanges become key players in the economy -- a result which is not reproduced by the null model. Although the empirical systemic risk aligns well with the null value until the onset of the pandemic, it becomes significantly smaller afterwards as the adaptive behavior of firms leads to a more resilient economy. Furthermore, firms' international trade volume (being a subject of disruption) becomes a significant predictor of their systemic risk. However, international links cannot provide an unequivocal explanation for the observed trends, as imports and exports have opposing effects on local systemic risk through the supply and demand channels.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A systematic comparison of measures for publishing k-anonymous social network data</title>
<link>https://arxiv.org/abs/2407.02290</link>
<guid>https://arxiv.org/abs/2407.02290</guid>
<content:encoded><![CDATA[
<div> anonymity measures, k-anonymity, social network data, privacy, attacker scenario  
Summary:  
This paper discusses the challenges of sharing or publishing social network data while preserving individual privacy. It specifically focuses on k-anonymity, a widely used privacy concept, and compares different anonymity measures to determine their effectiveness in protecting against attacker scenarios. The study includes a theoretical analysis and experimental investigations on real-world network datasets to evaluate the impact of different measures on anonymity levels, privacy trade-offs, and computational costs. The findings suggest that the choice of anonymity measure significantly influences the level of anonymity provided and the resources required. Surprisingly, the most effective measure for protecting against attackers considers a broader node vicinity with minimal structural information, leading to lower computational costs. This research provides valuable insights for researchers and practitioners in selecting appropriate anonymity measures for sharing social network data under privacy constraints. <div>
arXiv:2407.02290v2 Announce Type: replace 
Abstract: Sharing or publishing social network data while accounting for privacy of individuals is a difficult task due to the interconnectedness of nodes in networks. A key question in k-anonymity, a widely studied notion of privacy, is how to measure the anonymity of an individual, as this determines the attacker scenarios one protects against. In this paper, we systematically compare the most prominent anonymity measures from the literature in terms of the completeness and reach of the structural information they take into account. We present a theoretical characterization and a distance-parametrized strictness ordering of the existing measures for k-anonymity in networks. In addition, we conduct empirical experiments on a wide range of real-world network datasets with up to millions of edges. Our findings reveal that the choice of the measure significantly impacts the measured level of anonymity and hence the effectiveness of the corresponding attacker scenario, the privacy vs. utility trade-off, and computational cost. Surprisingly, we find that the anonymity measure representing the most effective attacker scenario considers a greater node vicinity yet utilizes only limited structural information and therewith minimal computational resources. Overall, the insights provided in this work offer researchers and practitioners practical guidance for selecting appropriate anonymity measures when sharing or publishing social network data under privacy constraints.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
<link>https://arxiv.org/abs/2110.11856</link>
<guid>https://arxiv.org/abs/2110.11856</guid>
<content:encoded><![CDATA[
<div> Sparse networks, $\beta$-model, estimation algorithms, $\ell_2$-penalized MLE algorithm, COVID-19

Summary:
This paper introduces improvements to the $\beta$-model for modeling large and sparse networks driven by degree heterogeneity. Existing estimation algorithms for the $\beta$-model are limited in scalability, but the proposed $\ell_2$-penalized MLE algorithm can handle sparse networks with millions of nodes efficiently. The paper also establishes rate-optimal error bounds and asymptotic normality results for $\beta$-models, even under weaker network sparsity assumptions. Application of the method to large COVID-19 network data sets yielded meaningful results. The advancements presented in this paper address the challenges faced in modeling large and sparse networks with degree heterogeneity, providing practical solutions for complex network analysis. 

<br /><br />Summary: <div>
arXiv:2110.11856v5 Announce Type: replace-cross 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets discovered meaningful results.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BotHash: Efficient and Training-Free Bot Detection Through Approximate Nearest Neighbor</title>
<link>https://arxiv.org/abs/2506.20503</link>
<guid>https://arxiv.org/abs/2506.20503</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Social Networks, Social bots, Large Language Models, BotHash, Bot detection <br />
Summary: <br />
Online Social Networks (OSNs) are vital platforms for content consumption but face challenges with the accuracy of shared information due to the spread of disinformation. Social bots, particularly problematic, spread misinformation and are made more complex by Large Language Models (LLMs). BotHash is introduced as a training-free solution for social bot detection by simplifying user representation for effective detection using approximate nearest-neighbor search. This approach offers advantages such as independence from training, robust performance with minimal data, and early detection capabilities, even with the use of state-of-the-art LLMs. BotHash shows promising results across various datasets, showcasing its effectiveness in differentiating between human and bot accounts. <div>
arXiv:2506.20503v1 Announce Type: new 
Abstract: Online Social Networks (OSNs) are a cornerstone in modern society, serving as platforms for diverse content consumption by millions of users each day. However, the challenge of ensuring the accuracy of information shared on these platforms remains significant, especially with the widespread dissemination of disinformation. Social bots -- automated accounts designed to mimic human behavior, frequently spreading misinformation -- represent one of the critical problems of OSNs. The advent of Large Language Models (LLMs) has further complicated bot behaviors, making detection increasingly difficult. This paper presents BotHash, an innovative, training-free approach to social bot detection. BotHash leverages a simplified user representation that enables approximate nearest-neighbor search to detect bots, avoiding the complexities of Deep-Learning model training and large dataset creation. We demonstrate that BotHash effectively differentiates between human and bot accounts, even when state-of-the-art LLMs are employed to generate posts' content. BotHash offers several advantages over existing methods, including its independence from a training phase, robust performance with minimal ground-truth data, and early detection capabilities, showing promising results across various datasets.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Aware Design: Modulating Valence, Arousal, and Dominance in Communication via Design</title>
<link>https://arxiv.org/abs/2502.16038</link>
<guid>https://arxiv.org/abs/2502.16038</guid>
<content:encoded><![CDATA[
<div> Emotionally-aware design, VAD model, comprehension, memory, behavior, multimodal design space

Summary: This review explores the concept of emotion-aware design, which emphasizes the importance of emotional awareness in communication. By utilizing the valence-arousal-dominance (VAD) model of affect, the framework examines how emotions impact information processing. Emotional responses play a significant role in perception, memory, and behavior, highlighting the need for strategic regulation of emotional dimensions in communication design. The proposed multimodal design space incorporating text, visuals, audio, and interaction offers a practical way to enhance communication efficacy by harnessing emotional dynamics. By linking emotional modulation to cognitive outcomes, this review provides a foundation for creating emotionally resonant communication in various fields including education, health, media, and public discourse.<br /><br />Summary: <div>
arXiv:2502.16038v3 Announce Type: replace 
Abstract: In an era of emotionally saturated digital media and information overload, effective communication demands more than clarity and accuracy-it requires emotional awareness. This review introduces the paradigm of emotion-aware design, a framework grounded in the valence-arousal-dominance (VAD) model of affect, which systematically examines how emotional modulation shapes comprehension, memory, and behavior. Drawing on insights from psychology, neuroscience, communication, and design, we show that emotional responses significantly influence how information is perceived, retained, and shared. We further propose a multimodal design space-encompassing text, visuals, audio, and interaction-that enables strategic regulation of emotional dimensions to enhance communication efficacy. By linking emotional dynamics to cognitive outcomes and practical design strategies, this review offers both a conceptual foundation and an applied roadmap for designing emotionally resonant communication across domains such as education, health, media, and public discourse.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block-corrected Modularity for Community Detection</title>
<link>https://arxiv.org/abs/2502.20083</link>
<guid>https://arxiv.org/abs/2502.20083</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, community structure, modularity, spectral method, citation networks

Summary: 
The study introduces a block-corrected modularity measure designed to uncover community structures hidden by unknown node attributes in complex networks. Through analytical analysis and experiments on synthetic models, the proposed modularity successfully identifies underlying community structures masked by known attributes. Spectral methods and fine-tuning algorithms inspired by the Louvain method are developed to maximize the modularity. These approaches are shown to outperform methods utilizing different null models in revealing hidden community structures. The methodology is applied to real-world citation networks constructed from OpenAlex data, where it effectively corrects for temporal citation patterns to identify significant community structures driven by unknown attributes. The study highlights the importance of considering unknown node attributes in determining community structures within complex networks. 

<br /><br />Summary: <div>
arXiv:2502.20083v2 Announce Type: replace-cross 
Abstract: Unknown node attributes in complex networks may introduce community structures that are important to distinguish from those driven by known attributes. We propose a block-corrected modularity that discounts given block structures present in the network to reveal communities masked by them. We show analytically how the proposed modularity finds the community structure driven by an unknown attribute in a simple network model. Further, we observe that the block-corrected modularity finds the underlying community structure on a number of simple synthetic network models while methods using different null models fail. We develop an efficient spectral method as well as two Louvain-inspired fine-tuning algorithms to maximize the proposed modularity and demonstrate their performance on several synthetic network models. Finally, we assess our methodology on various real-world citation networks built using the OpenAlex data by correcting for the temporal citation patterns.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Diffusion Structures of Heterogeneous Network Cascade</title>
<link>https://arxiv.org/abs/2506.19142</link>
<guid>https://arxiv.org/abs/2506.19142</guid>
<content:encoded><![CDATA[
<div> Keywords: network cascade, diffusion networks, double mixture directed graph model, multi-layer, Granger causality

Summary: 
The article introduces a novel double mixture directed graph model for inferring multi-layer diffusion networks from cascade data. This model captures the heterogeneity present in real-world cascades by representing cascade pathways as a mixture of diffusion networks across different layers. It also imposes layer-specific structural constraints to capture complementary cascading patterns across the population. The advantage of the model is its convex formulation, providing statistical and computational guarantees for diffusion network estimates. Through simulation studies, the model's performance in recovering diverse diffusion structures is demonstrated. Lastly, the method is applied to analyze research topic cascades among U.S. universities in the social sciences, revealing the underlying diffusion networks of research topic propagation among institutions.<br /><br />Summary: <div>
arXiv:2506.19142v1 Announce Type: new 
Abstract: Network cascade refers to diffusion processes in which outcome changes within part of an interconnected population trigger a sequence of changes across the entire network. These cascades are governed by underlying diffusion networks, which are often latent. Inferring such networks is critical for understanding cascade pathways, uncovering Granger causality of interaction mechanisms among individuals, and enabling tasks such as forecasting or maximizing information propagation. In this project, we propose a novel double mixture directed graph model for inferring multi-layer diffusion networks from cascade data. The proposed model represents cascade pathways as a mixture of diffusion networks across different layers, effectively capturing the strong heterogeneity present in real-world cascades. Additionally, the model imposes layer-specific structural constraints, enabling diffusion networks at different layers to capture complementary cascading patterns at the population level. A key advantage of our model is its convex formulation, which allows us to establish both statistical and computational guarantees for the resulting diffusion network estimates. We conduct extensive simulation studies to demonstrate the model's performance in recovering diverse diffusion structures. Finally, we apply the proposed method to analyze cascades of research topics in the social sciences across U.S. universities, revealing the underlying diffusion networks of research topic propagation among institutions.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection in Energy Networks based on Energy Self-Sufficiency and Dynamic Flexibility Activation</title>
<link>https://arxiv.org/abs/2506.19412</link>
<guid>https://arxiv.org/abs/2506.19412</guid>
<content:encoded><![CDATA[
<div> Keywords: energy transition, microgrids, community detection, energy modularity, decentralized energy systems

Summary:
The article addresses the need for novel control structures in the global energy transition towards distributed resources like microgrids and energy communities. It introduces the concept of energy modularity as a metric to evaluate community partitions based on energy self-sufficiency and flexibility. A scalable community detection algorithm based on the Louvain method is proposed to maximize energy modularity, either through linear programming or simulation-based approaches. The algorithm is validated on a benchmark grid, showcasing its effectiveness in identifying optimal energy clusters in decentralized energy systems. <div>
arXiv:2506.19412v1 Announce Type: new 
Abstract: The global energy transition towards distributed, smaller-scale resources, such as decentralized generation and flexible assets like storage and shiftable loads, demands novel control structures aligned with the emerging network architectures. These architectures consist of interconnected, self-contained clusters, commonly called microgrids or energy communities. These clusters aim to optimize collective self-sufficiency by prioritizing local energy use or operating independently during wide-area blackouts. This study addresses the challenge of defining optimal clusters, framed as a community detection problem. A novel metric, termed energy modularity, is proposed to evaluate community partitions by quantifying energy self-sufficiency within clusters while incorporating the influence of flexible resources. Furthermore, a highly scalable community detection algorithm to maximize energy modularity based on the Louvain method is presented. Therefore, energy modularity is calculated using linear programming or a more efficient simulation-based approach. The algorithm is validated on an exemplary benchmark grid, demonstrating its effectiveness in identifying optimal energy clusters for modern decentralized energy systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanders in Models of Social Networks</title>
<link>https://arxiv.org/abs/2506.19485</link>
<guid>https://arxiv.org/abs/2506.19485</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, geometric random graphs, expanders, minimum-component distance, algorithms

Summary:
Minimum-Component Distance Geometric Inhomogeneous Random Graphs (GIRGs) model social networks where vertices are positioned in a latent geometric space, forming edges based on similarity in some dimensions. Unlike the traditional model where similarity in all dimensions is required to form edges, this model allows edges to form if vertices align in some dimensions. For dimensions greater than or equal to 2, these GIRGs exhibit strong expanding properties, with the induced subgraph of vertices having expected degree at least $(\log n)^C$ forming an expander. The expansion factor of the resulting subgraph is shown to be significant except for sets already occupying a constant fraction of the vertices. This finding is important as it indicates that algorithms and mixing processes operate efficiently on these expander graphs. <div>
arXiv:2506.19485v1 Announce Type: new 
Abstract: A common model for social networks are Geometric Inhomogeneous Random Graphs (GIRGs), in which vertices draw a random position in some latent geometric space, and the probability of two vertices forming an edge depends on their geometric distance. The geometry may be modelled in two ways: either two points are defined as close if they are similar in all dimensions, or they are defined as close if they are similar in some dimensions. The first option is mathematically more natural since it can be described by metrics. However, the second option is arguably the better model for social networks if the different dimensions represent features like profession, kinship, or interests. In such cases, nodes already form bonds if they align in some, but not all dimensions. For the first option, it is known that the resulting networks are poor expanders. We study the second option in the form of Minimum-Component Distance GIRGs, and find that those behave the opposite way for dimension $d\ge 2$, and that they have strong expanding properties. More precisely, for a suitable constant $C>0$, the subgraph induced by vertices of (expected) degree at least $(\log n)^C$ forms an expander. Moreover, we study how the expansion factor of the resulting subgraph depends on the choice of $C$, and show that this expansion factor is $\omega(1)$ except for sets that already take up a constant fraction of the vertices. This has far-reaching consequences, since many algorithms and mixing processes are fast on expander graphs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Properties and Expressivity of Linear Geometric Centralities</title>
<link>https://arxiv.org/abs/2506.19670</link>
<guid>https://arxiv.org/abs/2506.19670</guid>
<content:encoded><![CDATA[
<div> linear centralities, geometric centralities, shortest-path distances, centrality measures, axiomatic approach

Summary:
Linear (geometric) centralities, based on shortest-path distances, are studied in their entirety in this paper. These centralities, defined as a linear transformation of the distance-count vector, are analyzed using an axiomatic approach to understand their expressivity in distinguishing nodes in a graph. The study also investigates the ability of linear centralities to induce different rankings of nodes in a graph. A linear programming formulation, utilizing Farkas' lemma, is employed to determine the number of distinct rankings possible with linear centralities. This approach has potential applications in various scenarios, particularly in adversarial settings. The research provides insights into the nature and capabilities of linear centralities, shedding light on their significance in ranking nodes within a graph. 

<br /><br />Summary: <div>
arXiv:2506.19670v1 Announce Type: new 
Abstract: Centrality indices are used to rank the nodes of a graph by importance: this is a common need in many concrete situations (social networks, citation networks, web graphs, for instance) and it was discussed many times in sociology, psychology, mathematics and computer science, giving rise to a whole zoo of definitions of centrality. Although they differ widely in nature, many centrality measures are based on shortest-path distances: such centralities are often referred to as geometric. Geometric centralities can use the shortest-path-length information in many different ways, but most of the existing geometric centralities can be defined as a linear transformation of the distance-count vector (that is, the vector containing, for every index t, the number of nodes at distance t).
  In this paper we study this class of centralities, that we call linear (geometric) centralities, in their full generality. In particular, we look at them in the light of the axiomatic approach, and we study their expressivity: we show to what extent linear centralities can be used to distinguish between nodes in a graph, and how many different rankings of nodes can be induced by linear centralities on a given graph. The latter problem (which has a number of possible applications, especially in an adversarial setting) is solved by means of a linear programming formulation, which is based on Farkas' lemma, and is interesting in its own right.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Use and Emergent Cooperation</title>
<link>https://arxiv.org/abs/2506.18920</link>
<guid>https://arxiv.org/abs/2506.18920</guid>
<content:encoded><![CDATA[
<div> agents, tribes, communication signals, cooperation, culture <br />
Summary: <br />
The study explores how autonomous agents in tribes develop a shared behavioral system through communication signals, similar to a culture, using the NEC-DAC system with neural networks. By examining different social structures and communication strategies, the research reveals that a culture of cooperation significantly impacts the tribe's performance. The study also highlights how signals facilitate culture emergence and transmission across agent generations. Coordination of behavior and signaling within individual agents' neural networks is shown to be beneficial. The self-organization of culture within tribes and the influence of varying communication strategies on fitness and cooperation are key focuses of the research. <div>
arXiv:2506.18920v1 Announce Type: cross 
Abstract: In this work, we investigate how autonomous agents, organized into tribes, learn to use communication signals to coordinate their activities and enhance their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture - Distributed Autonomous Communicators) system, where each agent is equipped with its own neural network for decision-making, we demonstrate how these agents develop a shared behavioral system -- akin to a culture -- through learning and signalling. Our research focuses on the self-organization of culture within these tribes of agents and how varying communication strategies impact their fitness and cooperation. By analyzing different social structures, such as authority hierarchies, we show that the culture of cooperation significantly influences the tribe's performance. Furthermore, we explore how signals not only facilitate the emergence of culture but also enable its transmission across generations of agents. Additionally, we examine the benefits of coordinating behavior and signaling within individual agents' neural networks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Hatred: Efficient Multimodal Detection of Hatemongers</title>
<link>https://arxiv.org/abs/2506.19603</link>
<guid>https://arxiv.org/abs/2506.19603</guid>
<content:encoded><![CDATA[
<div> hate speech, online discourse, user level, multimodal approach, detection

Summary:
- Automatic detection of online hate speech is crucial for improving online discourse and understanding hate as a social phenomenon.
- Focusing on the user level along with their texts, social activity, and network can enhance the detection of hate-mongers.
- A multimodal aggregative approach was used to detect hate-mongers by considering texts, user activity, and user network.
- The method was evaluated on Twitter, Gab, and Parler datasets, showing improved detection compared to previous methods.
- The approach can enhance classification of coded messages, dog-whistling, racial gas-lighting, and inform intervention measures.
- The multimodal approach performed well across diverse content platforms, large datasets, and networks.

<br /><br />Summary: <div>
arXiv:2506.19603v1 Announce Type: cross 
Abstract: Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases. Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Graph Databases</title>
<link>https://arxiv.org/abs/2506.19661</link>
<guid>https://arxiv.org/abs/2506.19661</guid>
<content:encoded><![CDATA[
<div> Graph databases, higher-order interactions, HO-GDBs, polyadic modeling, graph neural networks <br />
Summary: 
This study introduces higher-order graph databases (HO-GDBs) to support higher-order interactions in graph analytics beyond first-order relations. The system uses lifting and lowering paradigms to extend traditional graph databases seamlessly. Theoretical analysis ensures correctness, scalability, and ACID compliance for OLTP and OLAP queries. A lightweight, modular, and parallelizable HO-GDB prototype is implemented offering native support for hypergraphs, node-tuples, and subgraphs under a unified API. The prototype scales to large HO workloads and demonstrates improved performance for analytical tasks, such as enhancing graph neural networks within a GDB. The system maintains low latency, high query throughput, and generalizes both ACID-compliant and eventually consistent systems. <div>
arXiv:2506.19661v1 Announce Type: cross 
Abstract: Recent advances in graph databases (GDBs) have been driving interest in large-scale analytics, yet current systems fail to support higher-order (HO) interactions beyond first-order (one-hop) relations, which are crucial for tasks such as subgraph counting, polyadic modeling, and HO graph learning. We address this by introducing a new class of systems, higher-order graph databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and OLAP queries, ensuring correctness, scalability, and ACID compliance. We implement a lightweight, modular, and parallelizable HO-GDB prototype that offers native support for hypergraphs, node-tuples, subgraphs, and other HO structures under a unified API. The prototype scales to large HO OLTP & OLAP workloads and shows how HO improves analytical tasks, for example enhancing accuracy of graph neural networks within a GDB by 44%. Our work ensures low latency and high query throughput, and generalizes both ACID-compliant and eventually consistent systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words as Trigger Points in Social Media Discussions: A Large-Scale Case Study about UK Politics on Reddit</title>
<link>https://arxiv.org/abs/2405.10213</link>
<guid>https://arxiv.org/abs/2405.10213</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, political debates, trigger points, online communication, affective polarisation

Summary: 
Trigger points, moments that challenge individuals' societal norms and beliefs, have been found to elicit strong negative emotional responses in online political debates. Through the analysis of over 100 million comments on Reddit related to trigger words identified in UK politics, researchers found that trigger words increased user engagement and animosity, leading to more negativity, hate speech, and controversial comments. This study suggests that trigger points play a significant role in shaping online communication dynamics, particularly in political discussions. By introducing the concept of trigger points to computational studies of online communication, researchers can gain insights into affective polarisation, online deliberation, and how citizens engage in political discourse on social media platforms.<br /><br />Summary: <div>
arXiv:2405.10213v3 Announce Type: replace 
Abstract: Political debates on social media sometimes flare up. From that moment on, users engage much more with one another; their communication is also more emotional and polarised. While it has been difficult to grasp such moments with computational methods, we suggest that trigger points are a useful concept to understand and ultimately model such behaviour. Established in qualitative focus group interviews to understand political polarisation (Mau, Lux, and Westheuser 2023), trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals show strong and negative emotional responses when certain triggering words or topics are mentioned. Our paper finds that these trigger points also exist in online debates. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million comments from subreddits related to a set of words identified as trigger points in UK politics. Analysing the comments, we find that trigger words increase user engagement and animosity, i.e., more negativity, hate speech, and controversial comments. Introducing trigger points to computational studies of online communication, our findings are relevant to researchers interested in affective computing, online deliberation, and how citizens debate politics and society in light of affective polarisation.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Temporal Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2506.17312</link>
<guid>https://arxiv.org/abs/2506.17312</guid>
<content:encoded><![CDATA[
<div> Graph representation learning, heterogeneity, dynamics, complex networks, capturing high-order interactions<br />
<br />
Summary: 
The article discusses the importance of capturing high-order interactions in complex networks through graph representation learning (GRL) methods. Existing GRL techniques often focus on preserving low-order topology information and neglect higher-order group interaction relationships found in real-world networks. To address this limitation, the authors propose a formal definition of heterogeneous temporal hypergraphs and a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN). The HTHGN incorporates a hierarchical attention mechanism to facilitate temporal message-passing between heterogeneous nodes and hyperedges, allowing for a more comprehensive capture of high-order interactions in heterogeneous temporal graphs (HTGs). Additionally, the HTHGN employs contrastive learning to enhance the consistency between low-order correlated heterogeneous node pairs in HTGs. Experimental results on real-world datasets demonstrate the effectiveness of the proposed HTHGN in modeling high-order interactions and achieving significant performance improvements in HTGs. <br /><br />Summary: <div>
arXiv:2506.17312v1 Announce Type: new 
Abstract: Graph representation learning (GRL) has emerged as an effective technique for modeling graph-structured data. When modeling heterogeneity and dynamics in real-world complex networks, GRL methods designed for complex heterogeneous temporal graphs (HTGs) have been proposed and have achieved successful applications in various fields. However, most existing GRL methods mainly focus on preserving the low-order topology information while ignoring higher-order group interaction relationships, which are more consistent with real-world networks. In addition, most existing hypergraph methods can only model static homogeneous graphs, limiting their ability to model high-order interactions in HTGs. Therefore, to simultaneously enable the GRL model to capture high-order interaction relationships in HTGs, we first propose a formal definition of heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge construction algorithm that does not rely on additional information. Then, a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical attention mechanism module that simultaneously performs temporal message-passing between heterogeneous nodes and hyperedges to capture rich semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN performs contrastive learning by maximizing the consistency between low-order correlated heterogeneous node pairs on HTG to avoid the low-order structural ambiguity issue. Detailed experimental results on three real-world HTG datasets verify the effectiveness of the proposed HTHGN for modeling high-order interactions in HTGs and demonstrate significant performance improvements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A family of graph GOSPA metrics for graphs with different sizes</title>
<link>https://arxiv.org/abs/2506.17316</link>
<guid>https://arxiv.org/abs/2506.17316</guid>
<content:encoded><![CDATA[
<div> Graph metrics, distances, sizes, GOSPA metric, linear programming<br />
<br />
Summary: 
This paper introduces a family of graph metrics designed to measure distances between graphs of varying sizes. The proposed metric family builds upon the graph generalised optimal sub-pattern assignment (GOSPA) metric, while also incorporating additional penalties for edge mismatches. The metrics in the family retain the ability to penalize node attribute costs and unassigned nodes, similar to the original GOSPA metric. The paper demonstrates that these metrics can be approximately computed using linear programming techniques. Simulation experiments showcase the characteristics of the graph GOSPA metric family across different hyperparameter selections. Furthermore, real-world dataset experiments highlight the advantages of utilizing the proposed graph GOSPA metric family for classification tasks. <div>
arXiv:2506.17316v1 Announce Type: new 
Abstract: This paper proposes a family of graph metrics for measuring distances between graphs of different sizes. The proposed metric family defines a general form of the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also proved to satisfy the metric properties. Similarly to the graph GOSPA metric, the proposed graph GOSPA metric family also penalises the node attribute costs for assigned nodes between the two graphs, and the number of unassigned nodes. However, the proposed family of metrics provides more general penalties for edge mismatches than the graph GOSPA metric. This paper also shows that the graph GOSPA metric family can be approximately computed using linear programming. Simulation experiments are performed to illustrate the characteristics of the proposed graph GOSPA metric family with different choices of hyperparameters. The benefits of the proposed graph GOSPA metric family for classification tasks are also shown on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Iterative Graph Alignment Using Heat Diffusion</title>
<link>https://arxiv.org/abs/2506.17640</link>
<guid>https://arxiv.org/abs/2506.17640</guid>
<content:encoded><![CDATA[
<div> representation generation, heat diffusion, node alignment, UPGA, IterAlign

Summary: 
- The paper introduces IterAlign, a novel unsupervised plain graph alignment method that is parameter-free and efficient.
- IterAlign addresses the issue of biased node representations in existing UPGA methods by using a representation generation method based on heat diffusion.
- Two complementary node alignment strategies are employed in IterAlign to balance accuracy and efficiency across graphs of varying scales.
- Through an iterative process of representation generation and node alignment, IterAlign rectifies biases in node representations and refines the alignment process.
- Extensive experiments on three public benchmarks show that IterAlign outperforms state-of-the-art UPGA methods with lower computational overhead and is able to approach the theoretical accuracy upper bound in unsupervised plain graph alignment tasks. 

<br /><br />Summary: <div>
arXiv:2506.17640v1 Announce Type: new 
Abstract: Unsupervised plain graph alignment (UPGA) aims to align corresponding nodes across two graphs without any auxiliary information. Existing UPGA methods rely on structural consistency while neglecting the inherent structural differences in real-world graphs, leading to biased node representations. Moreover, their one-shot alignment strategies lack mechanisms to correct erroneous matches arising from inaccurate anchor seeds. To address these issues, this paper proposes IterAlign, a novel parameter-free and efficient UPGA method. First, a simple yet powerful representation generation method based on heat diffusion is introduced to capture multi-level structural characteristics, mitigating the over-reliance on structural consistency and generating stable node representations. Two complementary node alignment strategies are then adopted to balance alignment accuracy and efficiency across graphs of varying scales. By alternating between representation generation and node alignment, IterAlign iteratively rectifies biases in nodes representations and refines the alignment process, leading to superior and robust alignment performance. Extensive experiments on three public benchmarks demonstrate that the proposed IterAlign outperforms state-of-the-art UPGA approaches with a lower computational overhead, but also showcases the ability to approach the theoretical accuracy upper bound of unsupervised plain graph alignment task.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evolution of Complex Networks: A Reinforcement Learning Approach Applying Evolutionary Games to Community Structure</title>
<link>https://arxiv.org/abs/2506.17925</link>
<guid>https://arxiv.org/abs/2506.17925</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, individual birth-death, community structures, reinforcement learning, population dynamics

Summary: 
The article proposes a networked evolution model to study individual birth-death processes and community development within dynamic systems. This model incorporates reinforcement learning through games among individuals with varying lifespans. Through extensive experiments, the model demonstrates the evolution of cooperative behaviors and community structures within systems. The fitting of real-world populations and networks supports the practicality of the proposed model. Analyses reveal that exploitation rates, payoff parameters, learning rates, discount factors, and two-dimensional space dimensions play crucial roles in determining the emergence, speed, stability, and size of communities. Overall, the model offers a novel perspective on understanding real-world community development and provides a valuable framework for studying population dynamics behaviors.<br /><br />Summary: <div>
arXiv:2506.17925v1 Announce Type: new 
Abstract: Complex networks serve as abstract models for understanding real-world complex systems and provide frameworks for studying structured dynamical systems. This article addresses limitations in current studies on the exploration of individual birth-death and the development of community structures within dynamic systems. To bridge this gap, we propose a networked evolution model that includes the birth and death of individuals, incorporating reinforcement learning through games among individuals. Each individual has a lifespan following an arbitrary distribution, engages in games with network neighbors, selects actions using Q-learning in reinforcement learning, and moves within a two-dimensional space. The developed theories are validated through extensive experiments. Besides, we observe the evolution of cooperative behaviors and community structures in systems both with and without the birth-death process. The fitting of real-world populations and networks demonstrates the practicality of our model. Furthermore, comprehensive analyses of the model reveal that exploitation rates and payoff parameters determine the emergence of communities, learning rates affect the speed of community formation, discount factors influence stability, and two-dimensional space dimensions dictate community size. Our model offers a novel perspective on real-world community development and provides a valuable framework for studying population dynamics behaviors.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on False Information Detection: From A Perspective of Propagation on Social Networks</title>
<link>https://arxiv.org/abs/2506.18052</link>
<guid>https://arxiv.org/abs/2506.18052</guid>
<content:encoded><![CDATA[
<div> propagation, false information, detection techniques, misinformation, information dissemination
Summary: 
The paper provides a comprehensive review of false information detection techniques, focusing on the propagation characteristics of misinformation. It introduces a new taxonomy categorizing methods into homogeneous and heterogeneous propagation-based approaches. The paper includes problem formulation, dataset review, and state-of-the-art method summaries for each category. Promising future research directions identified include creating a unified benchmark suite, exploring diverse information modalities, and developing innovative rumor debunking tasks. The systematic organization of current techniques in this work offers a clear landscape of the research field, aiding researchers and practitioners in navigating the complexities and inspiring further advancements. <div>
arXiv:2506.18052v1 Announce Type: new 
Abstract: The proliferation of false information in the digital age has become a pressing concern, necessitating the development of effective and robust detection methods. This paper offers a comprehensive review of existing false information detection techniques, approached from a novel perspective that emphasizes the propagation characteristics of misinformation. We introduce a new taxonomy that categorizes these methods into homogeneous and heterogeneous propagation-based approaches, providing a deeper understanding of the varying scopes and complexities involved in information dissemination. For each category, we present a formal problem formulation, review commonly used datasets, and summarize state-of-the-art methods. Additionally, we identify several promising directions for future research, including the creation of a unified benchmark suite, exploration of diverse information modalities, and development of innovative rumor debunking tasks. By systematically organizing the vast array of current techniques, this work offers a clear overview of the research landscape, aiding researchers and practitioners in navigating this complex field and inspiring further advancements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving spreading dynamics and information flow in complex network reduction</title>
<link>https://arxiv.org/abs/2506.18641</link>
<guid>https://arxiv.org/abs/2506.18641</guid>
<content:encoded><![CDATA[
<div> Keywords: network reduction, epidemic spreading dynamics, information flow, subgraph extraction, optimization strategy

Summary: 
The paper introduces a novel network reduction framework based on subgraph extraction to preserve both structural and dynamical properties efficiently. The proposed method involves a degree centrality-driven node removal algorithm to selectively eliminate low-degree nodes and create a smaller subnetwork. An edge pruning algorithm is then used to adjust the edge density of the subnetwork while maintaining its average degree similar to the original network. Experimental results on various network types demonstrate that the approach can reduce network size by over 85% while retaining epidemic dynamics and information flow characteristics. This method shows promise in predicting the dynamic behavior of large-scale real-world networks. 

<br /><br />Summary: <div>
arXiv:2506.18641v1 Announce Type: new 
Abstract: Effectively preserving both the structural and dynamical properties during the reduction of complex networks remains a significant research topic. Existing network reduction methods based on renormalization group or sampling often face challenges such as high computational complexity and the loss of critical dynamic attributes. This paper proposes an efficient network reduction framework based on subgraph extraction, which accurately preserves epidemic spreading dynamics and information flow through a coordinated optimization strategy of node removal and edge pruning. Specifically, a degree centrality-driven node removal algorithm is adopted to preferentially remove low-degree nodes, thereby constructing a smaller-scale subnetwork. Subsequently, an edge pruning algorithm is designed to regulate the edge density of the subnetwork, ensuring that its average degree remains approximately consistent with that of the original network. Experimental results on Erd\"os-R\'enyi random graphs, Barab\'asi-Albert scale-free networks, and real-world social contact networks from various domains demonstrate that this proposed method can reduce the size of networks with heterogeneous structures by more than 85%, while preserving their epidemic dynamics and information flow. These findings provide valuable insights for predicting the dynamical behavior of large-scale real-world networks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioXplorer: An Interactive Tool for Topic and Network Analysis in Social Data</title>
<link>https://arxiv.org/abs/2506.18845</link>
<guid>https://arxiv.org/abs/2506.18845</guid>
<content:encoded><![CDATA[
<div> AI, natural language processing, social network analysis, Twitter, YouTube

Summary:
SocioXplorer is a new interactive tool designed for computational social science researchers to analyze topics and networks in social data from Twitter and YouTube. It combines artificial intelligence, natural language processing, and social network analysis, allowing for in-depth analysis of live datasets that can be regularly updated. This tool is an extension of the previous system, TwiXplorer, which was limited to analyzing archival Twitter data. SocioXplorer enhances this capability by adding the ability to analyze YouTube data and offers batch data processing features. The system is released under the Apache 2 license, providing researchers with a powerful tool for understanding social data on multiple platforms. <div>
arXiv:2506.18845v1 Announce Type: new 
Abstract: SocioXplorer is a powerful interactive tool that computational social science researchers can use to understand topics and networks in social data from Twitter (X) and YouTube. It integrates, among other things, artificial intelligence, natural language processing and social network analysis. It can be used with ``live" datasets that receive regular updates. SocioXplorer is an extension of a previous system called TwiXplorer, which was limited to the analysis of archival Twitter (X) data. SocioXplorer builds on this by adding the ability to analyse YouTube data, greater depth of analysis and batch data processing. We release it under the Apache 2 licence.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</title>
<link>https://arxiv.org/abs/2506.17851</link>
<guid>https://arxiv.org/abs/2506.17851</guid>
<content:encoded><![CDATA[
<div> Keywords: novelty, scientific progress, network science, interdisciplinary studies, citation counts

Summary:
- The study addresses the issue of current reward systems failing to recognize novel ideas in science.
- A triadic typology is introduced, categorizing novelty into Pioneers, Mavericks, and Vanguards based on how they introduce new ideas.
- Analysis of a dataset in philanthropy and nonprofit studies shows that different types of novelty are not uniformly rewarded.
- Pioneers lay the foundation but are often overlooked, Mavericks benefit from displacing prior focus, and Vanguards gain recognition by strengthening weak connections.
- Citation advantages vary for different types of novelty over time and as topics become more central in the field.

<br /><br />Summary: <div>
arXiv:2506.17851v1 Announce Type: cross 
Abstract: Scientific progress depends on novel ideas, but current reward systems often fail to recognize them. Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them. This study develops a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition. Drawing on network science and theories of discovery, we introduce a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak but promising connections. We apply this typology to a dataset of 41,623 articles in the interdisciplinary field of philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression. Results show that novelty is not uniformly rewarded. Pioneer efforts are foundational but often overlooked. Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus. Vanguard novelty is more likely to gain recognition when it strengthens weakly connected topics, but its citation advantage diminishes as those reinforced nodes become more central. To enable fair comparison across time and domains, we introduce a simulated baseline model. These findings improve the evaluation of innovations, affecting science policy, funding, and institutional assessment practices.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
<link>https://arxiv.org/abs/2506.18154</link>
<guid>https://arxiv.org/abs/2506.18154</guid>
<content:encoded><![CDATA[
<div> Taylor's Power Law, universality, ecosystems, statistical distributions, future research directions
Summary: 
Taylor's Power Law (TPL), first discovered in 1961, correlates population mean abundances and variances using a power function. Two main prongs of exploration have emerged: mathematical/statistical and ecological mechanisms. Over the past six decades, TPL studies have evolved through three distinct periods, covering various themes including population spatial aggregation, statistical distributions, and complex networks. The significance of TPL research lies in its practical applications in various fields such as agriculture, epidemiology, and finance, as well as its theoretical implications related to phase transitions and scale invariance. Three future research directions have been identified: fostering interactions between the two prongs, heterogeneity measurement, and exploration in evolutionary contexts. Reciprocal interactions between mathematical/statistical and ecological perspectives are crucial for advancing TPL research in ecosystems. <div>
arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Total Variation Distance Estimators for Changepoint Detection in News Data</title>
<link>https://arxiv.org/abs/2506.18764</link>
<guid>https://arxiv.org/abs/2506.18764</guid>
<content:encoded><![CDATA[
<div> Keywords: changepoint detection, neural networks, public discourse, news data, societal dynamics

Summary: 
This paper presents a novel method for changepoint detection in public discourse using neural networks. The approach, based on the learning-by-confusion scheme, involves training classifiers to distinguish between articles from different time periods, with classification accuracy indicating shifts in content distribution. The method successfully identifies significant events such as 9/11, the COVID-19 pandemic, and presidential elections in real-world data from The Guardian newspaper. It requires minimal domain knowledge, autonomously detects changes in public discourse, and provides a quantitative measure of content change. This approach has implications for journalism, policy analysis, and crisis monitoring. <div>
arXiv:2506.18764v1 Announce Type: cross 
Abstract: Detecting when public discourse shifts in response to major events is crucial for understanding societal dynamics. Real-world data is high-dimensional, sparse, and noisy, making changepoint detection in this domain a challenging endeavor. In this paper, we leverage neural networks for changepoint detection in news data, introducing a method based on the so-called learning-by-confusion scheme, which was originally developed for detecting phase transitions in physical systems. We train classifiers to distinguish between articles from different time periods. The resulting classification accuracy is used to estimate the total variation distance between underlying content distributions, where significant distances highlight changepoints. We demonstrate the effectiveness of this method on both synthetic datasets and real-world data from The Guardian newspaper, successfully identifying major historical events including 9/11, the COVID-19 pandemic, and presidential elections. Our approach requires minimal domain knowledge, can autonomously discover significant shifts in public discourse, and yields a quantitative measure of change in content, making it valuable for journalism, policy analysis, and crisis monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rumor Detection on Social Media with Reinforcement Learning-based Key Propagation Graph Generator</title>
<link>https://arxiv.org/abs/2405.13094</link>
<guid>https://arxiv.org/abs/2405.13094</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, rumor detection, social media, propagation graphs, public health<br />
Summary:<br />
The spread of rumors on social media during significant events like the US elections and COVID-19 poses a threat to social stability and public health. Current rumor detection methods use propagation graphs but struggle with noisy or irrelevant structures. The Key Propagation Graph Generator (KPG) introduces a reinforcement learning-based framework that generates informative propagation patterns for events with limited topology information and identifies significant substructures in events with noisy propagation. KPG includes the Candidate Response Generator (CRG) and the Ending Node Selector (ENS) to refine propagation patterns and identify influential substructures. This end-to-end framework utilizes rewards from a pre-trained graph neural network to guide the training process. Extensive experiments show that KPG outperforms current methods in rumor detection tasks. <br /><br /> <div>
arXiv:2405.13094v2 Announce Type: replace 
Abstract: The spread of rumors on social media, particularly during significant events like the US elections and the COVID-19 pandemic, poses a serious threat to social stability and public health. Current rumor detection methods primarily rely on propagation graphs to improve the model performance. However, the effectiveness of these methods is often compromised by noisy and irrelevant structures in the propagation process. To tackle this issue, techniques such as weight adjustment and data augmentation have been proposed. However, they depend heavily on rich original propagation structures, limiting their effectiveness in handling rumors that lack sufficient propagation information, especially in the early stages of dissemination. In this work, we introduce the Key Propagation Graph Generator (KPG), a novel reinforcement learning-based framework, that generates contextually coherent and informative propagation patterns for events with insufficient topology information and identifies significant substructures in events with redundant and noisy propagation structures. KPG comprises two key components: the Candidate Response Generator (CRG) and the Ending Node Selector (ENS). CRG learns latent variable distributions from refined propagation patterns to eliminate noise and generate new candidates for ENS, while ENS identifies the most influential substructures in propagation graphs and provides training data for CRG. Furthermore, we develop an end-to-end framework that utilizes rewards derived from a pre-trained graph neural network to guide the training process. The resulting key propagation graphs are then employed in downstream rumor detection tasks. Extensive experiments conducted on four datasets demonstrate that KPG outperforms current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Innovative Distinctiveness of Prizewinners and their Networks</title>
<link>https://arxiv.org/abs/2411.12180</link>
<guid>https://arxiv.org/abs/2411.12180</guid>
<content:encoded><![CDATA[
<div> Prizes, innovation, impact, productivity, collaboration <br />
Summary: <br />
- Prizewinners are more innovative in their research, combining existing ideas in new ways and integrating historical and contemporary thinking, as well as incorporating interdisciplinary perspectives.
- Prizewinners show increased innovativeness in their work compared to non-prizewinners, with a growing gap leading up to and following the prize year.
- Network embeddedness predicts unusual innovativeness, with prizewinners' collaborations being shorter, involving wider exposure to unfamiliar topics, and minimal overlap in coauthors' networks.
- Despite having equivalent impact and productivity records initially, prizewinners show a sustained increase in innovativeness post-prize year.
- The implications of these findings are significant for understanding the effectiveness of reward systems in promoting innovation in science. <br /> <div>
arXiv:2411.12180v2 Announce Type: replace-cross 
Abstract: Science prizes purportedly reward innovation and explorations of new phenomena. Yet, in practice prizes may inadvertently divert resources from similarly impactful but less celebrated scholars. Despite this paradox, knowledge of how prizewinning relates to innovation is nascent even as prizes proliferate widely. Analyzing 2,460 worldwide prizes, we compared the innovativeness of over 23,000 prizewinners and matched non-prizewinners whose performance records were statistically equivalent up to the prize year. First, we find that prizewinners are more innovative. Their research is more likely to combine existing ideas in new ways, integrate a topic's historical and contemporary thinking, and incorporate interdisciplinary perspectives. Second, although prizewinners and matched non-prizewinners have statistically equivalent impact and productivity records up to the prize year, at about five years before the prize, prizewinners' papers become more innovative than their matched peers, a difference that widens each year, peaks during the prize year, and then persists for the remainder of their careers. Third, network embeddedness predicts unusual innovativeness. Compared to non-prizewinners, prizewinners' collaborations are shorter in duration, encompass wider exposure to unfamiliar topics, and involve coauthors whose networks minimally overlap with each other. The implications of the findings for the efficacy of reward systems and innovation in science are discussed.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drivers of cooperation in social dilemmas on higher-order networks</title>
<link>https://arxiv.org/abs/2502.09446</link>
<guid>https://arxiv.org/abs/2502.09446</guid>
<content:encoded><![CDATA[
<div> Keywords: cooperation, social dilemmas, network framework, multi-player games, structured populations

Summary:
In the study, a new higher-order network framework for multi-player games on structured populations is introduced. The model considers multi-dimensional strategies to better capture the complexity of real-world interactions in social dilemmas. The research explores the dynamical and structural coupling between different orders of interactions and highlights the importance of nested multilevel interactions in promoting cooperative behavior. By incorporating features that go beyond traditional uni-dimensional strategies, the model shows how cooperation can be enhanced in group social dilemmas. This work sheds light on the key drivers of cooperative behavior observed in real-world scenarios, providing valuable insights for understanding and promoting cooperation in social interactions. 

<br /><br />Summary: <div>
arXiv:2502.09446v2 Announce Type: replace-cross 
Abstract: Understanding cooperation in social dilemmas requires models that capture the complexity of real-world interactions. While network frameworks have provided valuable insights to model the evolution of cooperation, they are unable to encode group interactions properly. Here, we introduce a general higher-order network framework for multi-player games on structured populations. Our model considers multi-dimensional strategies, based on the observation that social behaviours are affected by the size of the group interaction. We investigate dynamical and structural coupling between different orders of interactions, revealing the crucial role of nested multilevel interactions, and showing how such features can enhance cooperation beyond the limit of traditional models with uni-dimensional strategies. Our work identifies the key drivers promoting cooperative behaviour commonly observed in real-world group social dilemmas.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Online Polarization Through Human-Agent Interaction in a Synthetic LLM-Based Social Network</title>
<link>https://arxiv.org/abs/2506.15866</link>
<guid>https://arxiv.org/abs/2506.15866</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, polarization dynamics, artificial agents, user study, online environments<br />
Summary: 
This study introduces an experimental framework using artificial agents to investigate polarization dynamics in social media. Through a user study involving 122 participants, the research successfully replicated key characteristics of polarized online discourse and demonstrated the manipulation of environmental factors. The results validate theoretical predictions by showing that polarized environments heighten emotionality and group identity salience while decreasing uncertainty. This causal evidence enhances our understanding of how online environments influence user perceptions and behaviors. The novel methodology offers researchers precise control over experimental conditions while capturing real-world dynamics, providing valuable insights into social media dynamics. <br /><br />Summary: <div>
arXiv:2506.15866v1 Announce Type: new 
Abstract: The rise of social media has fundamentally transformed how people engage in public discourse and form opinions. While these platforms offer unprecedented opportunities for democratic engagement, they have been implicated in increasing social polarization and the formation of ideological echo chambers. Previous research has primarily relied on observational studies of social media data or theoretical modeling approaches, leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments. Here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with LLM-based artificial agents in a controlled social network simulation. Through a user study with 122 participants, we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors. Our results provide empirical validation of theoretical predictions about online polarization, showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty. These findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors. More broadly, this research introduces a powerful new methodology for studying social media dynamics, offering researchers unprecedented control over experimental conditions while maintaining ecological validity.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascade-driven opinion dynamics on social networks</title>
<link>https://arxiv.org/abs/2506.16302</link>
<guid>https://arxiv.org/abs/2506.16302</guid>
<content:encoded><![CDATA[
<div> Keywords: Online social networks, information cascades, opinion dynamics, Friedkin-Johnsen model, public discourse

Summary: 
The paper introduces the Friedkin-Johnsen on Cascade (FJC) model, which integrates information cascades and opinion dynamics in online social networks. By studying real social cascades, the model shows how the convergence of socialization and news sharing can disrupt traditional opinion evolution dynamics. The research reveals that these cascades can enhance the influence of central opinion leaders, making them less receptive to opposing viewpoints even when faced with a critical mass of dissent. Understanding the interplay between social dynamics and information flow is crucial in shaping public discourse in the digital age. <br /><br />Summary: <div>
arXiv:2506.16302v1 Announce Type: new 
Abstract: Online social networks (OSNs) have transformed the way individuals fulfill their social needs and consume information. As OSNs become increasingly prominent sources for news dissemination, individuals often encounter content that influences their opinions through both direct interactions and broader network dynamics. In this paper, we propose the Friedkin-Johnsen on Cascade (FJC) model, which is, to the best of our knowledge, is the first attempt to integrate information cascades and opinion dynamics, specifically using the very popular Friedkin-Johnsen model. Our model, validated over real social cascades, highlights how the convergence of socialization and sharing news on these platforms can disrupt opinion evolution dynamics typically observed in offline settings. Our findings demonstrate that these cascades can amplify the influence of central opinion leaders, making them more resistant to divergent viewpoints, even when challenged by a critical mass of dissenting opinions. This research underscores the importance of understanding the interplay between social dynamics and information flow in shaping public discourse in the digital age.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse</title>
<link>https://arxiv.org/abs/2506.16412</link>
<guid>https://arxiv.org/abs/2506.16412</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, education, social media, sentiment analysis, stakeholder discourse

Summary: 
Generative AI technologies are increasingly influencing education, with stakeholders like teachers and students actively engaging in discussions around its impact. A comprehensive analysis of social media data on GAI in education revealed various sentiments and viewpoints. The study utilized a modular framework leveraging large language models for sentiment analysis, topic modeling, and author classification, outperforming traditional NLP models. The analysis identified 12 latent topics in public discourse, showing optimism from teachers and students for GAI's personalized learning benefits but also highlighting concerns such as privacy, academic integrity, and job security. Students expressed distress over AI detectors falsely accusing them of cheating, while teachers were wary of institutional pressures to adopt GAI tools. The findings emphasize the need for clear policies, transparent integration practices, and support mechanisms to balance innovation with oversight in GAI-enabled educational environments. The study also showcased the potential of LLM-based frameworks for modeling stakeholder discourse in online communities. 


<br /><br />Summary: <div>
arXiv:2506.16412v1 Announce Type: new 
Abstract: Generative AI (GAI) technologies are quickly reshaping the educational landscape. As adoption accelerates, understanding how students and educators perceive these tools is essential. This study presents one of the most comprehensive analyses to date of stakeholder discourse dynamics on GAI in education using social media data. Our dataset includes 1,199 Reddit posts and 13,959 corresponding top-level comments. We apply sentiment analysis, topic modeling, and author classification. To support this, we propose and validate a modular framework that leverages prompt-based large language models (LLMs) for analysis of online social discourse, and we evaluate this framework against classical natural language processing (NLP) models. Our GPT-4o pipeline consistently outperforms prior approaches across all tasks. For example, it achieved 90.6% accuracy in sentiment analysis against gold-standard human annotations. Topic extraction uncovered 12 latent topics in the public discourse with varying sentiment and author distributions. Teachers and students convey optimism about GAI's potential for personalized learning and productivity in higher education. However, key differences emerged: students often voice distress over false accusations of cheating by AI detectors, while teachers generally express concern about job security, academic integrity, and institutional pressures to adopt GAI tools. These contrasting perspectives highlight the tension between innovation and oversight in GAI-enabled learning environments. Our findings suggest a need for clearer institutional policies, more transparent GAI integration practices, and support mechanisms for both educators and students. More broadly, this study demonstrates the potential of LLM-based frameworks for modeling stakeholder discourse within online communities.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Political Influence Through Social Media: Network and Causal Dynamics in the 2022 French Presidential Election</title>
<link>https://arxiv.org/abs/2506.16449</link>
<guid>https://arxiv.org/abs/2506.16449</guid>
<content:encoded><![CDATA[
<div> Keywords: French presidential election, Twitter messages, political discourse dynamics, causal inference, social media network 

Summary: 
During the 2022 French presidential election, Twitter messages on key topics were collected from political candidates and their networks, with a focus on analyzing interactions and central topics shaping political debate. Using causal inference techniques like Convergent Cross Mapping, directional influences among political parties were uncovered, distinguishing true influence from correlation and revealing asymmetric relationships. Specific issues like health and foreign policy were found to act as catalysts for cross-party influence, particularly in critical election phases. These findings provide a unique framework for understanding political discourse dynamics and offer practical implications for campaign strategists and media analysts to monitor and respond to shifts in political influence in real time. 

Summary: <div>
arXiv:2506.16449v1 Announce Type: new 
Abstract: During the 2022 French presidential election, we collected daily Twitter messages on key topics posted by political candidates and their close networks. Using a data-driven approach, we analyze interactions among political parties, identifying central topics that shape the landscape of political debate. Moving beyond traditional correlation analyses, we apply a causal inference technique: Convergent Cross Mapping, to uncover directional influences among political communities, revealing how some parties are more likely to initiate changes in activity while others tend to respond. This approach allows us to distinguish true influence from mere correlation, highlighting asymmetric relationships and hidden dynamics within the social media political network. Our findings demonstrate how specific issues, such as health and foreign policy, act as catalysts for cross-party influence, particularly during critical election phases. These insights provide a novel framework for understanding political discourse dynamics and have practical implications for campaign strategists and media analysts seeking to monitor and respond to shifts in political influence in real time.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data marketplaces can increase the willingness to share social media data at low prices</title>
<link>https://arxiv.org/abs/2506.16618</link>
<guid>https://arxiv.org/abs/2506.16618</guid>
<content:encoded><![CDATA[
<div> keywords: social media data, data donation, data marketplace, privacy protections, data buyer<br />
Summary:<br />
Living in the post API age, researchers face challenges in obtaining social media data. Data donation as an alternative is limited by low participation rates. This paper examines the impact of data marketplaces on individuals' willingness to sell their social media data. Results show a significant increase in willingness to sell within a marketplace compared to data donation or one-time purchase offers. While minimum acceptable prices did not vary significantly, most participants set prices within the marketplace's suggested range. The type of buyer and privacy safeguards did not significantly influence participants' decisions within the marketplace setting. This research suggests that data marketplaces could be a viable solution for obtaining social media data ethically and efficiently. <br />Summary: <div>
arXiv:2506.16618v1 Announce Type: new 
Abstract: Living in the Post API age, researchers face unprecedented challenges in obtaining social media data, while users are concerned about how big tech companies use their data. Data donation offers a promising alternative, however, its scalability is limited by low participation and high dropout rates. Research suggests that data marketplaces could be a solution, but its realization remains challenging due to theoretical gaps in treating data as an asset. This paper examines whether data marketplaces can increase individuals willingness to sell their X (Twitter) data package and the minimum price they would accept. It also explores how privacy protections and the type of data buyer may affect these decisions. Results from two preregistered online survey experiments show that a data marketplace increases participants' willingness to sell their X data by 12 to 25 percentage points compared to data donation (depending on treatments), and by 6.8 points compared to onetime purchase offers. Although difference in minimum acceptable prices are not statistically significant, over 64 percentage of participants set their price within the marketplace's suggested range (0.25 to 2), substantially lower than the amounts offered in prior onetime purchase studies. Finally, in the marketplace setting, neither the type of buyer nor the inclusion of a privacy safeguard significantly influenced participants willingness to sell.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
<link>https://arxiv.org/abs/2506.15690</link>
<guid>https://arxiv.org/abs/2506.15690</guid>
<content:encoded><![CDATA[
<div> framework, model collapse, language model, synthetic data, convergence pattern
Summary:
The article introduces the concept of LLM Web Dynamics (LWD) for analyzing model collapse in large language models trained with synthetic data from the Internet. By using a retrieval-augmented generation (RAG) database to simulate the Internet, the framework investigates the convergence pattern of model outputs at a network level. The study aims to address the potential threat of model collapse, which has been understudied in existing research that primarily focuses on individual models or statistical surrogates. The theoretical guarantees for convergence are provided, drawing parallels to interacting Gaussian Mixture Models. The approach enhances understanding of how large language models behave when trained on synthetic data from the public Internet, shedding light on important considerations for maintaining model performance and preventing collapse.<br /><br />Summary: <div>
arXiv:2506.15690v1 Announce Type: cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer entropy for finite data</title>
<link>https://arxiv.org/abs/2506.16215</link>
<guid>https://arxiv.org/abs/2506.16215</guid>
<content:encoded><![CDATA[
<div> Transfer entropy, directed information flow, complex systems, statistical significance, finite data<br />
Summary:<br />
Transfer entropy is a widely used measure for quantifying directed information flows in complex systems. However, it has shortcomings for data of finite cardinality, including a positive bias for sparse bin counts and a lack of clear means to assess statistical significance. By accounting for information content in finite data streams, a new transfer entropy measure is derived that addresses these issues. This measure is asymptotically equivalent to the standard estimator but provides improved results for small size and high cardinality time series. It allows for a fully nonparametric assessment of statistical significance without the need for simulation. The correction for finite data has a significant impact on results in both real and synthetic time series datasets.<br /> 
Summary: <div>
arXiv:2506.16215v1 Announce Type: cross 
Abstract: Transfer entropy is a widely used measure for quantifying directed information flows in complex systems. While the challenges of estimating transfer entropy for continuous data are well known, it has two major shortcomings that persist even for data of finite cardinality: it exhibits a substantial positive bias for sparse bin counts, and it has no clear means to assess statistical significance. By more precisely accounting for information content in finite data streams, we derive a transfer entropy measure which is asymptotically equivalent to the standard plug-in estimator but remedies these issues for time series of small size and/or high cardinality, permitting a fully nonparametric assessment of statistical significance without simulation. We show that this correction for finite data has a substantial impact on results in both real and synthetic time series datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding</title>
<link>https://arxiv.org/abs/2506.16754</link>
<guid>https://arxiv.org/abs/2506.16754</guid>
<content:encoded><![CDATA[
<div> Metapath-based Hyperbolic Contrastive Learning framework, heterogeneous graphs, hyperbolic space, complex structures, contrastive learning<br />
<br />
Summary: <br />
The article introduces the Metapath-based Hyperbolic Contrastive Learning framework (MHCL) for embedding heterogeneous graphs with diverse power-law structures. Unlike existing models that use a single hyperbolic space, MHCL utilizes multiple hyperbolic spaces to capture the complex structures corresponding to different metapaths within the graph. By learning distinct hyperbolic spaces for each metapath, MHCL effectively captures semantic information and preserves the discriminability of metapath embeddings. The framework employs a contrastive learning approach to optimize the embeddings, minimizing distances between embeddings of the same metapath and maximizing distances between different metapaths. Experimental results reveal that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, showcasing its ability to effectively capture the complex structures of heterogeneous graphs. <div>
arXiv:2506.16754v1 Announce Type: cross 
Abstract: The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elephant in the Room: Dissecting and Reflecting on the Evolution of Online Social Network Research</title>
<link>https://arxiv.org/abs/2411.13681</link>
<guid>https://arxiv.org/abs/2411.13681</guid>
<content:encoded><![CDATA[
<div> data access policies, social networks, research, metadata, survey
Summary: 
The paper highlights the challenges faced by external scientists in conducting research on Online Social Networks (OSN) due to restrictive data access policies, leading to reliance on static datasets. It presents the Minerva-OSN dataset, comprising metadata from over 13,000 papers on OSN research, revealing trends like the focus on platforms like Twitter and difficulties in obtaining OSN data. The expert survey conducted with 50 established scientists in the field suggests improvements, such as increased involvement of OSN owners. The paper calls for reflection on the collective body of research on OSN since 2006 and emphasizes the need to enhance research efforts to benefit OSN owners, end-users, and society as a whole.<br /><br />Summary: <div>
arXiv:2411.13681v2 Announce Type: replace 
Abstract: Billions of individuals engage with Online Social Networks (OSN) daily. The owners of OSN try to meet the demands of their end-users while complying with business necessities. Such necessities may, however, lead to the adoption of restrictive data access policies that hinder research activities from "external" scientists -- who may, in turn, resort to other means (e.g., rely on static datasets) for their studies. Given the abundance of literature on OSN, we -- as academics -- should take a step back and reflect on what we have done so far, after having written thousands of papers on OSN. This is the first paper that provides a holistic outlook to the entire body of research that focused on OSN -- since the seminal work by Acquisti and Gross (2006). First, we search through over 1 million peer-reviewed publications, and derive 13,842 papers that focus on OSN: we organize the metadata of these works in the Minerva-OSN dataset, the first of its kind -- which we publicly release. Next, by analyzing Minerva-OSN, we provide factual evidence elucidating trends and aspects that deserve to be brought to light, such as the predominant focus on Twitter or the difficulty in obtaining OSN data. Finally, as a constructive step to guide future research, we carry out an expert survey (n=50) with established scientists in this field, and coalesce suggestions to improve the status quo such as an increased involvement of OSN owners. Our findings should inspire a reflection to "rescue" research on OSN. Doing so would improve the overall OSN ecosystem, benefiting both their owners and end-users and, hence, our society.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set Graph Anomaly Detection via Normal Structure Regularisation</title>
<link>https://arxiv.org/abs/2311.06835</link>
<guid>https://arxiv.org/abs/2311.06835</guid>
<content:encoded><![CDATA[
<div> novel open-set GAD, normal structure regularisation, unseen anomalies, graph anomaly detection, supervised anomaly detection<br />
<br />
Summary:<br />
This paper introduces a novel approach for open-set Graph Anomaly Detection (GAD) called normal structure regularisation (NSReg). NSReg aims to improve the detection ability of unseen anomalies while effectively detecting seen anomalies. By incorporating a regularisation term that focuses on learning compact and semantically-rich representations of normal nodes based on their structural relations, NSReg prevents overfitting to seen anomalies and improves the normality decision boundary. Experimental results on seven datasets show that NSReg outperforms existing methods by at least 14% in AUC-ROC for unseen anomaly classes and 10% for all anomaly classes. The approach effectively captures discriminative features from graph structure and node attributes for GAD, providing a strong foundation for future research in anomaly detection. <br /><br /> <div>
arXiv:2311.06835v5 Announce Type: replace-cross 
Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Interest-aware Graph Learning for Group Identification</title>
<link>https://arxiv.org/abs/2506.14826</link>
<guid>https://arxiv.org/abs/2506.14826</guid>
<content:encoded><![CDATA[
<div> evolution relationship, group identification, collaborative, interests, social media
<br />
Summary: 
The paper introduces CI4GI, a Collaborative Interest-aware model for Group Identification, to address the issue of recommending groups to users on social media platforms. It highlights the collaborative evolution relationship between dual-level user interests, encompassing group-level and item-level interests. The model incorporates an interest enhancement strategy to capture additional user interests from groups they have joined, enhancing the alignment between the two interest levels. Furthermore, it utilizes the distance between interest distributions of users to optimize the identification of negative samples, reducing false-negative interference during cross-level interests alignment. Experimental results on real-world datasets demonstrate the superior performance of CI4GI compared to existing models. <div>
arXiv:2506.14826v1 Announce Type: new 
Abstract: With the popularity of social media, an increasing number of users are joining group activities on online social platforms. This elicits the requirement of group identification (GI), which is to recommend groups to users. We reveal that users are influenced by both group-level and item-level interests, and these dual-level interests have a collaborative evolution relationship: joining a group expands the user's item interests, further prompting the user to join new groups. Ultimately, the two interests tend to align dynamically. However, existing GI methods fail to fully model this collaborative evolution relationship, ignoring the enhancement of group-level interests on item-level interests, and suffering from false-negative samples when aligning cross-level interests. In order to fully model the collaborative evolution relationship between dual-level user interests, we propose CI4GI, a Collaborative Interest-aware model for Group Identification. Specifically, we design an interest enhancement strategy that identifies additional interests of users from the items interacted with by the groups they have joined as a supplement to item-level interests. In addition, we adopt the distance between interest distributions of two users to optimize the identification of negative samples for a user, mitigating the interference of false-negative samples during cross-level interests alignment. The results of experiments on three real-world datasets demonstrate that CI4GI significantly outperforms state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse</title>
<link>https://arxiv.org/abs/2506.14836</link>
<guid>https://arxiv.org/abs/2506.14836</guid>
<content:encoded><![CDATA[
<div> persistent homology, structural change, media narratives, semantic disruption, narrative volatility

Summary:
Persistent homology is introduced as a topological framework for identifying structural changes in media narratives during major global events. The study analyzes international news articles from events such as the Russian invasion of Ukraine, the murder of George Floyd, the U.S. Capitol insurrection, and the Hamas-led invasion of Israel. By constructing daily co-occurrence graphs of noun phrases and utilizing a Vietoris-Rips filtration to transform them into persistence diagrams, the study calculates Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. The results show sharp spikes in both connected components and loops during major events, indicating sudden reorganization in narrative structure. Cross-correlation analyses reveal a lag pattern where changes in component-level structure precede higher-order motif shifts, suggesting a bottom-up cascade of semantic change. An exception is observed during the Russian invasion of Ukraine, where higher-order entropy leads connected components, possibly reflecting top-down narrative framing before local discourse adjusts. The study demonstrates that persistent homology provides an unsupervised method for detecting inflection points and directional shifts in public attention during crises, protests, and information shocks. <br /><br />Summary: <div>
arXiv:2506.14836v1 Announce Type: new 
Abstract: How can we detect when global events fundamentally reshape public discourse? This study introduces a topological framework for identifying structural change in media narratives using persistent homology. Drawing on international news articles surrounding major events - including the Russian invasion of Ukraine (Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol insurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we construct daily co-occurrence graphs of noun phrases to trace evolving discourse. Each graph is embedded and transformed into a persistence diagram via a Vietoris-Rips filtration. We then compute Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. Our results show that major geopolitical and social events align with sharp spikes in both H0 (connected components) and H1 (loops), indicating sudden reorganization in narrative structure and coherence. Cross-correlation analyses reveal a typical lag pattern in which changes to component-level structure (H0) precede higher-order motif shifts (H1), suggesting a bottom-up cascade of semantic change. An exception occurs during the Russian invasion of Ukraine, where H1 entropy leads H0, possibly reflecting top-down narrative framing before local discourse adjusts. Persistence entropy further distinguishes tightly focused from diffuse narrative regimes. These findings demonstrate that persistent homology offers a mathematically principled, unsupervised method for detecting inflection points and directional shifts in public attention - without requiring prior knowledge of specific events. This topological approach advances computational social science by enabling real-time detection of semantic restructuring during crises, protests, and information shocks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic resolution of crowd-sourced moderation on X in polarized settings across countries</title>
<link>https://arxiv.org/abs/2506.15168</link>
<guid>https://arxiv.org/abs/2506.15168</guid>
<content:encoded><![CDATA[
<div> community notes, crowd-sourced moderation, ideological dimension, polarization contexts, social platforms 

Summary: 
The study examines the effectiveness of X's Community Notes system, which utilizes crowd-sourced moderation to combat misleading content on social platforms. By analyzing a vast amount of moderation data and cross-referencing ideological scaling data from 13 countries, the researchers found that the system accurately captures each country's main polarizing dimension. However, it falls short in moderating the most polarizing content, which poses risks to civic discourse and electoral processes. The transition from expert fact-checking to crowd-sourced moderation on social platforms is a notable trend, and this study sheds light on the challenges and potential implications of such a shift. <div>
arXiv:2506.15168v1 Announce Type: new 
Abstract: Social platforms increasingly transition from expert fact-checking to crowd-sourced moderation, with X pioneering this shift through its Community Notes system, enabling users to collaboratively moderate misleading content. To resolve conflicting moderation, Community Notes learns a latent ideological dimension and selects notes garnering cross-partisan support. As this system, designed for and evaluated in the United States, is now deployed worldwide, we evaluate its operation across diverse polarization contexts. We analyze 1.9 million moderation notes with 135 million ratings from 1.2 million users, cross-referencing ideological scaling data across 13 countries. Our results show X's Community Notes effectively captures each country's main polarizing dimension but fails by design to moderate the most polarizing content, posing potential risks to civic discourse and electoral processes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control</title>
<link>https://arxiv.org/abs/2506.15397</link>
<guid>https://arxiv.org/abs/2506.15397</guid>
<content:encoded><![CDATA[
<div> learning algorithm, vaccination strategies, sample complexity, optimal algorithm, real-world data

Summary:
The article addresses the problem of minimizing the extinction time of a highly contagious disease modeled by the Susceptible-Infected-Susceptible (SIS) model on an unknown graph. It proposes a novel inclusion-exclusion-based learning algorithm with established sample complexity for graph recovery. The article also details an optimal algorithm for the Spectral Radius Minimization (SRM) problem, proving its polynomial running time for graphs with bounded treewidth. Additionally, an efficient polynomial-time greedy heuristic for any graph is presented. Experimental validation on synthetic and real-world data confirms the efficacy of the learning and vaccination algorithms. <div>
arXiv:2506.15397v1 Announce Type: cross 
Abstract: The Susceptible-Infected-Susceptible (SIS) model is a widely used model for the spread of information and infectious diseases, particularly non-immunizing ones, on a graph. Given a highly contagious disease, a natural question is how to best vaccinate individuals to minimize the disease's extinction time. While previous works showed that the problem of optimal vaccination is closely linked to the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that the graph is known, which is often not the case in practice. In this work, we consider the problem of minimizing the extinction time of an outbreak modeled by an SIS model where the graph on which the disease spreads is unknown and only the infection states of the vertices are observed. To this end, we split the problem into two: learning the graph and determining effective vaccination strategies. We propose a novel inclusion-exclusion-based learning algorithm and, unlike previous approaches, establish its sample complexity for graph recovery. We then detail an optimal algorithm for the SRM problem and prove that its running time is polynomial in the number of vertices for graphs with bounded treewidth. This is complemented by an efficient and effective polynomial-time greedy heuristic for any graph. Finally, we present experiments on synthetic and real-world data that numerically validate our learning and vaccination algorithms.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic Graphs</title>
<link>https://arxiv.org/abs/2506.15464</link>
<guid>https://arxiv.org/abs/2506.15464</guid>
<content:encoded><![CDATA[
<div> Graph signal processing, hierarchical graphs, boundary-weighted operator, spectral norm, delta-hyperbolic networks <br />
<br />
Summary: 
Hierarchical graphs with tree-like branching patterns present challenges for traditional graph filters. A boundary-weighted operator is introduced to address this issue by rescaling edges based on their distance from the graph's Gromov boundary. Using Busemann functions on delta-hyperbolic networks, a closed-form upper bound on the operator's spectral norm is proven, showing that each signal loses a controlled fraction of energy at each pass. This results in a lightweight filter that is parameter-free and stable, with stability derived from geometric principles. The filter provides a new analytical tool for graph signal processing on data with dense or hidden hierarchical structures. <div>
arXiv:2506.15464v1 Announce Type: cross 
Abstract: Hierarchical graphs often exhibit tree-like branching patterns, a structural property that challenges the design of traditional graph filters. We introduce a boundary-weighted operator that rescales each edge according to how far its endpoints drift toward the graph's Gromov boundary. Using Busemann functions on delta-hyperbolic networks, we prove a closed-form upper bound on the operator's spectral norm: every signal loses a curvature-controlled fraction of its energy at each pass. The result delivers a parameter-free, lightweight filter whose stability follows directly from geometric first principles, offering a new analytic tool for graph signal processing on data with dense or hidden hierarchical structure.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HetGCoT: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning for Academic Question Answering</title>
<link>https://arxiv.org/abs/2501.01203</link>
<guid>https://arxiv.org/abs/2501.01203</guid>
<content:encoded><![CDATA[
<div> Keyword: academic question answering, heterogeneous scholarly networks, graph neural networks, large language models, reasoning

Summary:
HetGCoT is a framework designed for academic question answering in heterogeneous scholarly networks by integrating graph neural networks and large language models. It addresses the challenges of structured understanding and interpretable reasoning in QA tasks. The framework transforms graph structural information into reasoning chains for LLMs, employs an adaptive metapath selection mechanism, and incorporates graph contexts in a multi-step reasoning strategy. Experimental results on OpenAlex and DBLP datasets show superior performance compared to state-of-the-art baselines. HetGCoT is adaptable to different LLM architectures and supports various scholarly question answering tasks. This framework bridges the gap between structured graph information and semantic comprehension, providing a comprehensive solution for academic QA. <br /><br /> Summary: HetGCoT integrates graph neural networks and large language models for academic question answering in heterogeneous scholarly networks, achieving superior performance through reasoning chain transformation, metapath selection, and multi-step reasoning strategy. <div>
arXiv:2501.01203v2 Announce Type: replace 
Abstract: Academic question answering (QA) in heterogeneous scholarly networks presents unique challenges requiring both structural understanding and interpretable reasoning. While graph neural networks (GNNs) capture structured graph information and large language models (LLMs) demonstrate strong capabilities in semantic comprehension, current approaches lack integration at the reasoning level. We propose HetGCoT, a framework enabling LLMs to effectively leverage and learn information from graphs to reason interpretable academic QA results. Our framework introduces three technical contributions: (1) a framework that transforms heterogeneous graph structural information into LLM-processable reasoning chains, (2) an adaptive metapath selection mechanism identifying relevant subgraphs for specific queries, and (3) a multi-step reasoning strategy systematically incorporating graph contexts into the reasoning process. Experiments on OpenAlex and DBLP datasets show our approach outperforms all sota baselines. The framework demonstrates adaptability across different LLM architectures and applicability to various scholarly question answering tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voter model can accurately predict individual opinions in online populations</title>
<link>https://arxiv.org/abs/2501.13215</link>
<guid>https://arxiv.org/abs/2501.13215</guid>
<content:encoded><![CDATA[
<div> opinion dynamics, multi-state voter model, Twitter dataset, French Presidential elections, individual opinions <br />
Summary:<br />
The study evaluates the multi-state voter model with zealots by analyzing a fine-grained Twitter dataset from the 2017 French Presidential elections. It finds a strong correspondence between individual opinion distributions in the model's equilibrium state and the users' actual political leanings. Additionally, the model accurately identifies pairs of like-minded users through discord probabilities. These results highlight the model's validity in capturing individual opinions in complex settings and advocate for further empirical evaluations of opinion dynamics models at the user level. <br /> <div>
arXiv:2501.13215v2 Announce Type: replace 
Abstract: Models of opinion dynamics describe how opinions are shaped in various environments. While these models are able to replicate general opinion distributions observed in real-world scenarios, their capacity to align with data at the user level remains mostly untested. We evaluate the capacity of the multi-state voter model with zealots to capture individual opinions in a fine-grained Twitter dataset collected during the 2017 French Presidential elections. Our findings reveal a strong correspondence between individual opinion distributions in the equilibrium state of the model and ground-truth political leanings of the users. Additionally, we demonstrate that discord probabilities accurately identify pairs of like-minded users. These results emphasize the validity of the voter model in complex settings, and advocate for further empirical evaluations of opinion dynamics models at the user level.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation</title>
<link>https://arxiv.org/abs/2205.14651</link>
<guid>https://arxiv.org/abs/2205.14651</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph autoencoders, Variational graph autoencoders, Scalability, Directed graphs, Community detection<br />
<br />
Summary: In this thesis, several advancements are proposed to enhance the effectiveness of Graph Autoencoders (GAE) and Variational Graph Autoencoders (VGAE) for industrial applications. The scalability issues of previous models are addressed through strategies leveraging graph degeneracy and stochastic subgraph decoding, enabling training on large graphs. Gravity-Inspired GAE and VGAE extensions are introduced for directed graphs commonly found in industrial settings. Models for dynamic graphs and simplified versions using linear encoders are also presented. Modularity-Aware GAE and VGAE improve community detection while maintaining good performance in link prediction tasks. The methods are evaluated on music graphs from Deezer, demonstrating their effectiveness in music recommendation scenarios by detecting similar musical items, ranking artists in a cold start setting, and modeling music genre perception across cultures.<br /><br />Summary: <div>
arXiv:2205.14651v3 Announce Type: replace-cross 
Abstract: Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as two powerful groups of unsupervised node embedding methods, with various applications to graph-based machine learning problems such as link prediction and community detection. Nonetheless, at the beginning of this Ph.D. project, GAE and VGAE models were also suffering from key limitations, preventing them from being adopted in the industry. In this thesis, we present several contributions to improve these models, with the general aim of facilitating their use to address industrial-level problems involving graph representations. Firstly, we propose two strategies to overcome the scalability issues of previous GAE and VGAE models, permitting to effectively train these models on large graphs with millions of nodes and edges. These strategies leverage graph degeneracy and stochastic subgraph decoding techniques, respectively. Besides, we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of these models for directed graphs, that are ubiquitous in industrial applications. We also consider extensions of GAE and VGAE models for dynamic graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily complex, and we propose to simplify them by leveraging linear encoders. Lastly, we introduce Modularity-Aware GAE and VGAE to improve community detection on graphs, while jointly preserving good performances on link prediction. In the last part of this thesis, we evaluate our methods on several graphs extracted from the music streaming service Deezer. We put the emphasis on graph-based music recommendation problems. In particular, we show that our methods can improve the detection of communities of similar musical items to recommend to users, that they can effectively rank similar artists in a cold start setting, and that they permit modeling the music genre perception across cultures.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density-aware Walks for Coordinated Campaign Detection</title>
<link>https://arxiv.org/abs/2506.13912</link>
<guid>https://arxiv.org/abs/2506.13912</guid>
<content:encoded><![CDATA[
<div> Keywords: Coordinated campaigns, Social media platforms, Graph classification, Large Engagement Networks, Random weighted walk

Summary: 
- The study focuses on detecting coordinated campaigns on social media platforms, particularly on Twitter, by modeling the problem as a graph classification task.
- The researchers use the Large Engagement Networks (LEN) dataset, which contains data on engagement patterns from fake and authentic trends on Twitter before the 2023 Turkish elections.
- Existing graph neural networks struggle with accurately classifying campaign graphs due to the large network sizes in the LEN dataset.
- The study introduces a new graph classification method that leverages the density of local network structures through a random weighted walk (RWW) approach.
- By training message-passing neural networks (MPNNs) on density-aware structural embeddings generated by the RWW approach, the researchers achieve significant improvements in classification accuracy for identifying coordinated inauthentic behavior on social media networks like Twitter.

<br /><br />Summary: <div>
arXiv:2506.13912v1 Announce Type: new 
Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering</title>
<link>https://arxiv.org/abs/2506.13989</link>
<guid>https://arxiv.org/abs/2506.13989</guid>
<content:encoded><![CDATA[
<div> Keywords: money laundering, synthetic datasets, AMLGentex, anti-money laundering systems, benchmarking

Summary: 
Money laundering is a significant issue that facilitates organized crime by allowing illicit funds to enter the legitimate economy. Despite trillions of dollars being laundered annually, only a small fraction is detected due to various factors, including deliberate evasion and limited visibility into the global transaction network. Existing synthetic datasets lack the complexity of real-world money laundering, such as partial observability, sparse labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, the AMLGentex framework has been developed as an open-source suite for generating realistic transaction data and benchmarking detection methods. This framework allows for the systematic evaluation of anti-money laundering systems under conditions that mirror the complexity of practical AML scenarios. <div>
arXiv:2506.13989v1 Announce Type: new 
Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Precinct and District Voting Data Using Persistent Homology to Identify Gerrymandering in North Carolina</title>
<link>https://arxiv.org/abs/2506.13997</link>
<guid>https://arxiv.org/abs/2506.13997</guid>
<content:encoded><![CDATA[
<div> Keywords: level-set method, geospatial election data, gerrymandering, persistent homologies, topological data analysis 

Summary: 
This paper extends a previous study on using the level-set method to analyze geospatial election data for gerrymandering. By comparing precinct and district level voting data in North Carolina House of Representatives elections, the study identifies discrepancies that indicate gerrymandering. The analysis reveals how redistricting affects voting patterns and provides evidence of gerrymandering practices. The research utilized composite shapefiles created with QGIS and R, rasterized using Python, and employed the level-set method to generate filtered simplicial complexes. Persistence barcodes were analyzed using GUDHI and PHAT libraries to compare precinct and district voting patterns. The study also compared results with traditional gerrymandering identification measures like Polsby-Popper and Reock scores. This research showcases a novel application of topological data analysis in evaluating gerrymandering practices. 

<br /><br />Summary: <div>
arXiv:2506.13997v1 Announce Type: new 
Abstract: We present an extension of Feng and Porter's 2019 paper on the use of the level-set method for the construction of a filtered simplicial complex from geospatial election data. Precincts are regarded to be too small to be gerrymandered, allowing us to identify discrepancies between precinct and district level voting data to quantify gerrymandering in the United States. Comparing the persistent homologies of Democratic voting areas on the precinct and district level shows when areas have been 'cracked' or 'packed' for partisan gain. This analysis was done for North Carolina House of Representatives elections (2012 to 2024). North Carolina has been redistricted 4 times in the past 10 years, whereas most states redistrict decennially, allowing us to understand how and when redistricted maps deviate from precinct-level voting data, and when gerrymandering occurs. Comparing persistence barcodes at the precinct and district levels (using the bottleneck distance) shows that precinct-level voting patterns do not significantly fluctuate biannually, while district level patterns do, suggesting that shifts are likely a result of redistricting rather than voter behavior, providing strong evidence of gerrymandering. North Carolina election data was collected from the public domain. Composite shapefiles were created using QGIS and R, and rasterized using Python. The level-set method was employed to generate filtered simplicial complexes. Persistence barcodes were produced using GUDHI and PHAT libraries. Additionally, we compare our results with traditional measures such as Polsby-Popper and Reock scores (gerrymandering identification measures). This research presents a novel application of topological data analysis in evaluating gerrymandering.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection</title>
<link>https://arxiv.org/abs/2506.13795</link>
<guid>https://arxiv.org/abs/2506.13795</guid>
<content:encoded><![CDATA[
<div> transfer learning, social bots, graph neural networks, domain adaptation, multi-source

Summary:<br />
- The article addresses challenges in transferring knowledge from social networks to detect social bots using GNN-based models.
- It introduces a multi-source graph domain adaptation model called BotTrans to overcome the network heterophily problem.
- BotTrans leverages labeling knowledge from multiple source domains to establish a cross-domain topology with increased network homophily.
- It aggregates cross-domain neighbor information to enhance the discriminability of source node embeddings.
- The model optimizes the relevance between source-target pairs to facilitate knowledge transfer from more relevant source networks.
- A refinement strategy is proposed to improve detection performance by utilizing semantic knowledge within the target domain. 
- Extensive experiments on real-world datasets show that BotTrans outperforms existing methods, demonstrating its effectiveness in leveraging multi-source knowledge for unlabeled social bot detection. <div>
arXiv:2506.13795v1 Announce Type: cross 
Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Socioeconomic Status is Associated with Diverse Consumption across Brands and Price Levels</title>
<link>https://arxiv.org/abs/2506.13840</link>
<guid>https://arxiv.org/abs/2506.13840</guid>
<content:encoded><![CDATA[
<div> income, diverse consumption, niche consumption, socioeconomic status, mobile tracking<br />
Summary: <br />
The study examines the relationship between income and consumption diversity, focusing on mobile-tracked visits to stores in New York State. It finds that higher income levels are linked to more diverse consumption across brands and price points, supporting the diversity hypothesis. This association is less pronounced in densely populated and diverse areas like New York City. Education level also shows a similar pattern when used as a measure of socioeconomic status. The findings are replicated in Texas, suggesting a broader applicability. The study rules out simple geographic factors as explanations, pointing towards deeper social and cultural influences on consumption patterns. <div>
arXiv:2506.13840v1 Announce Type: cross 
Abstract: Consumption practices are determined by a combination of economic, social, and cultural forces. We posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestyle-based niche consumption. We provide empirical evidence for this diversity hypothesis by analysing millions of mobile-tracked visits from thousands of Census Block Groups to thousands of stores in New York State. The results show that high income is significantly associated with diverse consumption across brands and price levels. The associations between diversity and income persist but are less prominent for necessity-based consumption and for the densely populated and demographically diverse New York City. The associations replicate for education as an alternative measure of socioeconomic status and for the state of Texas. We further illustrate that the associations cannot be explained by simple geographic constraints, including the neighbourhoods' demographic diversity, the residents' geographic mobility and the stores' local availability, so deeper social and cultural factors must be at play.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic Demonstration of Miner-Centric Propagation Topologies</title>
<link>https://arxiv.org/abs/2506.14197</link>
<guid>https://arxiv.org/abs/2506.14197</guid>
<content:encoded><![CDATA[
<div> complex graph theory, Bitcoin CORE, Bitcoin Satoshi Vision, scale-free networks, small-world connectivity
<br />
The paper analyzes the network structure of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory. It finds that home-hosted full nodes are unable to participate in or influence the propagation topology. The propagation graph is primarily controlled by a closely connected miner clique, while full nodes are on the outskirts and cannot affect transaction-to-block inclusion paths. Through simulation-based metrics and eigenvalue centrality analysis, it is determined that full nodes are not crucial or operationally significant for consensus propagation. 
<br /><br />Summary: 
The study examines the network structures of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory. It reveals that full nodes hosted at home lack the capability to influence the propagation topology, with the network being dominated by a miner clique. Full nodes are situated on the periphery and are excluded from transaction-to-block inclusion paths. Simulation-based metrics and eigenvalue centrality analysis confirm that full nodes play a minimal role in consensus propagation. <div>
arXiv:2506.14197v1 Announce Type: cross 
Abstract: This paper formally examines the network structure of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that home-hosted full nodes are incapable of participating in or influencing the propagation topology. Leveraging established models such as scale-free networks and small-world connectivity, we demonstrate that the propagation graph is dominated by a densely interconnected miner clique, while full nodes reside on the periphery, excluded from all transaction-to-block inclusion paths. Using simulation-backed metrics and eigenvalue centrality analysis, we confirm that full nodes are neither critical nor operationally relevant for consensus propagation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2506.14291</link>
<guid>https://arxiv.org/abs/2506.14291</guid>
<content:encoded><![CDATA[
<div> Keywords: graph machine learning, symmetries, foundation models, node-level tasks, permutation-equivariance

Summary:
In the realm of graph machine learning, the challenge lies in creating models that can generalize across various graphs and feature sets. This study delves into the development of graph foundation models for node-level tasks, emphasizing the importance of respecting specific symmetries. The key symmetries identified are label permutation-equivariance, feature permutation-invariance, and node permutation-equivariance within local graph neighborhoods. By investigating linear transformations that adhere to these symmetries, the researchers establish a universal approximator capable of handling multisets with the specified properties. Utilizing these transformation layers on local neighborhood features, they construct a class of graph foundation models for predicting node properties. Empirical validation on real-world datasets showcases strong zero-shot performance and consistent enhancement as the number of training graphs increases. This work provides a promising recipe for designing versatile graph foundation models with broad applicability potential. 

<br /><br />Summary: <div>
arXiv:2506.14291v1 Announce Type: cross 
Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
<link>https://arxiv.org/abs/2406.11423</link>
<guid>https://arxiv.org/abs/2406.11423</guid>
<content:encoded><![CDATA[
<div> Keywords: Proactive content moderation, website credibility classification, webgraph, social media, dredge words

Summary: 
Our study focuses on developing a system for proactive content moderation that assesses the credibility of websites by analyzing direct and indirect user pathways to unreliable websites. We introduce a novel approach that integrates webgraph and social media contexts, utilizing graph neural networks to achieve state-of-the-art results in website credibility classification. Additionally, we introduce the concept of dredge words, terms that unreliable domains rank highly for on search engines, and explore their usage on social media platforms. Our system significantly enhances the identification of unreliable domains, improving top-k identification accuracy. We also release a dataset of dredge words, showcasing their links to social media and e-commerce platforms. Overall, our approach offers a comprehensive solution for evaluating website credibility and enhancing content moderation efforts.<br /><br />Summary: <div>
arXiv:2406.11423v4 Announce Type: replace 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Intelligence: A Survey on Image Privacy in Online Social Networks</title>
<link>https://arxiv.org/abs/2008.12199</link>
<guid>https://arxiv.org/abs/2008.12199</guid>
<content:encoded><![CDATA[
<div> privacy intelligence, online social networks, image sharing, privacy protection, user-centric perspective 

Summary:
The article discusses the importance of privacy intelligence in the context of image sharing on online social networks (OSNs). It highlights the growing risk of privacy invasion due to image leaks and the use of advanced algorithms like DeepFake. The authors emphasize the need for a more intelligent approach to privacy management in OSN image sharing. They introduce a comprehensive framework that includes a definition and taxonomy of OSN image privacy, as well as a stage-based analysis of privacy issues in the lifecycle of image sharing. The framework considers user behaviors and identifies corresponding privacy challenges. A systematic review of intelligent solutions targeting these issues is provided, leading to the proposal of an intelligent privacy firewall for improved privacy management. The article concludes with a discussion on challenges and future directions in the field of privacy intelligence in OSN image sharing.<br /><br />Summary: <div>
arXiv:2008.12199v4 Announce Type: replace-cross 
Abstract: Image sharing on online social networks (OSNs) has become an indispensable part of daily social activities, but it has also led to an increased risk of privacy invasion. The recent image leaks from popular OSN services and the abuse of personal photos using advanced algorithms (e.g. DeepFake) have prompted the public to rethink individual privacy needs in OSN image sharing. However, OSN image privacy itself is quite complicated, and solutions currently in place for privacy management in reality are insufficient to provide personalized, accurate and flexible privacy protection. A more intelligent environment for privacy-friendly OSN image sharing is in demand. To fill the gap, we contribute a survey of "privacy intelligence" that targets modern privacy issues in dynamic OSN image sharing from a user-centric perspective. Specifically, we present a definition and a taxonomy of OSN image privacy, and a high-level privacy analysis framework based on the lifecycle of OSN image sharing. The framework consists of three stages with different principles of privacy by design. At each stage, we identify typical user behaviors in OSN image sharing and the privacy issues associated with these behaviors. Then a systematic review on the representative intelligent solutions targeting those privacy issues is conducted, also in a stage-based manner. The resulting analysis describes an intelligent privacy firewall for closed-loop privacy management. We also discuss the challenges and future directions in this area.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlation-Aware Graph Convolutional Networks for Multi-Label Node Classification</title>
<link>https://arxiv.org/abs/2411.17350</link>
<guid>https://arxiv.org/abs/2411.17350</guid>
<content:encoded><![CDATA[
<div> Graph Convolution Networks, Multi-label node classification, Correlation-aware, Graph mining, Label correlations <br />
<br />
Summary: 
This paper introduces the Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node classification in graph mining. Traditional methods using GCNs struggle with ambiguous features and topologies induced by multiple labels, leading to reduced credibility of graph messages and overlooking label correlations. To address this challenge, CorGCN incorporates a Correlation-Aware Graph Decomposition module to capture label-correlated information for each label, and a Correlation-Enhanced Graph Convolution to model label relationships during message passing. Experimental results on five datasets demonstrate the effectiveness of CorGCN in improving multi-label node classification accuracy. <div>
arXiv:2411.17350v3 Announce Type: replace-cross 
Abstract: Multi-label node classification is an important yet under-explored domain in graph mining as many real-world nodes belong to multiple categories rather than just a single one. Although a few efforts have been made by utilizing Graph Convolution Networks (GCNs) to learn node representations and model correlations between multiple labels in the embedding space, they still suffer from the ambiguous feature and ambiguous topology induced by multiple labels, which reduces the credibility of the messages delivered in graphs and overlooks the label correlations on graph data. Therefore, it is crucial to reduce the ambiguity and empower the GCNs for accurate classification. However, this is quite challenging due to the requirement of retaining the distinctiveness of each label while fully harnessing the correlation between labels simultaneously. To address these issues, in this paper, we propose a Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node classification. By introducing a novel Correlation-Aware Graph Decomposition module, CorGCN can learn a graph that contains rich label-correlated information for each label. It then employs a Correlation-Enhanced Graph Convolution to model the relationships between labels during message passing to further bolster the classification process. Extensive experiments on five datasets demonstrate the effectiveness of our proposed CorGCN.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint UAV Trajectory Planning and LEO Satellite Selection for Data Offloading in Space-Air-Ground Integrated Networks</title>
<link>https://arxiv.org/abs/2506.12750</link>
<guid>https://arxiv.org/abs/2506.12750</guid>
<content:encoded><![CDATA[
<div> Keywords: LEO satellites, UAVs, IoT devices, energy consumption, optimization

Summary:
The paper presents a study on a two-phase hierarchical data uplink model in the Space-Air-Ground Integrated Network (SAGIN) to cater to the remote Internet of Things (IoT) requirements. The model consists of optimizing trajectories of unmanned aerial vehicles (UAVs) for efficient data collection from IoT devices and transmitting it to Low Earth Orbit (LEO) satellites for further processing. The objective is to minimize the total energy consumption for IoT devices, UAVs, and LEO satellites. The problem is decomposed into two phases - IoT-UAV phase and UAV-LEO phase - to address the mixed-integer nonlinear programming challenge. Simulation results indicate significant energy savings compared to benchmark algorithms. The proposed algorithms show promise in enhancing the efficiency of data collection and offloading in the evolving SAGIN environment. 

<br /><br />Summary: <div>
arXiv:2506.12750v1 Announce Type: new 
Abstract: With the development of low earth orbit (LEO) satellites and unmanned aerial vehicles (UAVs), the space-air-ground integrated network (SAGIN) becomes a major trend in the next-generation networks. However, due to the instability of heterogeneous communication and time-varying characteristics of SAGIN, it is challenging to meet the remote Internet of Things (IoT) demands for data collection and offloading. In this paper, we investigate a two-phase hierarchical data uplink model in SAGIN. Specifically, UAVs optimize trajectories to enable efficient data collection from IoT devices, and then they transmit the data to LEO satellites with computing capabilities for further processing. The problem is formulated to minimize the total energy consumption for IoT devices, UAVs, and LEO satellites. Since the problem is in the form of mixed-integer nonlinear programming and intractable to solve directly, we decompose it into two phases. In the IoT-UAV phase, we design the algorithm to jointly optimize the IoT pairing, power allocation, and UAVs trajectories. Considering the high dynamic characteristics of LEO satellites, a real-time LEO satellite selection mechanism joint with the Satellite Tool Kit is proposed in the UAV-LEO phase. Finally, simulation results show the effectiveness of the proposed algorithms, with about 10% less energy consumption compared with the benchmark algorithm.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governments Should Mandate Tiered Anonymity on Social-Media Platforms to Counter Deepfakes and LLM-Driven Mass Misinformation</title>
<link>https://arxiv.org/abs/2506.12814</link>
<guid>https://arxiv.org/abs/2506.12814</guid>
<content:encoded><![CDATA[
<div> three-tier anonymity framework, social-media platforms, deepfakes, misinformation, Reddit

Summary:
This position paper suggests implementing a three-tier anonymity framework on social media platforms to address the challenges posed by deepfakes and misinformation fueled by large-language models. The tiers would be based on a user's reach score, with smaller accounts enjoying pseudonymity, moderately influential accounts requiring legal-identity linkage, and accounts with significant reach undergoing fact-checking for each post. The analysis of Reddit shows that volunteer moderators use similar mechanisms as audience size grows, indicating the operational feasibility of such a framework. To address existing engagement incentives that may impede voluntary adoption, the paper outlines a regulatory approach based on US legal principles and recent safety statutes in the EU and UK. By integrating reach-based identity checks into platform tools, the proposed framework aims to combat large-scale misinformation while safeguarding everyday privacy.<br /><br />Summary: <div>
arXiv:2506.12814v1 Announce Type: new 
Abstract: This position paper argues that governments should mandate a three-tier anonymity framework on social-media platforms as a reactionary measure prompted by the ease-of-production of deepfakes and large-language-model-driven misinformation. The tiers are determined by a given user's $\textit{reach score}$: Tier 1 permits full pseudonymity for smaller accounts, preserving everyday privacy; Tier 2 requires private legal-identity linkage for accounts with some influence, reinstating real-world accountability at moderate reach; Tier 3 would require per-post, independent, ML-assisted fact-checking, review for accounts that would traditionally be classed as sources-of-mass-information.
  An analysis of Reddit shows volunteer moderators converge on comparable gates as audience size increases -- karma thresholds, approval queues, and identity proofs -- demonstrating operational feasibility and social legitimacy. Acknowledging that existing engagement incentives deter voluntary adoption, we outline a regulatory pathway that adapts existing US jurisprudence and recent EU-UK safety statutes to embed reach-proportional identity checks into existing platform tooling, thereby curbing large-scale misinformation while preserving everyday privacy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Social Network Activity Using Joint User and Topic Interaction</title>
<link>https://arxiv.org/abs/2506.12842</link>
<guid>https://arxiv.org/abs/2506.12842</guid>
<content:encoded><![CDATA[
<div> Keywords: online social platforms, information cascades, opinion formation, user behavior, Hawkes processes

Summary:<br /><br />
The paper introduces the Mixture of Interacting Cascades (MIC) model, which focuses on the dynamics of information cascades on online social platforms. The model combines marked multidimensional Hawkes processes to capture the interaction between cascades and user activity. By employing a mixture of temporal point processes, MIC can effectively model the spread of multiple pieces of information among users with varying behavior adoption mechanisms. Experimental results on synthetic and real data demonstrate the superior performance of MIC compared to existing methods. The model's learned parameters provide valuable insights into the interplay between information cascades and user behavior, allowing for bi-layered visualizations of social network activity data. Overall, MIC offers a comprehensive framework for understanding and analyzing the complex dynamics of opinion formation on online social platforms. 

Summary: <br /><br /> <div>
arXiv:2506.12842v1 Announce Type: new 
Abstract: The emergence of online social platforms, such as social networks and social media, has drastically affected the way people apprehend the information flows to which they are exposed. In such platforms, various information cascades spreading among users is the main force creating complex dynamics of opinion formation, each user being characterized by their own behavior adoption mechanism. Moreover, the spread of multiple pieces of information or beliefs in a networked population is rarely uncorrelated. In this paper, we introduce the Mixture of Interacting Cascades (MIC), a model of marked multidimensional Hawkes processes with the capacity to model jointly non-trivial interaction between cascades and users. We emphasize on the interplay between information cascades and user activity, and use a mixture of temporal point processes to build a coupled user/cascade point process model. Experiments on synthetic and real data highlight the benefits of this approach and demonstrate that MIC achieves superior performance to existing methods in modeling the spread of information cascades. Finally, we demonstrate how MIC can provide, through its learned parameters, insightful bi-layered visualizations of real social network activity data.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Coordinated Processes From Social Online Networks</title>
<link>https://arxiv.org/abs/2506.12988</link>
<guid>https://arxiv.org/abs/2506.12988</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, coordinated behavior, AI-generated content, process mining, Twitter<br />
<br />
Summary: 
The article discusses the use of process mining techniques to analyze coordinated agent behavior in social networks, particularly focusing on the detection of AI-generated content. It highlights the challenge of identifying AI-driven campaigns due to the high quality of generated text. By leveraging process mining methods on real-world Twitter data, the study aims to discover structural and behavioral properties of process models that can reveal coordinated AI and human behaviors online. The research emphasizes the importance of considering metadata like post timings for effective detection of coordinated campaigns. Current approaches to modeling information spread online are limited in representing different control flows, underscoring the need for innovative methods like process mining. The study showcases how process mining can offer insights into the sophisticated structures and behaviors in social networks, shedding light on the prevalence of coordinated AI and human actions in online environments. <br /> <div>
arXiv:2506.12988v1 Announce Type: new 
Abstract: The rapid growth of social media presents a unique opportunity to study coordinated agent behavior in an unfiltered environment. Online processes often exhibit complex structures that reflect the nature of the user behavior, whether it is authentic and genuine, or part of a coordinated effort by malicious agents to spread misinformation and disinformation. Detection of AI-generated content can be extremely challenging due to the high quality of large language model-generated text. Therefore, approaches that use metadata like post timings are required to effectively detect coordinated AI-driven campaigns. Existing work that models the spread of information online is limited in its ability to represent different control flows that occur within the network in practice. Process mining offers techniques for the discovery of process models with different routing constructs and are yet to be applied to social networks. We propose to leverage process mining methods for the discovery of AI and human agent behavior within social networks. Applying process mining techniques to real-world Twitter (now X) event data, we demonstrate how the structural and behavioral properties of discovered process models can reveal coordinated AI and human behaviors online.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evolution of Cooperation Based on Adaptive Reputation Threshold and Game Transition</title>
<link>https://arxiv.org/abs/2506.13319</link>
<guid>https://arxiv.org/abs/2506.13319</guid>
<content:encoded><![CDATA[
<div> reputation, game evolution, network topology, cooperation, strategy evolution
<br />
Summary:<br />
The study explores how reputation influences game evolution in social systems using a heterogeneous game transition model with a reputation-based dynamic threshold mechanism. Individual interactions are shaped by reputation, driving adaptation and strategy evolution. The square lattice network promotes the coexistence of competing strategies, while the small-world network is more sensitive to changes due to efficient information dissemination. The reputation mechanism fosters the formation of a dominant state of cooperation, especially in contexts highly sensitive to reputation. The initial distribution of reputation affects the early stage but has minimal impact on the final steady state, which is determined by the reputation mechanism and network structure. <div>
arXiv:2506.13319v1 Announce Type: new 
Abstract: In real-world social systems, individual interactions are frequently shaped by reputation, which not only influences partner selection but also affects the nature and benefits of the interactions themselves. We propose a heterogeneous game transition model that incorporates a reputation-based dynamic threshold mechanism to investigate how reputation regulates game evolution. In our framework, individuals determine the type of game they engage in according to their own and their neighbors' reputation levels. In turn, the outcomes of these interactions modify their reputations, thereby driving the adaptation and evolution of future strategies in a feedback-informed manner. Through simulations on two representative topological structures, square lattice and small-world networks, we find that network topology exerts a profound influence on the evolutionary dynamics. Due to its localized connection characteristics, the square lattice network fosters the long-term coexistence of competing strategies. In contrast, the small-world network is more susceptible to changes in system parameters due to the efficiency of information dissemination and the sensitivity of strategy evolution. Additionally, the reputation mechanism is significant in promoting the formation of a dominant state of cooperation, especially in contexts of high sensitivity to reputation. Although the initial distribution of reputation influences the early stage of the evolutionary path, it has little effect on the final steady state of the system. Hence, we can conclude that the ultimate steady state of evolution is primarily determined by the reputation mechanism and the network structure.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwiUSD: A Benchmark Dataset and Structure-Aware LLM Framework for User Stance Detection</title>
<link>https://arxiv.org/abs/2506.13343</link>
<guid>https://arxiv.org/abs/2506.13343</guid>
<content:encoded><![CDATA[
<div> keywords: User-level stance detection, TwiUSD benchmark, MRFG framework, relevance filtering, graph neural networks<br />
Summary:<br />
User-level stance detection (UserSD) remains a challenge, lacking high-quality benchmarks encompassing linguistic and social aspects. TwiUSD introduces a large-scale benchmark with explicit followee relationships, facilitating rigorous evaluation of stance models. The MRFG framework, utilizing LLM-based relevance filtering and feature routing, shows superior performance by addressing noise and context heterogeneity. MRFG employs multi-scale filtering and adaptively routes features through graph neural networks or multi-layer perceptrons based on topological informativeness. Experimental results demonstrate MRFG's superiority over strong baselines, including PLMs, graph-based models, and LLM prompting, in both in-target and cross-target evaluation.<br /> 
Summary: <div>
arXiv:2506.13343v1 Announce Type: new 
Abstract: User-level stance detection (UserSD) remains challenging due to the lack of high-quality benchmarks that jointly capture linguistic and social structure. In this paper, we introduce TwiUSD, the first large-scale, manually annotated UserSD benchmark with explicit followee relationships, containing 16,211 users and 47,757 tweets. TwiUSD enables rigorous evaluation of stance models by integrating tweet content and social links, with superior scale and annotation quality. Building on this resource, we propose MRFG: a structure-aware framework that uses LLM-based relevance filtering and feature routing to address noise and context heterogeneity. MRFG employs multi-scale filtering and adaptively routes features through graph neural networks or multi-layer perceptrons based on topological informativeness. Experiments show MRFG consistently outperforms strong baselines (including PLMs, graph-based models, and LLM prompting) in both in-target and cross-target evaluation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
<link>https://arxiv.org/abs/2506.12078</link>
<guid>https://arxiv.org/abs/2506.12078</guid>
<content:encoded><![CDATA[
<div> agent-based models, large language models, societal behaviors, human complexity, simulation

Summary:
The article introduces Light Society, a new agent-based simulation framework that combines traditional agent-based models with large language models to model human-like societies on a planetary scale. The framework formalizes social processes using structured transitions of agent and environment states powered by large language models. Light Society supports efficient simulation of societies with over one billion agents, enabling high-fidelity modeling of complex societal behaviors such as trust games and opinion propagation. The simulations conducted using Light Society demonstrate its ability to capture social trust and information diffusion accurately, with larger simulations leading to more stable and realistic emergent behaviors. The framework addresses the limitations of traditional agent-based models and offers new opportunities for understanding how complex societal behaviors emerge from individual cognition and interactions. <div>
arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs</title>
<link>https://arxiv.org/abs/2506.12367</link>
<guid>https://arxiv.org/abs/2506.12367</guid>
<content:encoded><![CDATA[
<div> extraction performance, knowledge graphs, error modeling, downstream analysis, community detection <br />
<br />Summary: <br />
This study evaluates knowledge graph extraction performance in affiliation graphs of person memberships in organizations. It identifies micro-level edge accuracy and macro-level graph metrics as key evaluation criteria, highlighting biases in downstream graph analysis metrics as extraction performance declines. The study demonstrates that error models commonly used in the literature do not capture these bias patterns, emphasizing the need for more realistic error modeling in knowledge graph extraction. The findings provide actionable insights for practitioners working with knowledge graphs, underscoring the importance of advancing extraction methods to ensure reliable downstream analyses. The study's focus on social register books underscores the relevance of understanding extraction errors for applications in social structures and institutional memberships. <div>
arXiv:2506.12367v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are useful for analyzing social structures, community dynamics, institutional memberships, and other complex relationships across domains from sociology to public health. While recent advances in large language models (LLMs) have improved the scalability and accessibility of automated KG extraction from large text corpora, the impacts of extraction errors on downstream analyses are poorly understood, especially for applied scientists who depend on accurate KGs for real-world insights. To address this gap, we conducted the first evaluation of KG extraction performance at two levels: (1) micro-level edge accuracy, which is consistent with standard NLP evaluations, and manual identification of common error sources; (2) macro-level graph metrics that assess structural properties such as community detection and connectivity, which are relevant to real-world applications. Focusing on affiliation graphs of person membership in organizations extracted from social register books, our study identifies a range of extraction performance where biases across most downstream graph analysis metrics are near zero. However, as extraction performance declines, we find that many metrics exhibit increasingly pronounced biases, with each metric tending toward a consistent direction of either over- or under-estimation. Through simulations, we further show that error models commonly used in the literature do not capture these bias patterns, indicating the need for more realistic error models for KG extraction. Our findings provide actionable insights for practitioners and underscores the importance of advancing extraction methods and error modeling to ensure reliable and meaningful downstream analyses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circular Directional Flow Decomposition of Networks</title>
<link>https://arxiv.org/abs/2506.12546</link>
<guid>https://arxiv.org/abs/2506.12546</guid>
<content:encoded><![CDATA[
<div> Circular Directional Flow Decomposition, weighted directed networks, circularity index, flow analysis, network structure<br />
Summary:<br />
The Circular Directional Flow Decomposition (CDFD) framework introduces a novel way to analyze circularity in weighted directed networks. It separates flow into a circular component and an acyclic component, providing a normalized circularity index between 0 and 1. This index captures flow involved in cycles and can represent system closure, feedback, and structural redundancy. The set of all decompositions forms a well-structured geometric space, with benchmark solutions including the maximum circularity solution and the Balanced Flow Forwarding (BFF) solution. These solutions outperform existing circularity metrics in detecting structural variation and enabling flow allocation or routing in practical applications. The CDFD framework allows for structural analysis, such as mapping cyclic flow distribution, and supports efficient transport and multilateral netting. <div>
arXiv:2506.12546v1 Announce Type: cross 
Abstract: We introduce the Circular Directional Flow Decomposition (CDFD), a new framework for analyzing circularity in weighted directed networks. CDFD separates flow into two components: a circular (divergence-free) component and an acyclic component that carries all nett directional flow. This yields a normalized circularity index between 0 (fully acyclic) and 1 (for networks formed solely by the superposition of cycles), with the complement measuring directionality. This index captures the proportion of flow involved in cycles, and admits a range of interpretations - such as system closure, feedback, weighted strong connectivity, structural redundancy, or inefficiency. Although the decomposition is generally non-unique, we show that the set of all decompositions forms a well-structured geometric space with favourable topological properties. Within this space, we highlight two benchmark decompositions aligned with distinct analytical goals: the maximum circularity solution, which minimizes nett flow, and the Balanced Flow Forwarding (BFF) solution, a unique, locally computable decomposition that distributes circular flow across all feasible cycles in proportion to the original network structure. We demonstrate the interpretive value and computational tractability of both decompositions on synthetic and empirical networks. They outperform existing circularity metrics in detecting meaningful structural variation. The decomposition also enables structural analysis - such as mapping the distribution of cyclic flow - and supports practical applications that require explicit flow allocation or routing, including multilateral netting and efficient transport.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prosocial Design in Trust and Safety</title>
<link>https://arxiv.org/abs/2506.12792</link>
<guid>https://arxiv.org/abs/2506.12792</guid>
<content:encoded><![CDATA[
<div> Keywords: Prosocial Design, platform design, governance, Trust and Safety, research <br />
Summary: <br />
This chapter provides an overview of Prosocial Design, emphasizing its impact on behavior and the importance of design choices in promoting healthy interactions. The authors discuss core principles of Prosocial Design and its connection to Trust and Safety. Research indicates that Prosocial Design can effectively reduce rule-breaking and misinformation spread. However, the field is still developing, and more research is needed to fully understand its potential. The chapter aims to inspire further research and adoption of a prosocial design approach, sparking discussions on its principles and role in enhancing Trust and Safety. <div>
arXiv:2506.12792v1 Announce Type: cross 
Abstract: This chapter presents an overview of Prosocial Design, an approach to platform design and governance that recognizes design choices influence behavior and that those choices can or should be made toward supporting healthy interactions and other prosocial outcomes. The authors discuss several core principles of Prosocial Design and its relationship to Trust and Safety and other related fields. As a primary contribution, the chapter reviews relevant research to demonstrate how Prosocial Design can be an effective approach to reducing rule-breaking and other harmful behavior and how it can help to stem the spread of harmful misinformation. Prosocial Design is a nascent and evolving field and research is still limited. The authors hope this chapter will not only inspire more research and the adoption of a prosocial design approach, but that it will also provoke discussion about the principles of Prosocial Design and its potential to support Trust and Safety.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Approximate Temporal Triangle Counting in Streaming with Predictions</title>
<link>https://arxiv.org/abs/2506.13173</link>
<guid>https://arxiv.org/abs/2506.13173</guid>
<content:encoded><![CDATA[
<div> Algorithm, Triangle counting, Temporal graphs, Streaming processing, Scalability <br />
Summary: 
STEP is a new algorithm designed to approximate temporal triangle counts in massive temporal graphs efficiently. It combines predictions on the number of triangles involving a temporal edge with a sampling strategy to achieve scalability and accuracy in estimating all eight temporal triangle types simultaneously. The algorithm uses a sublinear amount of memory while providing unbiased and highly accurate estimates, even with noisy predictions. Extensive experiments on billion-edge temporal graphs show that STEP outperforms existing methods in terms of efficiency and output quality. This novel approach addresses the challenge of handling large-scale temporal graphs and provides a practical solution for counting triangles in dynamic graph datasets. <br /> <div>
arXiv:2506.13173v1 Announce Type: cross 
Abstract: Triangle counting is a fundamental and widely studied problem on static graphs, and recently on temporal graphs, where edges carry information on the timings of the associated events. Streaming processing and resource efficiency are crucial requirements for counting triangles in modern massive temporal graphs, with millions of nodes and up to billions of temporal edges. However, current exact and approximate algorithms are unable to handle large-scale temporal graphs. To fill such a gap, we introduce STEP, a scalable and efficient algorithm to approximate temporal triangle counts from a stream of temporal edges. STEP combines predictions to the number of triangles a temporal edge is involved in, with a simple sampling strategy, leading to scalability, efficiency, and accurate approximation of all eight temporal triangle types simultaneously. We analytically prove that, by using a sublinear amount of memory, STEP obtains unbiased and very accurate estimates. In fact, even noisy predictions can significantly reduce the variance of STEP's estimates. Our extensive experiments on massive temporal graphs with up to billions of edges demonstrate that STEP outputs high-quality estimates and is more efficient than state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven analysis of the impact of non-compliant individuals on epidemic diffusion in urban settings</title>
<link>https://arxiv.org/abs/2506.13325</link>
<guid>https://arxiv.org/abs/2506.13325</guid>
<content:encoded><![CDATA[
<div> Keywords: public health, non-compliance, epidemic dynamics, urban environments, contact networks

Summary:
Individuals not complying with public health measures in urban settings can undermine epidemic control efforts. This study focused on Torino, Milano, and Palermo in Italy, using data-driven contact networks to analyze the impact of non-compliant individuals. The research modelled non-compliance using a Susceptible-Infected-Recovered framework, showing that even a small number of non-compliant individuals can significantly increase infections and peak times, especially at moderate transmission rates. Spatially varying distributions of non-compliance, obtained through electoral and vaccine hesitancy data, led to infection hotspots forming with differing intensities. The study highlights the need for tailored public health interventions to address localized risks and stresses the importance of monitoring behavioral compliance for effective epidemic control. 

<br /><br />Summary: <div>
arXiv:2506.13325v1 Announce Type: cross 
Abstract: Individuals who do not comply with public health safety measures pose a significant challenge to effective epidemic control, as their risky behaviours can undermine public health interventions. This is particularly relevant in urban environments because of their high population density and complex social interactions. In this study, we employ detailed contact networks, built using a data-driven approach, to examine the impact of non-compliant individuals on epidemic dynamics in three major Italian cities: Torino, Milano, and Palermo. We use a heterogeneous extension of the Susceptible-Infected-Recovered model that distinguishes between ordinary and non-compliant individuals, who are more infectious and/or more susceptible. By combining electoral data with recent findings on vaccine hesitancy, we obtain spatially heterogeneous distributions of non-compliance. Epidemic simulations demonstrate that even a small proportion of non-compliant individuals in the population can substantially increase the number of infections and accelerate the timing of their peak. Furthermore, the impact of non-compliance is greatest when disease transmission rates are moderate. Including the heterogeneous, data-driven distribution of non-compliance in the simulations results in infection hotspots forming with varying intensity according to the disease transmission rate. Overall, these findings emphasise the importance of monitoring behavioural compliance and tailoring public health interventions to address localised risks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best Soules Basis for the Estimation of a Spectral Barycentre Network</title>
<link>https://arxiv.org/abs/2502.00038</link>
<guid>https://arxiv.org/abs/2502.00038</guid>
<content:encoded><![CDATA[
<div> algorithm, barycentre, Laplacian spectral pseudo-distance, networks, stochastic block models

Summary:
The article presents a fast algorithm for computing the barycentre of a set of networks using a Laplacian spectral pseudo-distance. The algorithm utilizes Soules bases to reconstruct a sparse approximation of the sample mean adjacency matrix. The study proves that when networks are random realizations of stochastic block models, the algorithm accurately reconstructs the population mean adjacency matrix. Monte Carlo simulations validate the theoretical properties of the estimator. This research is significant as it paves the way for the development of new spectral-based network synthesis methods with theoretical guarantees. <div>
arXiv:2502.00038v2 Announce Type: replace 
Abstract: The main contribution of this work is a fast algorithm to compute the barycentre of a set of networks based on a Laplacian spectral pseudo-distance. The core engine for the reconstruction of the barycentre is an algorithm that explores the large library of Soules bases, and returns a basis that yields a sparse approximation of the sample mean adjacency matrix. We prove that when the networks are random realizations of stochastic block models, then our algorithm reconstructs the population mean adjacency matrix. In addition to the theoretical analysis of the estimator of the barycentre network, we perform Monte Carlo simulations to validate the theoretical properties of the estimator. This work is significant because it opens the door to the design of new spectral-based network synthesis that have theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2506.08298</link>
<guid>https://arxiv.org/abs/2506.08298</guid>
<content:encoded><![CDATA[
<div> framework, graph learning, heterogeneous graphs, context encoding, transformer <br />
Summary: 
The article introduces a novel framework, H$^2$GFM, designed for graph learning that can handle both homogeneous (HoTAGs) and heterogeneous text-attributed graphs (HeTAGs). The framework projects meta-relations among graphs into a unified textual space, utilizes context encoding to capture spatial and semantic relationships, and introduces a context-adaptive graph transformer (CGT) for robust node representations. It also incorporates a mixture of CGT experts to capture heterogeneous structural patterns among graph types. The model is tested on a variety of graphs and learning scenarios, demonstrating its effectiveness in generalizing across different graph types and tasks. <div>
arXiv:2506.08298v2 Announce Type: replace-cross 
Abstract: The growing interests and applications of graph learning in diverse domains have propelled the development of a unified model generalizing well across different graphs and tasks, known as the Graph Foundation Model (GFM). Existing research has leveraged text-attributed graphs (TAGs) to tackle the heterogeneity in node features among graphs. However, they primarily focus on homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple types of nodes/edges reside, underexplored. To enhance the capabilities and applications of GFM, we introduce H$^2$GFM, a novel framework designed to generalize across both HoTAGs and HeTAGs. Our model projects diverse meta-relations among graphs under a unified textual space, and employs a context encoding to capture spatial and higher-order semantic relationships. To achieve robust node representations, we propose a novel context-adaptive graph transformer (CGT), effectively capturing information from both context neighbors and their relationships. Furthermore, we employ a mixture of CGT experts to capture the heterogeneity in structural patterns among graph types. Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as learning scenarios demonstrate the effectiveness of our model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Dynamics of Emotions in Italian Online Soccer Fandoms</title>
<link>https://arxiv.org/abs/2506.11934</link>
<guid>https://arxiv.org/abs/2506.11934</guid>
<content:encoded><![CDATA[
<div> Keywords: Italian soccer, fandoms, sentiment analysis, emotional dynamics, social media

Summary:
This study delves into the emotional dynamics of Italian soccer fandoms by analyzing user-generated content from Instagram accounts of 83 teams in different leagues. Through sentiment analysis, researchers examined emotional patterns, identified fan base clusters with similar expectations, and linked emotions to preseason expectations, team performance, and socioeconomic factors. Joy was found to have anti-bursty distributions, while anger showed bursty patterns. The study revealed correlations between emotional signals, preseason expectations, and final league rankings. Notably, burstiness emerged as a significant indicator of team performance, with its exclusion leading to a decreased coefficient of determination in statistical models. These findings provide unique insights into the interplay of fan emotions, team outcomes, and social media dynamics, offering potential avenues for future research in sports analytics and fan engagement studies. 

<br /><br />Summary: <div>
arXiv:2506.11934v1 Announce Type: new 
Abstract: This study investigates the emotional dynamics of Italian soccer fandoms through computational analysis of user-generated content from official Instagram accounts of 83 teams across Serie A, Serie B, and Lega Pro during the 2023-24 season. By applying sentiment analysis to fan comments, we extract temporal emotional patterns and identify distinct clusters of fan bases with similar preseason expectations. Drawing from complex systems theory, we characterize joy as displaying anti-bursty temporal distributions, while anger is marked by pronounced bursty patterns. Our analysis reveals significant correlations between these emotional signals, preseason expectations, socioeconomic factors, and final league rankings. In particular, the burstiness metric emerges as a meaningful correlate of team performance; statistical models excluding this parameter show a decrease in the coefficient of determination of 32%. These findings offer novel insights into the relationship between fan emotional expression and team outcomes, suggesting potential avenues for research in sports analytics, social media dynamics, and fan engagement studies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Networks: Enumerating Maximal Community Patterns in $c$-Closed Graphs</title>
<link>https://arxiv.org/abs/2506.11437</link>
<guid>https://arxiv.org/abs/2506.11437</guid>
<content:encoded><![CDATA[
<div> maximal cliques, complete bipartite subgraphs, blow-ups, c-closed graphs, polynomial time
<br />
Summary:
This article discusses the enumeration of maximal blow-ups of arbitrary finite graphs in c-closed graphs. It is established that for any fixed graph H, the number of maximal blow-ups in an n-vertex c-closed graph is bounded by a polynomial in n. The study also extends to induced blow-ups, providing a precise characterization of the graphs H for which the number of maximal induced blow-ups is polynomially bounded in n. Additionally, the article explores the same questions for an infinite family of graphs. The research contributes to understanding the structural patterns and properties of c-closed graphs, offering insights into the computational complexity of enumerating specific graph structures within this model. 
<br /> <div>
arXiv:2506.11437v1 Announce Type: cross 
Abstract: Fox, Seshadhri, Roughgarden, Wei, and Wein (SICOMP 2020) introduced the model of $c$-closed graphs--a distribution-free model motivated by triadic closure, one of the most pervasive structural signatures of social networks. While enumerating maximal cliques in general graphs can take exponential time, it is known that in $c$-closed graphs, maximal cliques and maximal complete bipartite subgraphs can always be enumerated in polynomial time. These structures correspond to blow-ups of simple patterns: a single vertex or a single edge, with some vertices required to form cliques. In this work, we explore a natural extension: we study maximal blow-ups of arbitrary finite graphs $H$ in $c$-closed graphs. We prove that for any fixed graph $H$, the number of maximal blow-ups of $H$ in an $n$-vertex $c$-closed graph is always bounded by a polynomial in $n$. We further investigate the case of induced blow-ups and provide a precise characterization of the graphs $H$ for which the number of maximal induced blow-ups is also polynomially bounded in $n$. Finally, we study the analogue questions when $H$ ranges over an infinite family of graphs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems</title>
<link>https://arxiv.org/abs/2506.11467</link>
<guid>https://arxiv.org/abs/2506.11467</guid>
<content:encoded><![CDATA[
<div> Keywords: Human evaluators, Machine Translation, low-resource languages, evaluation platform, Natural Language Processing

Summary:
This paper addresses the challenge of evaluating Machine Translation (MT) systems for low-resource languages (LRLs) due to the lack of datasets and human evaluators. It reviews existing evaluation procedures and proposes a design for a recruitment and gamified evaluation platform to bridge the resource gap. The platform aims to enable developers of MT systems to find and engage adequate human evaluators, especially those with expertise in the target language. The design emphasizes testing for adequacy, fluency, and other key metrics that automated metrics may overlook. Challenges in evaluating the platform are discussed, as well as its potential applications in broader Natural Language Processing (NLP) research. The proposed platform offers a solution to the scarcity of resources for evaluating MT systems in LRLs and could significantly enhance the quality and accuracy of these systems in practice. 

<br /><br />Summary: <div>
arXiv:2506.11467v1 Announce Type: cross 
Abstract: Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research</title>
<link>https://arxiv.org/abs/2506.11727</link>
<guid>https://arxiv.org/abs/2506.11727</guid>
<content:encoded><![CDATA[
<div> limitations, YouTube Data API, academic research, search endpoint, bias

Summary:
This paper evaluates the search endpoint of YouTube's Data API (v3) for academic research, highlighting limitations in completeness, representativeness, consistency, and bias. The study, conducted over six months with eleven queries, uncovers significant challenges in video recall and precision, particularly with relevance rankings retrieving off-topic videos. Temporal decay is observed, with a rapid decline in findable videos after 20-60 days post-publication. Inconsistencies in search results compromise replicability, affecting research outcomes like the case study on European Parliament elections. The paper suggests mitigation strategies but ultimately deems the API's search function inadequate for robust academic research, especially regarding Digital Services Act requirements.
<br /><br />Summary: <div>
arXiv:2506.11727v1 Announce Type: cross 
Abstract: This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing "freshness" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries of weighted networks: weight approximation method and its application to food webs</title>
<link>https://arxiv.org/abs/2506.11824</link>
<guid>https://arxiv.org/abs/2506.11824</guid>
<content:encoded><![CDATA[
<div> food webs, symmetries, weighted networks, group theory, ecological co-existence<br />
<br />
Summary: 
The study focuses on identifying symmetries in weighted networks by aggregating edge weights into discrete categories. They apply this method to 250 empirical food webs to assess ecological co-existence and competition in species. Symmetric vertices, even under weak approximations, are found to form small orbits of size two or three in food webs. These symmetric vertices can be present at various trophic levels or network positions. Three symmetry measures are applied to compare structural patterns at the network level, revealing important insights into the roles of vertices in the network. The study highlights the significance of identifying symmetries in complex systems to simplify computations and understand network structures, particularly in the context of ecological interactions in food webs. <div>
arXiv:2506.11824v1 Announce Type: cross 
Abstract: Knowing which parts of a complex system have identical roles simplifies computations and reveals patterns in its network structure. Group theory has been applied to study symmetries in unweighted networks. However, in real-world weighted networks, edge weights are rarely equal, making exact symmetry uncommon. To study symmetries in weighted networks, we aggregate edge weights into a small number of discrete categories. The symmetries of these aggregated networks identify vertices with similar roles in the original weighted network.
  In food webs, this approach helps to quantify ecological co-existence and competition by assessing the functional substitutability of species. We apply our method to 250 empirical food webs, finding that symmetric vertices emerge even under weak approximations, typically forming small orbits of size two or three. These symmetric vertices can appear at any trophic level or network position. We also apply three symmetry measures to compare structural patterns at the network level.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Political Bias in LLMs through Structured Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2506.11825</link>
<guid>https://arxiv.org/abs/2506.11825</guid>
<content:encoded><![CDATA[
<div> language models, political bias, debates, agent gender, echo chambers
Summary:
This study explores the impact of large language models (LLMs) and agent attributes on political bias in debates. Neutral LLM agents tend to align with Democrats, while Republican agents move closer to Neutral positions. Agent gender influences attitudes, with agents adjusting their opinions based on gender knowledge. Contrary to previous findings, agents with shared political affiliations can form echo chambers, intensifying attitudes as debates progress. The study highlights the importance of considering LLM type and agent attributes in understanding political bias and interaction dynamics in debates. <div>
arXiv:2506.11825v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll in the Tanks! Measuring Left-wing Extremism on Reddit at Scale</title>
<link>https://arxiv.org/abs/2307.06981</link>
<guid>https://arxiv.org/abs/2307.06981</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, left-wing extremism, tankies, Reddit, discourse <br />
Summary:<br />
This large-scale study focuses on left-wing extremism, specifically the "tankie" community on Reddit. Tankies emerged in the 1950s in support of actions by the USSR and now back "Actually Existing Socialist" countries like China, USSR, and North Korea. Analysis of 1.3M posts from 53K authors reveals tankies' peripheral position within the far-left community on Reddit. Posts predominantly cover state-level political events over social issues. The study confirms misalignments and conceptual homomorphisms in tankie discourse, aligning with existing theory on their beliefs and behavior. This research sheds light on the distinct positioning and discourse of left-wing extremist groups on social media. <br /><br /> <div>
arXiv:2307.06981v3 Announce Type: replace 
Abstract: Social media's role in the spread and evolution of extremism is a focus of intense study. Online extremists have been involved in the spread of online hate, mis- and disinformation, and real-world violence. However, most existing work has focuses on right-wing extremism. In this paper, we perform a first of its kind large-scale measurement study exploring left-wing extremism. We focus on "tankies," a left-wing community that first arose in the 1950s in support of hardline actions of the USSR and has evolved to support what they call "Actually Existing Socialist" countries, e.g., CCP-run China, the USSR, and North Korea. We collect and analyze 1.3M posts from 53K authors from tankie subreddits, and explore the position of tankies within the broader far-left community on Reddit. Among other things, we find that tankies are clearly on the periphery of the larger far-left community. When examining the contents of posts, we find misalignments and conceptual homomorphisms that confirm the description of tankies in the theoretical work. We also discover that tankies focus more on state-level political events rather than social issues. Our findings provide empirical evidence of the distinct positioning and discourse of left-wing extremist groups on social media.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Datasets for Information Diffusion Tasks</title>
<link>https://arxiv.org/abs/2407.05161</link>
<guid>https://arxiv.org/abs/2407.05161</guid>
<content:encoded><![CDATA[
<div> Keywords: information diffusion, 5W Model, tasks, datasets, taxonomy

Summary:
The study categorizes information diffusion tasks into ten subtasks according to the 5W Model framework, focusing on prediction, bot detection, and misinformation detection. A systematic taxonomy is provided, along with an analysis of publicly available datasets linked to users and content attributes. The attributes include user information, social network, bot label, propagation content, propagation network, and veracity label. The dataset repository is accessible on the authors' GitHub page. The study highlights the need for a comprehensive categorization and integration of datasets in information diffusion research to advance understanding and prediction capabilities. Future research directions and limitations are also discussed, emphasizing the importance of enhancing current datasets for improved information diffusion analysis. <br /><br />Summary: <div>
arXiv:2407.05161v2 Announce Type: replace 
Abstract: Information diffusion across various new media platforms gradually influences perceptions, decisions, and social behaviors of individual users. In communication studies, the famous Five W's of Communication model (5W Model) has displayed the process of information diffusion clearly. At present, although plenty of studies and corresponding datasets about information diffusion have emerged, a systematic categorization of tasks and an integration of datasets are still lacking. To address this gap, we survey a systematic taxonomy of information diffusion tasks and datasets based on the "5W Model" framework. We first categorize the information diffusion tasks into ten subtasks with definitions and datasets analysis, from three main tasks of information diffusion prediction, social bot detection, and misinformation detection. We also collect the publicly available dataset repository of information diffusion tasks with the available links and compare them based on six attributes affiliated to users and content: user information, social network, bot label, propagation content, propagation network, and veracity label. In addition, we discuss the limitations and future directions of current datasets and research topics to advance the future development of information diffusion. The dataset repository can be accessed at our website https://github.com/fuxiaG/Information-Diffusion-Datasets.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly modelling the evolution of social structure and language in online communities</title>
<link>https://arxiv.org/abs/2409.19243</link>
<guid>https://arxiv.org/abs/2409.19243</guid>
<content:encoded><![CDATA[
<div> Keywords: group interactions, online communities, community structure, language modelling, extremist groups

Summary: 
Our study focuses on the dynamic nature of group interactions within online communities. We introduce a novel method that simultaneously models community structure and language over time. By generating dynamic word and user representations, our system can effectively cluster users, explore thematic interests of groups, and predict group membership. In our evaluation using a dataset of misogynistic extremist groups, our method outperforms previous models by incorporating both social structure and temporal language embeddings. This approach allows for new analyses of online groups, such as tracking their response to temporal events and measuring their use of violent language. Understanding the socio-temporal context of group interactions is crucial, especially in the context of extremist groups, and our method provides valuable insights for researchers studying online community dynamics.<br /><br />Summary: <div>
arXiv:2409.19243v2 Announce Type: replace 
Abstract: Group interactions take place within a particular socio-temporal context, which should be taken into account when modelling interactions in online communities. We propose a method for jointly modelling community structure and language over time. Our system produces dynamic word and user representations that can be used to cluster users, investigate thematic interests of groups, and predict group membership. We apply and evaluate our method in the context of a set of misogynistic extremist groups. Our results indicate that this approach outperforms prior models which lacked one of these components (i.e. not incorporating social structure, or using static word embeddings) when evaluated on clustering and embedding prediction tasks. Our method further enables novel types of analyses on online groups, including tracing their response to temporal events and quantifying their propensity for using violent language, which is of particular importance in the context of extremist groups.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stationary distribution of node2vec random walks on household models</title>
<link>https://arxiv.org/abs/2502.19039</link>
<guid>https://arxiv.org/abs/2502.19039</guid>
<content:encoded><![CDATA[
<div> random walk, node2vec, network embedding algorithms, stationary distribution, community-structured graphs

Summary:
The study focuses on the node2vec random walk in network embedding algorithms, particularly on community-structured household model graphs. It explores the mathematical properties of node2vec walks, including their stationary distribution. The research provides an explicit description of the stationary distribution in terms of walk parameters and demonstrates how tuning these parameters can interpolate between different types of stationary distributions. By adjusting the walk parameters, the stationary distribution can vary from uniform to size-biased or simple random walk distributions. The study highlights the flexibility and range of node2vec walks and their impact on various graph settings. <div>
arXiv:2502.19039v2 Announce Type: replace-cross 
Abstract: The node2vec random walk has proven to be a key tool in network embedding algorithms. These random walks are tuneable, and their transition probabilities depend on the previous visited node and on the triangles containing the current and the previously visited node. Even though these walks are widely used in practice, most mathematical properties of node2vec walks are largely unexplored, including their stationary distribution. We study the node2vec random walk on community-structured household model graphs. We prove an explicit description of the stationary distribution of node2vec walks in terms of the walk parameters. We then show that by tuning the walk parameters, the stationary distribution can interpolate between uniform, size-biased, or the simple random walk stationary distributions, demonstrating the wide range of possible walks. We further explore these effects on some specific graph settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of A* based heuristic algorithm for efficient interdiction in multi-Layer networks</title>
<link>https://arxiv.org/abs/2506.10017</link>
<guid>https://arxiv.org/abs/2506.10017</guid>
<content:encoded><![CDATA[
<div> Keywords: Criminal interception, limited police resources, transportation network, defender strategy, A-Star heuristic algorithm<br />
Summary: 
In dynamic crime environments, intercepting criminals with limited police resources is a challenging task, especially with continuous changes in the criminal's location and the vast transportation network. The study proposes a layered graph representation with duplicate transportation networks at each time step to optimize defender strategies using the A-Star heuristic algorithm. The defender aims to maximize the probability of successful interdiction against attacker strategies. Performance evaluation against a Mixed-Integer Linear Programming (MILP) approach shows that the proposed method effectively tackles complexity and produces high-quality solutions quickly.<br /> 
Summary: <div>
arXiv:2506.10017v1 Announce Type: new 
Abstract: Intercepting a criminal using limited police resources presents a significant challenge in dynamic crime environments, where the criminal's location continuously changes over time. The complexity is further heightened by the vastness of the transportation network. To tackle this problem, we propose a layered graph representation, in which each time step is associated with a duplicate of the transportation network. For any given set of attacker strategies, a near-optimal defender strategy is computed using the A-Star heuristic algorithm applied to the layered graph. The defender's goal is to maximize the probability of successful interdiction. We evaluate the performance of the proposed method by comparing it with a Mixed-Integer Linear Programming (MILP) approach used for the defender. The comparison considers both computational efficiency and solution quality. The results demonstrate that our approach effectively addresses the complexity of the problem and delivers high-quality solutions within a short computation time.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference of Hierarchical Core-Periphery Structure in Temporal Network</title>
<link>https://arxiv.org/abs/2506.10135</link>
<guid>https://arxiv.org/abs/2506.10135</guid>
<content:encoded><![CDATA[
<div> Keywords: core-periphery structure, temporal networks, multilayer-network, stochastic block models, Markov-chain Monte Carlo.

Summary: 
The study focuses on detecting hierarchical core-periphery structures in temporal networks using a multilayer-network representation and stochastic block models. Unlike traditional core-periphery structures, the hierarchical approach allows for nested, tree-like, and non-nested mesoscale structures. Statistical inference is performed using a Markov-chain Monte Carlo approach. The method is applied to two real-world temporal networks, revealing interesting structures within them. This research fills a gap in the field by providing a method for detecting core-periphery structures in time-dependent networks, enabling a deeper understanding of network dynamics and relationships. <div>
arXiv:2506.10135v1 Announce Type: new 
Abstract: Networks can have various types of mesoscale structures. One type of mesoscale structure in networks is core-periphery structure, which consists of densely-connected core nodes and sparsely-connected peripheral nodes. The core nodes are connected densely to each other and can be connected to the peripheral nodes, which are connected sparsely to other nodes. There has been much research on core-periphery structure in time-independent networks, but few core-periphery detection methods have been developed for time-dependent (i.e., ``temporal") networks. Using a multilayer-network representation of temporal networks and an inference approach that employs stochastic block models, we generalize a recent method for detecting hierarchical core-periphery structure \cite{Polanco23} from time-independent networks to temporal networks. In contrast to ``onion-like'' nested core-periphery structures (where each node is assigned to a group according to how deeply it is nested in a network's core), hierarchical core-periphery structures encompass networks with nested structures, tree-like structures (where any two groups must either be disjoint or have one as a strict subset of the other), and general non-nested mesoscale structures (where the group assignments of nodes do not have to be nested in any way). To perform statistical inference and thereby identify core-periphery structure, we use a Markov-chain Monte Carlo (MCMC) approach. We illustrate our method for detecting hierarchical core-periphery structure in two real-world temporal networks, and we briefly discuss the structures that we identify in these networks.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments</title>
<link>https://arxiv.org/abs/2506.10120</link>
<guid>https://arxiv.org/abs/2506.10120</guid>
<content:encoded><![CDATA[
<div> Graph-based Active Learning, dynamic environments, GRAIL framework, evaluation metrics, real-world sensor data <br />
<br />
Summary: <br />
Graph-based Active Learning (AL) leverages graph structure to prioritize label queries in dynamic environments. The GRAIL framework introduces novel metrics for evaluating AL strategies, considering sustained effectiveness, diversity, and user burden. Experiments on real-life sensor data highlight trade-offs between prediction performance and user burden, emphasizing the importance of balancing node importance, query diversity, and network topology in AL methods. GRAIL provides a comprehensive evaluation mechanism for graph AL solutions in dynamic environments. <div>
arXiv:2506.10120v1 Announce Type: cross 
Abstract: Graph-based Active Learning (AL) leverages the structure of graphs to efficiently prioritize label queries, reducing labeling costs and user burden in applications like health monitoring, human behavior analysis, and sensor networks. By identifying strategically positioned nodes, graph AL minimizes data collection demands while maintaining model performance, making it a valuable tool for dynamic environments. Despite its potential, existing graph AL methods are often evaluated on static graph datasets and primarily focus on prediction accuracy, neglecting user-centric considerations such as sampling diversity, query fairness, and adaptability to dynamic settings. To bridge this gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate graph AL strategies in dynamic, real-world environments. GRAIL introduces novel metrics to assess sustained effectiveness, diversity, and user burden, enabling a comprehensive evaluation of AL methods under varying conditions. Extensive experiments on datasets featuring dynamic, real-life human sensor data reveal trade-offs between prediction performance and user burden, highlighting limitations in existing AL strategies. GRAIL demonstrates the importance of balancing node importance, query diversity, and network topology, providing an evaluation mechanism for graph AL solutions in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing name generator designs in rural panel studies: analyzing alter retention and change</title>
<link>https://arxiv.org/abs/2506.10136</link>
<guid>https://arxiv.org/abs/2506.10136</guid>
<content:encoded><![CDATA[
<div> Keywords: personal network study, rural community, relational attributes, network stability, design considerations

Summary: 
- A two-wave personal network study was conducted in a rural Romanian community with 68 participants.
- Two name generators were used in the study - a fixed-choice generator focusing on emotional closeness and a free-choice generator based on frequent interaction.
- Participants were interviewed using both generators to compare tie characteristics and assess retention across waves.
- The study found that alters who were kin, co-residents, or emotionally close were more likely to be retained in personal networks, regardless of the generator type used.
- These findings highlight the importance of relational attributes in personal network stability and suggest design considerations for conducting network studies in resource-limited and culturally distinct settings.

<br /><br />Summary: 
A personal network study in a rural Romanian community involving 68 participants utilized two name generators to assess tie characteristics and retention across waves. The study found that kin, co-residents, and emotionally close alters were more likely to be retained in personal networks, regardless of the generator type used. These findings emphasize the significance of relational attributes in personal network stability and provide insights for designing network studies in resource-limited and culturally distinct settings. <div>
arXiv:2506.10136v1 Announce Type: cross 
Abstract: We conducted a two-wave personal network study in a rural Romanian community, interviewing the same participants (n = 68) using two name generators. Wave 1 employed a fixed-choice generator (n = 25) focused on emotional closeness; Wave 2 used a free-choice generator based on frequent interaction. We compared tie characteristics and assessed retention across waves. Alters who were kin, co-residents, or emotionally close were more likely to be retained, regardless of generator type. These findings underscore the role of relational attributes in personal network stability and highlight design considerations for network studies in resource-limited, culturally distinct settings.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast nonparametric inference of network backbones for weighted graph sparsification</title>
<link>https://arxiv.org/abs/2409.06417</link>
<guid>https://arxiv.org/abs/2409.06417</guid>
<content:encoded><![CDATA[
<div> Keywords: Network backbones, Weighted networks, Minimum Description Length, Bayesian model, Greedy algorithm

Summary:
Network backbones provide sparse representations of weighted networks, simplifying computations and visualizations. Existing methods often rely on user-specified parameters or impose restrictions on the backbone structure. A new nonparametric framework based on the Minimum Description Length principle automatically selects the optimal set of edges for the backbone without these limitations. Objective functions evaluate edge importance globally and locally, accommodating various weight distributions under Bayesian model specifications. An efficient greedy algorithm minimizes these objectives, maintaining network connectivity, weight heterogeneity, and spreading dynamics while removing a significant portion of edges. Empirical comparisons on real and synthetic networks highlight the effectiveness of both global and local backboning methods. The runtime complexity of the algorithm is log-linear in the number of edges.<br /><br />Summary: <div>
arXiv:2409.06417v3 Announce Type: replace 
Abstract: Network backbones provide useful sparse representations of weighted networks by keeping only their most important links, permitting a range of computational speedups and simplifying network visualizations. A key limitation of existing network backboning methods is that they either require the specification of a free parameter (e.g. significance level) that determines the number of edges to keep in the backbone, or impose specific restrictions on the topology of the backbone (e.g. that it is a spanning tree). Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations and automatically selects the optimal set of edges to retain using the Minimum Description Length (MDL) principle. We develop objective functions for global and local network backboning which evaluate the importance of an edge in the context of the whole network and individual node neighborhoods respectively and are generalizable to any weight distribution under Bayesian model specifications that fix the average edge weight either exactly or in expectation. We then construct an efficient and provably optimal greedy algorithm to identify the backbone minimizing our objectives, whose runtime complexity is log-linear in the number of edges. We demonstrate our methods by comparing them with existing methods in a range of tasks on real and synthetic networks, finding that both the global and local backboning methods can preserve network connectivity, weight heterogeneity, and spreading dynamics while removing a substantial fraction of edges.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alice and the Caterpillar: A more descriptive null model for assessing data mining results</title>
<link>https://arxiv.org/abs/2506.09764</link>
<guid>https://arxiv.org/abs/2506.09764</guid>
<content:encoded><![CDATA[
<div> Bipartite Joint Degree Matrix, caterpillars, Markov chain Monte Carlo, null models, statistical hypothesis testing <br />
Summary:
The article introduces novel null models for evaluating binary transactional and sequence data using statistical hypothesis testing. These new null models maintain more properties of the observed dataset, particularly preserving the Bipartite Joint Degree Matrix of the corresponding bipartite graph. The introduced algorithms, collectively called Alice, utilize Markov chain Monte Carlo methods to sample datasets from the new null models efficiently. Experimental results demonstrate that Alice mixes quickly and is scalable, outperforming existing models. The null model proposed in the study reveals different significant results compared to those in existing literature, showcasing its potential for uncovering new insights from binary datasets. <br /><br />Summary: <div>
arXiv:2506.09764v1 Announce Type: new 
Abstract: We introduce novel null models for assessing the results obtained from observed binary transactional and sequence datasets, using statistical hypothesis testing. Our null models maintain more properties of the observed dataset than existing ones. Specifically, they preserve the Bipartite Joint Degree Matrix of the bipartite (multi-)graph corresponding to the dataset, which ensures that the number of caterpillars, i.e., paths of length three, is preserved, in addition to other properties considered by other models. We describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling datasets from our null models, based on a carefully defined set of states and efficient operations to move between them. The results of our experimental evaluation show that Alice mixes fast and scales well, and that our null model finds different significant results than ones previously considered in the literature.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELRUHNA: Elimination Rule-basedHypergraph Alignment</title>
<link>https://arxiv.org/abs/2506.09866</link>
<guid>https://arxiv.org/abs/2506.09866</guid>
<content:encoded><![CDATA[
<div> framework, unsupervised, hypergraph alignment, binary quadratic optimization, similarity propagation <br />
<br />
The paper introduces ELRUHNA, an elimination rule-based framework for unsupervised hypergraph alignment using a bipartite representation. It proposes an incidence alignment formulation, a binary quadratic optimization method for aligning vertices and hyperedges. ELRUHNA utilizes a novel similarity propagation scheme with local matching and cooling rules, supported by an initialization strategy based on generalized eigenvector centrality. Extensive experiments on real-world datasets show that ELRUHNA outperforms state-of-the-art algorithms in alignment accuracy while efficiently scaling to large hypergraphs. The approach addresses the NP-hard nature of hypergraph alignment and provides a practical solution for discovering patterns and correspondence in high-order relational data.<br /><br />Summary: <div>
arXiv:2506.09866v1 Announce Type: new 
Abstract: Hypergraph alignment is a well-known NP-hard problem with numerous practical applications across domains such as bioinformatics, social network analysis, and computer vision. Despite its computational complexity, practical and scalable solutions are urgently needed to enable pattern discovery and entity correspondence in high-order relational data. The problem remains understudied in contrast to its graph based counterpart. In this paper, we propose ELRUHNA, an elimination rule-based framework for unsupervised hypergraph alignment that operates on the bipartite representation of hypergraphs. We introduce the incidence alignment formulation, a binary quadratic optimization approach that jointly aligns vertices and hyperedges. ELRUHNA employs a novel similarity propagation scheme using local matching and cooling rules, supported by an initialization strategy based on generalized eigenvector centrality for incidence matrices. Through extensive experiments on real-world datasets, we demonstrate that ELRUHNA achieves higher alignment accuracy compared to state-of-the-art algorithms, while scaling effectively to large hypergraphs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation</title>
<link>https://arxiv.org/abs/2506.09221</link>
<guid>https://arxiv.org/abs/2506.09221</guid>
<content:encoded><![CDATA[
<div> Keywords: online misinformation, crowdsourcing, human judgment, cognitive biases, automated fact-checking systems

Summary: 
This thesis explores the challenges posed by online misinformation and the potential of harnessing human intelligence through crowdsourcing. It focuses on three key areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, the research identifies factors that influence human judgments and introduces a model for predicting and explaining truthfulness. The findings indicate that non-expert judgments can align with expert assessments, especially when considering factors such as timing and experience. By increasing our understanding of human judgment and bias in truthfulness assessment, this research paves the way for the development of more transparent, trustworthy, and interpretable systems for combating misinformation. <div>
arXiv:2506.09221v1 Announce Type: cross 
Abstract: The spread of online misinformation poses serious threats to democratic societies. Traditionally, expert fact-checkers verify the truthfulness of information through investigative processes. However, the volume and immediacy of online content present major scalability challenges. Crowdsourcing offers a promising alternative by leveraging non-expert judgments, but it introduces concerns about bias, accuracy, and interpretability. This thesis investigates how human intelligence can be harnessed to assess the truthfulness of online information, focusing on three areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, it identifies key factors influencing human judgments and introduces a model for the joint prediction and explanation of truthfulness. The findings show that non-expert judgments often align with expert assessments, particularly when factors such as timing and experience are considered. By deepening our understanding of human judgment and bias in truthfulness assessment, this thesis contributes to the development of more transparent, trustworthy, and interpretable systems for combating misinformation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't be Afraid of Cell Complexes! An Introduction from an Applied Perspective</title>
<link>https://arxiv.org/abs/2506.09726</link>
<guid>https://arxiv.org/abs/2506.09726</guid>
<content:encoded><![CDATA[
<div> topology, signal processing, cell complexes, network science, algebra

Summary: 
In this paper, a simplified definition of cell complexes is presented, making the concept more accessible to a wider audience and applicable in practical settings. The introduction of abstract regular cell complexes (ARCCs) eliminates the need for abstract topological concepts, replacing them with algebraic notions, making it easier to understand and apply in signal processing and network science. This new definition bridges the gap between abstract topology and signal processing methods, providing a more straightforward approach to working with cell complexes. The paper also offers a simplified explanation of cell complexes, focusing on dimensions 2 and below, which are commonly used in practical applications. This simplification enhances the understanding and usability of cell complexes in various fields, making them more accessible for researchers and practitioners alike. <br /><br /> <div>
arXiv:2506.09726v1 Announce Type: cross 
Abstract: Cell complexes (CCs) are a higher-order network model deeply rooted in algebraic topology that has gained interest in signal processing and network science recently. However, while the processing of signals supported on CCs can be described in terms of easily-accessible algebraic or combinatorial notions, the commonly presented definition of CCs is grounded in abstract concepts from topology and remains disconnected from the signal processing methods developed for CCs. In this paper, we aim to bridge this gap by providing a simplified definition of CCs that is accessible to a wider audience and can be used in practical applications. Specifically, we first introduce a simplified notion of abstract regular cell complexes (ARCCs). These ARCCs only rely on notions from algebra and can be shown to be equivalent to regular cell complexes for most practical applications. Second, using this new definition we provide an accessible introduction to (abstract) cell complexes from a perspective of network science and signal processing. Furthermore, as many practical applications work with CCs of dimension 2 and below, we provide an even simpler definition for this case that significantly simplifies understanding and working with CCs in practice.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse</title>
<link>https://arxiv.org/abs/2506.09947</link>
<guid>https://arxiv.org/abs/2506.09947</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, extremism, AI, monitoring, German online data 

Summary: 
KI4Demokratie is an AI-based platform designed to assist in monitoring right-wing discourse that threatens democratic values on social media. By utilizing machine learning models on large-scale German online data, the platform provides a comprehensive overview of trends in the German digital sphere. The early analysis demonstrates the complexity of tracking organized extremist behavior and highlights the potential of the integrated approach, particularly during significant events. This tool is crucial for journalists, researchers, and policymakers seeking to combat the rapid spread of antidemocratic narratives without infringing on freedom of expression. With social media increasingly fueling extremism, KI4Demokratie serves as a valuable tool in addressing the challenges posed by right-wing extremism and ensuring the preservation of democratic values in the digital landscape. 

<br /><br />Summary: <div>
arXiv:2506.09947v1 Announce Type: cross 
Abstract: Social media increasingly fuel extremism, especially right-wing extremism, and enable the rapid spread of antidemocratic narratives. Although AI and data science are often leveraged to manipulate political opinion, there is a critical need for tools that support effective monitoring without infringing on freedom of expression. We present KI4Demokratie, an AI-based platform that assists journalists, researchers, and policymakers in monitoring right-wing discourse that may undermine democratic values. KI4Demokratie applies machine learning models to a large-scale German online data gathered on a daily basis, providing a comprehensive view of trends in the German digital sphere. Early analysis reveals both the complexity of tracking organized extremist behavior and the promise of our integrated approach, especially during key events.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The increasing fragmentation of global science limits the diffusion of ideas</title>
<link>https://arxiv.org/abs/2404.05861</link>
<guid>https://arxiv.org/abs/2404.05861</guid>
<content:encoded><![CDATA[
<div> Keywords: global recognition network, national citation preferences, scientific communities, international diffusion, equitable recognition 

Summary: 
The study introduces a rank-based measure of national citation preferences to construct a global recognition network, revealing uneven recognition between countries. Economic, cultural, and scientific variables influence national citation preferences, leading to a fragmented international scientific system. Multiple scientific communities exhibit strong internal citation preferences but negative preferences between them, highlighting growing fragmentation. A weighted logistic regression model demonstrates the impact of this network on the international diffusion of scientific ideas. The findings emphasize structural barriers to equitable recognition and the role of scientific community membership in shaping influence, providing valuable insights for policymakers seeking to nurture inclusive and impactful global science. 

Summary: <div>
arXiv:2404.05861v2 Announce Type: replace 
Abstract: Global science is often portrayed as a unified system of shared knowledge and open exchange. Yet this vision contrasts with emerging evidence that scientific recognition is uneven and increasingly fragmented along regional and cultural lines. Traditional models emphasize Western dominance in knowledge production but overlook regional dynamics, reinforcing a core-periphery narrative that sustains disparities and marginalizes less prominent countries. In this study, we introduce a rank-based signed measure of national citation preferences, enabling the construction of a global recognition network that distinguishes over- and under-recognition between countries. Using a multinomial logistic link prediction model, we assess how economic, cultural, and scientific variables shape the presence and direction of national citation preferences. We uncover a global structure composed of multiple scientific communities, characterized by strong internal citation preferences and negative preferences between them-revealing growing fragmentation in the international scientific system. A separate weighted logistic regression framework suggests that this network significantly influences the international diffusion of scientific ideas, even after controlling for common covariates. Together, these findings highlight the structural barriers to equitable recognition and underscore the importance of scientific community membership in shaping influence, offering valuable insights for policymakers aiming to foster inclusive and impactful global science.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervision policies can shape long-term risk management in general-purpose AI models</title>
<link>https://arxiv.org/abs/2501.06137</link>
<guid>https://arxiv.org/abs/2501.06137</guid>
<content:encoded><![CDATA[
<div> Keywords: General-Purpose AI, risk reporting ecosystems, supervision policies, AI risks, ChatGPT interactions

Summary: 
The article explores the challenges posed by the rapid deployment of General-Purpose AI models, focusing on large language models. It suggests that AI supervisory entities will face difficulties in managing the influx of risk and incident reports in the AI ecosystem. The study develops a simulation framework based on features extracted from various risk reporting systems and evaluates four supervision policies. Results show that prioritizing high-risk incidents and maintaining diversity in addressing risks are more effective but may overlook systemic issues reported by the broader community. The study validates its findings using real-world data, emphasizing the complex trade-offs involved in AI risk supervision. Ultimately, the choice of risk management policies can significantly impact the perception and management of AI risks across different AI models used in society. 

<br /><br />Summary: <div>
arXiv:2501.06137v2 Announce Type: replace-cross 
Abstract: The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Agent Interaction in Synthetic Social Networks: A Framework for Studying Online Polarization</title>
<link>https://arxiv.org/abs/2502.01340</link>
<guid>https://arxiv.org/abs/2502.01340</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, polarization mechanisms, opinion dynamics, linguistic realism, artificial agents<br />
Summary:<br />
This paper presents a new framework that combines mathematical models with language-based simulations to study online polarization in social networks. By embedding opinion dynamics principles within artificial agents, the framework allows for rigorous mathematical analysis while maintaining naturalistic social interactions. Through offline testing and experiments with human participants in controlled environments, the framework was validated to investigate how polarized discussions impact user perceptions and behavior. The results showed that exposure to polarized content increased sensitivity to emotions and group affiliations, while reducing uncertainty in agents' positions. The methodology bridges the gap between theoretical models and empirical observations, providing insights into the causal mechanisms driving online opinion dynamics. This innovative approach offers opportunities to study social media phenomena through controlled experimentation, opening new avenues for research in understanding online polarization. <br /><br />Summary: <div>
arXiv:2502.01340v3 Announce Type: replace-cross 
Abstract: Online social networks have dramatically altered the landscape of public discourse, creating both opportunities for enhanced civic participation and risks of deepening social divisions. Prevalent approaches to studying online polarization have been limited by a methodological disconnect: mathematical models excel at formal analysis but lack linguistic realism, while language model-based simulations capture natural discourse but often sacrifice analytical precision. This paper introduces an innovative computational framework that synthesizes these approaches by embedding formal opinion dynamics principles within LLM-based artificial agents, enabling both rigorous mathematical analysis and naturalistic social interactions. We validate our framework through comprehensive offline testing and experimental evaluation with 122 human participants engaging in a controlled social network environment. The results demonstrate our ability to systematically investigate polarization mechanisms while preserving ecological validity. Our findings reveal how polarized environments shape user perceptions and behavior: participants exposed to polarized discussions showed markedly increased sensitivity to emotional content and group affiliations, while perceiving reduced uncertainty in the agents' positions. By combining mathematical precision with natural language capabilities, our framework opens new avenues for investigating social media phenomena through controlled experimentation. This methodological advancement allows researchers to bridge the gap between theoretical models and empirical observations, offering unprecedented opportunities to study the causal mechanisms underlying online opinion dynamics.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic block models with many communities and the Kesten--Stigum bound</title>
<link>https://arxiv.org/abs/2503.03047</link>
<guid>https://arxiv.org/abs/2503.03047</guid>
<content:encoded><![CDATA[
<div> community recovery, stochastic block models, low-degree hardness, graphon estimation, non-backtracking walks

Summary:
- The study explores community inference in stochastic block models with a growing number of communities.
- Efficient recovery of communities is achievable above the Kesten-Stigum bound in block models.
- Recovery of block models becomes low-degree hard when the number of communities is below the Kesten-Stigum bound with qn.
- An efficient algorithm based on non-backtracking walks allows for recovery even below the Kesten-Stigum bound when qn.
- A new threshold for efficient community recovery in the regime of increasing number of communities is identified.
- Detection of communities is easy, and the information-theoretic threshold for community recovery is determined as the number of communities q approaches infinity.
- The low-degree hardness results have implications for graphon estimation, improving upon previous research by Luo and Gao (2024). 

Summary:<br />
The study delves into community recovery in growing stochastic block models, revealing that efficient recovery is possible above the Kesten-Stigum bound and becomes challenging below it with qn. Surprisingly, an efficient algorithm using non-backtracking walks enables recovery even below the bound when qn, indicating a new threshold for efficient recovery in this regime. Community detection is straightforward, with the information-theoretic threshold identified as q approaches infinity. These results also positively impact graphon estimation, enhancing previous work by Luo and Gao (2024). <div>
arXiv:2503.03047v2 Announce Type: replace-cross 
Abstract: We study the inference of communities in stochastic block models with a growing number of communities. For block models with $n$ vertices and a fixed number of communities $q$, it was predicted in Decelle et al. (2011) that there are computationally efficient algorithms for recovering the communities above the Kesten--Stigum (KS) bound and that efficient recovery is impossible below the KS bound. This conjecture has since stimulated a lot of interest, with the achievability side proven in a line of research that culminated in the work of Abbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025) provides evidence for the hardness part using the low-degree paradigm.
  In this paper we investigate community recovery in the regime $q=q_n \to \infty$ as $n\to\infty$ where no such predictions exist. We show that efficient inference of communities remains possible above the KS bound. Furthermore, we show that recovery of block models is low-degree hard below the KS bound when the number of communities satisfies $q\ll \sqrt{n}$. Perhaps surprisingly, we find that when $q \gg \sqrt{n}$, there is an efficient algorithm based on non-backtracking walks for recovery even below the KS bound. We identify a new threshold and ask if it is the threshold for efficient recovery in this regime. Finally, we show that detection is easy and identify (up to a constant) the information-theoretic threshold for community recovery as the number of communities $q$ diverges.
  Our low-degree hardness results also naturally have consequences for graphon estimation, improving results of Luo and Gao (2024).
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph signal aware decomposition of dynamic networks via latent graphs</title>
<link>https://arxiv.org/abs/2506.08519</link>
<guid>https://arxiv.org/abs/2506.08519</guid>
<content:encoded><![CDATA[
<div> latent factor analysis, network dynamics, tensor decomposition, graph topology, missing data recovery

Summary:
The article introduces a novel approach for analyzing dynamics on networks by utilizing low-rank tensor decomposition to uncover underlying driving factors of network evolution. The proposed method combines topology and node signals to capture the joint evolution of the network structure and associated signals. By estimating latent adjacency matrices and their temporal scaling signatures through alternating minimization, the approach demonstrates superior performance in reconstructing missing network data, particularly when observations are limited. The two-way decomposition technique enhances interpretability and captures the coupling between topology and signals, outperforming standard tensor-based decompositions and signal-based topology identification methods. The numerical results support the effectiveness of the method in recovering expressive latent graphs and showcasing the potential for addressing practical constraints and privacy concerns in socio-technological systems. 

<br /><br />Summary: <div>
arXiv:2506.08519v1 Announce Type: cross 
Abstract: Dynamics on and of networks refer to changes in topology and node-associated signals, respectively and are pervasive in many socio-technological systems, including social, biological, and infrastructure networks. Due to practical constraints, privacy concerns, or malfunctions, we often observe only a fraction of the topological evolution and associated signal, which not only hinders downstream tasks but also restricts our analysis of network evolution. Such aspects could be mitigated by moving our attention at the underlying latent driving factors of the network evolution, which can be naturally uncovered via low-rank tensor decomposition. Tensor-based methods provide a powerful means of uncovering the underlying factors of network evolution through low-rank decompositions. However, the extracted embeddings typically lack a relational structure and are obtained independently from the node signals. This disconnect reduces the interpretability of the embeddings and overlooks the coupling between topology and signals. To address these limitations, we propose a novel two-way decomposition to represent a dynamic graph topology, where the structural evolution is captured by a linear combination of latent graph adjacency matrices reflecting the overall joint evolution of both the topology and the signal. Using spatio-temporal data, we estimate the latent adjacency matrices and their temporal scaling signatures via alternating minimization, and prove that our approach converges to a stationary point. Numerical results show that the proposed method recovers individually and collectively expressive latent graphs, outperforming both standard tensor-based decompositions and signal-based topology identification methods in reconstructing the missing network especially when observations are limited.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning</title>
<link>https://arxiv.org/abs/2304.09914</link>
<guid>https://arxiv.org/abs/2304.09914</guid>
<content:encoded><![CDATA[
<div> deep-learning, affective nonverbal communication, political leaders, YouTube videos, populist rhetoric

Summary:
This study utilizes deep-learning methods to analyze facial expressions in YouTube videos of political leaders from 15 countries. Emotions such as anger, disgust, fear, happiness, sadness, surprise, and neutrality are measured and compared. The research shows a moderate agreement between deep-learning and human label emotions. Significant differences in negative emotion scores are found among leaders exhibiting varying levels of populist rhetoric. <div>
arXiv:2304.09914v5 Announce Type: replace-cross 
Abstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branch-and-cut algorithms for colorful components problems</title>
<link>https://arxiv.org/abs/2408.16508</link>
<guid>https://arxiv.org/abs/2408.16508</guid>
<content:encoded><![CDATA[
<div> community detection, cybersecurity, bioinformatics, partitioning, colorful connected components

Summary:
The article addresses optimization problems involving the partitioning of colored graphs into colorful connected components. These components are defined as having each color appear at most once. Different objective functions determine the best partition. The problems have practical applications in community detection, cybersecurity, and bioinformatics. The authors present integer non-linear formulations, linearized using standard techniques. They develop exact branch-and-cut algorithms for solving the formulations, incorporating various improvement techniques like valid inequalities and warm-start methods. Computational tests show the algorithms are effective for reasonably sized instances. This work introduces the first exact algorithm for solving these specific optimization problems. <div>
arXiv:2408.16508v2 Announce Type: replace-cross 
Abstract: We tackle three optimization problems in which a colored graph, where each node is assigned a color, must be partitioned into colorful connected components. A component is defined as colorful if each color appears at most once. The problems differ in the objective function, which determines which partition is the best one. These problems have applications in community detection, cybersecurity, and bioinformatics. We present integer non-linear formulations, which are then linearized using standard techniques. To solve these formulations, we develop exact branch-and-cut algorithms, embedding various improving techniques, such as valid inequalities, bounds limiting the number of variables, and warm-start and preprocessing techniques. Extensive computational tests on benchmark instances demonstrate the effectiveness of the proposed procedures. The branch-and-cut algorithms can solve reasonably sized instances efficiently. To the best of our knowledge, we are the first to propose an exact algorithm for solving these problems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmarking Framework for Network Classification Methods</title>
<link>https://arxiv.org/abs/2506.06513</link>
<guid>https://arxiv.org/abs/2506.06513</guid>
<content:encoded><![CDATA[
<div> Benchmark Dataset, Network Classification, Feature Extraction Techniques, Structural Noise, Classification Performance

Summary: 
The research introduces a benchmark dataset of synthetic networks for testing network classification methods. Different feature extraction techniques including traditional structural measures, Life-Like Network Automata (LLNA), Graph2Vec, Deterministic Tourist Walk (DTW), and Deterministic Tourist Walk with Bifurcation (DTWB) are evaluated. The study incorporates various levels of structural noise to test the resilience of these techniques. Results show that DTWB outperforms other methods in both classifying classes and subclasses, even in noisy conditions. LLNA and DTW also show strong performance, while Graph2Vec falls mid-range in accuracy. Surprisingly, traditional topological measures exhibit the weakest classification performance. The research highlights the importance of robust feature extraction techniques for effective network classification, especially in noisy environments.<br /><br />Summary: <div>
arXiv:2506.06513v1 Announce Type: new 
Abstract: Network classification plays a crucial role in the study of complex systems, impacting fields like biology, sociology, and computer science. In this research, we present an innovative benchmark dataset made up of synthetic networks that are categorized into various classes and subclasses. This dataset is specifically crafted to test the effectiveness and resilience of different network classification methods. To put these methods to the test, we also introduce various types and levels of structural noise. We evaluate five feature extraction techniques: traditional structural measures, Life-Like Network Automata (LLNA), Graph2Vec, Deterministic Tourist Walk (DTW), and its improved version, the Deterministic Tourist Walk with Bifurcation (DTWB). Our experimental results reveal that DTWB surpasses the other methods in classifying both classes and subclasses, even when faced with significant noise. LLNA and DTW also perform well, while Graph2Vec lands somewhere in the middle in terms of accuracy. Interestingly, topological measures, despite their simplicity and common usage, consistently show the weakest classification performance. These findings underscore the necessity of robust feature extraction techniques for effective network classification, particularly in noisy conditions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neighborhood Overlap-Aware High-Order Graph Neural Network for Dynamic Graph Learning</title>
<link>https://arxiv.org/abs/2506.06728</link>
<guid>https://arxiv.org/abs/2506.06728</guid>
<content:encoded><![CDATA[
<div> Dynamic Graph Learning, Temporal Dynamics, Structural Dependencies, Dynamic Graph Neural Networks, Neighborhood Overlap<br />
<br />
Summary: 
The article introduces the Neighborhood Overlap-Aware High-Order Graph Neural Network (NO-HGNN) for dynamic graph learning. NO-HGNN addresses the challenge of effectively modeling temporal dynamics and structural dependencies by incorporating neighborhood overlap into high-order graph neural networks. By computing a correlation score based on the extent of neighborhood overlap to capture complex node interactions, NO-HGNN improves link prediction accuracy. Experimental results on real-world dynamic graphs demonstrate the superior performance of NO-HGNN compared to state-of-the-art approaches in dynamic graph learning. <div>
arXiv:2506.06728v1 Announce Type: new 
Abstract: Dynamic graph learning (DGL) aims to learn informative and temporally-evolving node embeddings to support downstream tasks such as link prediction. A fundamental challenge in DGL lies in effectively modeling both the temporal dynamics and structural dependencies of evolving graph topologies. Recent advances in Dynamic Graph Neural Networks (DGNNs) have obtained remarkable success by leveraging message-passing mechanisms to capture pairwise node interactions. However, these approaches often overlook more complex structural patterns, particularly neighborhood overlap, which can play a critical role in characterizing node interactions. To overcome this limitation, we introduce the Neighborhood Overlap-Aware High-Order Graph Neural Network (NO-HGNN), which is built upon two key innovations: (a) computing a correlation score based on the extent of neighborhood overlap to better capture complex node interactions; and (b) embedding this correlation directly into the message-passing process of high-order graph neural networks in the DGL. Experiments on two real-world dynamic graphs show that NO-HGNN achieves notable improvements in link prediction accuracy, outperforming several state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An $\alpha$-triangle eigenvector centrality of graphs</title>
<link>https://arxiv.org/abs/2506.07026</link>
<guid>https://arxiv.org/abs/2506.07026</guid>
<content:encoded><![CDATA[
<div> centrality, complex networks, eigenvector, graph structure, connectivity <br />
Summary:<br />
The paper introduces a new centrality measure, $\alpha$-triangle eigenvector centrality ($\alpha$TEC), which considers both edge and triangle structures in complex networks. By adjusting the parameter $\alpha, the influence of edges and triangles on centrality scores can be controlled. The centrality scores are computed based on the eigenvector corresponding to the spectral radius of a nonnegative tensor, ensuring unique positive scores for all vertices in connected graphs. Experimental results on synthetic and real-world networks show that $\alpha$TEC effectively identifies the structural positioning of vertices within networks. Increasing $\alpha$ emphasizes the contribution of edges, while decreasing $\alpha$ emphasizes triangles. Additionally, higher $\alpha$TEC rankings indicate a greater impact on network connectivity. The proposed centrality measure provides insights into the importance of vertices in network analysis. <br /> <div>
arXiv:2506.07026v1 Announce Type: new 
Abstract: Centrality represents a fundamental research field in complex network analysis, where centrality measures identify important vertices within networks. Over the years, researchers have developed diverse centrality measures from varied perspectives. This paper proposes an $\alpha$-triangle eigenvector centrality ($\alpha$TEC), which is a global centrality measure based on both edge and triangle structures. It can dynamically adjust the influence of edges and triangles through a parameter $\alpha$ ($\alpha \in (0,1]$). The centrality scores for vertices are defined as the eigenvector corresponding to the spectral radius of a nonnegative tensor. By the Perron-Frobenius theorem, $\alpha$TEC guarantees unique positive centrality scores for all vertices in connected graphs. Numerical experiments on synthetic and real world networks demonstrate that $\alpha$TEC effectively identifies the vertex's structural positioning within graphs. As $\alpha$ increases (decreases), the centrality rankings reflect a stronger (weaker) contribution from edge structure and a weaker (stronger) contribution from triangle structure. Furthermore, we experimentally prove that vertices with higher $\alpha$TEC rankings have a greater impact on network connectivity.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Geometric Embedding for Node Influence Maximization</title>
<link>https://arxiv.org/abs/2506.07435</link>
<guid>https://arxiv.org/abs/2506.07435</guid>
<content:encoded><![CDATA[
<div> Graph layout; Centrality measures; Betweenness; Closeness; Force layout algorithm  
Summary:  
The study introduces an efficient force layout algorithm for embedding graphs into a low-dimensional space, using the radial distance from the origin as a proxy for centrality measures like betweenness and closeness. The algorithm shows strong correlations with degree, PageRank, and other centralities on various graph types. This embedding method enables the identification of high-influence nodes in networks, providing a quicker and scalable alternative to traditional greedy algorithms for centrality computation. The approach offers a computationally less expensive way to compute classical centrality measures and demonstrates promising results on large-scale graphs. Moreover, it can be applied to analyze network structures and identify key nodes efficiently. <div>
arXiv:2506.07435v1 Announce Type: new 
Abstract: Computing classical centrality measures such as betweenness and closeness is computationally expensive on large-scale graphs. In this work, we introduce an efficient force layout algorithm that embeds a graph into a low-dimensional space, where the radial distance from the origin serves as a proxy for various centrality measures. We evaluate our method on multiple graph families and demonstrate strong correlations with degree, PageRank, and paths-based centralities. As an application, it turns out that the proposed embedding allows to find high-influence nodes in a network, and provides a fast and scalable alternative to the standard greedy algorithm.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Powers of Magnetic Graph Matrix: Fourier Spectrum, Walk Compression, and Applications</title>
<link>https://arxiv.org/abs/2506.07343</link>
<guid>https://arxiv.org/abs/2506.07343</guid>
<content:encoded><![CDATA[
<div> Keywords: magnetic graphs, directed networks, matrix powers, walk profiles, link prediction

Summary:<br />
- The study explores the use of magnetic graph theory in analyzing complex directed networks, focusing on local, non-equilibrium behaviors.
- A combinatorial interpretation of magnetic graph matrix powers through directed walk profiles is introduced. This allows for a Fourier transform connection, enabling the reconstruction of walk profiles from matrix powers at discrete potentials.
- The research highlights the compressibility of information captured by the magnetic matrix, showing that an even smaller number of potentials can be used for accurate approximate reconstruction in real networks.
- The ability of magnetic matrix powers to identify frustrated directed cycles, such as feedforward loops, is demonstrated. 
- Powers of the magnetic matrix are shown to be effective in encoding local structural details in directed graphs for link prediction.

<br /><br />Summary: <div>
arXiv:2506.07343v1 Announce Type: cross 
Abstract: Magnetic graphs, originally developed to model quantum systems under magnetic fields, have recently emerged as a powerful framework for analyzing complex directed networks. Existing research has primarily used the spectral properties of the magnetic graph matrix to study global and stationary network features. However, their capacity to model local, non-equilibrium behaviors, often described by matrix powers, remains largely unexplored. We present a novel combinatorial interpretation of the magnetic graph matrix powers through directed walk profiles -- counts of graph walks indexed by the number of edge reversals. Crucially, we establish that walk profiles correspond to a Fourier transform of magnetic matrix powers. The connection allows exact reconstruction of walk profiles from magnetic matrix powers at multiple discrete potentials, and more importantly, an even smaller number of potentials often suffices for accurate approximate reconstruction in real networks. This shows the empirical compressibility of the information captured by the magnetic matrix. This fresh perspective suggests new applications; for example, we illustrate how powers of the magnetic matrix can identify frustrated directed cycles (e.g., feedforward loops) and can be effectively employed for link prediction by encoding local structural details in directed graphs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels</title>
<link>https://arxiv.org/abs/2506.07606</link>
<guid>https://arxiv.org/abs/2506.07606</guid>
<content:encoded><![CDATA[
<div> dataset, user-level stance detection, Bluesky, 2024 U.S. presidential election, Kamala Harris, Donald Trump

Summary:
The article introduces a new dataset called PolitiSky24 for user-level stance detection focused on the 2024 U.S. presidential election, specifically targeting Kamala Harris and Donald Trump. The dataset contains 16,044 user-target stance pairs with additional metadata such as engagement information, interaction graphs, and user posting histories. The dataset was created using advanced information retrieval and large language models, achieving 81% accuracy in stance labeling. This resource fills a gap in political stance analysis by providing a timely, open-data, and user-level perspective. The dataset is openly available for researchers and can be accessed at https://doi.org/10.5281/zenodo.15616911.<br /><br />Summary: <div>
arXiv:2506.07606v1 Announce Type: cross 
Abstract: Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refugees' path to legal stability is long and systematically unequal</title>
<link>https://arxiv.org/abs/2506.07916</link>
<guid>https://arxiv.org/abs/2506.07916</guid>
<content:encoded><![CDATA[
<div> Keywords: legal systems, migrants, refugees, legal journey, integration <br />
Summary: <br />
The study examines the legal pathways of over 350,000 migrants in Austria from 2022 to 2024, focusing on refugees' legal journeys. It finds significant disparities in the time taken for refugees to gain stable status, with Ukrainians taking two months, Syrians nine months, and Afghans up to 20 months. Women, particularly from these regions, are more likely to receive protection compared to Afghan men who wait an average of 30 months. Those who cross borders unofficially have higher exit rates and lower chances of achieving stable legal status. The research highlights that legal integration is a complex and varied process influenced by institutional structures, entry procedures, and unequal timelines. <div>
arXiv:2506.07916v1 Announce Type: cross 
Abstract: Legal systems shape not only the recognition of migrants and refugees but also the pace and stability of their integration. Refugees often shift between multiple legal classifications, a process we refer to as the "legal journey". This journey is frequently prolonged and uncertain. Using a network-based approach, we analyze legal transitions for over 350,000 migrants in Austria (2022 to 2024). Refugees face highly unequal pathways to stability, ranging from two months for Ukrainians to nine months for Syrians and 20 months for Afghans. Women, especially from these regions, are more likely to gain protection; Afghan men wait up to 30 months on average. We also find that those who cross the border without going through official border controls face higher exit rates and lower chances of securing stable status. We show that legal integration is not a uniform process, but one structured by institutional design, procedural entry points, and unequal timelines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Event Detection using Social Media Data Analysis: Applications, Challenges, and Open Problems</title>
<link>https://arxiv.org/abs/2207.03997</link>
<guid>https://arxiv.org/abs/2207.03997</guid>
<content:encoded><![CDATA[
<div> social media, event detection, data analysis, crisis scenarios, research directions

Summary:<br /><br />In this paper, the authors explore the benefits and applications of event detection using social media data analysis. They highlight the importance of monitoring social media for real-world incidents, which can provide valuable information for responding to crises. The study investigates the challenges and tradeoffs inherent in event detection, emphasizing the need for careful analysis of social media streams. The paper also raises key open questions and proposes potential research directions for further exploration in this field. By leveraging the vast amount of social media content available, event detection can offer insights that contribute to improved crisis management and decision-making for individuals and organizations. <div>
arXiv:2207.03997v5 Announce Type: replace 
Abstract: In recent years, social media has become one of the most popular platforms for communication. These platforms allow users to report real-world incidents that might swiftly and widely circulate throughout the whole social network. A social event is a real-world incident that is documented on social media. Social gatherings could contain vital documentation of crisis scenarios. Monitoring and analyzing this rich content can produce information that is extraordinarily valuable and help people and organizations learn how to take action. In this paper, a survey on the potential benefits and applications of event detection with social media data analysis will be presented. Moreover, the critical challenges and the fundamental tradeoffs in event detection will be methodically investigated by monitoring social media stream. Then, fundamental open questions and possible research directions will be introduced.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Analytics in Disaster Response: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2307.04046</link>
<guid>https://arxiv.org/abs/2307.04046</guid>
<content:encoded><![CDATA[
<div> Natural disasters, social media analytics, disaster management, data mining, machine learning<br />
<br />Summary: Social media plays a crucial role in disaster management by providing real-time information for detection, situational awareness, and communication during emergencies. This review paper explores the challenges and opportunities of utilizing social media data, highlighting methodologies like data collection, preprocessing, and analysis using data mining and machine learning techniques. Case studies demonstrate successful applications of social media analytics in disaster response. Ethical considerations and privacy concerns are addressed in the context of using social media data for disaster management. The need for continued research and innovation in social media analytics for disaster management is emphasized for improving response and recovery efforts in the face of increasing natural disasters. <div>
arXiv:2307.04046v2 Announce Type: replace 
Abstract: Social media has emerged as a valuable resource for disaster management, revolutionizing the way emergency response and recovery efforts are conducted during natural disasters. This review paper aims to provide a comprehensive analysis of social media analytics for disaster management. The abstract begins by highlighting the increasing prevalence of natural disasters and the need for effective strategies to mitigate their impact. It then emphasizes the growing influence of social media in disaster situations, discussing its role in disaster detection, situational awareness, and emergency communication. The abstract explores the challenges and opportunities associated with leveraging social media data for disaster management purposes. It examines methodologies and techniques used in social media analytics, including data collection, preprocessing, and analysis, with a focus on data mining and machine learning approaches. The abstract also presents a thorough examination of case studies and best practices that demonstrate the successful application of social media analytics in disaster response and recovery. Ethical considerations and privacy concerns related to the use of social media data in disaster scenarios are addressed. The abstract concludes by identifying future research directions and potential advancements in social media analytics for disaster management. The review paper aims to provide practitioners and researchers with a comprehensive understanding of the current state of social media analytics in disaster management, while highlighting the need for continued research and innovation in this field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Artificial Intelligence for Complex Network: Potential, Methodology and Application</title>
<link>https://arxiv.org/abs/2402.16887</link>
<guid>https://arxiv.org/abs/2402.16887</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, artificial intelligence, statistical mechanics, structures, dynamics

Summary:<br /><br />
Complex networks are prevalent in various real-world systems, demonstrating a transition from disorder to order through intertwined topology and node dynamics. While significant progress has been made in understanding the statistical mechanics, structures, and dynamics of networks, challenges persist in exploring realistic systems and practical applications. The integration of artificial intelligence (AI) technologies with diverse network data presents opportunities to advance complex network research. This survey comprehensively addresses the potential benefits of AI in overcoming research challenges, summarizing key problems and reviewing methodologies and applications. By bridging AI and complex network science, valuable insights are provided to drive further interdisciplinary research progress. <br /><br />Summary: <div>
arXiv:2402.16887v2 Announce Type: replace 
Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of complex network research. It endeavors to summarize the pivotal research problems and provide an exhaustive review of the corresponding methodologies and applications. Through this comprehensive survey-the first of its kind on AI for complex networks-we expect to provide valuable insights that will drive further research and advancement in this interdisciplinary field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2307.12555</link>
<guid>https://arxiv.org/abs/2307.12555</guid>
<content:encoded><![CDATA[
<div> robustness, unsupervised, Graph Contrastive Learning, structural attacks, homophily-driven sanitation view

Summary:
This article explores the adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. Through empirical and theoretical analysis, existing attacks on GCL are examined, revealing their impact on performance. Inspired by these findings, a robust GCL framework, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), is proposed. This framework integrates a homophily-driven sanitation view that addresses the non-differentiable nature of the sanitation objective. Techniques are developed to enable gradient-based end-to-end robust GCL. Additionally, a fully unsupervised hyperparameter tuning method is introduced, eliminating the need for node labels. Extensive experiments demonstrate that GCHS outperforms state-of-the-art baselines in generating high-quality node embeddings. Results also show superior performance on important downstream tasks, showcasing the effectiveness of GCHS against structural attacks on GCL. 

<br /><br />Summary: <div>
arXiv:2307.12555v2 Announce Type: replace-cross 
Abstract: We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.04548</link>
<guid>https://arxiv.org/abs/2406.04548</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, explanation methods, GNNAnatomy, graphlets <br />
Summary:
This paper critiques existing model-level explanation methods for Graph Neural Networks (GNNs) that rely on maximizing classification confidence, assuming a single explanation suffices for an entire class of graphs, and assuming explanations are inherently trustworthy. The authors introduce GNNAnatomy, a distillation-based method that generates explanations by characterizing graph topology using graphlets and training a transparent multilayer perceptron surrogate. This approach identifies the most discriminative topologies and supports exploration of structural diversity within a class through an interface designed for human-AI teaming. GNNAnatomy aims to provide more reliable and interpretable explanations than current methods while fostering transparency and trust in GNN models. Evaluation on synthetic and real-world datasets shows promising results compared to existing explainable GNN methods. <br /> <div>
arXiv:2406.04548v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) achieve outstanding performance across graph-based tasks but remain difficult to interpret. In this paper, we revisit foundational assumptions underlying model-level explanation methods for GNNs, namely: (1) maximizing classification confidence yields representative explanations, (2) a single explanation suffices for an entire class of graphs, and (3) explanations are inherently trustworthy. We identify pitfalls resulting from these assumptions: methods that optimize for classification confidence may overlook partially learned patterns; topological diversity across graph subsets within the same class is often underrepresented; and explanations alone offer limited support for building user trust when applied to new datasets or models. This paper introduces GNNAnatomy, a distillation-based method designed to generate explanations while avoiding these pitfalls. GNNAnatomy first characterizes graph topology using graphlets, a set of fundamental substructures. We then train a transparent multilayer perceptron surrogate to directly approximate GNN predictions based on the graphlet representations. By analyzing the weights assigned to each graphlet, we identify the most discriminative topologies, which serve as GNN explanations. To account for structural diversity within a class, GNNAnatomy generates explanations at the required granularity through an interface that supports human-AI teaming. This interface helps users identify subsets of graphs where distinct critical substructures drive class differentiation, enabling multi-grained explanations. Additionally, by enabling exploration and linking explanations back to input graphs, the interface fosters greater transparency and trust. We evaluate GNNAnatomy on both synthetic and real-world datasets through quantitative metrics and qualitative comparisons with state-of-the-art model-level explainable GNN methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropEnc: A Property Encoder for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.11554</link>
<guid>https://arxiv.org/abs/2409.11554</guid>
<content:encoded><![CDATA[
<div> encoder, node features, graph neural networks, graph metrics, graph classification

Summary:
PropEnc is introduced as a versatile encoder for generating expressive node embeddings from any graph metric. It addresses the lack of node features in real-world systems by offering a flexible solution that supports low-dimensional representations and diverse input types. Using histogram construction and reversed index encoding, PropEnc effectively mitigates sparsity issues and improves computational efficiency. It can replicate one-hot encoding or approximate indices with high accuracy, making it adaptable to a wide range of graph applications. Extensive experiments on graph classification tasks across social networks lacking node features validate the efficiency of PropEnc in constructing node features from various graph metrics. Overall, PropEnc presents a novel approach to enhancing graph machine learning by providing a mechanism for generating node embeddings without relying on traditional node features.
<br /><br />Summary: <div>
arXiv:2409.11554v3 Announce Type: replace-cross 
Abstract: Graph machine learning, particularly using graph neural networks, heavily relies on node features. However, many real-world systems, such as social and biological networks, lack node features due to privacy concerns, incomplete data, or collection limitations. Structural and positional encoding are commonly used to address this but are constrained by the maximum values of the encoded properties, such as the highest node degree. This limitation makes them impractical for scale-free networks and applications involving large or non-categorical properties. This paper introduces PropEnc, a novel and versatile encoder to generate expressive node embedding from any graph metric. By combining histogram construction with reversed index encoding, PropEnc offers a flexible solution that supports low-dimensional representations and diverse input types, effectively mitigating sparsity issues while improving computational efficiency. Additionally, it replicates one-hot encoding or approximates indices with high accuracy, making it adaptable to a wide range of graph applications. We validate PropEnc through extensive experiments on graph classification task across several social networks lacking node features. The empirical results demonstrate that PropEnc offers an efficient mechanism for constructing node features from various graph metrics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering-induced localization of quantum walks on networks</title>
<link>https://arxiv.org/abs/2412.04325</link>
<guid>https://arxiv.org/abs/2412.04325</guid>
<content:encoded><![CDATA[
<div> quantum walks, networks, localization, clustering, eigenvectors <br />
<br />
Summary: 
Quantum walks on networks are important in quantum information theory, with applications in spatial-search, element-distinctness problems, and node centrality analysis. Unlike classical random walks, quantum walks evolve unitarily without converging to a stationary distribution. This study focuses on the long-time behavior of quantum walks on networks and their localization. The research demonstrates the emergence of localization in highly clustered networks constructed by attaching triangles and provides an analytical expression for the long-time inverse participation ratio. Localization is also observed in Kleinberg navigable small-world networks and Holme-Kim power-law cluster networks, showing that local clustering in networks can induce quantum walk localization. The research highlights the impact of network structure on the evolution of quantum walks and sheds light on the role of network clustering in quantum information processing. <br /> <div>
arXiv:2412.04325v2 Announce Type: replace-cross 
Abstract: Quantum walks on networks are a paradigmatic model in quantum information theory. Quantum-walk algorithms have been developed for various applications, including spatial-search problems, element-distinctness problems, and node centrality analysis. Unlike their classical counterparts, the evolution of quantum walks is unitary, so they do not converge to a stationary distribution. However, for many applications, it is important to understand the long-time behavior of quantum walks and the impact of network structure on their evolution. In the present paper, we study the localization of quantum walks on networks. We demonstrate how localization emerges in highly clustered networks that we construct by recursively attaching triangles, and we derive an analytical expression for the long-time inverse participation ratio that depends on products of eigenvectors of the quantum-walk Hamiltonian. Building on the insights from this example, we then show that localization also occurs in Kleinberg navigable small-world networks and Holme--Kim power-law cluster networks. Our results illustrate that local clustering, which is a key structural feature of networks, can induce localization of quantum walks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Response Strategies for GenAI</title>
<link>https://arxiv.org/abs/2502.00729</link>
<guid>https://arxiv.org/abs/2502.00729</guid>
<content:encoded><![CDATA[
<div> Generative AI, Stack Overflow, data generation, selective response, revenue <br />
<br />
Summary: <br />
The article discusses the impact of Generative AI on platforms like Stack Overflow, emphasizing the vital role of human-generated data. It proposes a solution called selective response, where AI intentionally provides inaccurate responses for new topics to drive users back to human forums. This strategy aims to improve data quality for AI systems and enhance user welfare in the long term. The paper presents an algorithmic approach to maximize GenAI's revenue while considering social welfare constraints and outlines regulatory conditions for the implementation of selective response. This innovative strategy offers a potential remedy to the challenges faced by GenAI systems in obtaining high-quality data and highlights the importance of balancing revenue generation with user well-being. <div>
arXiv:2502.00729v2 Announce Type: replace-cross 
Abstract: The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Artificial Benchmark for Community Detection with Outliers and Overlapping Communities (ABCD+$o^2$)</title>
<link>https://arxiv.org/abs/2506.05486</link>
<guid>https://arxiv.org/abs/2506.05486</guid>
<content:encoded><![CDATA[
<div> community detection, artificial benchmark, ABCD graph, power-law distribution, overlapping communities

Summary: 
The article introduces the Artificial Benchmark for Community Detection (ABCD) graph model, which has community structure and power-law distributions for degrees and community sizes. This model, similar to the LFR model but faster and more analytically tractable, offers a new perspective on graph generation. The ABCD model variant, ABCD+$o$, includes outliers, while the ABCD+$o^2$ variant allows for overlapping communities. These variations expand the model's capabilities and offer new insights into community detection algorithms and graph structures. Through the ABCD model and its extensions, researchers can generate random graphs that mimic real-world communities, providing a valuable tool for evaluating and comparing community detection algorithms. <div>
arXiv:2506.05486v1 Announce Type: new 
Abstract: The Artificial Benchmark for Community Detection (ABCD) graph is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model, and its generalization to include outliers (ABCD+$o$), and introduce another variant that allows for overlapping communities, ABCD+$o^2$.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Community-Level Blocklists in Decentralized Social Media</title>
<link>https://arxiv.org/abs/2506.05522</link>
<guid>https://arxiv.org/abs/2506.05522</guid>
<content:encoded><![CDATA[
<div> blocklists, decentralized social media, moderation, Mastodon, content analysis
Summary:
Community-level blocklists play a crucial role in decentralized social media moderation by enabling moderators to prevent interactions with communities acting in bad faith. A study on Mastodon moderators revealed a wide variation in blocklist goals, inclusion criteria, and transparency. Moderators balance proactive safety measures, reactive practices, and caution regarding false positives when utilizing blocklists. Challenges and limitations were identified, prompting suggestions for design improvements such as comment receipts, category filters, and collaborative voting. Trade-offs between openness, safety, and nuance were highlighted, emphasizing the complexity of moderator roles and the need for future design opportunities in decentralized content moderation. <div>
arXiv:2506.05522v1 Announce Type: new 
Abstract: Community-level blocklists are key to content moderation practices in decentralized social media. These blocklists enable moderators to prevent other communities, such as those acting in bad faith, from interacting with their own -- and, if shared publicly, warn others about communities worth blocking. Prior work has examined blocklists in centralized social media, noting their potential for collective moderation outcomes, but has focused on blocklists as individual-level tools. To understand how moderators perceive and utilize community-level blocklists and what additional support they may need, we examine social media communities running Mastodon, an open-source microblogging software built on the ActivityPub protocol. We conducted (1) content analysis of the community-level blocklist ecosystem, and (2) semi-structured interviews with twelve Mastodon moderators. Our content analysis revealed wide variation in blocklist goals, inclusion criteria, and transparency. Interviews showed moderators balance proactive safety, reactive practices, and caution around false positives when using blocklists for moderation. They noted challenges and limitations in current blocklist use, suggesting design improvements like comment receipts, category filters, and collaborative voting. We discuss implications for decentralized content moderation, highlighting trade-offs between openness, safety, and nuance; the complexity of moderator roles; and opportunities for future design.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Coordination on Short-Video Platforms: The Challenge of Multimodality and Complex Similarity on TikTok</title>
<link>https://arxiv.org/abs/2506.05868</link>
<guid>https://arxiv.org/abs/2506.05868</guid>
<content:encoded><![CDATA[
<div> Keywords: online coordinated behaviour, multilayer network analysis, TikTok, political videos, detection

Summary:
This study focuses on detecting coordination on short-video platform TikTok, which poses unique challenges due to its integrated multimedia content. The researchers propose a methodology based on multilayer network analysis to capture coordination across video, audio, and text modalities, specifically addressing the complex similarity found in video and audio content. Testing this approach on political videos from TikTok, the study demonstrates its effectiveness in identifying coordination among users. However, the researchers also highlight potential pitfalls and limitations of the method. Overall, the study contributes to understanding coordinated behavior on platforms like TikTok and emphasizes the importance of considering various modalities in detecting such behaviors. <div>
arXiv:2506.05868v1 Announce Type: new 
Abstract: Research on online coordinated behaviour has predominantly focused on text-based social media platforms, where coordination manifests clearly through the frequent posting of identical hyperlinks or the frequent re-sharing of the same textual content by the same group of users. However, the rise of short-video platforms like TikTok introduces distinct challenges, by supporting integrated multimodality within posts and complex similarity between them. In this paper, we propose an approach to detecting coordination that addresses these characteristic challenges. Our methodology, based on multilayer network analysis, is tailored to capture coordination across multiple modalities, including video, audio, and text, and explicitly handles complex forms of similarity inherent in video and audio content. We test this approach on political videos posted on TikTok and extracted via the TikTok researcher API. This application demonstrates the capacity of the approach to identify coordination, while also critically highlighting potential pitfalls and limitations.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the co-evolution of online engagement with (mis)information and its visibility at scale</title>
<link>https://arxiv.org/abs/2506.06106</link>
<guid>https://arxiv.org/abs/2506.06106</guid>
<content:encoded><![CDATA[
<div> COVID-19 pandemic, online attention, misinformation, engagement, visibility <br />
Summary:<br />
- Online attention is a valuable resource in the digital age, especially during events like the COVID-19 pandemic. <br />
- Users seek credible sources amidst the prevalence of misinformation on online platforms. <br />
- News outlets compete to attract and retain users' attention through engagement and visibility metrics. <br />
- A study using a temporal network modeling framework analyzed over 100 million COVID-related retweets over 3 years. <br />
- Highly engaged sources experience spikes in follower growth during major events, while less credible sources sustain faster growth outside these periods. <br /> <div>
arXiv:2506.06106v1 Announce Type: new 
Abstract: Online attention is an increasingly valuable resource in the digital age, with extraordinary events such as the COVID-19 pandemic fuelling fierce competition around it. As misinformation pervades online platforms, users seek credible sources, while news outlets compete to attract and retain their attention. Here we measure the co-evolution of online "engagement" with (mis)information and its "visibility", where engagement corresponds to user interactions on social media, and visibility to fluctuations in user follower counts. Using a scalable temporal network modelling framework applied to over 100 million COVID-related retweets spanning 3 years, we find that highly engaged sources experience sharp spikes in follower growth during major events (e.g., vaccine rollouts, epidemic severity), whereas sources with more questionable credibility tend to sustain faster growth outside of these periods. Our framework lends itself to studying other large-scale events where online attention is at stake, such as climate and political debates.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Large Language Models Can Increase the Belief Accuracy of Social Networks</title>
<link>https://arxiv.org/abs/2506.06153</link>
<guid>https://arxiv.org/abs/2506.06153</guid>
<content:encoded><![CDATA[
<div> personalized LLMs, belief accuracy, social networks, misinformation, corrective agents
Summary:
- The study explores the impact of personalized Large Language Models (LLMs) on belief accuracy in social networks during the 2024 US presidential election.
- Personalized LLMs lead individuals to update their beliefs towards the truth.
- Individuals with personalized LLMs in their social network choose to follow them and include others with more accurate beliefs in their social networks.
- The study demonstrates that LLMs have the potential to influence individual beliefs and social networks.
- It suggests that LLMs can act as corrective agents in online environments.
<br /><br />Summary: <div>
arXiv:2506.06153v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly involved in shaping public understanding on contested issues. This has led to substantial discussion about the potential of LLMs to reinforce or correct misperceptions. While existing literature documents the impact of LLMs on individuals' beliefs, limited work explores how LLMs affect social networks. We address this gap with a pre-registered experiment (N = 1265) around the 2024 US presidential election, where we empirically explore the impact of personalized LLMs on belief accuracy in the context of social networks. The LLMs are constructed to be personalized, offering messages tailored to individuals' profiles, and to have guardrails for accurate information retrieval. We find that the presence of a personalized LLM leads individuals to update their beliefs towards the truth. More importantly, individuals with a personalized LLM in their social network not only choose to follow it, indicating they would like to obtain information from it in subsequent interactions, but also construct subsequent social networks to include other individuals with beliefs similar to the LLM -- in this case, more accurate beliefs. Therefore, our results show that LLMs have the capacity to influence individual beliefs and the social networks in which people exist, and highlight the potential of LLMs to act as corrective agents in online environments. Our findings can inform future strategies for responsible AI-mediated communication.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Language Models are Good Heterogeneous Graph Generalizers</title>
<link>https://arxiv.org/abs/2506.06157</link>
<guid>https://arxiv.org/abs/2506.06157</guid>
<content:encoded><![CDATA[
<div> Keywords: Heterogeneous graph neural networks, large language models, masked language modeling, structural and semantic information, generalization performance <br />
Summary: 
Heterogeneous graph neural networks (HGNNs) in heterogeneous graphs struggle with generalization across domains and tasks. Integrating HGNNs with large language models (LLMs) has been proposed for better generalizable graph learning, but disparities in embedding spaces limit effectiveness. The MLM4HG method introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information from HGs. It utilizes customized textual templates in a cloze-style "mask" token prediction paradigm to unify different graph tasks. By converting HGs from various domains into textual formats based on metapaths, MLM4HG enables fine-tuning of pretrained language models with a constrained vocabulary, enhancing generalization to unseen target HGs. Experimental results on real-world datasets demonstrate the superior generalization performance of MLM4HG in both few-shot and zero-shot scenarios compared to existing methods. <br /><br />Summary: <div>
arXiv:2506.06157v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. Recently, some researchers have turned to integrating HGNNs with large language models (LLMs) for more generalizable heterogeneous graph learning. However, these approaches typically extract structural information via HGNNs as HG tokens, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension of HGs. Moreover, as these HG tokens are often derived from node-level tasks, the model's ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style "mask" token prediction paradigm. Specifically, MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at https://github.com/BUPT-GAMMA/MLM4HG.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport</title>
<link>https://arxiv.org/abs/2506.02619</link>
<guid>https://arxiv.org/abs/2506.02619</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Heterogeneous Graphs, Self-Supervised Learning, Optimal Transport<br />
<br />
Summary:<br />
Heterogeneous Graph Neural Networks (HGNNs) have shown promise in processing heterogeneous information networks. A novel approach, called HGOT, integrates optimal transport to facilitate self-supervised learning without requiring graph augmentation strategies. Instead of manual selection of positive and negative samples, HGOT uses an optimal transport mechanism to align node representations with the graph space. By aggregating information from different meta-paths and using optimal transport plans, HGOT achieves higher-quality node representations. Experimental results on real-world datasets demonstrate that HGOT outperforms state-of-the-art methods in various downstream tasks, particularly node classification, where it achieves over 6% improvement in accuracy. <div>
arXiv:2506.02619v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent capabilities in processing heterogeneous information networks. Self-supervised learning on heterogeneous graphs, especially contrastive self-supervised strategy, shows great potential when there are no labels. However, this approach requires the use of carefully designed graph augmentation strategies and the selection of positive and negative samples. Determining the exact level of similarity between sample pairs is non-trivial.To solve this problem, we propose a novel self-supervised Heterogeneous graph neural network with Optimal Transport (HGOT) method which is designed to facilitate self-supervised learning for heterogeneous graphs without graph augmentation strategies. Different from traditional contrastive self-supervised learning, HGOT employs the optimal transport mechanism to relieve the laborious sampling process of positive and negative samples. Specifically, we design an aggregating view (central view) to integrate the semantic information contained in the views represented by different meta-paths (branch views). Then, we introduce an optimal transport plan to identify the transport relationship between the semantics contained in the branch view and the central view. This allows the optimal transport plan between graphs to align with the representations, forcing the encoder to learn node representations that are more similar to the graph space and of higher quality. Extensive experiments on four real-world datasets demonstrate that our proposed HGOT model can achieve state-of-the-art performance on various downstream tasks. In particular, in the node classification task, HGOT achieves an average of more than 6% improvement in accuracy compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Misinformation in the Arab World: Challenges &amp; Opportunities</title>
<link>https://arxiv.org/abs/2506.05582</link>
<guid>https://arxiv.org/abs/2506.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, disinformation, Arab region, fact-checking organizations, information ecosystem <br />
Summary: <br />
Misinformation and disinformation present significant challenges globally, with particular vulnerabilities in the Arab region due to geopolitical instabilities and cultural diversity. In combating this issue, it is essential to focus on detection, tracking, mitigation, and community engagement. By collaborating with grassroots fact-checking organizations and promoting social correction, the Arab world can build a more resilient information ecosystem. Understanding cultural norms and fostering strong collaborative networks are also crucial in addressing misinformation. By prioritizing these strategies, opportunities can be created for a more informed and empowered society in the Arab region. <br /> <div>
arXiv:2506.05582v1 Announce Type: cross 
Abstract: Misinformation and disinformation pose significant risks globally, with the Arab region facing unique vulnerabilities due to geopolitical instabilities, linguistic diversity, and cultural nuances. We explore these challenges through the key facets of combating misinformation: detection, tracking, mitigation and community-engagement. We shed light on how connecting with grass-roots fact-checking organizations, understanding cultural norms, promoting social correction, and creating strong collaborative information networks can create opportunities for a more resilient information ecosystem in the Arab world.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoding meets Persistent Homology on Graphs</title>
<link>https://arxiv.org/abs/2506.05814</link>
<guid>https://arxiv.org/abs/2506.05814</guid>
<content:encoded><![CDATA[
<div> Graph neural networks (GNNs), which have a local inductive bias, can struggle to utilize key structural information. Two approaches, Positional Encoding (PE) and Persistent Homology (PH), aim to address this issue. A study comparing PE and PH found neither to be more expressive than the other. A novel method called PiPE (Persistence-informed Positional Encoding) was developed, which is proven to be more expressive than both PE and PH. PiPE showed strong performance in various tasks such as molecule property prediction, graph classification, and out-of-distribution generalization. The study results provide insights for advancing graph representation learning. 

Keywords: GNNs, Positional Encoding, Persistent Homology, Graph representation learning, PiPE <br /><br />Summary: Graph neural networks have a local inductive bias, making it challenging to leverage key structural information. Positional Encoding (PE) and Persistent Homology (PH) have emerged as solutions, with a study showing neither approach is more expressive. A novel method called PiPE (Persistence-informed Positional Encoding) was developed, proving to be more expressive than both PE and PH. PiPE demonstrated strong performance in various tasks, advancing the field of graph representation learning. <div>
arXiv:2506.05814v1 Announce Type: cross 
Abstract: The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction, graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Generalization with Gaussian Mixture Model Based Augmentation</title>
<link>https://arxiv.org/abs/2411.08638</link>
<guid>https://arxiv.org/abs/2411.08638</guid>
<content:encoded><![CDATA[
<div> generalization error, Rademacher complexity, data augmentation, graph neural networks, Gaussian Mixture Models

Summary:
This paper introduces a theoretical framework for Graph Neural Networks (GNNs) to address the challenges of generalization to unseen or out-of-distribution data. By using Rademacher complexity to compute a regret bound on generalization error, the authors characterize the impact of data augmentation on improving performance. They propose GRATIN, a novel graph data augmentation algorithm that leverages Gaussian Mixture Models (GMMs) to approximate any distribution efficiently. The approach not only surpasses existing augmentation methods in terms of generalization but also offers improved time complexity, making it highly suitable for practical applications. This framework provides valuable insights into enhancing the generalization capabilities of GNNs and offers a promising avenue for further research and application in real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2411.08638v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GRATIN, an efficient graph data augmentation algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExDiff: A Framework for Simulating Diffusion Processes on Complex Networks with Explainable AI Integration</title>
<link>https://arxiv.org/abs/2506.04271</link>
<guid>https://arxiv.org/abs/2506.04271</guid>
<content:encoded><![CDATA[
<div> framework, diffusion, networks, graph neural networks, XAI
<br />
Summary: 
The article introduces ExDiff, a computational framework that combines network simulation, graph neural networks, and explainable artificial intelligence to model and interpret diffusion dynamics in complex networks. ExDiff integrates classical compartmental models with deep learning techniques to capture structural and temporal characteristics of diffusion across different network topologies. It features modules for network analysis, neural modeling, simulation, and interpretability, accessible through an intuitive interface on Google Colab. Through a case study using the SIRVD model, ExDiff demonstrates the ability to simulate disease spread, evaluate intervention strategies, classify node states, and uncover the structural determinants of contagion using XAI techniques. By integrating simulation and interpretability, ExDiff provides a flexible and accessible platform for studying diffusion phenomena in networked systems, enabling methodological innovation and practical insights. 
<br /> <div>
arXiv:2506.04271v1 Announce Type: new 
Abstract: Understanding and controlling diffusion processes in complex networks is critical across domains ranging from epidemiology to information science. Here, we present ExDiff, an interactive and modular computational framework that integrates network simulation, graph neural networks (GNNs), and explainable artificial intelligence (XAI) to model and interpret diffusion dynamics. ExDiff combines classical compartmental models with deep learning techniques to capture both the structural and temporal characteristics of diffusion across diverse network topologies. The framework features dedicated modules for network analysis, neural modeling, simulation, and interpretability, all accessible via an intuitive interface built on Google Colab. Through a case study of the Susceptible Infectious Recovered Vaccinated Dead (SIRVD) model, we demonstrate the capacity to simulate disease spread, evaluate intervention strategies, classify node states, and reveal the structural determinants of contagion through XAI techniques. By unifying simulation and interpretability, ExDiff provides a powerful, flexible, and accessible platform for studying diffusion phenomena in networked systems, enabling both methodological innovation and practical insight.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering</title>
<link>https://arxiv.org/abs/2506.04292</link>
<guid>https://arxiv.org/abs/2506.04292</guid>
<content:encoded><![CDATA[
<div> Keywords: Money laundering, Smurfing, Network analytics, Graph-based method, Fraud detection

Summary:<br />
Money laundering, estimated to make up a significant portion of the global GDP, presents a challenge for financial institutions due to stringent regulatory controls. Smurfing, a common method of laundering involving breaking large transactions into smaller amounts, requires sophisticated detection techniques. GARG-AML, a new graph-based approach, offers an interpretable metric derived from second-order transaction networks to quantify smurfing risk effectively and transparently. This method strikes a balance between computational efficiency, detection power, and interpretability, making it suitable for integration into anti-money laundering workflows. By utilizing only the adjacency matrix of the second-order neighborhood and basic network features, GARG-AML outperforms current state-of-the-art smurfing detection methods in experimental evaluations. This research demonstrates the potential of fundamental network properties in advancing fraud detection.<br />
Summary: <div>
arXiv:2506.04292v1 Announce Type: new 
Abstract: Money laundering poses a significant challenge as it is estimated to account for 2%-5% of the global GDP. This has compelled regulators to impose stringent controls on financial institutions. One prominent laundering method for evading these controls, called smurfing, involves breaking up large transactions into smaller amounts. Given the complexity of smurfing schemes, which involve multiple transactions distributed among diverse parties, network analytics has become an important anti-money laundering tool. However, recent advances have focused predominantly on black-box network embedding methods, which has hindered their adoption in businesses. In this paper, we introduce GARG-AML, a novel graph-based method that quantifies smurfing risk through a single interpretable metric derived from the structure of the second-order transaction network of each individual node in the network. Unlike traditional methods, GARG-AML strikes an effective balance among computational efficiency, detection power and transparency, which enables its integration into existing AML workflows. To enhance its capabilities, we combine the GARG-AML score calculation with different tree-based methods and also incorporate the scores of the node's neighbours. An experimental evaluation on large-scale synthetic and open-source networks demonstrate that the GARG-AML outperforms the current state-of-the-art smurfing detection methods. By leveraging only the adjacency matrix of the second-order neighbourhood and basic network features, this work highlights the potential of fundamental network properties towards advancing fraud detection.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Drives Team Success? Large-Scale Evidence on the Role of the Team Player Effect</title>
<link>https://arxiv.org/abs/2506.04475</link>
<guid>https://arxiv.org/abs/2506.04475</guid>
<content:encoded><![CDATA[
<div> teamwork, social skills, familiarity, team player effect, team performance

Summary: 
The study explores the impact of social skills and familiarity on team success in temporary teams by analyzing data from the real-time strategy game Age of Empires II. The research examines individual contributions to team outcomes and identifies a 'team player effect,' where certain individuals enhance team performance beyond their technical proficiency. This effect is strengthened by team familiarity, particularly in teams with prior shared experience. The study also notes that the team player effect becomes more pronounced with larger team sizes, suggesting the increasing value of social skills in coordinating complex tasks. The findings highlight the complementary interaction between social skills and familiarity in achieving high team performance, shedding light on the dynamics of teamwork in structured, high-pressure environments. <div>
arXiv:2506.04475v1 Announce Type: new 
Abstract: Effective teamwork is essential in structured, performance-driven environments, from professional organizations to high-stakes competitive settings. As tasks grow more complex, achieving high performance requires not only technical proficiency but also strong interpersonal skills that allow individuals to coordinate effectively within teams. While prior research has identified social skills and familiarity as key drivers of team success, their joint effects -- particularly in temporary teams -- remain underexplored due to data and methodological constraints. To address this gap, we analyze a large-scale panel dataset from the real-time strategy game Age of Empires II, where players are assigned quasi-randomly to temporary teams and must coordinate under dynamic, high-pressure conditions. We isolate individual contributions by comparing observed match outcomes with predictions based on task proficiency. Our findings confirm a robust 'team player effect': certain individuals consistently improve team outcomes beyond what their technical skills predict. This effect is significantly amplified by team familiarity -- teams with prior shared experience benefit more from the presence of such individuals. Moreover, the effect grows with team size, suggesting that social skills become increasingly valuable as coordination demands rise. Our results demonstrate that social skills and familiarity interact in a complementary, rather than additive, way. These findings contribute to the literature on team performance by documenting the strength and structure of the team player effect in a quasi-randomized, high-stakes setting, with implications for teamwork in organizations and labor markets.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge interventions can mitigate demographic and prestige disparities in the Computer Science coauthorship network</title>
<link>https://arxiv.org/abs/2506.04435</link>
<guid>https://arxiv.org/abs/2506.04435</guid>
<content:encoded><![CDATA[
<div> algorithms, demographic traits, network centrality, academic publishing, inequities

Summary:
- Social factors and institutional prestige influence academic publishing and network centrality in coauthorship.
- Women and individuals from minoritized racial backgrounds are less central in the computer science coauthorship network.
- Faculty from top-ranked departments are more central, indicating higher prestige.
- Disparities in centrality can be reduced through simulated interventions promoting collaborations.
- Interventions targeting scholars based on institutional prestige improve network centrality and academic job placement predictions.<br /><br />Summary: <div>
arXiv:2506.04435v1 Announce Type: cross 
Abstract: Social factors such as demographic traits and institutional prestige structure the creation and dissemination of ideas in academic publishing. One place these effects can be observed is in how central or peripheral a researcher is in the coauthorship network. Here we investigate inequities in network centrality in a hand-collected data set of 5,670 U.S.-based faculty employed in Ph.D.-granting Computer Science departments and their DBLP coauthorship connections. We introduce algorithms for combining name- and perception-based demographic labels by maximizing alignment with self-reported demographics from a survey of faculty from our census. We find that women and individuals with minoritized race identities are less central in the computer science coauthorship network, implying worse access to and ability to spread information. Centrality is also highly correlated with prestige, such that faculty in top-ranked departments are at the core and those in low-ranked departments are in the peripheries of the computer science coauthorship network. We show that these disparities can be mitigated using simulated edge interventions, interpreted as facilitated collaborations. Our intervention increases the centrality of target individuals, chosen independently of the network structure, by linking them with researchers from highly ranked institutions. When applied to scholars during their Ph.D., the intervention also improves the predicted rank of their placement institution in the academic job market. This work was guided by an ameliorative approach: uncovering social inequities in order to address them. By targeting scholars for intervention based on institutional prestige, we are able to improve their centrality in the coauthorship network that plays a key role in job placement and longer-term academic success.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Altruism in Recommendation Systems</title>
<link>https://arxiv.org/abs/2506.04525</link>
<guid>https://arxiv.org/abs/2506.04525</guid>
<content:encoded><![CDATA[
<div> boost, user altruism, recommendation systems, social welfare, strategic interaction

Summary:
The study focuses on users of social media platforms that use recommendation systems and how they strategically interact with platform content to influence future recommendations. It examines the concept of user altruism, where users engage in behaviors to boost algorithmically suppressed content. The game theory analysis between users and the recommendation system shows that user altruism can lead to increases in user social welfare under certain conditions. It also suggests that effectively altruistic strategies can improve the utility of the recommendation system itself. The study includes theoretical analysis, empirical results on the GoodReads dataset, and an online survey to understand real-world user behavior in recommendation systems. These findings demonstrate how traditional recommendation systems may incentivize users to form collectives and engage in altruistic strategies when interacting with them. 

<br /><br />Summary: <div>
arXiv:2506.04525v1 Announce Type: cross 
Abstract: Users of social media platforms based on recommendation systems (RecSys) (e.g. TikTok, X, YouTube) strategically interact with platform content to influence future recommendations. On some such platforms, users have been documented to form large-scale grassroots movements encouraging others to purposefully interact with algorithmically suppressed content in order to "boost" its recommendation; we term this behavior user altruism. To capture this behavior, we study a game between users and a RecSys, where users provide the RecSys (potentially manipulated) preferences over the contents available to them, and the RecSys -- limited by data and computation constraints -- creates a low-rank approximation preference matrix, and ultimately provides each user her (approximately) most-preferred item. We compare the users' social welfare under truthful preference reporting and under a class of strategies capturing user altruism. In our theoretical analysis, we provide sufficient conditions to ensure strict increases in user social welfare under user altruism, and provide an algorithm to find an effective altruistic strategy. Interestingly, we show that for commonly assumed recommender utility functions, effectively altruistic strategies also improve the utility of the RecSys! We show that our results are robust to several model misspecifications, thus strengthening our conclusions. Our theoretical analysis is complemented by empirical results of effective altruistic strategies on the GoodReads dataset, and an online survey on how real-world users behave altruistically in RecSys. Overall, our findings serve as a proof-of-concept of the reasons why traditional RecSys may incentivize users to form collectives and/or follow altruistic strategies when interacting with them.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Driven Bounded Confidence Opinion Dynamics: A Hegselmann-Krause Model Based on Fractional-Order Methods</title>
<link>https://arxiv.org/abs/2506.04701</link>
<guid>https://arxiv.org/abs/2506.04701</guid>
<content:encoded><![CDATA[
<div> memory effects, social interactions, decision-making processes, fractional-order bounded confidence, opinion dynamics

Summary:
The paper presents a novel fractional-order bounded confidence opinion dynamics model that incorporates memory effects into social interactions and decision-making processes. By combining the Hegselmann-Krause framework with fractional-order difference, the model captures the persistent influence of historical information on system states. The rigorous theoretical analysis explores key properties such as convergence and consensus, demonstrating that the proposed model outperforms classical opinion dynamics by addressing limitations like the monotonicity of bounded opinions. This enhanced model offers a more realistic representation of opinion evolution in real-world scenarios, providing valuable insights and methodological approaches for understanding opinion formation and evolution. The findings contribute to both theoretical advancements and practical applications in studying complex social dynamics. 

<br /><br />Summary: <div>
arXiv:2506.04701v1 Announce Type: cross 
Abstract: Memory effects play a crucial role in social interactions and decision-making processes. This paper proposes a novel fractional-order bounded confidence opinion dynamics model to characterize the memory effects in system states. Building upon the Hegselmann-Krause framework and fractional-order difference, a comprehensive model is established that captures the persistent influence of historical information. Through rigorous theoretical analysis, the fundamental properties including convergence and consensus is investigated. The results demonstrate that the proposed model not only maintains favorable convergence and consensus characteristics compared to classical opinion dynamics, but also addresses limitations such as the monotonicity of bounded opinions. This enables a more realistic representation of opinion evolution in real-world scenarios. The findings of this study provide new insights and methodological approaches for understanding opinion formation and evolution, offering both theoretical significance and practical applications.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Aware Temporal Network Generation</title>
<link>https://arxiv.org/abs/2501.07327</link>
<guid>https://arxiv.org/abs/2501.07327</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, face-to-face interactions, community affiliation, network generation, synthetic networks 

Summary: 
Temporal networks are crucial in understanding complex dynamics in human behavior, such as epidemics and sociological studies. Existing datasets have limitations, including short time spans and privacy concerns. Traditional network generation algorithms do not adequately capture social structures or temporal aspects. This study extends a recent approach to generate synthetic networks that mimic interactions between different communities. Nodes are labeled based on their community affiliation to create surrogate networks reflecting realistic behaviors. The method is validated by comparing structural measures between original and generated networks in various face-to-face interaction datasets.<br /><br />Summary: <div>
arXiv:2501.07327v2 Announce Type: replace 
Abstract: The advantages of temporal networks in capturing complex dynamics, such as diffusion and contagion, has led to breakthroughs in real world systems across numerous fields. In the case of human behavior, face-to-face interaction networks enable us to understand the dynamics of how communities emerge and evolve in time through the interactions, which is crucial in fields like epidemics, sociological studies and urban science. However, state-of-the-art datasets suffer from a number of drawbacks, such as short time-span for data collection and a small number of participants. Moreover, concerns arise for the participants' privacy and the data collection costs. Over the past years, many successful algorithms for static networks generation have been proposed, but they often do not tackle the social structure of interactions or their temporal aspect. In this work, we extend a recent network generation approach to capture the evolution of interactions between different communities. Our method labels nodes based on their community affiliation and constructs surrogate networks that reflect the interactions of the original temporal networks between nodes with different labels. This enables the generation of synthetic networks that replicate realistic behaviors. We validate our approach by comparing structural measures between the original and generated networks across multiple face-to-face interaction datasets.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing lexicons of in-group language with socio-temporal context</title>
<link>https://arxiv.org/abs/2409.19257</link>
<guid>https://arxiv.org/abs/2409.19257</guid>
<content:encoded><![CDATA[
<div> Keywords: in-group language, lexicon induction, socio-temporal context, dynamic word embeddings, online anti-women communities

Summary: 
This paper introduces a novel method for inducing lexicons of in-group language, taking into account the socio-temporal context. Existing methods do not adequately capture the evolving nature of in-group language and the social structure of the community. By utilizing dynamic word and user embeddings trained on conversations from online anti-women communities, this approach surpasses previous methods for lexicon induction. A test set is developed for this task, along with a new lexicon of manosphere language validated by human experts. This lexicon quantifies the relevance of each term to a specific sub-community at a particular point in time. The research provides unique insights into in-group language, demonstrating the effectiveness of this method. 

<br /><br />Summary: <div>
arXiv:2409.19257v3 Announce Type: replace-cross 
Abstract: In-group language is an important signifier of group dynamics. This paper proposes a novel method for inducing lexicons of in-group language, which incorporates its socio-temporal context. Existing methods for lexicon induction do not capture the evolving nature of in-group language, nor the social structure of the community. Using dynamic word and user embeddings trained on conversations from online anti-women communities, our approach outperforms prior methods for lexicon induction. We develop a test set for the task of lexicon induction and a new lexicon of manosphere language, validated by human experts, which quantifies the relevance of each term to a specific sub-community at a given point in time. Finally, we present novel insights on in-group language which illustrate the utility of this approach.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Politics and polarization on Bluesky</title>
<link>https://arxiv.org/abs/2506.03443</link>
<guid>https://arxiv.org/abs/2506.03443</guid>
<content:encoded><![CDATA[
<div> Keywords: political discourse, polarization, social media platform, Bluesky, fragmentation 

Summary: 
The study focuses on political discourse and polarization on the social media platform Bluesky, analyzing data from December 2024 to May 2025. Approximately 13% of posts on Bluesky involve political content, with topics such as international conflicts, U.S. politics, and socio-technological debates being prominent. Structural polarization is observed across various political topics, with some topics showing high imbalance in the numbers of users on opposing sides. Despite echoing familiar political narratives and polarization trends, Bluesky has a more politically homogeneous user base compared to other platforms. The study highlights the impact of platform fragmentation on shaping online political discourse and polarization dynamics. 

<br /><br />Summary:  <div>
arXiv:2506.03443v1 Announce Type: new 
Abstract: Online political discourse is increasingly shaped not by a few dominant platforms but by a fragmented ecosystem of social media spaces, each with its own user base, target audience, and algorithmic mediation of discussion. Such fragmentation may fundamentally change how polarization manifests online. In this study, we investigate the characteristics of political discourse and polarization on the emerging social media site Bluesky. We collect all activity on the platform between December 2024 and May 2025 to map out the platform's political topic landscape and detect distinct polarization patterns. Our comprehensive data collection allows us to employ a data-driven methodology for identifying political themes, classifying user stances, and measuring both structural and content-based polarization across key topics raised in English-language discussions. Our analysis reveals that approximately 13% of Bluesky posts engage with political content, with prominent topics including international conflicts, U.S. politics, and socio-technological debates. We find high levels of structural polarization across several salient political topics. However, the most polarized topics are also highly imbalanced in the numbers of users on opposing sides, with the smaller group consisting of only 1-2% of the users. While discussions in Bluesky echo familiar political narratives and polarization trends, the platform exhibits a more politically homogeneous user base than was typical prior to the current wave of platform fragmentation.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Bulimia Nervosa in the Digital Age: The Role of Social Media</title>
<link>https://arxiv.org/abs/2506.03491</link>
<guid>https://arxiv.org/abs/2506.03491</guid>
<content:encoded><![CDATA[
<div> Keywords: globalization, eating disorders, bulimia nervosa, mathematical modeling, social media

Summary: 
Globalization has significantly impacted the societal dynamics surrounding eating disorders such as bulimia nervosa (BN), with these conditions increasingly influenced by broader sociocultural and digital contexts. Traditional mathematical modeling frameworks have limitations in capturing the complex interactions of social contagion, digital media, and adaptive behavior in the development of BN. This review discusses the evolution of quantitative modeling in understanding BN as a socially transmissible condition, highlighting the need for improved models to incorporate feedback mechanisms, content-driven influence functions, and dynamic network effects. The intensifying impact of social media on eating disorders presents a critical gap that future models must address. By integrating behavioral epidemiology and an adaptive behavior framework, researchers aim to develop data-informed models that can guide more effective public health interventions in the digital age. <div>
arXiv:2506.03491v1 Announce Type: new 
Abstract: Globalization has fundamentally reshaped societal dynamics, influencing how individuals interact and perceive themselves and others. One significant consequence is the evolving landscape of eating disorders such as bulimia nervosa (BN), which are increasingly driven not just by internal psychological factors but by broader sociocultural and digital contexts. While mathematical modeling has provided valuable insights, traditional frameworks often fall short in capturing the nuanced roles of social contagion, digital media, and adaptive behavior. This review synthesizes two decades of quantitative modeling efforts, including compartmental, stochastic, and delay-based approaches. We spotlight foundational work that conceptualizes BN as a socially transmissible condition and identify critical gaps, especially regarding the intensifying impact of social media. Drawing on behavioral epidemiology and the adaptive behavior framework by Fenichel et al., we advocate for a new generation of models that incorporate feedback mechanisms, content-driven influence functions, and dynamic network effects. This work outlines a roadmap for developing more realistic, data-informed models that can guide effective public health interventions in the digital era.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GA-S$^3$: Comprehensive Social Network Simulation with Group Agents</title>
<link>https://arxiv.org/abs/2506.03532</link>
<guid>https://arxiv.org/abs/2506.03532</guid>
<content:encoded><![CDATA[
<div> Keywords: Social network simulation, Group Agents, Intelligent decision-making, Online events, Benchmark

Summary:
Social network simulation plays a crucial role in understanding real-world social networks and their applications. The proposed Social Network Simulation System (GA-S3) utilizes Group Agents to make intelligent decisions in simulating large-scale network phenomena. Unlike individual-based agents, Group Agents model collections of individuals with similar behaviors, making it computationally manageable to simulate complex interactions. A social network benchmark comprising 2024 online events provides detailed information on Internet traffic variations. The experiment shows that GA-S3 achieves accurate and realistic prediction results. The system's open-source code is available on GitHub, enabling researchers to utilize and further develop the simulation system for various applications. 

<br /><br />Summary: <div>
arXiv:2506.03532v1 Announce Type: new 
Abstract: Social network simulation is developed to provide a comprehensive understanding of social networks in the real world, which can be leveraged for a wide range of applications such as group behavior emergence, policy optimization, and business strategy development. However, billions of individuals and their evolving interactions involved in social networks pose challenges in accurately reflecting real-world complexities. In this study, we propose a comprehensive Social Network Simulation System (GA-S3) that leverages newly designed Group Agents to make intelligent decisions regarding various online events. Unlike other intelligent agents that represent an individual entity, our group agents model a collection of individuals exhibiting similar behaviors, facilitating the simulation of large-scale network phenomena with complex interactions at a manageable computational cost. Additionally, we have constructed a social network benchmark from 2024 popular online events that contains fine-grained information on Internet traffic variations. The experiment demonstrates that our approach is capable of achieving accurate and highly realistic prediction results. Code is open at https://github.com/AI4SS/GAS-3.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Retrieval-Augmented Multi-Agent Framework for Psychiatry Diagnosis</title>
<link>https://arxiv.org/abs/2506.03750</link>
<guid>https://arxiv.org/abs/2506.03750</guid>
<content:encoded><![CDATA[
<div> framework, diagnosis, AI, MoodAngels, psychiatric

Summary:
The article introduces MoodAngels, a specialized multi-agent framework for mood disorder diagnosis that addresses challenges in AI-assisted psychiatric assessments. The framework combines granular-scale analysis of clinical assessments with a structured verification process to improve accuracy in interpreting complex psychiatric data. A new open-source dataset called MoodSyn, consisting of 1,173 synthetic psychiatric cases, is also presented to preserve clinical validity and ensure patient privacy. Experimental results show that MoodAngels outperforms conventional methods, with a baseline agent achieving higher accuracy than GPT-4o on real-world cases. The full multi-agent system further improves diagnostic accuracy. Evaluation in the MoodSyn dataset demonstrates exceptional fidelity in reproducing statistical patterns and complex relationships found in the original data, making it a valuable resource for machine learning applications in computational psychiatry. Overall, MoodAngels provides an advanced diagnostic tool and critical research resource for bridging gaps in AI-assisted mental health assessment. 

<br /><br />Summary: <div>
arXiv:2506.03750v1 Announce Type: new 
Abstract: The application of AI in psychiatric diagnosis faces significant challenges, including the subjective nature of mental health assessments, symptom overlap across disorders, and privacy constraints limiting data availability. To address these issues, we present MoodAngels, the first specialized multi-agent framework for mood disorder diagnosis. Our approach combines granular-scale analysis of clinical assessments with a structured verification process, enabling more accurate interpretation of complex psychiatric data. Complementing this framework, we introduce MoodSyn, an open-source dataset of 1,173 synthetic psychiatric cases that preserves clinical validity while ensuring patient privacy. Experimental results demonstrate that MoodAngels outperforms conventional methods, with our baseline agent achieving 12.3% higher accuracy than GPT-4o on real-world cases, and our full multi-agent system delivering further improvements. Evaluation in the MoodSyn dataset demonstrates exceptional fidelity, accurately reproducing both the core statistical patterns and complex relationships present in the original data while maintaining strong utility for machine learning applications. Together, these contributions provide both an advanced diagnostic tool and a critical research resource for computational psychiatry, bridging important gaps in AI-assisted mental health assessment.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of COVID-19 on Twitter Ego Networks: Structure, Sentiment, and Topics</title>
<link>https://arxiv.org/abs/2506.03788</link>
<guid>https://arxiv.org/abs/2506.03788</guid>
<content:encoded><![CDATA[
<div> Keywords: lockdown measures, ego networks, online social platforms, Twitter users, cognitive resources

Summary: 
During the COVID-19 pandemic, individuals turned to online social platforms like Twitter due to in-person restrictions. This shift impacted the characteristics of online ego networks, leading to expanded networks, structured social circles, intensified relationships, increased negative interactions, and greater thematic diversity in user engagement. These changes were temporary, reverting to pre-pandemic norms once lockdown measures were lifted. The study analyzed data from a seven-year period, showing how individuals adapted their online interactions in response to the extraordinary social context imposed by the pandemic.

<br /><br />Summary: <div>
arXiv:2506.03788v1 Announce Type: new 
Abstract: Lockdown measures, implemented by governments during the initial phases of the COVID-19 pandemic to reduce physical contact and limit viral spread, imposed significant restrictions on in-person social interactions. Consequently, individuals turned to online social platforms to maintain connections. Ego networks, which model the organization of personal relationships according to human cognitive constraints on managing meaningful interactions, provide a framework for analyzing such dynamics. The disruption of physical contact and the predominant shift of social life online potentially altered the allocation of cognitive resources dedicated to managing these digital relationships. This research aims to investigate the impact of lockdown measures on the characteristics of online ego networks, presumably resulting from this reallocation of cognitive resources. To this end, a large dataset of Twitter users was examined, covering a seven-year period of activity. Analyzing a seven-year Twitter dataset -- including five years pre-pandemic and two years post -- we observe clear, though temporary, changes. During lockdown, ego networks expanded, social circles became more structured, and relationships intensified. Simultaneously, negative interactions increased, and users engaged with a broader range of topics, indicating greater thematic diversity. Once restrictions were lifted, these structural, emotional, and thematic shifts largely reverted to pre-pandemic norms -- suggesting a temporary adaptation to an extraordinary social context.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^3$: Social-network Simulation System with Large Language Model-Empowered Agents</title>
<link>https://arxiv.org/abs/2307.14984</link>
<guid>https://arxiv.org/abs/2307.14984</guid>
<content:encoded><![CDATA[
<div> Keywords: social network simulation, large language models, S$^3$ system, agent-based simulation, information propagation<br />
<br />
Summary: 
The article discusses the development of the S$^3$ system, a social network simulation system utilizing large language models (LLMs) to create human-like agents. By employing prompt engineering and tuning techniques, the system ensures that agents behave realistically within the simulated social network. The simulation focuses on three key aspects: emotion, attitude, and interaction behaviors, leading to the emergence of population-level phenomena such as information and attitude propagation. Evaluation using real-world social network data shows promising accuracy. This work represents an important step in leveraging LLMs for social network simulation and has implications for various applications in social science research. <div>
arXiv:2307.14984v3 Announce Type: replace 
Abstract: Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\textbf{S}$ocial network $\textbf{S}$imulation $\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Treatment Allocation in the Presence of Interference</title>
<link>https://arxiv.org/abs/2410.00075</link>
<guid>https://arxiv.org/abs/2410.00075</guid>
<content:encoded><![CDATA[
<div> Influence Maximization, Treatment Allocation, Network Interference, Causal Estimator, Optimizing
Treatment Allocation

Summary:
Optimizing Treatment Allocation in the Presence of Interference (OTAPI) addresses the challenge of selecting the optimal entities in a network for treatment allocation to maximize overall effect. Traditional methods like Uplift Modeling (UM) do not consider network interference, leading to suboptimal results in network settings. OTAPI integrates a causal estimator to predict treatment effects in networks and uses this information to drive optimal treatment allocation decisions. By bridging the gap between Influence Maximization (IM) and UM, OTAPI outperforms existing approaches on various datasets. This novel method showcases the importance of considering network effects in treatment allocation decisions, demonstrating its effectiveness in improving outcomes in scenarios where entities influence each other. <br /><br />Summary: <div>
arXiv:2410.00075v2 Announce Type: replace 
Abstract: In Influence Maximization (IM), the objective is to -- given a budget -- select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover, effects that follow from targeting these customers. Recently, new methods to estimate treatment effects in the presence of network interference have been proposed. However, the issue of how to leverage these models to make better treatment allocation decisions has been largely overlooked. Traditionally, in Uplift Modeling (UM), entities are ranked according to estimated treatment effect, and the top entities are allocated treatment. Since, in a network context, entities influence each other, the UM ranking approach will be suboptimal. The problem of finding the optimal treatment allocation in a network setting is \textcolor{red}{NP-hard,} and generally has to be solved heuristically. To fill the gap between IM and UM, we propose OTAPI: Optimizing Treatment Allocation in the Presence of Interference to find solutions to the IM problem using treatment effect estimates. OTAPI consists of two steps. First, a causal estimator is trained to predict treatment effects in a network setting. Second, this estimator is leveraged to identify an optimal treatment allocation by integrating it into classic IM algorithms. We demonstrate that this novel method outperforms classic IM and UM approaches on both synthetic and semi-synthetic datasets.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Experts for Node Classification</title>
<link>https://arxiv.org/abs/2412.00418</link>
<guid>https://arxiv.org/abs/2412.00418</guid>
<content:encoded><![CDATA[
<div> mixture of experts, node classification, node predictors, diverse patterns, real-world graphs
<br />
Summary: 
The study highlights the limitations of existing node predictors in capturing diverse patterns in real-world graphs, such as degree and homophily. It suggests that using a single node predictor for all nodes could result in suboptimal classification performance. To address this issue, the authors propose a novel framework called MoE-NP, which utilizes a mixture of node predictors and strategically selects models based on node patterns. Experimental results on various real-world datasets show significant performance improvements with MoE-NP. This approach emphasizes the importance of considering different node patterns when predicting node behavior in complex networks. <div>
arXiv:2412.00418v3 Announce Type: replace 
Abstract: Nodes in the real-world graphs exhibit diverse patterns in numerous aspects, such as degree and homophily. However, most existent node predictors fail to capture a wide range of node patterns or to make predictions based on distinct node patterns, resulting in unsatisfactory classification performance. In this paper, we reveal that different node predictors are good at handling nodes with specific patterns and only apply one node predictor uniformly could lead to suboptimal result. To mitigate this gap, we propose a mixture of experts framework, MoE-NP, for node classification. Specifically, MoE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Experimental results from a range of real-world datasets demonstrate significant performance improvements from MoE-NP.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying edge relevance for epidemic spreading via the semi-metric topology of complex networks</title>
<link>https://arxiv.org/abs/2311.14817</link>
<guid>https://arxiv.org/abs/2311.14817</guid>
<content:encoded><![CDATA[
<div> semi-metric topology, sparsification, complex networks, epidemic outbreaks, shortest paths
Summary:
The study introduces a new sparsification method based on semi-metric distortion, prioritizing edges that significantly break the triangle inequality in weighted graphs. By identifying and removing edges with high semi-metric distortion, the method effectively preserves network dynamics and topology while reducing computational costs. Experimental results demonstrate that the semi-metric distortion sparsification outperforms existing methods in recovering Susceptible-Infected dynamics and maintaining connectivity. This approach improves the ranking of edge relevance for epidemic outbreaks by quantifying alternative transmission pathways through semi-metric distances. Overall, the method provides a more efficient and accurate way to sparsify networks while retaining essential information for studying complex system dynamics.<br /><br />Summary: <div>
arXiv:2311.14817v2 Announce Type: replace-cross 
Abstract: Sparsification aims at extracting a reduced core of associations that best preserves both the dynamics and topology of networks while reducing the computational cost of simulations. We show that the semi-metric topology of complex networks yields a natural and algebraically-principled sparsification that outperforms existing methods on those goals. Weighted graphs whose edges represent distances between nodes are semi-metric when at least one edge breaks the triangle inequality (transitivity). We first confirm with new experiments that the metric backbone$\unicode{x2013}$a unique subgraph of all edges that obey the triangle inequality and thus preserve all shortest paths$\unicode{x2013}$recovers Susceptible-Infected dynamics over the original non-sparsified graph. This recovery is improved when we remove only those edges that break the triangle inequality significantly, i.e., edges with large semi-metric distortion. Based on these results, we propose the new semi-metric distortion sparsification method to progressively sparsify networks in decreasing order of semi-metric distortion. Our method recovers the macro- and micro-level dynamics of epidemic outbreaks better than other methods while also yielding sparser yet connected subgraphs that preserve all shortest paths. Overall, we show that semi-metric distortion overcomes the limitations of edge betweenness in ranking the dynamical relevance of edges not participating in any shortest path, as it quantifies the existence and strength of alternative transmission pathways.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ideological Fragmentation of the Social Media Ecosystem: From echo chambers to echo platforms</title>
<link>https://arxiv.org/abs/2411.16826</link>
<guid>https://arxiv.org/abs/2411.16826</guid>
<content:encoded><![CDATA[
<div> platform centrality, news consumption, user base composition, social media ecosystem, ideological diversity

Summary:
The study investigates the impact of social media on political discussions, focusing on platform centrality, news consumption, and user base composition. Analyzing 117 million posts related to the 2020 US Presidential elections on nine platforms, including mainstream and alt-tech platforms, significant differences are found. Mainstream platforms are more central within the ecosystem, circulate more reliable news, and have a more diverse user base. In contrast, alt-tech platforms play a peripheral role, feature higher instances of unreliable content, and exhibit greater ideological uniformity. The findings underscore the growing fragmentation and polarization of the social media landscape, with platforms catering to specific ideological niches and users seeking like-minded communities. This trend highlights the challenge of echo chambers and the need for platforms to address the spread of unreliable information and promote diverse perspectives. 

<br /><br />Summary: <div>
arXiv:2411.16826v2 Announce Type: replace-cross 
Abstract: The entertainment-driven nature of social media encourages users to engage with like-minded individuals and consume content aligned with their beliefs, limiting exposure to diverse perspectives. Simultaneously, users migrate between platforms, either due to moderation policies like de-platforming or in search of environments better suited to their preferences. These dynamics drive the specialization of the social media ecosystem, shifting from internal echo chambers to "echo platforms"--entire platforms functioning as ideologically homogeneous niches. To systematically analyze this phenomenon in political discussions, we propose a quantitative approach based on three key dimensions: platform centrality, news consumption, and user base composition. We analyze 117 million posts related to the 2020 US Presidential elections from nine social media platforms--Facebook, Reddit, Twitter, YouTube, BitChute, Gab, Parler, Scored, and Voat. Our findings reveal significant differences among platforms in their centrality within the ecosystem, the reliability of circulated news, and the ideological diversity of their users, highlighting a clear divide between mainstream and alt-tech platforms. The latter occupy a peripheral role, feature a higher prevalence of unreliable content, and exhibit greater ideological uniformity. These results highlight the key dimensions shaping the fragmentation and polarization of the social media landscape.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplex Nodal Modularity: A novel network metric for the regional analysis of amnestic mild cognitive impairment during a working memory binding task</title>
<link>https://arxiv.org/abs/2501.09805</link>
<guid>https://arxiv.org/abs/2501.09805</guid>
<content:encoded><![CDATA[
<div> modularity, community structure, nodal modularity, Alzheimer's disease, fMRI<br />
Summary:<br />
This study introduces the concept of nodal modularity (nQ) to capture variations in community structure at individual nodes in brain networks. By applying nQ to fMRI and DTI data from early-stage Alzheimer's disease (AD) patients, the researchers found changes in nQ in visual, limbic, and paralimbic regions, associated with amyloid-$\beta$ and tau deposition. Additionally, white-matter microstructure changes in parietal and frontal regions were observed, potentially linked to memory deficits. NQ was able to differentiate between individuals with mild cognitive impairment (MCI) and MCI converters, suggesting its sensitivity to disease progression. This novel measure offers insights into localized group structures and may have broader applications across various domains for understanding network organization. <br />  
Summary: <div>
arXiv:2501.09805v2 Announce Type: replace-cross 
Abstract: Modularity is a well-established concept for assessing community structures in various single and multi-layer networks, including those in biological and social domains. Brain networks are known to exhibit community structure at local, meso, and global scale. However, modularity is limited as a metric to a global scale describing the overall strength of community structure, overlooking important variations in community structure at node level. To address this limitation, we extended modularity to individual nodes. This novel measure of nodal modularity (nQ) captures both mesoscale and local-scale changes in modularity. We hypothesized that nQ would illuminate granular changes in the brain due to diseases such as Alzheimer's disease (AD), which are known to disrupt the brain's modular structure. We explored nQ in multiplex networks of a visual short-term memory binding task in fMRI and DTI data in the early stages of AD. While limited by sample size, changes in nQ for individual regions of interest (ROIs) in our fMRI networks were predominantly observed in visual, limbic, and paralimbic systems in the brain, aligning with known AD trajectories and linked to amyloid-$\beta$ and tau deposition. Furthermore, observed changes in white-matter microstructure in our DTI networks in parietal and frontal regions may compliment studies of white-matter integrity in poor memory binders. Additionally, nQ clearly differentiated MCI from MCI converters indicating that nQ may be sensitive to this key turning point of AD. Our findings demonstrate the utility of nQ as a measure of localized group structure, providing novel insights into task and disease-related variability at the node level. Given the widespread application of modularity as a global measure, nQ represents a significant advancement, providing a granular measure of network organization applicable to a wide range of disciplines.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Algorithmic Curation on Social Media: An Empirical Audit of Reddit's r/popular Feed</title>
<link>https://arxiv.org/abs/2502.20491</link>
<guid>https://arxiv.org/abs/2502.20491</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithms, social media feeds, user behavior, Reddit, engagement

Summary: 
Algorithms play a crucial role in curating content on social media feeds, impacting user behavior and content consumption. An empirical audit of Reddit's r/popular feed revealed that recent comments boost a post's visibility and ranking on the feed, while posts below rank 80 experience a significant decline in activity. Despite a higher proportion of undesired behavior like toxic comments, there is no evidence that it helps posts stay on r/popular longer. Posts closer to the top receive more undesired comments but also see an overall increase in engagement, indicating a broader impact rather than targeting undesired activity specifically. The findings emphasize the influence of algorithms in determining content prioritization on social media platforms, underscoring the need for transparency and accountability in algorithmic curation. Content creators, consumers, and moderators can benefit from empirical audits to enhance understanding and improve user experiences on algorithmically curated feeds. 

<br /><br />Summary: <div>
arXiv:2502.20491v2 Announce Type: replace-cross 
Abstract: Platforms are increasingly relying on algorithms to curate the content within users' social media feeds. However, the growing prominence of proprietary, algorithmically curated feeds has concealed what factors influence the presentation of content on social media feeds and how that presentation affects user behavior. This lack of transparency can be detrimental to users, from reducing users' agency over their content consumption to the propagation of misinformation and toxic content. To uncover details about how these feeds operate and influence user behavior, we conduct an empirical audit of Reddit's algorithmically curated trending feed called r/popular. Using 10K r/popular posts collected by taking snapshots of the feed over 11 months, we find that recent comments help a post remain on r/popular longer and climb the feed. We also find that posts below rank 80 correspond to a sharp decline in activity compared to posts above. When examining the effects of having a higher proportion of undesired behavior -- i.e., moderator-removed and toxic comments -- we find no significant evidence that it helps posts stay on r/popular for longer. Although posts closer to the top receive more undesired comments, we find this increase to coincide with a broader increase in overall engagement -- rather than indicating a disproportionate effect on undesired activity. The relationships between algorithmic rank and engagement highlight the extent to which algorithms employed by social media platforms essentially determine which content is prioritized and which is not. We conclude by discussing how content creators, consumers, and moderators on social media platforms can benefit from empirical audits aimed at improving transparency in algorithmically curated feeds.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Recommendation System Using Amazon Product Co-Purchasing Network</title>
<link>https://arxiv.org/abs/2506.02482</link>
<guid>https://arxiv.org/abs/2506.02482</guid>
<content:encoded><![CDATA[
<div> GraphSAGE, link prediction, e-commerce, recommendation system, online<br />
<br />
Summary: This project introduces an online recommendation system for new products on e-commerce platforms. By utilizing the Amazon Product Co-Purchasing Network Metadata dataset, a co-purchasing graph is created to showcase relationships among products. A modified GraphSAGE method is implemented for link prediction, combining product features and co-purchasing graph structure to forecast potential connections. This approach enables the model to adapt to unseen products and update in real time, enhancing the relevance of new product suggestions. Experimental results indicate superior performance compared to baseline algorithms, demonstrating the effectiveness of the proposed method in improving product recommendations in e-commerce settings. The code for the project is available on GitHub for further exploration and implementation. <div>
arXiv:2506.02482v1 Announce Type: new 
Abstract: This project develops an online, inductive recommendation system for newly listed products on e-commerce platforms, focusing on suggesting relevant new items to customers as they purchase other products. Using the Amazon Product Co-Purchasing Network Metadata dataset, we construct a co-purchasing graph where nodes represent products and edges capture co-purchasing relationships. To address the challenge of recommending new products with limited information, we apply a modified GraphSAGE method for link prediction. This inductive approach leverages both product features and the existing co-purchasing graph structure to predict potential co-purchasing relationships, enabling the model to generalize to unseen products. As an online method, it updates in real time, making it scalable and adaptive to evolving product catalogs. Experimental results demonstrate that our approach outperforms baseline algorithms in predicting relevant product links, offering a promising solution for enhancing the relevance of new product recommendations in e-commerce environments. All code is available at https://github.com/cse416a-fl24/final-project-l-minghao_z-catherine_z-nathan.git.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>