<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.SI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.SI</link>


<item>
<title>LLMs Between the Nodes: Community Discovery Beyond Vectors</title>
<link>https://arxiv.org/abs/2507.22955</link>
<guid>https://arxiv.org/abs/2507.22955</guid>
<content:encoded><![CDATA[
<div> Keyword: Community detection, Social network graphs, Large Language Models, Prompt-based reasoning, Graph-aware strategies

Summary:
Community detection in social network graphs is crucial for understanding group dynamics and information spread. This paper explores the integration of Large Language Models (LLMs) in identifying communities within social graphs. The proposed CommLLM framework combines the GPT-4o model with prompt-based reasoning to incorporate semantic and contextual information. Evaluations on real-world datasets show that LLMs, especially when guided by graph-aware strategies, can effectively detect communities in small to medium-sized graphs. Integration of instruction-tuned models and carefully crafted prompts improves accuracy and coherence of detected communities. These findings emphasize the potential of LLMs in graph-based research and highlight the importance of tailoring model interactions to the structure of graph data. 

<br /><br />Summary: <div>
arXiv:2507.22955v1 Announce Type: new 
Abstract: Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices</title>
<link>https://arxiv.org/abs/2507.23025</link>
<guid>https://arxiv.org/abs/2507.23025</guid>
<content:encoded><![CDATA[
<div> framework, directed graphs, sampling algorithm, Joint Degree Matrix, Degree Correlation Matrix
Summary:
The article proposes a new framework for sampling directed graphs to speed up the analysis of large networks. By rescaling the Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM), a sample graph can be constructed while preserving important graph properties. Experiments on real-world datasets show that the number of non-zero entries in JDM and DCM is small compared to the number of edges and nodes. The proposed algorithm can preserve in-degree and out-degree distributions, as well as joint degree distribution and degree correlation distribution. The algorithm's performance is expected to exceed theoretical expectations due to the negative correlation between deviations and the sparsity of JDM and DCM. The framework offers a promising approach for analyzing large directed networks efficiently.<br /><br />Summary: <div>
arXiv:2507.23025v1 Announce Type: new 
Abstract: In recent years, many large directed networks such as online social networks are collected with the help of powerful data engineering and data storage techniques. Analyses of such networks attract significant attention from both the academics and industries. However, analyses of large directed networks are often time-consuming and expensive because the complexities of a lot of graph algorithms are often polynomial with the size of the graph. Hence, sampling algorithms that can generate graphs preserving properties of original graph are of great importance because they can speed up the analysis process. We propose a promising framework to sample directed graphs: Construct a sample graph with linearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM). Previous work shows that graphs with the same JDM and DCM will have a range of very similar graph properties. We also conduct experiments on real-world datasets to show that the numbers of non-zero entries in JDM and DCM are quite small compared to the number of edges and nodes. Adopting this framework, we propose a novel graph sampling algorithm that can provably preserves in-degree and out-degree distributions, which are two most fundamental properties of a graph. We also prove the upper bound for deviations in the joint degree distribution and degree correlation distribution, which correspond to JDM and DCM. Besides, we prove that the deviations in these distributions are negatively correlated with the sparsity of the JDM and DCM. Considering that these two matrices are always quite sparse, we believe that proposed algorithm will have a better-than-theory performance on real-world large directed networks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Countering the Forgetting of Novel Health Information with 'Social Boosting'</title>
<link>https://arxiv.org/abs/2507.23148</link>
<guid>https://arxiv.org/abs/2507.23148</guid>
<content:encoded><![CDATA[
<div> Keywords: intervention techniques, social structure, knowledge retention, social interactions, health knowledge 

Summary: 
The study investigates the impact of social structure on the retention of knowledge interventions in isolated Honduran villages. By focusing on maternal and child health care information delivery, the researchers found that individuals with friendship ties within a social network experienced enhanced effectiveness of knowledge interventions. This was attributed to the opportunities for social interactions, such as discussing and reinforcing information with others, leading to deeper cognitive processing and memory retention. The concept of "social boosting" emerged, where well-connected individuals could internalize and retain information better due to increased social interactions. The findings emphasize the significant role of social interactions in reinforcing health knowledge interventions over the long term. This study has implications for health policy, global health workforce, healthcare professionals working with disadvantaged populations, and UN missions addressing infodemics.<br /><br />Summary: <div>
arXiv:2507.23148v1 Announce Type: new 
Abstract: To mitigate the adverse effects of low-quality or false information, studies have shown the effectiveness of various intervention techniques through debunking or so-called pre-bunking. However, the effectiveness of such interventions can decay. Here, we investigate the role of the detailed social structure of the local villages within which the intervened individuals live, which provides opportunities for the targeted individuals to discuss and internalize new knowledge. We evaluated this with respect to a critically important topic, information about maternal and child health care, delivered via a 22-month in-home intervention. Specifically, we examined the effect of having friendship ties on the retention of knowledge interventions among targeted individuals in 110 isolated Honduran villages. We hypothesize that individuals who receive specific knowledge can internalize and consolidate this information by engaging in social interactions where, for instance, they have an opportunity to discuss it with others in the process. The opportunity to explain information to others (knowledge sharing) promotes deeper cognitive processing and elaborative encoding, which ultimately enhances memory retention. We found that well-connected individuals within a social network experience an enhanced effectiveness of knowledge interventions. These individuals may be more likely to internalize and retain the information and reinforce it in others, due to increased opportunities for social interaction where they teach others or learn from them, a mechanism we refer to as "social boosting". These findings underscore the role of social interactions in reinforcing health knowledge interventions over the long term. We believe these findings would be of interest to the health policy, the global health workforce, and healthcare professionals focusing on disadvantaged populations and UN missions on infodemics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical cross-system meta-analysis of long-term transmission grid evolution</title>
<link>https://arxiv.org/abs/2507.23546</link>
<guid>https://arxiv.org/abs/2507.23546</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-side flexibility, transmission network, reconfiguration, real-world grids, empirical studies 

Summary: 
Grid-side flexibility, the ability to reconfigure transmission network topology, is not fully utilized in real-world grids due to limited empirical studies. The potential for grid-side flexibility remains underexplored, hindering its full implementation. The lack of empirical research hinders understanding on how real-world grids evolve over time. By examining real-world grids, insights can be gained into the practical implications of implementing grid-side flexibility. Analyzing the evolution of transmission network topology can provide valuable knowledge for enhancing grid efficiency and resilience. Empirical studies on real-world grids are crucial for unlocking the full potential of grid-side flexibility and optimizing grid operations for the future. 

Summary: <div>
arXiv:2507.23546v1 Announce Type: new 
Abstract: The potential of grid-side flexibility, the latent ability to reconfigure transmission network topology remains under-used partly because of the lack of empirical studies on how real-world grids evolve.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Left-Wing Extremism on the Decentralized Web: An Analysis of Lemmygrad.ml</title>
<link>https://arxiv.org/abs/2507.23699</link>
<guid>https://arxiv.org/abs/2507.23699</guid>
<content:encoded><![CDATA[
<div> extremism, Lemmy, social media, toxicity, authoritarian

Summary:<br />
This study examines left-wing extremism on the Lemmygrad.ml instance of the social media platform Lemmy. The research covers user activity and toxicity levels, particularly after the migration of certain subreddits. It uncovers an increase in user engagement and harmful content, including support for authoritarian regimes and anti-Zionist and antisemitic posts. By using a transformer-based topic modeling approach, the study offers insights into the nature of content on Lemmygrad.ml. The findings highlight the need for a comprehensive analysis of political extremism on decentralized social networks, stressing the importance of studying extremism across the political spectrum. <div>
arXiv:2507.23699v1 Announce Type: new 
Abstract: This study investigates the presence of left-wing extremism on the Lemmygrad.ml instance of the decentralized social media platform Lemmy, from its launch in 2019 up to a month after the bans of the subreddits r/GenZedong and r/GenZhou. We conduct a temporal analysis on Lemmygrad.ml's user activity, with also measuring the degree of highly abusive or hateful content. Furthermore, we explore the content of their posts using a transformer-based topic modeling approach. Our findings reveal a substantial increase in user activity and toxicity levels following the migration of these subreddits to Lemmygrad.ml. We also identify posts that support authoritarian regimes, endorse the Russian invasion of Ukraine, and feature anti-Zionist and antisemitic content. Overall, our findings contribute to a more nuanced understanding of political extremism within decentralized social networks and emphasize the necessity of analyzing both ends of the political spectrum in research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting stock prices with ChatGPT-annotated Reddit sentiment</title>
<link>https://arxiv.org/abs/2507.22922</link>
<guid>https://arxiv.org/abs/2507.22922</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, sentiment analysis, stock market, predictive power, retail investors

Summary:
- The study examines the relationship between social media sentiment, particularly from Reddit's r/wallstreetbets, and stock market movements for GameStop and AMC Entertainment.
- Three sentiment analysis methods are employed, including a model fine-tuned for interpreting informal language and emojis on social media.
- Surprisingly, social media sentiment shows only a weak correlation with stock prices, with simpler metrics like comment volume and Google search trends offering stronger predictive signals.
- The results suggest that traditional sentiment analysis may not fully capture the complexity of retail investor behavior and market dynamics.
- The study underscores the need to consider various factors beyond sentiment analysis when predicting stock market movements. 

<br /><br />Summary: <div>
arXiv:2507.22922v1 Announce Type: cross 
Abstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection</title>
<link>https://arxiv.org/abs/2507.22930</link>
<guid>https://arxiv.org/abs/2507.22930</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, Personal Information Identifiers (PIIs), synthetic dataset, privacy risks, online social media<br />
Summary:<br />
- The study focuses on the risk of privacy breaches on social platforms like Reddit due to users' self-disclosures of Personal Information Identifiers (PIIs).
- A lack of open-source labeled datasets hinders research into identifying and retrieving PII-revealing text.
- The researchers developed a methodology to create synthetic PII-labeled datasets from large language models to address this issue.
- They created a taxonomy of 19 PII-revealing categories for vulnerable populations.
- The evaluation of the synthetic dataset was based on reproducibility equivalence, unlinkability to original users, and indistinguishability from the original data.
- The dataset and code were released to facilitate further research into PII privacy risks on online social media platforms. <br />Summary: <div>
arXiv:2507.22930v1 Announce Type: cross 
Abstract: Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion formation in Wikipedia Ising networks</title>
<link>https://arxiv.org/abs/2507.22254</link>
<guid>https://arxiv.org/abs/2507.22254</guid>
<content:encoded><![CDATA[
<div> opinion formation, Wikipedia Ising Networks, competition, political leaders, social concepts

Summary:
The study focuses on opinion formation on Wikipedia Ising Networks, where each node represents a Wikipedia article and links are generated by citations. Ising spins at each node determine their orientation based on a majority vote of connected neighbors, leading to a spin polarized steady-state phase with stable opinion polarization. The model explores competition between political leaders, world countries, and social concepts, using examples like Donald Trump, Vladimir Putin, and Xi Jinping. The approach is extended to three groups with different opinions represented by colors within English, Russian, and Chinese editions of Wikipedia. The research suggests that this Ising Network Opinion Formation model offers a generic description of opinion formation in complex networks. <br /><br />Summary: <div>
arXiv:2507.22254v1 Announce Type: new 
Abstract: We study properties of opinion formation
  on Wikipedia Ising Networks. Each Wikipedia article
  is represented as a node and links are formed by citations of
  one article to another generating a directed network
  of a given language edition with millions of nodes.
  Ising spins are placed at each node
  and their orientation up or down is determined by a majority vote
  of connected neighbors. At the initial stage there are only
  a few nodes from two groups with fixed competing opinions up and down
  while other nodes are assumed to have no initial opinion with no
  effect on the vote. The competition of two opinions is modeled by
  an asynchronous Monte Carlo process converging to a spin polarized
  steady-state phase.
  This phase remains stable with respect to small fluctuations
  induced by an effective temperature of the Monte Carlo process.
  The opinion polarization at the steady-state provides
  opinion (spin) preferences for each node. In the framework of
  this Ising Network
  Opinion Formation model we analyze the influence and competition between
  political leaders, world countries and social concepts.
  This approach is also generalized to the competition between
  three groups of
  different opinions described by three colors, for example
  Donald Trump, Vladimir Putin, Xi Jinping or USA, Russia, China
  within English, Russian and Chinese editions of Wikipedia of March 2025.
  We argue that this approach provides a generic description of
  opinion formation in various complex networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Influence Maximization on Temporal Networks: A Guide to Make the Best Choice</title>
<link>https://arxiv.org/abs/2507.22589</link>
<guid>https://arxiv.org/abs/2507.22589</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, influence maximization, diffusion models, seed selection, network settings

Summary: 
This article presents a comprehensive guide for selecting the most suitable diffusion model for influence maximization on temporal networks. The categorization of existing models based on underlying mechanisms and effectiveness in different network settings aids in making informed decisions. The analysis of seed selection strategies emphasizes the development of efficient algorithms for finding near-optimal influential node sets. The comparison of key advancements, challenges, and practical applications provides a roadmap for researchers and practitioners to navigate the temporal influence maximization landscape effectively. Overall, this structured guide offers valuable insights into maximizing influence in dynamic communication systems and online social platforms. 

<br /><br />Summary:  <div>
arXiv:2507.22589v1 Announce Type: new 
Abstract: The increasing prominence of temporal networks in online social platforms and dynamic communication systems has made influence maximization a critical research area. Various diffusion models have been proposed to capture the spread of information, yet selecting the most suitable model for a given scenario remains challenging. This article provides a structured guide to making the best choice among diffusion models for influence maximization on temporal networks. We categorize existing models based on their underlying mechanisms and assess their effectiveness in different network settings. We analyze seed selection strategies, highlighting how the inherent properties of influence spread enable the development of efficient algorithms that can find near-optimal sets of influential nodes. By comparing key advancements, challenges, and practical applications, we offer a comprehensive roadmap for researchers and practitioners to navigate the landscape of temporal influence maximization effectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility in Epidemic Modeling</title>
<link>https://arxiv.org/abs/2507.22799</link>
<guid>https://arxiv.org/abs/2507.22799</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, epidemic modeling, contact tracing, data-driven methodologies, disease transmission dynamics

Summary: 
This review explores the integration of high-resolution human mobility data into epidemic modeling, highlighting the importance of considering the complex and heterogeneous nature of real-world human interactions. The review covers diverse sources and representations of human mobility data, and examines the behavioral and structural roles of mobility in shaping disease transmission dynamics. It discusses various epidemic modeling approaches, including compartmental models, network-based models, agent-based models, and machine learning models. The review also addresses how integrating mobility data improves risk management and response strategies during epidemics. By synthesizing these insights, the review serves as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while providing clear directions for future research.

<br /><br />Summary: <div>
arXiv:2507.22799v1 Announce Type: new 
Abstract: Human mobility forms the backbone of contact patterns through which infectious diseases propagate, fundamentally shaping the spatio-temporal dynamics of epidemics and pandemics. While traditional models are often based on the assumption that all individuals have the same probability of infecting every other individual in the population, a so-called random homogeneous mixing, they struggle to capture the complex and heterogeneous nature of real-world human interactions. Recent advancements in data-driven methodologies and computational capabilities have unlocked the potential of integrating high-resolution human mobility data into epidemic modeling, significantly improving the accuracy, timeliness, and applicability of epidemic risk assessment, contact tracing, and intervention strategies. This review provides a comprehensive synthesis of the current landscape in human mobility-informed epidemic modeling. We explore diverse sources and representations of human mobility data, and then examine the behavioral and structural roles of mobility and contact in shaping disease transmission dynamics. Furthermore, the review spans a wide range of epidemic modeling approaches, ranging from classical compartmental models to network-based, agent-based, and machine learning models. And we also discuss how mobility integration enhances risk management and response strategies during epidemics. By synthesizing these insights, the review can serve as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while charting clear directions for future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of media memorability in facilitating startups' access to venture capital funding</title>
<link>https://arxiv.org/abs/2507.22201</link>
<guid>https://arxiv.org/abs/2507.22201</guid>
<content:encoded><![CDATA[
<div> memedia, memorability, venture capital, investment, startup  
Summary:  
- Media memorability, related to imprinting a startup's name in investor memory, influences venture capital investment in startups, with detailed cues such as distinctiveness and connectivity key factors.  
- Venture capitalists consider nuanced aspects of media content, beyond general exposure, when making funding decisions.  
- Data from 197 UK micro and nanotechnology startups funded between 1995 and 2004 supports the significant impact of media memorability on investment outcomes.  
- Startups should focus on strengthening brand memorability through targeted, meaningful media coverage highlighting uniqueness and industry relevance to attract venture capital.  
- The study contributes to understanding how media legitimation influences entrepreneurial finance and emphasizes the importance of strategic media engagement for startups seeking investment.<br /><br /> <div>
arXiv:2507.22201v1 Announce Type: cross 
Abstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From News Source Sharers to Post Viewers: How Topic Diversity and Conspiracy Theories Shape Engagement With Misinformation During a Health Crisis</title>
<link>https://arxiv.org/abs/2401.08832</link>
<guid>https://arxiv.org/abs/2401.08832</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, COVID-19, social media, conspiracy theories, engagement

Summary:
The study examines online engagement with misinformation during the COVID-19 pandemic on social media platform X. False news with conspiracy theories is found to have higher topic diversity than true news. False news also has a longer lifespan and receives more engagement on X, especially when conspiracy theories are involved. News source sharers' engagement is not significantly influenced by topic diversity. In contrast, post viewers engage more with posts that have diverse topics, with misinformation containing conspiracy theories receiving significantly more reposts, likes, and replies. These findings highlight distinct engagement patterns between news source sharers and post viewers on X, providing insights for refining interventions against misinformation at both user levels. <div>
arXiv:2401.08832v3 Announce Type: replace 
Abstract: Online engagement with misinformation threatens societal well-being, particularly during health crises when susceptibility to misinformation is heightened in a multi-topic context. Here, we focus on the COVID-19 pandemic and address a critical gap in understanding engagement with multi-topic misinformation on social media at two user levels: news source sharers (who post news items) and post viewers (who engage with news posts). To this end, we analyze 7273 fact-checked source news items and their associated posts on X through the lens of topic diversity and conspiracy theories. We find that false news, especially those containing conspiracy theories, exhibits higher topic diversity than true news. At news source sharer level, false news has a longer lifetime and receives more posts on X than true news, with conspiracy theories further extending its longevity. However, topic diversity does not significantly influence news source sharers' engagement. At post viewer level, contrary to news source sharer level, posts characterized by heightened topic diversity receive more reposts, likes, and replies. Notably, post viewers tend to engage more with misinformation containing conspiracy narratives: false news posts that contain conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes, and 44.1% more replies compared to those without conspiracy theories. Our findings suggest that news source sharers and post viewers exhibit distinct engagement patterns on X, offering valuable insights into refining misinformation interventions at these two user levels.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted-Median Model of Opinion Dynamics on Networks</title>
<link>https://arxiv.org/abs/2406.17552</link>
<guid>https://arxiv.org/abs/2406.17552</guid>
<content:encoded><![CDATA[
<div> consensus opinion, opinion fragmentation, social network, opinion model, echo chambers  
Summary:  
The study explores how social interactions in a network influence individuals' opinions, leading to either a consensus opinion or opinion fragmentation forming echo chambers. Unlike traditional models that rely on mean opinion, the study considers an update rule based on weighted median opinion for a more realistic approach. Numerical simulations investigate how the limit opinion distribution is impacted by network structure. For configuration-model networks, a mean-field approximation for the opinion distribution dynamics with infinitely many individuals is derived. The research sheds light on the dynamics of opinions in social networks and provides insights into the formation of echo chambers and consensus opinions. <div>
arXiv:2406.17552v2 Announce Type: replace-cross 
Abstract: Social interactions influence people's opinions. In some situations, these interactions result in a consensus opinion; in others, they result in opinion fragmentation and the formation of different opinion groups in the form of "echo chambers". Consider a social network of individuals, who hold continuous-valued scalar opinions and change their opinions when they interact with each other. In such an opinion model, it is common for an opinion-update rule to depend on the mean opinion of interacting individuals. However, we consider an alternative update rule - which may be more realistic in some situations - that instead depends on a weighted median opinion of interacting individuals. Through numerical simulations of our opinion model, we investigate how the limit opinion distribution depends on network structure. For configuration-model networks, we also derive a mean-field approximation for the asymptotic dynamics of the opinion distribution when there are infinitely many individuals in a network.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Half-life of Youtube News Videos: Diffusion Dynamics and Predictive Factors</title>
<link>https://arxiv.org/abs/2507.21187</link>
<guid>https://arxiv.org/abs/2507.21187</guid>
<content:encoded><![CDATA[
<div> diffusion patterns, dispersion rate, 24-hour half-life, prediction models, Explainable AI

Summary:
The study investigates the early-stage diffusion patterns and dispersion rate of news videos on YouTube within the first 24 hours. It analyzes a dataset of over 50,000 videos from 75 countries and six continents, revealing the average 24-hour half-life of YouTube news videos to be around 7 hours, with variability across regions. The research also delves into predicting the 24-hour half-lives of news videos using 6 different models based on statistical and Deep Learning techniques. The performance differences and the importance of video- and channel-related predictors are examined through Explainable AI techniques. The dataset, analysis codebase, and trained models are made publicly available to support further research in this field. 

<br /><br />Summary: <div>
arXiv:2507.21187v1 Announce Type: new 
Abstract: Consumption of YouTube news videos significantly shapes public opinion and political narratives. While prior works have studied the longitudinal dissemination dynamics of YouTube News videos across extended periods, limited attention has been paid to the short-term trends. In this paper, we investigate the early-stage diffusion patterns and dispersion rate of news videos on YouTube, focusing on the first 24 hours. To this end, we introduce and analyze a rich dataset of over 50,000 videos across 75 countries and six continents. We provide the first quantitative evaluation of the 24-hour half-life of YouTube news videos as well as identify their distinct diffusion patterns. According to the findings, the average 24-hour half-life is approximately 7 hours, with substantial variance both within and across countries, ranging from as short as 2 hours to as long as 15 hours. Additionally, we explore the problem of predicting the latency of news videos' 24-hour half-lives. Leveraging the presented datasets, we train and contrast the performance of 6 different models based on statistical as well as Deep Learning techniques. The difference in prediction results across the models is traced and analyzed. Lastly, we investigate the importance of video- and channel-related predictors through Explainable AI (XAI) techniques. The dataset, analysis codebase and the trained models are released at http://bit.ly/3ILvTLU to facilitate further research in this area.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Growing Toxicity Manifests: A Topic Trajectory Analysis of U.S. Immigration Discourse on Social Media</title>
<link>https://arxiv.org/abs/2507.21418</link>
<guid>https://arxiv.org/abs/2507.21418</guid>
<content:encoded><![CDATA[
<div> Keywords: online public sphere, immigration discourse, toxicity, topic discovery, trajectory analysis

Summary: 
The research examines how shifts in toxicity in discussions about U.S. immigration correspond to changes in topics. By analyzing 4 million online posts over six months, the study identifies 157 fine-grained subtopics within the immigration discourse. Users with increasing toxicity tend to adopt alarmist and fear-based perspectives, while those with decreasing toxicity gravitate towards legal and policy-focused themes. These patterns are statistically significantly different from two reference groups with stable toxicity levels. The study employs a novel method that combines hierarchical topic discovery with trajectory analysis to understand dynamic conversations around social issues in a scalable way. This approach can provide valuable insights into understanding polarization and toxicity in online discussions about immigration. 

<br /><br />Summary: <div>
arXiv:2507.21418v1 Announce Type: new 
Abstract: In the online public sphere, discussions about immigration often become increasingly fractious, marked by toxic language and polarization. Drawing on 4 million X posts over six months, we combine a user- and topic-centric approach to study how shifts in toxicity manifest as topical shifts. Our topic discovery method, which leverages instruction-based embeddings and recursive HDBSCAN, uncovers 157 fine-grained subtopics within the U.S. immigration discourse. We focus on users in four groups: (1) those with increasing toxicity, (2) those with decreasing toxicity, and two reference groups with no significant toxicity trend but matched toxicity levels. Treating each posting history as a trajectory through a five-dimensional topic space, we compare average group trajectories using permutational MANOVA. Our findings show that users with increasing toxicity drift toward alarmist, fear-based frames, whereas those with decreasing toxicity pivot toward legal and policy-focused themes. Both patterns diverge statistically significantly from their reference groups. This pipeline, which combines hierarchical topic discovery with trajectory analysis, offers a replicable method for studying dynamic conversations around social issues at scale.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation</title>
<link>https://arxiv.org/abs/2507.21903</link>
<guid>https://arxiv.org/abs/2507.21903</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Graphical methods, news summarization, stakeholders, events<br />
Summary:<br />
- Existing news summarization methods focusing only on textual content of articles, lacking analysis on parties involved.
- SUnSET framework introduced for Timeline Summarization, leveraging Large Language Models to build SET triplets.
- Stakeholder-based ranking used to construct Relevancy metric, outperforming prior baselines and becoming State-of-the-Art.
- Importance of stakeholders highlighted in news articles, impacting the understanding of events.
- SUnSET provides a novel approach to tracking events across sources, considering parties involved and connection of related events. <br />
Summary: <div>
arXiv:2507.21903v1 Announce Type: new 
Abstract: As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21055</link>
<guid>https://arxiv.org/abs/2507.21055</guid>
<content:encoded><![CDATA[
<div> Keywords: news media, comprehension gaps, large language models, agent-based framework, diverse audiences

Summary:
Large language models (LLMs) are used in an agent-based framework to address comprehension gaps in news media among diverse audiences. The framework simulates communication behaviors among agents representing experts from various occupations or different age groups. Through iterative discussions, the framework identifies confusion and misunderstandings in news content for the agents. Supplemental materials specific to these gaps are then designed and provided to the agents, leading to significantly improved news comprehension. This study demonstrates the utility and efficiency of the framework in enhancing news understanding for audiences with varying levels of expertise and age. <div>
arXiv:2507.21055v1 Announce Type: cross 
Abstract: In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos</title>
<link>https://arxiv.org/abs/2507.21467</link>
<guid>https://arxiv.org/abs/2507.21467</guid>
<content:encoded><![CDATA[
<div> algorithm, recommendation, YouTube, bias, content diversity

Summary:
This study explores the impact of recommendation algorithms on user engagement and content diversity on YouTube, focusing on short-form and long-form videos. By developing a novel data collection framework, the researchers analyze the distinct behavioral patterns of the algorithms in recommending content. They find that short-form videos lead to quicker shifts towards engaging but less diverse content compared to long-form videos. Moreover, the study investigates biases in politically sensitive topics like the South China Sea dispute, shedding light on how these algorithms shape narratives and amplify specific viewpoints. The research emphasizes the importance of responsible AI practices in creating equitable and transparent recommendation systems to address concerns about biases, echo chambers, and content diversity in digital media platforms. This study provides actionable insights for designing recommendation algorithms that promote fairness and transparency, highlighting the need for responsible AI practices in the evolving digital landscape. 

<br /><br />Summary: <div>
arXiv:2507.21467v1 Announce Type: cross 
Abstract: The growing popularity of short-form video content, such as YouTube Shorts, has transformed user engagement on digital platforms, raising critical questions about the role of recommendation algorithms in shaping user experiences. These algorithms significantly influence content consumption, yet concerns about biases, echo chambers, and content diversity persist. This study develops an efficient data collection framework to analyze YouTube's recommendation algorithms for both short-form and long-form videos, employing parallel computing and advanced scraping techniques to overcome limitations of YouTube's API. The analysis uncovers distinct behavioral patterns in recommendation algorithms across the two formats, with short-form videos showing a more immediate shift toward engaging yet less diverse content compared to long-form videos. Furthermore, a novel investigation into biases in politically sensitive topics, such as the South China Sea dispute, highlights the role of these algorithms in shaping narratives and amplifying specific viewpoints. By providing actionable insights for designing equitable and transparent recommendation systems, this research underscores the importance of responsible AI practices in the evolving digital media landscape.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Tight Bounds for Estimating Degree Distribution in Streaming and Query Models</title>
<link>https://arxiv.org/abs/2507.21784</link>
<guid>https://arxiv.org/abs/2507.21784</guid>
<content:encoded><![CDATA[
<div> degree distribution, complementary cumulative degree histogram, approximation algorithm, lower bounds, sublinear models

Summary:
An algorithm is proposed to approximate the complementary cumulative degree histogram (ccdh) of a graph by obtaining suitable vertex and edge samples, independent of any sublinear model. Efficient methods for obtaining these samples in streaming and query models are discussed. The complexity of the problem across sublinear models is nearly settled with the first lower bounds established in both query and streaming scenarios. This work addresses an open problem posed in a previous conference, providing insights into approximating ccdh and advancing the understanding of graph structure analysis. <div>
arXiv:2507.21784v1 Announce Type: cross 
Abstract: The degree distribution of a graph $G=(V,E)$, $|V|=n$, $|E|=m$ is one of the most fundamental objects of study in the analysis of graphs as it embodies relationship among entities. In particular, an important derived distribution from degree distribution is the complementary cumulative degree histogram (ccdh). The ccdh is a fundamental summary of graph structure, capturing, for each threshold $d$, the number of vertices with degree at least $d$. For approximating ccdh, we consider the $(\varepsilon_D,\varepsilon_R)$-BiCriteria Multiplicative Approximation, which allows for controlled multiplicative slack in both the domain and the range. The exact complexity of the problem was not known and had been posed as an open problem in WOLA 2019 [Sublinear.info, Problem 98].
  In this work, we first design an algorithm that can approximate ccdh if a suitable vertex sample and an edge sample can be obtained and thus, the algorithm is independent of any sublinear model. Next, we show that in the streaming and query models, these samples can be obtained efficiently. On the other end, we establish the first lower bounds for this problem in both query and streaming models, and (almost) settle the complexity of the problem across both the sublinear models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Misconceptions in Social Bots Research</title>
<link>https://arxiv.org/abs/2303.17251</link>
<guid>https://arxiv.org/abs/2303.17251</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots research, biases, misconceptions, online manipulation, methodological issues <br />
Summary: This article discusses the prevalent biases, hyped results, and misconceptions that plague social bots research, leading to ambiguities and unrealistic expectations. It highlights the need for rigorous, unbiased, and responsible discussions on online disinformation and manipulation. The analysis addresses methodological and conceptual issues affecting current research and refutes common fallacious arguments used by proponents and opponents alike. By demystifying misconceptions and providing guidelines for future research, the article aims to ensure reliable solutions and reaffirm the validity of the scientific method. <div>
arXiv:2303.17251v4 Announce Type: replace 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
<div> social media, ideology classification, Large Language Models, in-context learning, metadata

Summary:
Large Language Models (LLMs) show promise in classifying political ideology in online content within the context of the two-party US political spectrum through in-context learning (ICL). The study conducted experiments on news articles and YouTube videos, demonstrating that the approach outperforms zero-shot and traditional supervised methods. The influence of metadata, such as content source and descriptions, on ideological classification was also evaluated. Providing source information for political and non-political content was found to impact the LLM's classification accuracy. The research addresses concerns of radicalization, filter bubbles, and content bias in the rapidly expanding social media landscape. <div>
arXiv:2503.20797v2 Announce Type: replace-cross 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green building blocks reveal the complex anatomy of climate change mitigation technologies</title>
<link>https://arxiv.org/abs/2504.06834</link>
<guid>https://arxiv.org/abs/2504.06834</guid>
<content:encoded><![CDATA[
<div> Keywords: Green Building Blocks, innovation, climate change, technology, international collaboration

Summary: 
The study focuses on identifying "Green Building Blocks" (GBBs) as modular components that can be incorporated into existing technologies to reduce their carbon footprint, thus facilitating the transition to net-zero emissions. By comparing green and nongreen patents, the researchers construct a network that connects nongreen technologies to GBBs, highlighting areas with varying potential for climate-change mitigating innovation. The unequal distribution of node degrees in the network underscores the differing opportunities for innovation across domains. Additionally, the study reveals the importance of international collaboration in driving innovation, with a significant proportion of firms in the US, Germany, and China relying on foreign partners for optimal development of green technologies. The findings emphasize the critical role of global cooperation in advancing sustainable technological solutions and warn against the risks of economic nationalism hindering progress towards achieving climate goals. 

<br /><br />Summary: <div>
arXiv:2504.06834v2 Announce Type: replace-cross 
Abstract: Achieving net-zero emissions requires rapid innovation, yet the necessary technological knowhow is scattered across industries and countries. Comparing functionally similar green and nongreen patents, we identify "Green Building Blocks" (GBBs): modular components that can be added to reduce existing technologies' carbon footprints. These GBBs depict the anatomy of the green transition as a network that connects problems -- nongreen technologies -- to GBBs that mitigate their climate-change impact. Node degrees in this network are highly unequal, showing that the scope for climate-change mitigating innovation varies substantially across domains. The network also helps predict which green technologies firms develop themselves, and which alliances they form to do so. This reveals a critical dependence on international collaboration: optimal innovation partners for 84% of US, 87% of German, and 92% of Chinese firms are foreign, providing quantitative evidence that rising economic nationalism threatens the pace of innovation required to meet global climate goals.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dual Personas of Social Media Bots</title>
<link>https://arxiv.org/abs/2504.12498</link>
<guid>https://arxiv.org/abs/2504.12498</guid>
<content:encoded><![CDATA[
<div> Keywords: social media bots, personas, bot detection regulation, good-bad duality, metrics

Summary:
Social media bots, AI agents that engage in online interactions, have various personas tailored for specific behaviors or content types. This article introduces fifteen agent personas categorized into Content-Based and Behavior-Based Bot Personas. The bots can serve both positive and negative purposes, challenging the common perception that all bots are malicious. To better understand bot behavior, metrics for assessing the good and bad aspects of bot agents are outlined. The research highlights the importance of considering how bots are utilized rather than labeling them universally as harmful. This guideline aims to inform bot detection regulation and underscores the need for nuanced approaches to assessing and addressing the impacts of social media bots. <div>
arXiv:2504.12498v2 Announce Type: replace-cross 
Abstract: Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks</title>
<link>https://arxiv.org/abs/2507.19702</link>
<guid>https://arxiv.org/abs/2507.19702</guid>
<content:encoded><![CDATA[
<div> 1D-CGS; influential nodes; complex networks; node ranking; deep learning
<br />
<br />
Summary: 
The article introduces 1D-CGS, a hybrid model combining 1D convolutional neural networks (1D-CNN) with GraphSAGE for efficient node ranking in complex networks. It utilizes node degree and average neighbor degree as input features, processed through 1D convolutions and GraphSAGE layers. The model is trained on synthetic networks and tested on real-world networks, outperforming traditional centrality measures and deep learning models in accuracy while maintaining fast runtime. Results show a substantial improvement in ranking accuracy compared to baselines, with high correlation and Jaccard Similarity scores. The model exhibits unique and discriminative rankings with near-perfect rank distributions and high Monotonicity Index scores. Importantly, 1D-CGS operates in a highly reasonable time frame, making it suitable for large-scale applications. <div>
arXiv:2507.19702v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a critical task with a wide range of applications across different domains. However, existing approaches often face trade-offs between accuracy and computational efficiency. To address these challenges, we propose 1D-CGS, a lightweight and effective hybrid model that integrates the speed of one-dimensional convolutional neural networks (1D-CNN) with the topological representation power of GraphSAGE for efficient node ranking. The model uses a lightweight input representation built on two straightforward and significant topological features: node degree and average neighbor degree. These features are processed through 1D convolutions to extract local patterns, followed by GraphSAGE layers to aggregate neighborhood information. We formulate the node ranking task as a regression problem and use the Susceptible-Infected-Recovered (SIR) model to generate ground truth influence scores. 1D-CGS is initially trained on synthetic networks generated by the Barabasi-Albert model and then applied to real world networks for identifying influential nodes. Experimental evaluations on twelve real world networks demonstrate that 1D-CGS significantly outperforms traditional centrality measures and recent deep learning models in ranking accuracy, while operating in very fast runtime. The proposed model achieves an average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard Similarity over the best performing deep learning baselines. It also achieves an average Monotonicity Index (MI) score 0.99 and produces near perfect rank distributions, indicating highly unique and discriminative rankings. Furthermore, all experiments confirm that 1D-CGS operates in a highly reasonable time, running significantly faster than existing deep learning methods, making it suitable for large scale applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Closed Loop Dynamics Between a Social Media Recommender System and Users' Opinions</title>
<link>https://arxiv.org/abs/2507.19792</link>
<guid>https://arxiv.org/abs/2507.19792</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender System, User Opinions, Monte Carlo simulations, Polarisation, Radicalisation

Summary:
The paper presents a mathematical model that analyzes the interaction between a Recommender System (RS) algorithm and content consumers. The model considers a large user population with varying opinions, consuming personalized content recommended by the RS. Through Monte Carlo simulations, the study explores how RS impacts user opinions and the subsequent content recommendations based on user engagement. The research delves into the performance of the RS in influencing user engagement and how user opinions evolve, particularly focusing on polarization and radicalization. It identifies certain opinion distributions as more prone to polarization, highlights ineffective content stances in changing opinions, and underscores the effectiveness of viral content in combating polarization. The findings provide insights into the dynamics of RS-user interactions and the role of viral content in mitigating opinion polarization.<br /><br />Summary: <div>
arXiv:2507.19792v1 Announce Type: new 
Abstract: This paper proposes a mathematical model to study the coupled dynamics of a Recommender System (RS) algorithm and content consumers (users). The model posits that a large population of users, each with an opinion, consumes personalised content recommended by the RS. The RS can select from a range of content to recommend, based on users' past engagement, while users can engage with the content (like, watch), and in doing so, users' opinions evolve. This occurs repeatedly to capture the endless content available for user consumption on social media. We employ a campaign of Monte Carlo simulations using this model to study how recommender systems influence users' opinions, and in turn how users' opinions shape the subsequent recommended content. We take an interest in both the performance of the RS (e.g., how users engage with the content) and the user's opinions, focusing on polarisation and radicalisation of opinions. We find that different opinion distributions are more susceptible to becoming polarised than others, many content stances are ineffective in changing user opinions, and creating viral content is an effective measure in combating polarisation of opinions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity</title>
<link>https://arxiv.org/abs/2507.20066</link>
<guid>https://arxiv.org/abs/2507.20066</guid>
<content:encoded><![CDATA[
<div> scale measurement, similarity, disinformation, narratives, detection

Summary:
The thesis presents a continuous scale measurement of similarity to disinformation narratives for detecting and capturing partial truths. Two tools, a tracing tool and a narrative synthesis tool, are developed to analyze tweets and target narratives. The tracing tool rates similarities and graphs them over time, while the narrative synthesis tool clusters tweets and identifies dominant narratives. The tools are integrated into a Tweet Narrative Analysis Dashboard. Validation on the GLUE STS-B benchmark is followed by case studies on "The 2020 election was stolen" narrative using Donald Trump's tweets and "Transgender people are harmful to society" narrative using tweets from media outlets. The empirical findings support semantic similarity for nuanced disinformation detection, tracing, and characterization. Access to the tools is available upon request to the author. 

<br /><br />Summary: <div>
arXiv:2507.20066v1 Announce Type: new 
Abstract: This thesis develops a continuous scale measurement of similarity to disinformation narratives that can serve to detect disinformation and capture the nuanced, partial truths that are characteristic of it. To do so, two tools are developed and their methodologies are documented. The tracing tool takes tweets and a target narrative, rates the similarities of each to the target narrative, and graphs it as a timeline. The second narrative synthesis tool clusters tweets above a similarity threshold and generates the dominant narratives within each cluster. These tools are combined into a Tweet Narrative Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark, and then the two tools are used to analyze two case studies for further empirical validation. The first case study uses the target narrative "The 2020 election was stolen" and analyzes a dataset of Donald Trump's tweets during 2020. The second case study uses the target narrative, "Transgender people are harmful to society" and analyzes tens of thousands of tweets from the media outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News. Together, the empirical findings from these case studies demonstrate semantic similarity for nuanced disinformation detection, tracing, and characterization.
  The tools developed in this thesis are hosted and can be accessed through the permission of the author. Please explain your use case in your request. The HTML friendly version of this paper is at https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Blockchain-Based Quality Control Model for Online Collaboration Systems</title>
<link>https://arxiv.org/abs/2507.20265</link>
<guid>https://arxiv.org/abs/2507.20265</guid>
<content:encoded><![CDATA[
<div> blockchain, quality control, collaborative content generation, decentralized trust, informetrics

Summary: 
The manuscript introduces a blockchain-based quality control model for collaborative content generation (CCG), addressing challenges such as citation manipulation, transparency, and decentralized trust in metric computation. The model uses a semi-iterative algorithm to compute quality scores of artifacts and reputation of nodes. It is agile in processing latency and shows comparable performance to PageRank and HITS baselines in evaluating quality scores. The model also demonstrates throughput, latency, and robustness against malicious nodes, confirming its reliability. Theoretical comparison with recent studies validates its feasibility for real-world informetric application. <div>
arXiv:2507.20265v1 Announce Type: new 
Abstract: Collaborative content generation (CCG) enables collective creation of artifacts like scientific articles. Quality is a paramount concern in CCG, and a multitude of methods have been proposed to evaluate the quality of artifacts. Nevertheless, the majority of these methods are reliant on centralized architectures, which present challenges pertaining to security, privacy, and availability. Blockchain technology proffers a potential resolution to these challenges, by furnishing a decentralized and immutable ledger of quality scores. In this manuscript, we introduce a blockchain-based quality control model for CCG that uses a semi-iterative algorithm to interdependently compute quality scores of artifacts and reputation of nodes. Our model addresses critical challenges in academic informetrics, such as citation manipulation, transparency in collaborative scholarship, and decentralized trust in metric computation. Our model also exhibits sensitivity to processing latency, rendering it more agile in the presence of delays. Our model's quality scores, evaluated against PageRank and HITS baselines, show comparable performance, with additional assessments of throughput, latency, and robustness against malicious nodes confirming its reliability. A theoretical comparison with recent studies validates its feasibility for real world informetric application.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural-Aware Key Node Identification in Hypergraphs via Representation Learning and Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20682</link>
<guid>https://arxiv.org/abs/2507.20682</guid>
<content:encoded><![CDATA[
<div> Autoencoder, Hypergraph neural network, Active learning, Node importance, Node identification

Summary:
The article introduces a new framework, AHGA, for evaluating key nodes in hypergraphs, capturing polyadic interactions in real-world systems. AHGA combines an Autoencoder for higher-order structural features, a HyperGraph neural network for pre-training, and an Active learning-based fine-tuning process. The fine-tuning step improves robustness and generalization across diverse hypergraph topologies. Experimental results on empirical hypergraphs show AHGA outperforms centrality-based baselines by 37.4%. Nodes identified by AHGA exhibit high influence and strong structural disruption capability, highlighting their ability to detect multifunctional nodes. <div>
arXiv:2507.20682v1 Announce Type: new 
Abstract: Evaluating node importance is a critical aspect of analyzing complex systems, with broad applications in digital marketing, rumor suppression, and disease control. However, existing methods typically rely on conventional network structures and fail to capture the polyadic interactions intrinsic to many real-world systems. To address this limitation, we study key node identification in hypergraphs, where higher-order interactions are naturally modeled as hyperedges. We propose a novel framework, AHGA, which integrates an Autoencoder for extracting higher-order structural features, a HyperGraph neural network-based pre-training module (HGNN), and an Active learning-based fine-tuning process. This fine-tuning step plays a vital role in mitigating the gap between synthetic and real-world data, thereby enhancing the model's robustness and generalization across diverse hypergraph topologies. Extensive experiments on eight empirical hypergraphs show that AHGA outperforms classical centrality-based baselines by approximately 37.4%. Furthermore, the nodes identified by AHGA exhibit both high influence and strong structural disruption capability, demonstrating their superiority in detecting multifunctional nodes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wardropian Cycles make traffic assignment both optimal and fair by eliminating price-of-anarchy with Cyclical User Equilibrium for compliant connected autonomous vehicles</title>
<link>https://arxiv.org/abs/2507.19675</link>
<guid>https://arxiv.org/abs/2507.19675</guid>
<content:encoded><![CDATA[
<div> Keywords: Connected and Autonomous Vehicles, Wardropian cycles, System Optimal, User Equilibrium, Traffic Performance

Summary:
In this study, the concept of Wardropian cycles is proposed to achieve fair and optimal traffic assignment in the context of Connected and Autonomous Vehicles (CAVs). These cycles ensure both fairness and efficiency in routing, satisfying Wardrop's principles while equalizing average travel times among users. The researchers develop exact methods and a greedy heuristic to compute and optimize these cycles efficiently. They introduce the concept of Cyclical User Equilibrium for stability under deviations. Large-scale simulations in Barcelona, Berlin, Anaheim, and Sioux Falls show significant reductions in traffic inefficiencies and inequities with the implementation of Wardropian cycles, with the approach displaying more social acceptability. In Barcelona, up to 670 vehicle-hours of inefficiencies were eliminated, while in other cities, initial inequities were significantly reduced within a short timeframe. Overall, the study demonstrates the potential of Wardropian cycles in improving traffic performance and equality in CAV systems.<br /><br />Summary: <div>
arXiv:2507.19675v1 Announce Type: cross 
Abstract: Connected and Autonomous Vehicles (CAVs) open the possibility for centralised routing with full compliance, making System Optimal traffic assignment attainable. However, as System Optimum makes some drivers better off than others, voluntary acceptance seems dubious. To overcome this issue, we propose a new concept of Wardropian cycles, which, in contrast to previous utopian visions, makes the assignment fair on top of being optimal, which amounts to satisfaction of both Wardrop's principles. Such cycles, represented as sequences of permutations to the daily assignment matrices, always exist and equalise, after a limited number of days, average travel times among travellers (like in User Equilibrium) while preserving everyday optimality of path flows (like in System Optimum). We propose exact methods to compute such cycles and reduce their length and within-cycle inconvenience to the users. As identification of optimal cycles turns out to be NP-hard in many aspects, we introduce a greedy heuristic efficiently approximating the optimal solution. Finally, we introduce and discuss a new paradigm of Cyclical User Equilibrium, which ensures stability of optimal Wardropian Cycles under unilateral deviations.
  We complement our theoretical study with large-scale simulations. In Barcelona, 670 vehicle-hours of Price-of-Anarchy are eliminated using cycles with a median length of 11 days-though 5% of cycles exceed 90 days. However, in Berlin, just five days of applying the greedy assignment rule significantly reduces initial inequity. In Barcelona, Anaheim, and Sioux Falls, less than 7% of the initial inequity remains after 10 days, demonstrating the effectiveness of this approach in improving traffic performance with more ubiquitous social acceptability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy</title>
<link>https://arxiv.org/abs/2507.19692</link>
<guid>https://arxiv.org/abs/2507.19692</guid>
<content:encoded><![CDATA[
<div> photosensitive epilepsy, FlashGuard, color analysis, seizure prevention, digital media<br />
<br />
Summary: <br />
Individuals with photosensitive epilepsy (PSE) face challenges in the virtual realm due to unpredictable seizure-causing visual stimuli. Current solutions detect flashes asynchronously, lacking real-time and computational efficiency. FlashGuard, a novel approach, assesses color change rates in frames to mitigate stimuli in real-time. It uses CIELAB color space analysis to analyze color differences, reducing luminance and smoothing transitions. This study highlights how color properties impact flashing perception for PSE individuals, advocating for broader WCAG guidelines. Implementing these insights can better protect individuals with PSE from digital media triggers, enhancing accessibility and safety. <div>
arXiv:2507.19692v1 Announce Type: cross 
Abstract: In the virtual realm, individuals with photosensitive epilepsy (PSE) encounter challenges when using devices, resulting in exposure to unpredictable seizure-causing visual stimuli. The current norm for preventing epileptic flashes in media is to detect asynchronously when a flash will occur in a video, then notifying the user. However, there is a lack of a real-time and computationally efficient solution for dealing with this issue. To address this issue and enhance accessibility for photosensitive viewers, FlashGuard, a novel approach, was devised to assess the rate of change of colors in frames across the user's screen and appropriately mitigate stimuli, based on perceptually aligned color space analysis in the CIELAB color space. The detection system is built on analyzing differences in color, and the mitigation system works by reducing luminance and smoothing color transitions. This study provides novel insight into how intrinsic color properties contribute to perceptual differences in flashing for PSE individuals, calling for the adoption of broadened WCAG guidelines to better account for risk. These insights and implementations pave the way for stronger protections for individuals with PSE from dangerous triggers in digital media, both in policy and in software.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Bidding in Service-Oriented Combinatorial Spectrum Forward Auctions</title>
<link>https://arxiv.org/abs/2507.19720</link>
<guid>https://arxiv.org/abs/2507.19720</guid>
<content:encoded><![CDATA[
<div> auctions, spectrum, combinatorial, bidding mechanism, social welfare

Summary:
The article introduces a novel combinatorial forward auction scheme that allows for flexible bidding in dynamic spectrum sharing environments. Participants can submit bids consisting of base spectrum demand and adjustable demand ranges, improving resource efficiency and maximizing social welfare. A Spectrum Equivalent Mapping (SEM) coefficient is used to standardize valuation across frequency bands. A greedy matching algorithm sorts buyers by equivalent unit bid prices to determine winning bids and allocate resources within supply constraints. Simulation results show that the proposed flexible bidding mechanism outperforms existing methods, achieving higher social welfare in dynamic spectrum sharing scenarios. <div>
arXiv:2507.19720v1 Announce Type: cross 
Abstract: Traditional combinatorial spectrum auctions mainly rely on fixed bidding and matching processes, which limit participants' ability to adapt their strategies and often result in suboptimal social welfare in dynamic spectrum sharing environments. To address these limitations, we propose a novel approximately truthful combinatorial forward auction scheme with a flexible bidding mechanism aimed at enhancing resource efficiency and maximizing social welfare. In the proposed scheme, each buyer submits a combinatorial bid consisting of the base spectrum demand and adjustable demand ranges, enabling the auctioneer to dynamically optimize spectrum allocation in response to market conditions. To standardize the valuation across heterogeneous frequency bands, we introduce a Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is employed to determine winning bids by sorting buyers based on their equivalent unit bid prices and allocating resources within supply constraints. Simulation results demonstrate that the proposed flexible bidding mechanism significantly outperforms existing benchmark methods, achieving notably higher social welfare in dynamic spectrum sharing scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)</title>
<link>https://arxiv.org/abs/2507.20234</link>
<guid>https://arxiv.org/abs/2507.20234</guid>
<content:encoded><![CDATA[
<div> Decentralized Autonomous Organizations, DAOs, governance mechanism, user behavior, Internet Computer Protocol, SNS DAO framework<br />
Summary: 
This empirical study explores user behavior in decentralized autonomous organizations (DAOs) using the Internet Computer Protocol DAO framework SNS. The analysis includes participation rates, proposal submission frequency, voter approval rates, decision duration times, and metric shifts. Over 3,000 proposals from 14 SNS DAOs were evaluated, showing high approval rates and alignment in governance mechanisms. SNS DAOs demonstrate higher activity, lower costs, and faster decisions compared to other blockchain platforms. Importantly, SNS DAOs exhibit sustained or increasing engagement levels over time, contrary to declines seen in other frameworks. This study highlights the effectiveness of SNS governance mechanisms in promoting user engagement and agility in decision-making processes. <br /><br /> <div>
arXiv:2507.20234v1 Announce Type: cross 
Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism without centralized leadership. This paper presents an empirical study of user behavior in governance for a variety of DAOs, ranging from DeFi to gaming, using the Internet Computer Protocol DAO framework called SNS (Service Nervous System). To analyse user engagement, we measure participation rates and frequency of proposals submission and voter approval rates. We evaluate decision duration times to determine DAO agility. To investigate dynamic aspects, we also measure metric shifts in time. We evaluate over 3,000 proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected DAO have been existing between 6 and 20 months and cover a wide spectrum of use cases, treasury sizes, and number of participants. We also compare our results for SNS DAOs with DAOs from other blockchain platforms. While approval rates are generally high for all DAOs studied, SNS DAOs show slightly more alignment. We observe that the SNS governance mechanisms and processes in ICP lead to higher activity, lower costs and faster decisions. Most importantly, in contrast to studies which report a decline in participation over time for other frameworks, SNS DAOs exhibit sustained or increasing engagement levels over time.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Community Detection in Academic Networks by Handling Publication Bias</title>
<link>https://arxiv.org/abs/2507.20449</link>
<guid>https://arxiv.org/abs/2507.20449</guid>
<content:encoded><![CDATA[
<div> Keywords: research collaboration, topic-based network, BERTopic, SciBERT model, interdisciplinary links

Summary: 
The research article introduces a novel approach to identifying potential research collaborators based on publication content rather than traditional methods like co-authorships and citations. By utilizing BERTopic with a fine-tuned SciBERT model, a topic-based research network is built to connect researchers across disciplines who share topical interests. The main challenge addressed is publication imbalance, where some researchers publish more frequently across various topics, making their less frequent interests less visible. To overcome this, a cloning strategy is proposed, clustering a researcher's publications to treat each cluster as a separate node, allowing researchers to be part of multiple communities and improving the detection of interdisciplinary links. The evaluation of this method demonstrates that the cloned network structure leads to more meaningful communities and reveals a wider range of collaboration opportunities.<br /><br />Summary: <div>
arXiv:2507.20449v1 Announce Type: cross 
Abstract: Finding potential research collaborators is a challenging task, especially in today's fast-growing and interdisciplinary research landscape. While traditional methods often rely on observable relationships such as co-authorships and citations to construct the research network, in this work, we focus solely on publication content to build a topic-based research network using BERTopic with a fine-tuned SciBERT model that connects and recommends researchers across disciplines based on shared topical interests. A major challenge we address is publication imbalance, where some researchers publish much more than others, often across several topics. Without careful handling, their less frequent interests are hidden under dominant topics, limiting the network's ability to detect their full research scope. To tackle this, we introduce a cloning strategy that clusters a researcher's publications and treats each cluster as a separate node. This allows researchers to be part of multiple communities, improving the detection of interdisciplinary links. Evaluation on the proposed method shows that the cloned network structure leads to more meaningful communities and uncovers a broader set of collaboration opportunities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
<div> Identification, classification, sexism, social media, models
Summary:
The paper discusses the fifth Sexism Identification in Social Networks (EXIST) challenge at CLEF 2025, focusing on identifying and classifying sexism in social media posts. Three subtasks are addressed through the implementation of three models: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses adjectives as bottleneck concepts for interpretability, while SCBMT combines this with transformer contextual embeddings for improved performance. The models offer insights at both instance and class levels. Additional metadata like annotators' profiles are explored for leveraging in the classification task. Results show competitive performance, with SCBMT ranking 7th and 6th for English and Spanish, respectively, in Subtask 1.1. In comparison, the fine-tuned XLM-RoBERTa model achieves 6th and 4th for English in the Soft-Soft evaluation of Subtask 1.1. This research contributes to combating sexism on social media through advanced modeling techniques and data analysis. 
<br /><br />Summary: <div>
arXiv:2507.20924v1 Announce Type: cross 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Heterogeneous Networks, Link Prediction, Contrastive Learning, Spatial Heterogeneity, Temporal Heterogeneity

Summary:
This study introduces a novel Contrastive Learning-based Link Prediction model (CLP) designed to address the shortcomings of existing methods in capturing spatial and temporal heterogeneity in Temporal Heterogeneous Networks. The model employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. It includes a spatial feature modeling layer for capturing fine-grained topological distribution patterns and a temporal information modeling layer to perceive evolutionary dependencies. By encoding spatial and temporal distribution heterogeneity from a contrastive learning perspective, the model enables comprehensive self-supervised hierarchical relation modeling for link prediction. Experimental results on four real-world dynamic heterogeneous network datasets show that CLP consistently outperforms state-of-the-art models, demonstrating significant improvements in AUC and AP metrics. This research showcases the effectiveness of CLP in capturing complex system dynamics and heterogeneity in network link prediction tasks.

<br /><br />Summary: <div>
arXiv:2411.00612v2 Announce Type: replace 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixed points of Personalized PageRank centrality: From irreducible to reducible networks</title>
<link>https://arxiv.org/abs/2507.18652</link>
<guid>https://arxiv.org/abs/2507.18652</guid>
<content:encoded><![CDATA[
<div> PageRank, complex network, personalization vector, fixed points, strongly connected components <br />
Summary: <br />
- The study focuses on analyzing the PageRank of a complex network based on its personalization vector.
- The research provides a comprehensive understanding of the existence and uniqueness of fixed points of PageRank in a graph.
- The number and nature of strongly connected components play a crucial role in determining the fixed points of PageRank.
- A feedback-PageRank method is introduced to accurately compute the fixed points using Power's Method and the Perron vector of each strongly connected component.
- The approach presented in the paper offers a systematic way to analyze the PageRank behavior in complex networks. <div>
arXiv:2507.18652v1 Announce Type: new 
Abstract: In this paper we analyze the PageRank of a complex network as a function of its personalization vector. By using this approach, a complete characterization of the existence and uniqueness of fixed points of PageRank of a graph is given in terms of the number and nature of its strongly connected components. The method presented includes the use of a feedback-PageRank in order to compute exactly the fixed points following the classic Power's Method in terms of the (left-hand) Perron vector of each strongly connected components.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative news posts are less prevalent and generate lower user engagement than non-negative news posts across six countries</title>
<link>https://arxiv.org/abs/2507.19300</link>
<guid>https://arxiv.org/abs/2507.19300</guid>
<content:encoded><![CDATA[
<div> Facebook, news posts, negativity, engagement, multilingual classifiers  
Summary:  
Negative news posts on Facebook make up a small fraction (12.6%) of all posts, with political news posts not being more negative than non-political ones. In the U.S., political news posts are relatively less negative compared to other countries. Negative news posts receive fewer likes and comments compared to non-negative posts. Only a small proportion (10.2% to 13.1%) of user engagement with news posts comes from negative posts by analyzed news organizations. This comparative study sheds light on the prevalence of negative news on social media and its impact on user engagement, suggesting that negativity does not always correlate with higher engagement levels. <br /><br />Summary: <div>
arXiv:2507.19300v1 Announce Type: new 
Abstract: Although news negativity is often studied, missing is comparative evidence on the prevalence of and engagement with negative political and non-political news posts on social media. We use 6,081,134 Facebook posts published between January 1, 2020, and April 1, 2024, by 97 media organizations in six countries (U.S., UK, Ireland, Poland, France, Spain) and develop two multilingual classifiers for labeling posts as (non-)political and (non-)negative. We show that: (1) negative news posts constitute a relatively small fraction (12.6%); (2) political news posts are neither more nor less negative than non-political news posts; (3) U.S. political news posts are less negative relative to the other countries on average (40% lower odds); (4) Negative news posts get 15% fewer likes and 13% fewer comments than non-negative news posts. Lastly, (5) we provide estimates of the proportion of the total volume of user engagement with negative news posts and show that only between 10.2% to 13.1% of engagement is linked to negative posts by the analyzed news organizations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Changes to the Facebook Algorithm Decreased News Visibility Between 2021-2024</title>
<link>https://arxiv.org/abs/2507.19373</link>
<guid>https://arxiv.org/abs/2507.19373</guid>
<content:encoded><![CDATA[
<div> Facebook, news, algorithm, visibility, reactions <br />
Summary: The study examines the impact of Meta's algorithm changes on news visibility on Facebook between 2016 and 2025. Data from news and non-news pages show a significant decline in user reactions to news, particularly between 2021 and 2024, indicating targeted suppression. Low-quality news sources were disproportionately affected. However, the end of the "War on News" in 2025 led to an increase in user reactions to news, especially low-quality sources. This suppression of news visibility did not align with a decrease in news supply, Facebook user base, or interest in news. The findings suggest that Meta's algorithm changes had a substantial impact on the visibility and engagement with news content on the platform. <div>
arXiv:2507.19373v1 Announce Type: new 
Abstract: Platforms, especially Facebook, are primary news sources in the US. In its widely criticized "War on News," Meta algorithmically deprioritized news and political content. We use data from 40 news organizations (5,243,302 Facebook posts, 7,875,372,958 user reactions) and 21 non-news pages (396,468 posts; 1,909,088,308 reactions) between January 1, 2016 and February 13, 2025 to examine how these changes influenced news visibility on the platform. Reactions to news declined by 78% between 2021 and 2024 while reactions to non-news pages increased, indicating targeted suppression of news visibility. Low-quality sources were especially suppressed, yet the 2025 end to "War on News" increased user reactions to news, especially low-quality ones. These changes do not reflect decreased news supply, Facebook user base, or interest in news over this period.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods</title>
<link>https://arxiv.org/abs/2507.18778</link>
<guid>https://arxiv.org/abs/2507.18778</guid>
<content:encoded><![CDATA[
<div> recommendation system, CityHood, interactive, explainable, personalized

Summary:
CityHood is an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' interests. It leverages large-scale Google Places reviews enriched with various indicators to provide personalized recommendations at city and neighborhood levels. The system uses the LIME technique for explainability and offers natural-language explanations for each suggestion. Users can explore recommendations based on their preferences and understand the reasoning behind each suggestion through a visual interface. CityHood combines interest modeling, multi-scale analysis, and explainability to make travel recommendations transparent and engaging. It bridges gaps in location-based recommendations by considering spatial similarity, cultural alignment, and user interests. <div>
arXiv:2507.18778v1 Announce Type: cross 
Abstract: We present CityHood, an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' areas of interest. The system models user interests leveraging large-scale Google Places reviews enriched with geographic, socio-demographic, political, and cultural indicators. It provides personalized recommendations at city (Core-Based Statistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an explainable technique (LIME) and natural-language explanations. Users can explore recommendations based on their stated preferences and inspect the reasoning behind each suggestion through a visual interface. The demo illustrates how spatial similarity, cultural alignment, and interest understanding can be used to make travel recommendations transparent and engaging. This work bridges gaps in location-based recommendation by combining a kind of interest modeling, multi-scale analysis, and explainability in a user-facing system.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Approach for Agile Supply Chain Decision-Making Based on Network Attributes</title>
<link>https://arxiv.org/abs/2507.19038</link>
<guid>https://arxiv.org/abs/2507.19038</guid>
<content:encoded><![CDATA[
<div> network attributes, supply chain, disruption mitigation, distributed decision-making, performance evaluation

Summary:
This paper addresses the issue of disruptions in global supply chains and the need for agile decision-making strategies to mitigate their impact. The study characterizes supply chains from both a capability and network topological perspective and explores the use of distributed decision-making approaches to improve supply chain resilience. A comprehensive case study is conducted, evaluating the performance of a distributed framework in response to disruptions based on the network structure and agent attributes. Comparisons with a centralized decision-making approach highlight trade-offs between performance, computation time, and network communication. The findings provide valuable insights for practitioners looking to design effective response strategies that leverage agent capabilities, network attributes, and desired supply chain performance. <div>
arXiv:2507.19038v1 Announce Type: cross 
Abstract: In recent years, the frequent occurrence of disruptions has had a negative impact on global supply chains. To stay competitive, enterprises strive to remain agile through the implementation of efficient and effective decision-making strategies in reaction to disruptions. A significant effort has been made to develop these agile disruption mitigation approaches, leveraging both centralized and distributed decision-making strategies. Though trade-offs of centralized and distributed approaches have been analyzed in existing studies, no related work has been found on understanding supply chain performance based on the network attributes of the disrupted supply chain entities. In this paper, we characterize supply chains from a capability and network topological perspective and investigate the use of a distributed decision-making approach based on classical multi-agent frameworks. The performance of the distributed framework is evaluated through a comprehensive case study that investigates the performance of the supply chain as a function of the network structure and agent attributes within the network in the presence of a disruption. Comparison to a centralized decision-making approach highlights trade-offs between performance, computation time, and network communication based on the decision-making strategy and network architecture. Practitioners can use the outcomes of our studies to design response strategies based on agent capabilities, network attributes, and desired supply chain performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epidemiology-informed Network for Robust Rumor Detection</title>
<link>https://arxiv.org/abs/2411.12949</link>
<guid>https://arxiv.org/abs/2411.12949</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, rumor detection, information cascade, epidemiology, social media<br />
<br />
Summary:<br />
The article discusses the challenges posed by the rapid spread of rumors on social media and the limitations of existing graph-based rumor detection models. It introduces a novel approach called Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to improve performance by addressing issues related to data quality and varying depths of propagation trees. The EIN model utilizes large language models to generate stance labels from user responses, eliminating the need for manual annotation. Experimental results show that EIN outperforms state-of-the-art methods on real-world datasets and demonstrates enhanced robustness across different tree depths. By incorporating epidemiological principles into rumor detection, the EIN approach offers a more effective and efficient way to detect and monitor the spread of rumors on social media platforms. <br /> <div>
arXiv:2411.12949v3 Announce Type: replace 
Abstract: The rapid spread of rumors on social media has posed significant challenges to maintaining public trust and information integrity. Since an information cascade process is essentially a propagation tree, recent rumor detection models leverage graph neural networks to additionally capture information propagation patterns, thus outperforming text-only solutions. Given the variations in topics and social impact of the root node, different source information naturally has distinct outreach capabilities, resulting in different heights of propagation trees. This variation, however, impedes the data-driven design of existing graph-based rumor detectors. Given a shallow propagation tree with limited interactions, it is unlikely for graph-based approaches to capture sufficient cascading patterns, questioning their ability to handle less popular news or early detection needs. In contrast, a deep propagation tree is prone to noisy user responses, and this can in turn obfuscate the predictions. In this paper, we propose a novel Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to enhance performance by overcoming data-driven methods sensitivity to data quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is expected that each users stance toward the source information will be annotated. To bypass the costly and time-consuming human labeling process, we take advantage of large language models to generate stance labels, facilitating optimization objectives for learning epidemiology-informed representations. Our experimental results demonstrate that the proposed EIN not only outperforms state-of-the-art methods on real-world datasets but also exhibits enhanced robustness across varying tree depths.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Climate Change Debate in Italy through Information Supply and Demand</title>
<link>https://arxiv.org/abs/2503.17026</link>
<guid>https://arxiv.org/abs/2503.17026</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, social media, information circulation, information voids, Italian climate transition discourse

Summary: 
The study focuses on the dynamics of information circulation and the emergence of information voids in the context of the Italian climate-transition discourse. It highlights the importance of public understanding of climate issues and the role of social media platforms in disseminating information. The model proposed in the study takes into account the supply and demand of information on platforms like Facebook, Instagram, and GDELT, as well as Google searches to capture information demand. The findings reveal responsiveness and temporal coupling between supply and demand, especially during moments of heightened public attention due to significant external events. The study identifies an adaptive information ecosystem but also notes persistent information voids that could limit public understanding and hinder meaningful engagement with climate policy. <div>
arXiv:2503.17026v2 Announce Type: replace 
Abstract: Climate change is one of the most critical challenges of the twenty-first century. Public understanding of climate issues and of the goals regarding the climate transition is essential to translate awareness into concrete actions. In this context, social media platforms play a crucial role in disseminating information about climate change and climate policy. To better understand the dynamics of information circulation and the emergence of information voids we propose a model that takes into account the supply and demand of information related to the Italian climate-transition discourse. We conceptualise information supply as the production of content on Facebook, Instagram and GDELT (an online news database) while leveraging Google searches to capture information demand. Our findings highlight responsiveness and temporal coupling between supply and demand, particularly during moments of heightened public attention triggered by significant external events. These responsive interactions reveal an overall adaptive information ecosystem. However, we also observe persistent information voids which may limit public understanding and delay meaningful engagement.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image</title>
<link>https://arxiv.org/abs/2507.17755</link>
<guid>https://arxiv.org/abs/2507.17755</guid>
<content:encoded><![CDATA[
<div> Keywords: Douyin, WeChat, body image, male adolescents, social media <br />
Summary: <br />
This study explores the impact of two Chinese social media platforms, Douyin and WeChat, on body image perceptions among male adolescents. Through surveys with 395 participants, it was found that Douyin usage was significantly linked to appearance evaluation and body area satisfaction, while WeChat usage showed no significant correlation with any body image dimensions. The results suggest that Douyin's algorithm-driven, video-centric environment may contribute to heightened exposure to idealized body standards, influencing users at a cognitive level. The study emphasizes the need to consider platform-specific characteristics when examining social media's influence on body image. By highlighting how technological design and content modalities shape psychological outcomes, the research provides valuable insights for addressing body image concerns among male adolescents in China. <br /> <div>
arXiv:2507.17755v1 Announce Type: cross 
Abstract: In the digital era, social media platforms play a pivotal role in shaping adolescents' body image perceptions. This study examines how Douyin and WeChat, two contrasting Chinese social media platforms, influence body image among Chinese male adolescents. Employing a platformization perspective, we surveyed 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction. Our findings reveal that Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat usage shows no significant correlation with any body image dimensions. These results suggest that Douyin's algorithm-driven, video-centric environment intensifies exposure to idealized body standards, impacting users at a cognitive level. This study underscores the importance of considering platform-specific characteristics in understanding social media's impact on body image. It contributes to the broader discourse on how technological design and content modalities mediate psychological outcomes, offering insights for addressing body image concerns among male adolescents in China.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Uniqueness and Divisiveness of Presidential Discourse</title>
<link>https://arxiv.org/abs/2401.01405</link>
<guid>https://arxiv.org/abs/2401.01405</guid>
<content:encoded><![CDATA[
<div> Keywords: American presidents, speech patterns, divisive language, uniqueness, language models

Summary:
This study analyzes the speaking styles of American presidents using a unique metric based on language models. The research examines whether presidents speak differently from each other and in what ways, particularly focusing on Donald Trump's distinctiveness. The findings suggest that Trump's speech patterns diverge significantly from other major party nominees, with an emphasis on divisive and antagonistic language towards political opponents. These differences are consistent across various measurement strategies, occurring both in campaign speeches and official addresses. The study concludes that Trump's uniqueness in speech is not simply a result of changes in presidential communications over time. Overall, this research sheds light on the distinct ways in which presidents communicate and highlights Trump's particularly unique and divisive language compared to his predecessors.<br /><br />Summary: <div>
arXiv:2401.01405v2 Announce Type: replace-cross 
Abstract: Do American presidents speak discernibly different from each other? If so, in what ways? And are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for assessing the distinctive ways in which presidents speak about their political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Donald Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values appear closer to those of the Democrats. Contributing to these differences is Trump's employment of divisive and antagonistic language, particularly when targeting his political opponents. These differences hold across a variety of measurement strategies, arise on both the campaign trail and in official presidential addresses, and do not appear to be an artifact of secular changes in presidential communications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature</title>
<link>https://arxiv.org/abs/2507.16820</link>
<guid>https://arxiv.org/abs/2507.16820</guid>
<content:encoded><![CDATA[
<div> countries, institutions, authors, collaboration networks, topics<br />
<br />
Summary: 
Countries most impacted by COVID-19 were highly active in disaster informatics research, each with specific interests. Regional and language-based collaborations were common among countries and institutions. Top authors tended to partner closely with a few key collaborators, focused on specific topics, with institutions having diverse interests. The pandemic shifted research priorities towards public health in disaster informatics. The field is emphasizing multidimensional resilience strategies and data-sharing collaborations, reflecting global vulnerability awareness. Strategies, practices, and tools used in this study can be applied to similar datasets. The analysis provides insights for policymakers, practitioners, and scholars looking to enhance disaster informatics capacities in complex risk landscapes. <br /><br /> <div>
arXiv:2507.16820v1 Announce Type: new 
Abstract: This study presents a comprehensive bibliometric and topic analysis of the disaster informatics literature published between January 2020 to September 2022. Leveraging a large-scale corpus and advanced techniques such as pre-trained language models and generative AI, we identify the most active countries, institutions, authors, collaboration networks, emergent topics, patterns among the most significant topics, and shifts in research priorities spurred by the COVID-19 pandemic. Our findings highlight (1) countries that were most impacted by the COVID-19 pandemic were also among the most active, with each country having specific research interests, (2) countries and institutions within the same region or share a common language tend to collaborate, (3) top active authors tend to form close partnerships with one or two key partners, (4) authors typically specialized in one or two specific topics, while institutions had more diverse interests across several topics, and (5) the COVID-19 pandemic has influenced research priorities in disaster informatics, placing greater emphasis on public health. We further demonstrate that the field is converging on multidimensional resilience strategies and cross-sectoral data-sharing collaborations or projects, reflecting a heightened awareness of global vulnerability and interdependency. Collecting and quality assurance strategies, data analytic practices, LLM-based topic extraction and summarization approaches, and result visualization tools can be applied to comparable datasets or solve similar analytic problems. By mapping out the trends in disaster informatics, our analysis offers strategic insights for policymakers, practitioners, and scholars aiming to enhance disaster informatics capacities in an increasingly uncertain and complex risk landscape.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media</title>
<link>https://arxiv.org/abs/2507.16847</link>
<guid>https://arxiv.org/abs/2507.16847</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, user behavior, prediction, open-source models, cross-modal configurations

Summary: 
This study explores the evolution of user behavior on social media platforms over their lifetime using a novel approach. By leveraging open-source models and advanced language processing techniques like GPT-2, BERT, and RoBERTa, the research aims to predict future stages of user social evolution, including network changes, future connections, and shifts in activities. Experimental results show that GPT-2 outperforms other models in a Cross-modal configuration, emphasizing the importance of this approach for superior performance. The study addresses critical challenges in social media, such as friend recommendations and activity predictions, providing insights into user behavior trajectories. By anticipating future interactions and activities, the research aims to offer early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.<br /><br />Summary: <div>
arXiv:2507.16847v1 Announce Type: new 
Abstract: Social media platforms serve as a significant medium for sharing personal emotions, daily activities, and various life events, ensuring individuals stay informed about the latest developments. From the initiation of an account, users progressively expand their circle of friends or followers, engaging actively by posting, commenting, and sharing content. Over time, user behavior on these platforms evolves, influenced by demographic attributes and the networks they form. In this study, we present a novel approach that leverages open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint embedding technique, to analyze and predict the evolution of user behavior on social media over their lifetime. Our experiments demonstrate the potential of these models to forecast future stages of a user's social evolution, including network changes, future connections, and shifts in user activities. Experimental results highlight the effectiveness of our approach, with GPT-2 achieving the lowest perplexity (8.21) in a Cross-modal configuration, outperforming RoBERTa (9.11) and BERT, and underscoring the importance of leveraging Cross-modal configurations for superior performance. This approach addresses critical challenges in social media, such as friend recommendations and activity predictions, offering insights into the trajectory of user behavior. By anticipating future interactions and activities, this research aims to provide early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots</title>
<link>https://arxiv.org/abs/2507.16848</link>
<guid>https://arxiv.org/abs/2507.16848</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, disinformation dissemination, multi-agent framework, correction strategies, network modeling <br />
<br />
Summary: 
The study focuses on understanding the influence of social bots in spreading and correcting disinformation in the human-bot symbiotic information ecosystem. The proposed Multi Agent based framework for Disinformation Dissemination (MADD) aims to address the limitations of current studies by utilizing a more realistic propagation network model that integrates scale-free topology and community structures. By incorporating both malicious and legitimate bots with controlled dynamic participation, MADD enables quantitative analysis of correction strategies. The framework was evaluated using individual and group-level metrics, and experiments verified the consistency of user attributes and network structure with real-world data. The simulation of disinformation dissemination showcased the varying effects of fact-based and narrative-based correction strategies. MADD provides a valuable tool for better understanding and managing the impact of social bots in the dissemination of disinformation. <br /> <div>
arXiv:2507.16848v1 Announce Type: new 
Abstract: In the human-bot symbiotic information ecosystem, social bots play key roles in spreading and correcting disinformation. Understanding their influence is essential for risk control and better governance. However, current studies often rely on simplistic user and network modeling, overlook the dynamic behavior of bots, and lack quantitative evaluation of correction strategies. To fill these gaps, we propose MADD, a Multi Agent based framework for Disinformation Dissemination. MADD constructs a more realistic propagation network by integrating the Barabasi Albert Model for scale free topology and the Stochastic Block Model for community structures, while designing node attributes based on real world user data. Furthermore, MADD incorporates both malicious and legitimate bots, with their controlled dynamic participation allows for quantitative analysis of correction strategies. We evaluate MADD using individual and group level metrics. We experimentally verify the real world consistency of MADD user attributes and network structure, and we simulate the dissemination of six disinformation topics, demonstrating the differential effects of fact based and narrative based correction strategies.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Subreddit Behavior as Open-Source Indicators of Coordinated Influence: A Case Study of r/Sino &amp; r/China</title>
<link>https://arxiv.org/abs/2507.16857</link>
<guid>https://arxiv.org/abs/2507.16857</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated influence activity, Reddit communities, sentiment analysis, behavioral profiling, online influence behavior

Summary: 
This study examines indicators of coordinated influence activity in users of the Reddit communities r/Sino and r/China, which focus on Chinese political discourse. The research uses topic modeling and sentiment analysis to analyze posts from users who participate in both communities, creating a user-topic sentiment matrix. Behavioral profiling is conducted using various measures such as lexical diversity, language consistency, and posting frequency. Users with anomalous behavior are identified and examined within a subreddit co-participation network. The study integrates linguistic and behavioral analysis to identify patterns indicative of inauthentic or strategically structured participation. The findings showcase the importance of combining content and activity-based signals in analyzing online influence behavior, particularly within contested information environments. <div>
arXiv:2507.16857v1 Announce Type: new 
Abstract: This study investigates potential indicators of coordinated influence activity among users participating in both r/Sino and r/China, two ideologically divergent Reddit communities focused on Chinese political discourse. Topic modeling and sentiment analysis are applied to all posts and comments authored by dual-subreddit users to construct a user-topic sentiment matrix. Individual sentiment patterns are compared to global topic baselines derived from the broader r/Sino and r/China populations. Behavioral profiling is performed using full user activity histories and metadata, incorporating measures such as lexical diversity, language consistency, account age, posting frequency, and karma distribution. Users exhibiting multiple behavioral anomalies are identified and examined within a subreddit co-participation network to assess structural overlap. The combined linguistic and behavioral analysis enables the identification of patterns consistent with inauthentic or strategically structured participation. These findings demonstrate the utility of integrating content and activity-based signals in the analysis of online influence behavior within contested information environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky</title>
<link>https://arxiv.org/abs/2507.16858</link>
<guid>https://arxiv.org/abs/2507.16858</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, Congressional Democrats, social network analysis, decentralized platforms, political messaging

Summary:
In the wake of the 2024 U.S. presidential election, Democratic lawmakers shifted from mainstream social media to Bluesky, a decentralized platform. A study on 182 verified Democratic members of Congress analyzes their use of Bluesky to form influence networks and disseminate political messaging. Party leaders like Hakeem Jeffries and Elizabeth Warren are prominent, but lesser-known figures like Marcy Kaptur and Donald Beyer hold significant influence positions. Analysis reveals homophily based on ideology, state, and leadership lines, with Senate leaders less connected. Topic modeling uncovers shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad standing out. The study highlights how decentralized platforms reshape intra-party communication and emphasizes the importance of computational research on elite political behavior in digital environments. <br /><br />Summary: <div>
arXiv:2507.16858v1 Announce Type: new 
Abstract: Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and transformer-based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party communication dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs</title>
<link>https://arxiv.org/abs/2507.16860</link>
<guid>https://arxiv.org/abs/2507.16860</guid>
<content:encoded><![CDATA[
<div> Language Models, Fake Profile Detection, Adversarial Training, Robustness, GPT<br />
<br />
Summary: 
The study evaluates the robustness of existing fake profile detectors against Large Language Models (LLMs) like GPT. While current detectors are effective against manually created fake profiles, they struggle to detect LLM-generated profiles. The proposed solution involves GPT-assisted adversarial training, which reduces the False Accept Rate significantly without affecting the False Reject Rates. Ablation studies show that detectors utilizing combined numerical and textual embeddings perform the best. Analysis also highlights the importance of automated detectors over prompt-based language models like GPT-4Turbo and human evaluators for detecting fake profiles effectively. The study emphasizes the necessity to enhance fake profile detection mechanisms to combat the risk posed by LLMs in creating realistic fake profiles on platforms like LinkedIn.<br /> <div>
arXiv:2507.16860v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made it easier to create realistic fake profiles on platforms like LinkedIn. This poses a significant risk for text-based fake profile detectors. In this study, we evaluate the robustness of existing detectors against LLM-generated profiles. While highly effective in detecting manually created fake profiles (False Accept Rate: 6-7%), the existing detectors fail to identify GPT-generated profiles (False Accept Rate: 42-52%). We propose GPT-assisted adversarial training as a countermeasure, restoring the False Accept Rate to between 1-7% without impacting the False Reject Rates (0.5-2%). Ablation studies revealed that detectors trained on combined numerical and textual embeddings exhibit the highest robustness, followed by those using numerical-only embeddings, and lastly those using textual-only embeddings. Complementary analysis on the ability of prompt-based GPT-4Turbo and human evaluators affirms the need for robust automated detectors such as the one proposed in this study.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of temporal influence in polarised networks</title>
<link>https://arxiv.org/abs/2507.17177</link>
<guid>https://arxiv.org/abs/2507.17177</guid>
<content:encoded><![CDATA[
<div> influential users, social networks, polarised networks, temporal dynamics, community structure

Summary: 
This study investigates the dynamics of users' influence in polarised social networks, where information is often segregated within communities. It highlights the importance of identifying the most influential users across different communities and acknowledges that users' influence can change over time as opinions evolve. The research compares the stability of influence rankings using temporal centrality measures, considering community structure and network evolution behaviors. The study successfully groups nodes into influence bands and demonstrates how centrality scores can be aggregated to analyze community influence over time. The modified temporal independent cascade model and temporal degree centrality prove to be effective in isolating nodes into their respective influence bands, providing valuable insights into the changing dynamics of user influence in polarized social networks. <div>
arXiv:2507.17177v1 Announce Type: new 
Abstract: In social networks, it is often of interest to identify the most influential users who can successfully spread information to others. This is particularly important for marketing (e.g., targeting influencers for a marketing campaign) and to understand the dynamics of information diffusion (e.g., who is the most central user in the spreading of a certain type of information). However, different opinions often split the audience and make the network polarised. In polarised networks, information becomes soiled within communities in the network, and the most influential user within a network might not be the most influential across all communities. Additionally, influential users and their influence may change over time as users may change their opinion or choose to decrease or halt their engagement on the subject. In this work, we aim to study the temporal dynamics of users' influence in a polarised social network. We compare the stability of influence ranking using temporal centrality measures, while extending them to account for community structure across a number of network evolution behaviours. We show that we can successfully aggregate nodes into influence bands, and how to aggregate centrality scores to analyse the influence of communities over time. A modified version of the temporal independent cascade model and the temporal degree centrality perform the best in this setting, as they are able to reliably isolate nodes into their bands.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quotegraph: A Social Network Extracted from Millions of News Quotations</title>
<link>https://arxiv.org/abs/2507.17626</link>
<guid>https://arxiv.org/abs/2507.17626</guid>
<content:encoded><![CDATA[
<div> network, Quotegraph, social, news articles, computational social scientists  
Summary:  
Quotegraph is a new large-scale social network created from speaker-attributed quotations in English news articles from 2008 to 2020. It comprises 528 thousand unique nodes and 8.63 million directed edges, connecting speakers to the individuals they mention. Nodes are linked to Wikidata entries, providing detailed biographic entity information like nationality, gender, and political affiliation. Derived from Quotebank, Quotegraph includes context information, enriching its relations. The network construction process is language-agnostic, facilitating the creation of similar datasets from non-English news sources. Quotegraph offers valuable insights for computational social scientists, complementing online social networks and offering new perspectives on public figure behavior as portrayed in news media.  

Summary: <div>
arXiv:2507.17626v1 Announce Type: new 
Abstract: We introduce Quotegraph, a novel large-scale social network derived from speaker-attributed quotations in English news articles published between 2008 and 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million directed edges, pointing from speakers to persons they mention. The nodes are linked to their corresponding items in Wikidata, thereby endowing the dataset with detailed biographic entity information, including nationality, gender, and political affiliation. Being derived from Quotebank, a massive corpus of quotations, relations in Quotegraph are additionally enriched with the information about the context in which they are featured. Each part of the network construction pipeline is language agnostic, enabling the construction of similar datasets based on non-English news corpora. We believe Quotegraph is a compelling resource for computational social scientists, complementary to online social networks, with the potential to yield novel insights into the behavior of public figures and how it is captured in the news.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic First-Order Logic Queries in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.17215</link>
<guid>https://arxiv.org/abs/2507.17215</guid>
<content:encoded><![CDATA[
<div> Thresholded First Order Logic, Motif Analysis, Temporal Networks, Algorithm, Triadic Queries<br />
<br />Summary:
In the study of motif counting in network analysis, the concept of thresholded First Order Logic (FOL) Motif Analysis has been introduced for massive temporal networks. This approach combines ideas from logic and database theory to extract richer information from networks by incorporating temporal constraints into motif queries. The FOLTY algorithm was developed to efficiently mine thresholded FOL triadic queries, with theoretical running time matching the best known for temporal triangle counting in sparse graphs. The algorithm's implementation using specialized temporal data structures allows for fast processing, enabling the analysis of networks with millions of edges in a short time frame on standard hardware. This work has the potential to open up new avenues for research in the established field of motif analysis. <div>
arXiv:2507.17215v1 Announce Type: cross 
Abstract: Motif counting is a fundamental problem in network analysis, and there is a rich literature of theoretical and applied algorithms for this problem. Given a large input network $G$, a motif $H$ is a small "pattern" graph indicative of special local structure. Motif/pattern mining involves finding all matches of this pattern in the input $G$. The simplest, yet challenging, case of motif counting is when $H$ has three vertices, often called a "triadic" query. Recent work has focused on "temporal graph mining", where the network $G$ has edges with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of "thresholded First Order Logic (FOL) Motif Analysis" for massive temporal networks. A typical triadic motif query asks for the existence of three vertices that form a desired temporal pattern. An "FOL" motif query is obtained by having both existential and thresholded universal quantifiers. This allows for query semantics that can mine richer information from networks. A typical triadic query would be "find all triples of vertices $u,v,w$ such that they form a triangle within one hour". A thresholded FOL query can express "find all pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$ also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic queries. The theoretical running time of FOLTY matches the best known running time for temporal triangle counting in sparse graphs. We give an efficient implementation of FOLTY using specialized temporal data structures. FOLTY has excellent empirical behavior, and can answer triadic FOL queries on graphs with nearly 70M edges is less than hour on commodity hardware. Our work has the potential to start a new research direction in the classic well-studied problem of motif analysis.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors</title>
<link>https://arxiv.org/abs/2412.20936</link>
<guid>https://arxiv.org/abs/2412.20936</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence maximization, temporal social networks, dynamic interactions, active-inactive transitions, seed selection

Summary:<br />
Influence maximization in temporal social networks is challenging due to dynamic interactions and active-inactive transitions among nodes. The Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R) addresses this by incorporating active-inactive transitions and node reinforcement, improving influence spread understanding. This model leads to a submodular and monotone objective function, allowing for efficient seed selection optimization. A temporal snapshot sampling method simplifies network analysis, enhancing seed selection efficiency. Adapting prior seed selection algorithms to the cpSI-R model and sampling strategy leads to reduced computational costs and improved performance. Experimental evaluations on various datasets show significant performance enhancements over baseline methods, proving the effectiveness of cpSI-R for real-world temporal networks.<br /><br />Summary: <div>
arXiv:2412.20936v2 Announce Type: replace 
Abstract: Influence maximization in temporal social networks presents unique challenges due to the dynamic interactions that evolve over time. Traditional diffusion models often fall short in capturing the real-world complexities of active-inactive transitions among nodes, obscuring the true behavior of influence spread. In dynamic networks, nodes do not simply transition to an active state once; rather, they can oscillate between active and inactive states, with the potential for reactivation and reinforcement over time. This reactivation allows previously influenced nodes to regain influence potency, enhancing their ability to spread influence to others and amplifying the overall diffusion process. Ignoring these transitions can thus conceal the cumulative impact of influence, making it essential to account for them in any effective diffusion model. To address these challenges, we introduce the Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R), which explicitly incorporates active-inactive transitions, capturing the progressive reinforcement that makes nodes more potent spreaders upon reactivation. This model naturally leads to a submodular and monotone objective function, which supports efficient optimization for seed selection in influence maximization tasks. Alongside cpSI-R, we propose an efficient temporal snapshot sampling method, simplifying the analysis of evolving networks. We then adapt the prior algorithms of seed selection to our model and sampling strategy, resulting in reduced computational costs and enhanced seed selection efficiency. Experimental evaluations on diverse datasets demonstrate substantial improvements in performance over baseline methods, underscoring the effectiveness of cpSI-R for real-world temporal networks
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Matching in Correlated Networks with Node Attributes for Improved Community Recovery</title>
<link>https://arxiv.org/abs/2501.02851</link>
<guid>https://arxiv.org/abs/2501.02851</guid>
<content:encoded><![CDATA[
<div> community detection, multiple networks, correlated node attributes, Stochastic Block Model, Gaussian Mixture Models

Summary:
The study investigates community detection in multiple networks with correlated node attributes and edges, such as social platforms. They introduce the correlated Contextual Stochastic Block Model (CSBM) to incorporate both structural and attribute correlations across graphs. By analyzing correlated Gaussian Mixture Models, they establish conditions for exact node matching using attribute distances. Their two-step algorithm combines edge information and attribute distances through $k$-core matching to accurately identify node correspondence and merge correlated edges for improved community detection. The research shows how incorporating side information from correlated graphs can make community detection feasible even in cases where it may be impossible in a single graph. This approach leverages the interplay between graph matching and community recovery to enhance performance in attribute-based community detection across multiple networks. <br /><br />Summary: <div>
arXiv:2501.02851v2 Announce Type: replace 
Abstract: We study community detection in multiple networks with jointly correlated node attributes and edges. This setting arises naturally in applications such as social platforms, where a shared set of users may exhibit both correlated friendship patterns and correlated attributes across different platforms. Extending the classical Stochastic Block Model (SBM) and its contextual counterpart (Contextual SBM or CSBM), we introduce the correlated CSBM, which incorporates structural and attribute correlations across graphs. To build intuition, we first analyze correlated Gaussian Mixture Models, wherein only correlated node attributes are available without edges, and identify the conditions under which an estimator minimizing the distance between attributes achieves exact matching of nodes across the two databases. For the correlated CSBMs, we develop a two-step procedure that first applies $k$-core matching to most nodes using edge information, then refines the matching for the remaining unmatched nodes by leveraging their attributes with a distance-based estimator. We identify the conditions under which the algorithm recovers the exact node correspondence, enabling us to merge the correlated edges and average the correlated attributes for enhanced community detection. Crucially, by aligning and combining graphs, we identify regimes in which community detection is impossible in a single graph but becomes feasible when side information from correlated graphs is incorporated. Our results illustrate how the interplay between graph matching and community recovery can boost performance, broadening the scope of multi-graph, attribute-based community detection.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, image memorability, virality, neural network, online engagement <br />
<br />
Summary: <br />
This study explores the connection between image memorability and social media virality. By analyzing over 1,200 Reddit image posts, the researchers found that memorable images tend to attract more engagement, specifically in the form of comments. They used a neural network called ResMem to predict image memorability and correlated these scores with virality metrics. Memorable images were linked to neutral-affect comments, indicating a unique pathway to virality compared to emotion-driven content. The study also revealed that visually consistent, memorable posts prompted a variety of externally-associated comments. Analysis of ResMem's layers emphasized the importance of semantic distinctiveness in both memorability and virality, even when accounting for image categories. Overall, the findings suggest that image memorability can be a valuable predictor of online engagement on social media platforms. <br /> <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief Alignment vs Opinion Leadership: Understanding Cross-linguistic Digital Activism in K-pop and BLM Communities</title>
<link>https://arxiv.org/abs/2507.16046</link>
<guid>https://arxiv.org/abs/2507.16046</guid>
<content:encoded><![CDATA[
<div> activism, internet, K-pop fans, Black Lives Matter, belief alignment
Summary: 
- The study explores the motivations behind cross-cultural activism, focusing on the interaction between K-pop fans and the Black Lives Matter movement on Twitter after George Floyd's murder.
- It suggests that belief alignment, where individuals resonate with shared beliefs, drives cross-cultural interactions in digital activism.
- The study indicates that influential online opinion leaders, such as K-pop entertainers, may amplify activism through their actions, but are not the direct cause of activism.
- The findings show a slight increase in belief similarity between BLM and K-pop fans following their interaction on Twitter.
- Overall, the research highlights the importance of belief resonance in driving global movements, transcending geographical boundaries through online platforms. 
<br /><br />Summary: <div>
arXiv:2507.16046v1 Announce Type: new 
Abstract: The internet has transformed activism, giving rise to more organic, diverse, and dynamic social movements that transcend geo-political boundaries. Despite extensive research on the role of social media and the internet in cross-cultural activism, the fundamental motivations driving these global movements remain poorly understood. This study examines two plausible explanations for cross-cultural activism: first, that it is driven by influential online opinion leaders, and second, that it results from individuals resonating with emergent sets of beliefs, values, and norms. We conduct a case study of the interaction between K-pop fans and the Black Lives Matter (BLM) movement on Twitter following the murder of George Floyd. Our findings provide strong evidence that belief alignment, where people resonate with common beliefs, is a primary driver of cross-cultural interactions in digital activism. We also demonstrate that while the actions of potential opinion leaders--in this case, K-pop entertainers--may amplify activism and lead to further expressions of love and admiration from fans, they do not appear to be a direct cause of activism. Finally, we report some initial evidence that the interaction between BLM and K-pop led to slight increases in their overall belief similarity.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections</title>
<link>https://arxiv.org/abs/2507.16298</link>
<guid>https://arxiv.org/abs/2507.16298</guid>
<content:encoded><![CDATA[
<div> WhatsApp tiplines, misinformation, fact-checkers, Indian assembly elections, content analysis

Summary:
The study examines WhatsApp tiplines used during the 2021 Indian assembly elections to combat misinformation. It analyzes 580 claims from 451 users in English, Hindi, and Telugu, categorizing them into election, COVID-19, and other topics. Similarities in claims are found across languages, with some users submitting tips in multiple languages. Fact-checkers take a couple of days on average to debunk claims. Users do not submit claims to multiple fact-checking organizations, indicating unique audiences. Practical recommendations for using tiplines during elections with ethical considerations for users' information are provided. <div>
arXiv:2507.16298v1 Announce Type: new 
Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable users to interact with fact-checkers to verify misleading content. This study analyzes 580 unique claims (tips) from 451 users, covering both high-resource languages (English, Hindi) and a low-resource language (Telugu) during the 2021 Indian assembly elections using a mixed-method approach. We categorize the claims into three categories, election, COVID-19, and others, and observe variations across languages. We compare content similarity through frequent word analysis and clustering of neural sentence embeddings. We also investigate user overlap across languages and fact-checking organizations. We measure the average time required to debunk claims and inform tipline users. Results reveal similarities in claims across languages, with some users submitting tips in multiple languages to the same fact-checkers. Fact-checkers generally require a couple of days to debunk a new claim and share the results with users. Notably, no user submits claims to multiple fact-checking organizations, indicating that each organization maintains a unique audience. We provide practical recommendations for using tiplines during elections with ethical consideration of users' information.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SASH: Decoding Community Structure in Graphs</title>
<link>https://arxiv.org/abs/2507.16583</link>
<guid>https://arxiv.org/abs/2507.16583</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Graph, Error correcting codes, Simulations
Summary:
The paper introduces an encoding method for community structure in graphs and presents a decoding algorithm called SASH to estimate communities from observed data. The problem of detecting communities in a graph is important and has various applications. The approach views a graph as a noisy version of the underlying communities and utilizes error correcting codes. SASH is demonstrated through simulations on an assortative planted partition model and Zachary's Karate Club dataset, showing its performance in identifying densely connected clusters of vertices. Overall, the study provides a novel perspective on community detection in graphs and offers a promising algorithm for accurately decoding community structures from noisy data.<br /><br />Summary: <div>
arXiv:2507.16583v1 Announce Type: new 
Abstract: Detection of communities in a graph entails identifying clusters of densely connected vertices; the area has a variety of important applications and a rich literature. The problem has previously been situated in the realm of error correcting codes by viewing a graph as a noisy version of the assumed underlying communities. In this paper, we introduce an encoding of community structure along with the resulting code's parameters. We then present a novel algorithm, SASH, to decode to estimated communities given an observed dataset. We demonstrate the performance of SASH via simulations on an assortative planted partition model and on the Zachary's Karate Club dataset.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Data-centric Overview of Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.16541</link>
<guid>https://arxiv.org/abs/2507.16541</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Graph Learning, Data Characteristics, Data Utilization, Pretrained Large Models, Realistic Applications

Summary: 
Federated Graph Learning (FGL) is a solution for reconciling decentralized datasets while preserving sensitive information. This survey introduces a two-level data-centric taxonomy for FGL research, focusing on Data Characteristics and Data Utilization. Data Characteristics categorize studies based on dataset properties, while Data Utilization analyzes training procedures to overcome data-centric challenges. The survey also examines FGL integration with Pretrained Large Models, showcases realistic applications, and suggests future research directions aligned with Graph Machine Learning trends. Ultimately, this comprehensive review aims to organize FGL research around data-centric constraints to enhance model performance. 

<br /><br />Summary: <div>
arXiv:2507.16541v1 Announce Type: cross 
Abstract: In the era of big data applications, Federated Graph Learning (FGL) has emerged as a prominent solution that reconcile the tradeoff between optimizing the collective intelligence between decentralized datasets holders and preserving sensitive information to maximum. Existing FGL surveys have contributed meaningfully but largely focus on integrating Federated Learning (FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that emphasis on methodology and simulated scenarios. Notably, a data centric perspective, which systematically examines FGL methods through the lens of data properties and usage, remains unadapted to reorganize FGL research, yet it is critical to assess how FGL studies manage to tackle data centric constraints to enhance model performances. This survey propose a two-level data centric taxonomy: Data Characteristics, which categorizes studies based on the structural and distributional properties of datasets used in FGL, and Data Utilization, which analyzes the training procedures and techniques employed to overcome key data centric challenges. Each taxonomy level is defined by three orthogonal criteria, each representing a distinct data centric configuration. Beyond taxonomy, this survey examines FGL integration with Pretrained Large Models, showcases realistic applications, and highlights future direction aligned with emerging trends in GML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Analytics for Anti-Money Laundering -- A Systematic Literature Review and Experimental Evaluation</title>
<link>https://arxiv.org/abs/2405.19383</link>
<guid>https://arxiv.org/abs/2405.19383</guid>
<content:encoded><![CDATA[
<div> Keywords: money laundering, network analytics, anti-money laundering, deep learning, fraud analytics

Summary: 
This study reviews existing research on using network analytics for anti-money laundering (AML) to combat the pervasive issue of money laundering funding illegal activities. The literature review covers 97 papers and presents a taxonomy following a fraud analytics framework. The research found that while most studies rely on expert-based rules and manual features, deep learning methods are gaining popularity. A comprehensive framework for evaluating and comparing the performance of different methods was also developed. The study compared manual feature engineering, random walk-based, and deep learning methods on two public datasets, concluding that network analytics enhances predictive power but caution is needed in applying Graph Neural Networks (GNNs) due to class imbalance and network topology issues. The study warns against relying on synthetic data for overly optimistic results. An open-source implementation is provided to facilitate further research and standardize the analysis and evaluation of network analytics for AML. 

<br /><br />Summary: <div>
arXiv:2405.19383v4 Announce Type: replace 
Abstract: Money laundering presents a pervasive challenge, burdening society by financing illegal activities. The use of network information is increasingly being explored to effectively combat money laundering, given it involves connected parties. This led to a surge in research on network analytics for anti-money laundering (AML). The literature is, however, fragmented and a comprehensive overview of existing work is missing. This results in limited understanding of the methods to apply and their comparative detection power. This paper presents an extensive and unique literature review, based on 97 papers from Web of Science and Scopus, resulting in a taxonomy following a recently proposed fraud analytics framework. We conclude that most research relies on expert-based rules and manual features, while deep learning methods have been gaining traction. This paper also presents a comprehensive framework to evaluate and compare the performance of prominent methods in a standardized setup. We compare manual feature engineering, random walk-based, and deep learning methods on two publicly available data sets. We conclude that (1) network analytics increases the predictive power, but caution is needed when applying GNNs in the face of class imbalance and network topology, and that (2) care should be taken with synthetic data as this can give overly optimistic results. The open-source implementation facilitates researchers and practitioners to extend this work on proprietary data, promoting a standardised approach for the analysis and evaluation of network analytics for AML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why We Experience Society Differently: Intrinsic Dispositions as Drivers of Ideological Complexity in Adaptive Social Networks</title>
<link>https://arxiv.org/abs/2504.07848</link>
<guid>https://arxiv.org/abs/2504.07848</guid>
<content:encoded><![CDATA[
<div> behavioral tendencies, opinion dynamics, complexity, social networks, inequality

Summary:
- The study focuses on understanding how inequality emerges in complex systems by considering both structural dynamics and intrinsic heterogeneity in opinion dynamics.
- Traditional models overlook how diverse cognitive dispositions influence belief evolution, instead assuming homogeneous agent behavior.
- An adaptive social network model is analyzed, where agents exhibit one of three behavioral tendencies - homophily, neophily, or social conformity.
- The study measures individual opinion trajectories using normalized Lempel-Ziv complexity and finds counterintuitive patterns such as homophilic agents becoming unpredictable, neophilic agents stabilizing, and conformic agents showing a U-shaped trajectory.
- The results show that internal behavioral dispositions primarily govern long-term opinion unpredictability rather than external environmental factors.
- Individuals' experiences of ideological volatility or stability are self-structured through their own cognitive tendencies, leading to persistent disparities in dynamical experience within social systems. <div>
arXiv:2504.07848v2 Announce Type: replace 
Abstract: Understanding the emergence of inequality in complex systems requires attention to both structural dynamics and intrinsic heterogeneity. In the context of opinion dynamics, traditional models relied on static snapshots or assumed homogeneous agent behavior, overlooking how diverse cognitive dispositions shape belief evolution. While some recent models introduce behavioral heterogeneity, they typically focus on macro-level patterns, neglecting the unequal and individualized dynamics that unfold at the agent level. In this study, we analyze an adaptive social network model where each agent exhibits one of three behavioral tendencies-homophily, neophily (attention to novelty), or social conformity-and measure the complexity of individual opinion trajectories using normalized Lempel-Ziv (nLZ) complexity. We find that the resulting dynamics are often counterintuitive-homophilic agents, despite seeking similarity, become increasingly unpredictable; neophilic agents, despite pursuing novelty, stabilize; and conformic agents follow a U-shaped trajectory, transitioning from early stability to later unpredictability. More fundamentally, these patterns remain robust across diverse network settings, showing that internal behavioral dispositions - not external environment - primarily govern long-term opinion unpredictability. The broader implication is that individuals' experiences of ideological volatility, uncertainty, or stability are not merely environmental, but endogenously self-structured through their own cognitive tendencies. These results establish a novel individual-level lens on opinion dynamics, where the behavioral identity of agents serves as a dynamical fingerprint in the evolution of belief systems, and gives rise to persistent disparities in dynamical experience within self-organizing social systems, even in structurally similar environments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study</title>
<link>https://arxiv.org/abs/2502.02451</link>
<guid>https://arxiv.org/abs/2502.02451</guid>
<content:encoded><![CDATA[
<div> machine translation, local lexicon, multilingual language models, large language models, moral foundations

Summary:<br />
- The study examines computational methods for analyzing moral foundations (MFs) in non-English texts, using Chinese as a test case.
- Machine translation and local lexicons are not effective for capturing cultural nuances in moral assessments.
- Multilingual language models and large language models (LLMs) show promise in cross-language MF measurement with transfer learning.
- LLMs are particularly efficient in data usage for this task.
- It is crucial to have human validation in automated MF assessment to ensure cultural nuances are not overlooked.
- The study emphasizes the potential of LLMs for cross-language MF measurements and other complex multilingual coding tasks. 

Summary: <div>
arXiv:2502.02451v3 Announce Type: replace-cross 
Abstract: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discipline and Resistance: The Construction of a Digital Home for TikTok Refugees on Xiaohongshu</title>
<link>https://arxiv.org/abs/2507.14465</link>
<guid>https://arxiv.org/abs/2507.14465</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, Xiaohongshu, heterotopia, cross-cultural discourse, digital migration

Summary: 
This study explores the migration of TikTok users to Xiaohongshu following a potential ban of TikTok in the US, utilizing Foucault's concept of heterotopia to analyze the platform as a crisis space for cross-cultural discussions. Through Critical Discourse Analysis of 586 user comments, the study uncovers how both Chinese and international users collaborated in constructing and challenging a new online order through language negotiation, identity positioning, and playful platform policing. The research highlights unique discursive tactics employed by domestic and overseas users, showcasing a blend of cultural resistance and adaptation. By shedding light on digital migration, heterotopic spaces in social media, and evolving dynamics of cross-cultural discourse amid geopolitical turmoil, this study offers valuable insights into the complexities of online interaction in a globalized world. <div>
arXiv:2507.14465v1 Announce Type: new 
Abstract: This study examines how TikTok refugees moved to Xiaohongshu after TikTok was about to be banned in the United States. It utilizes Foucault's idea of heterotopia to demonstrate how Xiaohongshu became a crisis space for cross-cultural discussions across the Great Firewall. Through Critical Discourse Analysis of 586 user comments, the study reveals how Chinese and international users collaboratively constructed and contested a new online order through language negotiation, identity positioning, and playful platform policing. The findings highlight distinct discursive strategies between domestic and overseas users, reflecting both cultural resistance and adaptation. This research contributes to the understanding of digital migration, heterotopic spaces in social media, and emerging dynamics of cross-cultural discourse during geopolitical crises.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote</title>
<link>https://arxiv.org/abs/2507.14623</link>
<guid>https://arxiv.org/abs/2507.14623</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, cross-cultural interactions, social media, Chinese users, TikTok Refugees  

Summary:  
- This study explores cross-cultural interactions between Chinese users and "TikTok Refugees" on RedNote after TikTok's U.S. ban, using sentiment analysis and topic modeling.  
- Analysis of 1,862 posts and 403,054 comments reveals emotional asymmetry with Chinese users showing pride and praise in cultural discussions, while political topics evoke contempt and anger, particularly from Pro-China users.  
- Pro-Foreign users exhibit strong negative emotions across all topics, while neutral users express curiosity and joy within mainstream norms.  
- Appearance-related content fosters emotionally balanced interactions, while political discussions lead to high polarization.  
- The findings highlight distinct emotion-stance structures in Sino-foreign online interactions, shedding light on identity negotiation in transnational digital publics.  

<br /><br /> <div>
arXiv:2507.14623v1 Announce Type: new 
Abstract: This study examines cross-cultural interactions between Chinese users and self-identified "TikTok Refugees"(foreign users who migrated to RedNote after TikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we use large language model-based sentiment classification and BERT-based topic modelling to explore how both groups engage with the TikTok refugee phenomenon. We analyse what themes foreign users express, how Chinese users respond, how stances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how affective responses differ across topics and identities. Results show strong affective asymmetry: Chinese users respond with varying emotional intensities across topics and stances: pride and praise dominate cultural threads, while political discussions elicit high levels of contempt and anger, especially from Pro-China commenters. Pro-Foreign users exhibit the strongest negative emotions across all topics, whereas neutral users express curiosity and joy but still reinforce mainstream discursive norms. Cross-topic comparisons reveal that appearance-related content produces the most emotionally balanced interactions, while politics generates the highest polarization. Our findings reveal distinct emotion-stance structures in Sino-foreign online interactions and offer empirical insights into identity negotiation in transnational digital publics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Faculty Placement from Patterns in Co-authorship Networks</title>
<link>https://arxiv.org/abs/2507.14696</link>
<guid>https://arxiv.org/abs/2507.14696</guid>
<content:encoded><![CDATA[
<div> faculty hiring, academic placement, co-authorship networks, institutional prestige, predictive accuracy

Summary:
- Faculty hiring plays a crucial role in shaping academic progress and individual career paths.
- Traditional indicators like doctoral department prestige and publication record are not sufficient predictors of faculty placement outcomes.
- The study introduces a novel approach by considering faculty placement as an individual-level prediction task.
- Analysis of temporal co-authorship networks reveals a significant improvement in predictive accuracy, especially for placements at top-tier departments.
- The findings highlight the importance of social networks, professional endorsements, and implicit advocacy in faculty hiring decisions, beyond traditional measures of productivity and prestige.

<br /><br />Summary: <div>
arXiv:2507.14696v1 Announce Type: new 
Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in academia, influencing not only individual career trajectories but also broader patterns of institutional prestige and scientific progress. While traditional studies have found strong correlations between faculty hiring and attributes such as doctoral department prestige and publication record, they rarely assess whether these associations generalize to individual hiring outcomes, particularly for future candidates outside the original sample. Here, we consider faculty placement as an individual-level prediction task. Our data consist of temporal co-authorship networks with conventional attributes such as doctoral department prestige and bibliometric features. We observe that using the co-authorship network significantly improves predictive accuracy by up to 10% over traditional indicators alone, with the largest gains observed for placements at the most elite (top-10) departments. Our results underscore the role that social networks, professional endorsements, and implicit advocacy play in faculty hiring beyond traditional measures of scholarly productivity and institutional prestige. By introducing a predictive framing of faculty placement and establishing the benefit of considering co-authorship networks, this work provides a new lens for understanding structural biases in academia that could inform targeted interventions aimed at increasing transparency, fairness, and equity in academic hiring practices.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Algorithms for Relevant Quantities of Friedkin-Johnsen Opinion Dynamics Model</title>
<link>https://arxiv.org/abs/2507.14864</link>
<guid>https://arxiv.org/abs/2507.14864</guid>
<content:encoded><![CDATA[
<div> Keywords: Online social networks, Friedkin-Johnsen model, equilibrium opinion vector, polarization, disagreement

Summary: 
The paper introduces a computational approach for efficiently determining the equilibrium opinion vector, polarization, and disagreement in online social networks. The Friedkin-Johnsen model is utilized as the basis for modeling opinion formation dynamics in both directed and undirected networks. A deterministic local algorithm with relative error guarantees is proposed, capable of scaling to networks with over ten million nodes. Integration with successive over-relaxation techniques further accelerates the computation process by optimizing convergence rates. Extensive experiments conducted on real-world networks demonstrate the practical effectiveness of the proposed approaches, showcasing significant improvements in computational efficiency and scalability compared to traditional methods. The strategies developed in this work have the potential to enhance our understanding of opinion dynamics in complex social networks and contribute to the development of more effective strategies for managing and analyzing online discourse. 

<br /><br />Summary: <div>
arXiv:2507.14864v1 Announce Type: new 
Abstract: Online social networks have become an integral part of modern society, profoundly influencing how individuals form and exchange opinions across diverse domains ranging from politics to public health. The Friedkin-Johnsen model serves as a foundational framework for modeling opinion formation dynamics in such networks. In this paper, we address the computational task of efficiently determining the equilibrium opinion vector and associated metrics including polarization and disagreement, applicable to both directed and undirected social networks. We propose a deterministic local algorithm with relative error guarantees, scaling to networks exceeding ten million nodes. Further acceleration is achieved through integration with successive over-relaxation techniques, where a relaxation factor optimizes convergence rates. Extensive experiments on diverse real-world networks validate the practical effectiveness of our approaches, demonstrating significant improvements in computational efficiency and scalability compared to conventional methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Privacy Risk Assessment in Social Networks Using User Attributes Social Graphs and Text Analysis</title>
<link>https://arxiv.org/abs/2507.15124</link>
<guid>https://arxiv.org/abs/2507.15124</guid>
<content:encoded><![CDATA[
<div> framework, privacy risk, social networking, user attributes, content 

Summary:
The Comprehensive Privacy Risk Scoring (CPRS) framework quantifies privacy risk by combining user attributes, social graph structures, and user-generated content. It calculates risk scores based on sensitivity, visibility, structural similarity, and entity-level analysis, culminating in a unified risk score. Validation on real-world datasets showed an average CPRS of 0.478, with graph-based risks posing a higher threat than content or profile attributes. Attributes such as email, date of birth, and mobile number were identified as high-risk factors. A user study demonstrated that 85% found the CPRS dashboard clear and actionable, affirming its practicality. This framework offers personalized privacy risk insights and a comprehensive approach to privacy management, with future plans to incorporate temporal dynamics and multimodal content for wider applicability.<br /><br />Summary: <div>
arXiv:2507.15124v1 Announce Type: new 
Abstract: The rise of social networking platforms has amplified privacy threats as users increasingly share sensitive information across profiles, content, and social connections. We present a Comprehensive Privacy Risk Scoring (CPRS) framework that quantifies privacy risk by integrating user attributes, social graph structures, and user-generated content. Our framework computes risk scores across these dimensions using sensitivity, visibility, structural similarity, and entity-level analysis, then aggregates them into a unified risk score. We validate CPRS on two real-world datasets: the SNAP Facebook Ego Network (4,039 users) and the Koo microblogging dataset (1M posts, 1M comments). The average CPRS is 0.478 with equal weighting, rising to 0.501 in graph-sensitive scenarios. Component-wise, graph-based risks (mean 0.52) surpass content (0.48) and profile attributes (0.45). High-risk attributes include email, date of birth, and mobile number. Our user study with 100 participants shows 85% rated the dashboard as clear and actionable, confirming CPRS's practical utility. This work enables personalized privacy risk insights and contributes a holistic, scalable methodology for privacy management. Future directions include incorporating temporal dynamics and multimodal content for broader applicability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Multimodal News Recommendation through Federated Learning</title>
<link>https://arxiv.org/abs/2507.15460</link>
<guid>https://arxiv.org/abs/2507.15460</guid>
<content:encoded><![CDATA[
<div> Multimodal, federated learning, news recommendation, privacy preservation, user interests <br />
<br />
Summary: This paper introduces a novel multimodal federated learning-based approach for personalized news recommendation. It addresses challenges faced by traditional systems by integrating textual and visual features of news items, balancing users' long-term and short-term interests, and enhancing privacy through a federated learning framework. The approach combines a multimodal model for more comprehensive content representation with a time-aware model using multi-head self-attention networks to improve recommendation accuracy. The federated learning framework divides the recommendation model into a server-maintained news model and a user model shared between the server and clients, ensuring collaborative model training without sharing user data. Additionally, a secure aggregation algorithm based on Shamir's secret sharing is employed to further safeguard user privacy. Experimental results on a real-world news dataset demonstrate significantly improved performance compared to existing systems, making it a substantial advancement in privacy-preserving personalized news recommendation. <br /> <div>
arXiv:2507.15460v1 Announce Type: new 
Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to information overload by predicting and suggesting news items tailored to individual user interests. However, traditional PNR systems face several challenges, including an overreliance on textual content, common neglect of short-term user interests, and significant privacy concerns due to centralized data storage. This paper addresses these issues by introducing a novel multimodal federated learning-based approach for news recommendation. First, it integrates both textual and visual features of news items using a multimodal model, enabling a more comprehensive representation of content. Second, it employs a time-aware model that balances users' long-term and short-term interests through multi-head self-attention networks, improving recommendation accuracy. Finally, to enhance privacy, a federated learning framework is implemented, enabling collaborative model training without sharing user data. The framework divides the recommendation model into a large server-maintained news model and a lightweight user model shared between the server and clients. The client requests news representations (vectors) and a user model from the central server, then computes gradients with user local data, and finally sends their locally computed gradients to the server for aggregation. The central server aggregates gradients to update the global user model and news model. The updated news model is further used to infer news representation by the server. To further safeguard user privacy, a secure aggregation algorithm based on Shamir's secret sharing is employed. Experiments on a real-world news dataset demonstrate strong performance compared to existing systems, representing a significant advancement in privacy-preserving personalized news recommendation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apology of Green Digitalization in the Context of Information and Climate Feedback Theory</title>
<link>https://arxiv.org/abs/2507.14162</link>
<guid>https://arxiv.org/abs/2507.14162</guid>
<content:encoded><![CDATA[
<div> digitalization, energy consumption, climate feedback, sustainable digitalization, Green Digital Accord
Summary:
The paper introduces the theory of information and climate feedback (ICF), which examines the impact of digitalization on the environment. It highlights the overlooked influence of information and communication technologies on the biosphere's thermal and energy balance. The ICF model, represented by differential equations, explores the interconnected relationship between digitalization, energy consumption, thermal footprint, climatic response, and infrastructure vulnerability. Through numerical analysis and thermal mapping, critical scenarios like digital overheating and infrastructural collapse are identified. The paper concludes with a call for the Green Digital Accord, an international agreement promoting sustainable digitalization. This interdisciplinary work combines climatology, information technologies, and political economy to address the environmental challenges posed by accelerating digitalization. 
<br /><br />Summary: <div>
arXiv:2507.14162v1 Announce Type: cross 
Abstract: Amid accelerated digitalization, not only is the scale of data processing and storage increasing, but so too is the associated infrastructure load on the climate. Current climate models and environmental protocols almost entirely overlook the impact of information and communication technologies on the thermal and energy balance of the biosphere.
  This paper proposes the theory of information and climate feedback (ICF) as a new nonlinear model describing the loop of digitalization, energy consumption, the thermal footprint, the climatic response, and the vulnerability of digital infrastructure. The system is formalized via differential equations with delays and parameters of sensitivity, greenness, and phase stability.
  A multiscenario numerical analysis, phase reconstructions, and thermal cartography were conducted. Critical regimes, including digital overheating, fluctuational instability, and infrastructural collapse in the absence of adaptive measures, were identified.
  The paper concludes with the proposal of an international agreement titled the Green Digital Accord and a set of metrics for sustainable digitalization. This work integrates climatology, information technologies, and the political economy of sustainability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module</title>
<link>https://arxiv.org/abs/2507.14612</link>
<guid>https://arxiv.org/abs/2507.14612</guid>
<content:encoded><![CDATA[
<div> keywords: POI recommendation, time information, graph representation, weighting factors, performance improvement
<br />
Summary: 
The paper proposes a novel framework, GDPW, for next point of interest (POI) recommendation that considers POI category information and multiple weighting factors. The framework utilizes global category and category-time graphs to learn category and time representations, disentangling them through contrastive learning. It incorporates weighting information such as POI popularity, transition relationships, and distances between POIs to improve prediction accuracy. By jointly considering POI categories and time information, GDPW addresses existing limitations in capturing user tendencies and time continuity. Experimental results on real-world datasets show that GDPW outperforms other models, achieving a performance improvement of 3% to 11%. <div>
arXiv:2507.14612v1 Announce Type: cross 
Abstract: Next point of interest (POI) recommendation primarily predicts future activities based on users' past check-in data and current status, providing significant value to users and service providers. We observed that the popular check-in times for different POI categories vary. For example, coffee shops are crowded in the afternoon because people like to have coffee to refresh after meals, while bars are busy late at night. However, existing methods rarely explore the relationship between POI categories and time, which may result in the model being unable to fully learn users' tendencies to visit certain POI categories at different times. Additionally, existing methods for modeling time information often convert it into time embeddings or calculate the time interval and incorporate it into the model, making it difficult to capture the continuity of time. Finally, during POI prediction, various weighting information is often ignored, such as the popularity of each POI, the transition relationships between POIs, and the distances between POIs, leading to suboptimal performance. To address these issues, this paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW). This framework aims to jointly consider POI category information and multiple POI weighting factors. Specifically, the proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs. We conducted experiments on two real-world datasets, and the results demonstrate that the proposed GDPW outperforms other existing models, improving performance by 3% to 11%.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model</title>
<link>https://arxiv.org/abs/2507.15067</link>
<guid>https://arxiv.org/abs/2507.15067</guid>
<content:encoded><![CDATA[
<div> transformer-based classification model, bad actor detection, robustness, adversarial attacks, sequence embedding  
Summary:  
- The study focuses on developing a robust deep learning model, ROBAD, for detecting bad actors on internet platforms.  
- ROBAD utilizes transformer encoder and decoder blocks to capture local and global information from user posts.  
- The model leverages sequence embeddings to detect potential modifications in input sequences.  
- By incorporating mimicked behaviors of bad actors in training, ROBAD enhances its knowledge and robustness against adversarial attacks.  
- Experimental results on Yelp and Wikipedia datasets demonstrate ROBAD's effectiveness in detecting bad actors under state-of-the-art adversarial attacks.  

<br /><br />Summary: <div>
arXiv:2507.15067v1 Announce Type: cross 
Abstract: Detecting bad actors is critical to ensure the safety and integrity of internet platforms. Several deep learning-based models have been developed to identify such users. These models should not only accurately detect bad actors, but also be robust against adversarial attacks that aim to evade detection. However, past deep learning-based detection models do not meet the robustness requirement because they are sensitive to even minor changes in the input sequence. To address this issue, we focus on (1) improving the model understanding capability and (2) enhancing the model knowledge such that the model can recognize potential input modifications when making predictions. To achieve these goals, we create a novel transformer-based classification model, called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection model), which uses the sequence of user posts to generate user embedding to detect bad actors. Particularly, ROBAD first leverages the transformer encoder block to encode each post bidirectionally, thus building a post embedding to capture the local information at the post level. Next, it adopts the transformer decoder block to model the sequential pattern in the post embeddings by using the attention mechanism, which generates the sequence embedding to obtain the global information at the sequence level. Finally, to enrich the knowledge of the model, embeddings of modified sequences by mimicked attackers are fed into a contrastive-learning-enhanced classification layer for sequence prediction. In essence, by capturing the local and global information (i.e., the post and sequence information) and leveraging the mimicked behaviors of bad actors in training, ROBAD can be robust to adversarial attacks. Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can effectively detect bad actors when under state-of-the-art adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Homophily and Heterophily in Multimodal Graph Clustering</title>
<link>https://arxiv.org/abs/2507.15253</link>
<guid>https://arxiv.org/abs/2507.15253</guid>
<content:encoded><![CDATA[
<div> framework, multimodal graph clustering, Disentangled Multimodal Graph Clustering, dual-frequency fusion, self-supervised alignment  
Summary:  
The article introduces the Disentangled Multimodal Graph Clustering (DMGC) framework for unsupervised learning on multimodal graphs. These graphs combine structured interconnections with heterogeneous data, exhibiting both homophilic and heterophilic relationships. The DMGC framework decomposes the hybrid graph into homophily-enhanced and heterophily-aware graphs, integrating them through a Multimodal Dual-frequency Fusion mechanism. This approach effectively combines different views while preventing category confusion. Self-supervised alignment objectives guide the learning process without requiring labels. Empirical analysis on various datasets shows that DMGC outperforms existing methods, demonstrating its effectiveness and generalizability. The code for DMGC is available on GitHub at https://github.com/Uncnbb/DMGC. 

<br /><br />Summary: <div>
arXiv:2507.15253v1 Announce Type: cross 
Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with structured interconnections, offer substantial real-world utility but remain insufficiently explored in unsupervised learning. In this work, we initiate the study of multimodal graph clustering, aiming to bridge this critical gap. Through empirical analysis, we observe that real-world multimodal graphs often exhibit hybrid neighborhood patterns, combining both homophilic and heterophilic relationships. To address this challenge, we propose a novel framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which decomposes the original hybrid graph into two complementary views: (1) a homophily-enhanced graph that captures cross-modal class consistency, and (2) heterophily-aware graphs that preserve modality-specific inter-class distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism that jointly filters these disentangled graphs through a dual-pass strategy, enabling effective multimodal integration while mitigating category confusion. Our self-supervised alignment objectives further guide the learning process without requiring labels. Extensive experiments on both multimodal and multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art performance, highlighting its effectiveness and generalizability across diverse settings. Our code is available at https://github.com/Uncnbb/DMGC.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicting narratives and polarization on social media</title>
<link>https://arxiv.org/abs/2507.15600</link>
<guid>https://arxiv.org/abs/2507.15600</guid>
<content:encoded><![CDATA[
<div> conflicting narratives, political reality, polarization, issue alignment, discursive mechanisms

Summary: 
This study examines how conflicting narratives in the public sphere contribute to political polarization and issue alignment. By analyzing tweets from opposing opinion groups in the German Twittersphere between 2021 and 2023, the researchers identify conflicting narratives on key issues such as the war in Ukraine, Covid, and climate change. They find that these conflicting narratives stem from differing attributions of actantial roles and the emplotment of different actants for the same events. Additionally, the study uncovers patterns of narrative alignment, wherein political actors strategically align opinions across different issues. These findings highlight the role of narratives as a valuable analytical lens for understanding discursive polarization mechanisms in political discourse. 

<br /><br />Summary: <div>
arXiv:2507.15600v1 Announce Type: cross 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. In this work, we show how the analysis of conflicting narratives, i.e. conflicting interpretive lenses through which political reality is experienced and told, provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere. Building upon previous work that has identified ideologically polarized issues in the German Twittersphere between 2021 and 2023, we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups. Focusing on a selection of salient issues and events (the war in Ukraine, Covid, climate change), we show evidence for conflicting narratives along two dimensions: (i) different attributions of actantial roles to the same set of actants (e.g. diverging interpretations of the role of NATO in the war in Ukraine), and (ii) emplotment of different actants for the same event (e.g. Bill Gates in the right-leaning Covid narrative). Furthermore, we provide first evidence for patterns of narrative alignment, a discursive strategy that political actors employ to align opinions across issues. These findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work</title>
<link>https://arxiv.org/abs/2507.15823</link>
<guid>https://arxiv.org/abs/2507.15823</guid>
<content:encoded><![CDATA[
<div> Keywords: AI for Good, collaboration, deployment, resource-constrained, continuous performance updates 

Summary: 
This work focuses on the deployment of AI models in collaboration with a humanitarian-to-humanitarian organization, emphasizing the importance of real-world impact in the AI for Good space. The paper discusses the process of deploying AI models in resource-constrained environments, highlighting the challenges and successes of maintaining the models for continuous performance updates. The close collaboration with the partner organization is key to ensuring the AI model's success and impact in high-impact applications. Practitioners can learn valuable insights from this work on how to effectively collaborate with organizations in deploying AI models and maintaining them for long-term success. Overall, this study sheds light on the importance of not only developing AI models for high-impact applications but also on the crucial process of deploying and collaborating with partner organizations for real-world impact. 

<br /><br />Summary: <div>
arXiv:2507.15823v1 Announce Type: cross 
Abstract: Publications in the AI for Good space have tended to focus on the research and model development that can support high-impact applications. However, very few AI for Good papers discuss the process of deploying and collaborating with the partner organization, and the resulting real-world impact. In this work, we share details about the close collaboration with a humanitarian-to-humanitarian (H2H) organization and how to not only deploy the AI model in a resource-constrained environment, but also how to maintain it for continuous performance updates, and share key takeaways for practitioners.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection for directed networks revisited using bimodularity</title>
<link>https://arxiv.org/abs/2502.04777</link>
<guid>https://arxiv.org/abs/2502.04777</guid>
<content:encoded><![CDATA[
<div> Keywords: Community structure, Directed networks, Bimodularity, Edge-based clustering, Convex relaxation

Summary:
Community structure, a common feature in networks, has been extensively studied for undirected graphs but not as satisfactorily for directed graphs. This study introduces the concept of bimodularity to represent directed communities by mapping sending and receiving communities. Using convex relaxation and the singular value decomposition of the directed modularity matrix, bimodularity can be optimized. An edge-based clustering approach is then proposed to uncover directed communities and their mappings. The effectiveness of this new framework is demonstrated on a synthetic model and applied to the neuronal network of \textit{C. elegans}, revealing meaningful feedforward loops in the motion systems of the head and body. This novel approach lays a foundation for identifying and understanding community structures in directed networks. 

<br /><br />Summary: Community structure in directed networks is addressed through the concept of bimodularity, optimizing the directed modularity matrix using convex relaxation. An edge-based clustering method reveals directed communities and their mappings, showcased on a synthetic model and the neuronal network of \textit{C. elegans}. This framework advances the detection and comprehension of community structures in directed networks. <div>
arXiv:2502.04777v2 Announce Type: replace 
Abstract: Community structure is a key feature omnipresent in real-world network data. Plethora of methods have been proposed to reveal subsets of densely interconnected nodes using criteria such as the modularity index. These approaches have been successful for undirected graphs, but directed edge information has not yet been dealt with in a satisfactory way. Here, we revisit the concept of directed communities as a mapping between sending and receiving communities. This translates into a new definition that we term bimodularity. Using convex relaxation, bimodularity can be optimized with the singular value decomposition of the directed modularity matrix. Subsequently, we propose an edge-based clustering approach to reveal the directed communities including their mappings. The feasibility of the new framework is illustrated on a synthetic model and further applied to the neuronal wiring diagram of the \textit{C. elegans}, for which it yields meaningful feedforward loops of the head and body motion systems. This framework sets the ground for the understanding and detection of community structures in directed networks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion</title>
<link>https://arxiv.org/abs/2507.13366</link>
<guid>https://arxiv.org/abs/2507.13366</guid>
<content:encoded><![CDATA[
<div> framework, trajectory, synthesis, privacy-preserving, mobility <br />
Summary: <br />
The paper introduces Cardiff, a novel trajectory synthesizing framework for generating fine-grained and privacy-preserving urban mobility data. Cardiff utilizes a cascaded hybrid diffusion-based approach, decomposing the generation process into segment-level and GPS-level synthesis for realistic trajectories. The segment-level encodes road segments into latent embeddings and employs a denoising network for synthesis, while the GPS-level network uses noise augmentation for high-fidelity generation. Cardiff offers a balance between privacy preservation and utility, outperforming existing methods on real-world trajectory datasets in various metrics. <div>
arXiv:2507.13366v1 Announce Type: new 
Abstract: Urban mobility data has significant connections with economic growth and plays an essential role in various smart-city applications. However, due to privacy concerns and substantial data collection costs, fine-grained human mobility trajectories are difficult to become publicly available on a large scale. A promising solution to address this issue is trajectory synthesizing. However, existing works often ignore the inherent structural complexity of trajectories, unable to handle complicated high-dimensional distributions and generate realistic fine-grained trajectories. In this paper, we propose Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory synthesizing framework for fine-grained and privacy-preserving mobility generation. By leveraging the hierarchical nature of urban mobility, Cardiff decomposes the generation process into two distinct levels, i.e., discrete road segment-level and continuous fine-grained GPS-level: (i) In the segment-level, to reduce computational costs and redundancy in raw trajectories, we first encode the discrete road segments into low-dimensional latent embeddings and design a diffusion transformer-based latent denoising network for segment-level trajectory synthesis. (ii) Taking the first stage of generation as conditions, we then design a fine-grained GPS-level conditional denoising network with a noise augmentation mechanism to achieve robust and high-fidelity generation. Additionally, the Cardiff framework not only progressively generates high-fidelity trajectories through cascaded denoising but also flexibly enables a tunable balance between privacy preservation and utility. Experimental results on three large real-world trajectory datasets demonstrate that our method outperforms state-of-the-art baselines in various metrics.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio</title>
<link>https://arxiv.org/abs/2507.13368</link>
<guid>https://arxiv.org/abs/2507.13368</guid>
<content:encoded><![CDATA[
<div> neighborhood search, multiple views, graph clustering, attribute-missing, performance improvement

Summary: 
The paper presents a new method called CMV-ND for deep graph clustering (DGC) on large-scale attribute-missing graphs. It preprocesses structural information into multiple non-redundant views through recursive neighborhood search and neighborhood differentiation strategies. By constructing complementary views from differential hop representations and node features, CMV-ND effectively enhances the performance of existing multi-view clustering and DGC methods. Experimental results on six graph datasets validate the significant performance improvement achieved by CMV-ND in various industrial scenarios like community detection and recommendation.<br /><br />Summary: <div>
arXiv:2507.13368v1 Announce Type: new 
Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes in an attribute graph into different clusters, has seen substantial potential in various industrial scenarios like community detection and recommendation. However, the real-world attribute graphs, e.g., social networks interactions, are usually large-scale and attribute-missing. To solve these two problems, we propose a novel DGC method termed \underline{\textbf{C}}omplementary \underline{\textbf{M}}ulti-\underline{\textbf{V}}iew \underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation (\textit{CMV-ND}), which preprocesses graph structural information into multiple views in a complete but non-redundant manner. First, to ensure completeness of the structural information, we propose a recursive neighborhood search that recursively explores the local structure of the graph by completely expanding node neighborhoods across different hop distances. Second, to eliminate the redundancy between neighborhoods at different hops, we introduce a neighborhood differential strategy that ensures no overlapping nodes between the differential hop representations. Then, we construct $K+1$ complementary views from the $K$ differential hop representations and the features of the target node. Last, we apply existing multi-view clustering or DGC methods to the views. Experimental results on six widely used graph datasets demonstrate that CMV-ND significantly improves the performance of various methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
<link>https://arxiv.org/abs/2507.13370</link>
<guid>https://arxiv.org/abs/2507.13370</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion evolution, consensus, non-intrusive, multi-agent reinforcement learning

Summary: 
The article introduces the H-NeiFi framework, a hierarchical and non-intrusive approach to guiding opinion evolution on social media towards global consensus. By considering social roles and behavioral characteristics, the framework aims to optimize information propagation paths without directly modifying user views or enforcing connections. Through a dynamic two-layer model and non-intrusive neighbor filtering method, H-NeiFi adaptsively controls user communication channels to enhance consensus speed by 22.0% to 30.7% and maintain global convergence even in the absence of experts. Utilizing multi-agent reinforcement learning, the framework offers a long-term reward function to guide opinions naturally and efficiently while preserving user interaction autonomy. This approach presents a new paradigm for social network governance, addressing challenges in promoting global consensus without intruding on individual opinions. 

Summary: <div>
arXiv:2507.13370v1 Announce Type: new 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patterns, Models, and Challenges in Online Social Media: A Survey</title>
<link>https://arxiv.org/abs/2507.13379</link>
<guid>https://arxiv.org/abs/2507.13379</guid>
<content:encoded><![CDATA[
<div> keywords: digital platforms, individual behavior, collective behavior, online social systems, model validation
Summary:
The article discusses the impact of digital platforms on observing individual and collective behavior through interaction data. It highlights the need for consolidation in the field due to methodological heterogeneity and weak integration across domains. The survey systematically synthesizes empirical findings and formal models to understand platform-level regularities and methodological architectures. It emphasizes the importance of current modeling frameworks in accounting for observed dynamics in online social systems. The goal is to establish a shared empirical baseline and identify structural constraints for more robust analyses. The article aims to pave the way for improved, comparable, and actionable analyses of online social systems.<br /><br />Summary: <div>
arXiv:2507.13379v1 Announce Type: new 
Abstract: The rise of digital platforms has enabled the large scale observation of individual and collective behavior through high resolution interaction data. This development has opened new analytical pathways for investigating how information circulates, how opinions evolve, and how coordination emerges in online environments. Yet despite a growing body of research, the field remains fragmented and marked by methodological heterogeneity, limited model validation, and weak integration across domains. This survey offers a systematic synthesis of empirical findings and formal models. We examine platform-level regularities, assess the methodological architectures that generate them, and evaluate the extent to which current modeling frameworks account for observed dynamics. The goal is to consolidate a shared empirical baseline and clarify the structural constraints that shape inference in this domain, laying the groundwork for more robust, comparable, and actionable analyses of online social systems.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Dynamics of Conspiracy Related German Telegram Conversations during COVID-19</title>
<link>https://arxiv.org/abs/2507.13398</link>
<guid>https://arxiv.org/abs/2507.13398</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Telegram chats, COVID-19, misinformation, information flow

Summary:
Conspiracy theories have seen a surge on German-language Telegram chats during the COVID-19 pandemic, raising concerns about their impact on trust, democracy, and public health. The study analyzed the structure of these chats, revealing spikes in activity during key pandemic events and highlighting the role of societal stressors in amplifying conspiratorial beliefs. Information flow was observed from larger national or transnational discourse to localized discussions, with a small number of key actors driving the dissemination of content. The top 10% of chats accounted for a majority of forwarded content, but they operated independently with minimal interconnection. A concerning finding was that a significant proportion of shared links pointed to untrustworthy sources, indicating Telegram's role in spreading misinformation. This research sheds light on how conspiracy-related discussions on the platform serve as vectors for the spread of false information.<br /><br />Summary: <div>
arXiv:2507.13398v1 Announce Type: new 
Abstract: Conspiracy theories have long drawn public attention, but their explosive growth on platforms like Telegram during the COVID-19 pandemic raises pressing questions about their impact on societal trust, democracy, and public health. We provide a geographical, temporal and network analysis of the structure of of conspiracy-related German-language Telegram chats in a novel large-scale data set. We examine how information flows between regional user groups and influential broadcasting channels, revealing the interplay between decentralized discussions and content spread driven by a small number of key actors.
  Our findings reveal that conspiracy-related activity spikes during major COVID-19-related events, correlating with societal stressors and mirroring prior research on how crises amplify conspiratorial beliefs. By analysing the interplay between regional, national and transnational chats, we uncover how information flows from larger national or transnational discourse to localised, community-driven discussions. Furthermore, we find that the top 10% of chats account for 94% of all forwarded content, portraying the large influence of a few actors in disseminating information. However, these chats operate independently, with minimal interconnection between each other, primarily forwarding messages to low-traffic groups. Notably, 43% of links shared in the data set point to untrustworthy sources as identified by NewsGuard, a proportion far exceeding their share on other platforms and in other discourse contexts, underscoring the role of conspiracy-related discussions on Telegram as vector for the spread of misinformation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Multi-Site Sex Ad Data at the Individual Level to Aid Counter-Trafficking Efforts</title>
<link>https://arxiv.org/abs/2507.13477</link>
<guid>https://arxiv.org/abs/2507.13477</guid>
<content:encoded><![CDATA[
<div> Keywords: sex trafficking, adult service websites, counter-trafficking efforts, data linking, artificial intelligence<br />
<br />
Summary: 
The study focuses on the use of adult service websites (ASWs) for sex trafficking and the challenges in collecting and linking data from these sites. The closure of Backpage.com has led to the expansion of ASWs outside US jurisdiction, making it difficult to gather intelligence for counter-trafficking efforts. The researchers have developed an end-to-end process using network science, information systems, and artificial intelligence to link and filter sex ad data efficiently. This process has been successful in identifying over 60 potential victims of sex trafficking, helping them transition out of the exploitative life. The key component of the process is an edge filtering procedure that removes erroneous links in the data. Compared to existing approaches, their process is more computationally efficient and provides more actionable intelligence. The proposed process is a valuable tool in transforming disparate sex ad data into actionable intelligence to combat sex trafficking and save lives.<br /><br /> <div>
arXiv:2507.13477v1 Announce Type: new 
Abstract: The Internet facilitates sex trafficking through adult service websites (ASWs) that host online advertisements for sexual services (sex ads). Since the closure of the popular site Backpage.com, the ecosystem of ASWs has expanded to include multiple competing sites that are hosted outside US jurisdiction. Gaining intelligence for counter-trafficking efforts requires collecting, linking, and cleaning the data from multiple sites. However, high ad volumes, disparate data types, and the existence of generic and misappropriated data make this process challenging. We present an end-to-end process for linking sex ad data and filtering potentially erroneous links. Outputs of the developed process have been used to inform counter-trafficking operations that have helped identify more than 60 potential victims of sex trafficking, some of whom are getting help to transition out of the life. Our process leverages concepts and techniques from network science, information systems, and artificial intelligence to link ads across sites at the level of an individual or unique posting entity. Our approach is computationally efficient, allowing millions of ads to be processed in under an hour. A key component of our process is an edge filtering procedure that identifies and removes potentially erroneous links in a graph representation of sex ad data. A comparison of the proposed process to an existing approach shows that our process is typically more computationally efficient and yields substantial increases in the number of individuals for which we can derive actionable intelligence. The proposed process is an efficient and effective approach for transforming the high volumes of disparate data from sex ads into intelligence that can save lives. It has been refined over years of collaboration with practitioners and represents a strong foundation upon which further counter-trafficking tools can be built.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures</title>
<link>https://arxiv.org/abs/2507.13577</link>
<guid>https://arxiv.org/abs/2507.13577</guid>
<content:encoded><![CDATA[
<div> Keywords: interdependent infrastructure systems, hetero-functional graph, community preferences, Large Language Model, learning algorithms 

Summary:
Interdependent infrastructure systems and communities are represented using a hetero-functional graph (HFG) to encode dependencies between functionalities. This graph establishes a partial order of functionalities to guide repair decisions during disasters. However, integrating community preferences is crucial to refine this order and enhance resilience. To accomplish this, a Large Language Model (LLM) is used as a proxy survey tool to gather community feedback on preferred repair sequences. Simulated personas representing diverse disaster experiences provide input on prioritizing infrastructure repair needs across communities. Learning algorithms are then employed to generate a global order based on aggregated responses from these LLM-generated personas. This approach combines technical criteria with community input to optimize repair strategies and enhance disaster resilience.<br /><br />Summary: Interdependent infrastructure systems are represented using a HFG, allowing for a partial order of repair functionalities. Community preferences are integrated via LLM proxy surveys to refine this order and improve resilience. Simulated personas provide diverse input on prioritizing repair needs, culminating in a global order generated by learning algorithms. <div>
arXiv:2507.13577v1 Announce Type: new 
Abstract: We represent interdependent infrastructure systems and communities alike with a hetero-functional graph (HFG) that encodes the dependencies between functionalities. This graph naturally imposes a partial order of functionalities that can inform the sequence of repair decisions to be made during a disaster across affected communities. However, using such technical criteria alone provides limited guidance at the point where the functionalities directly impact the communities, since these can be repaired in any order without violating the system constraints. To address this gap and improve resilience, we integrate community preferences to refine this partial order from the HFG into a total order. Our strategy involves getting the communities' opinions on their preferred sequence for repair crews to address infrastructure issues, considering potential constraints on resources. Due to the delay and cost associated with real-world survey data, we utilize a Large Language Model (LLM) as a proxy survey tool. We use the LLM to craft distinct personas representing individuals, each with varied disaster experiences. We construct diverse disaster scenarios, and each simulated persona provides input on prioritizing infrastructure repair needs across various communities. Finally, we apply learning algorithms to generate a global order based on the aggregated responses from these LLM-generated personas.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Duplicating Deceit: Inauthentic Behavior Among Indian Misinformation Duplicators on X/Twitter</title>
<link>https://arxiv.org/abs/2507.13636</link>
<guid>https://arxiv.org/abs/2507.13636</guid>
<content:encoded><![CDATA[
<div> Keywords: inauthentic duplication, social media, misinformation, TweeXster, duplication campaigns

Summary:
This paper examines the issue of inauthentic duplication on social media, specifically focusing on the spread of misinformation through multiple accounts sharing identical false tweets. The study utilizes a dataset from AltNews, an Indian fact-checking organization, containing over 12 million posts from 5,493 accounts known to be involved in duplicating such content. Surprisingly, the research reveals that less than 1% of these accounts exhibit bot-like behavior, dispelling the common misconception that bots are solely responsible for the dissemination of false information. The authors introduce TweeXster, a framework designed to detect and analyze duplication campaigns, which uncovers clusters of accounts engaged in the repeated and sometimes revived sharing of deceptive or harmful content. This research sheds light on the complex dynamics of misinformation spread on social media and offers a novel approach to combatting such campaigns. 

<br /><br />Summary: <div>
arXiv:2507.13636v1 Announce Type: new 
Abstract: This paper investigates inauthentic duplication on social media, where multiple accounts share identical misinformation tweets. Leveraging a dataset of misinformation verified by AltNews, an Indian fact-checking organization, we analyze over 12 million posts from 5,493 accounts known to have duplicated such content. Contrary to common assumptions that bots are primarily responsible for spreading false information, fewer than 1\% of these accounts exhibit bot-like behavior. We present TweeXster, a framework for detecting and analyzing duplication campaigns, revealing clusters of accounts involved in repeated and sometimes revived dissemination of false or abusive content.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Route-based Conflation Between Linear Referencing System Maps And OpenStreetMap Using Open-source Tools</title>
<link>https://arxiv.org/abs/2507.13939</link>
<guid>https://arxiv.org/abs/2507.13939</guid>
<content:encoded><![CDATA[
<div> conflation, basemaps, LRS, OpenStreetMap, automated<br />
<br />
Summary: 
This paper introduces an automated open-source process for conflation of two basemaps - the Virginia Department of Transportation's linear reference system (LRS) basemap and OpenStreetMap (OSM) for Virginia. The process involves loading one LRS route at a time, determining the direction of travel, interpolating to fill gaps, and using Valhalla's map-matching algorithm with a Hidden Markov Model (HMM) and Viterbi search to find corresponding points on OSM. Key contributions include successful conflation of Virginia's LRS map with OSM using an automated method, a replicable open-source processing pipeline, and achieving over 98% successful matches. This approach improves upon existing automated conflation processes and eliminates the need for proprietary licenses. <br /><br />Summary: <div>
arXiv:2507.13939v1 Announce Type: new 
Abstract: Transportation researchers and planners utilize a wide range of roadway metrics that are usually associated with different basemaps. Conflation is an important process for transferring these metrics onto a single basemap. However, conflation is often an expensive and time-consuming process based on proprietary algorithms that require manual verification.
  In this paper, an automated open-source process is used to conflate two basemaps: the linear reference system (LRS) basemap produced by the Virginia Department of Transportation and the OpenStreetMap (OSM) basemap for Virginia. This process loads one LRS route at a time, determines the correct direction of travel, interpolates to fill gaps larger than 12 meters, and then uses Valhalla's map-matching algorithm to find the corresponding points along OSM's segments. Valhalla's map-matching process uses a Hidden Markov Model (HMM) and Viterbi search-based approach to find the most likely OSM segments matching the LRS route.
  This work has three key contributions. First, it conflates the Virginia roadway network LRS map with OSM using an automated conflation method based on HMM and Viterbi search. Second, it demonstrates a novel open-source processing pipeline that could be replicated without the need for proprietary licenses. Finally, the overall conflation process yields over 98% successful matches, which is an improvement over most automated processes currently available for this type of conflation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rumour Spreading Depends on the Latent Geometry and Degree Distribution in Social Network Models</title>
<link>https://arxiv.org/abs/2408.01268</link>
<guid>https://arxiv.org/abs/2408.01268</guid>
<content:encoded><![CDATA[
<div> power-law distribution, rumour spreading, ultra-small-world models, geometric inhomogeneous random graphs, metric geometry 

Summary: 
- The study investigates rumour spreading in ultra-small-world models with power-law distribution degrees, focusing on Geometric Inhomogeneous Random Graphs (GIRGs).
- Rumour spreading speed varies in GIRGs with Euclidean geometry, showing slow, fast (polylogarithmic), or ultra-fast rates based on power law exponent and geometry strength.
- Rumour spreading in GIRGs doesn't align with graph distance rates, indicating that faster spreading may occur even with short graph distances.
- Non-metric geometry always leads to at least fast rumour spreading, depicting social connections based on single attributes like familial kinship.
- In Euclidean geometry, efficient rumour transmission pathways differ from known paths, showcasing chains of vertices with specific degree structures. <div>
arXiv:2408.01268v3 Announce Type: replace-cross 
Abstract: We study push-pull rumour spreading in ultra-small-world models for social networks where the degrees follow a power-law distribution. In a non-geometric setting, Fountoulakis, Panagiotou and Sauerwald have shown that rumours always spread ultra-fast (SODA 2012), i.e. in doubly logarithmic time. On the other hand, Janssen and Mehrabian have found that rumours spread slowly (polynomial time) in a spatial preferential attachment model (SIDMA 2017). We study the question systematically for the model of Geometric Inhomogeneous Random Graphs (GIRGs). Our results are two-fold: first, with Euclidean geometry slow, fast (polylogarithmic) and ultra-fast rumour spreading may occur, depending on the exponent of the power law and the strength of the geometry in the networks, and we fully characterise the phase boundaries in between. The regimes do not coincide with the graph distance regimes, i.e., polylogarithmic or even polynomial rumour spreading may occur even if graph distances are doubly logarithmic. We expect these results to hold with little effort for related models, e.g. Scale-Free Percolation. Second, we show that rumour spreading is always (at least) fast in a non-metric geometry. The considered non-metric geometry allows to model social connections where resemblance of vertices in a single attribute, such as familial kinship, already strongly indicates the presence of an edge. Euclidean geometry fails to capture such ties.
  For some regimes in the Euclidean setting, the efficient pathways for spreading rumours differ from previously identified paths. For example, a vertex of degree $d$ can transmit the rumour to a vertex of larger degree by a chain of length $3$, where one of the two intermediaries has constant degree, and the other has degree $d^{c}$ for some constant $c<1$. Similar but longer chains of vertices, all having non-constant degree, turn out to be useful as well.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification of Authoritative Nodes and Dismantling of Illicit Networks Using a Novel Metric for Measuring Strength of a Graph</title>
<link>https://arxiv.org/abs/2507.12711</link>
<guid>https://arxiv.org/abs/2507.12711</guid>
<content:encoded><![CDATA[
<div> metrics, criminal networks, node removal, human perception, network strength 
Summary: 
The study introduces a new metric for evaluating the strength of networks after node removal in scenarios like dismantling criminal networks or containing epidemics. Traditional metrics focus on structural properties of the graph, neglecting human perception. The proposed metric combines structural properties with human perception, aligning more closely with human judgment. Validation through human subject surveys shows that the new metric outperforms existing methods in identifying authoritative nodes and effectively dismantling networks. The metric's effectiveness is demonstrated through dismantling both synthetic and real-world networks. By integrating structural properties and human perception, the new metric offers a more comprehensive evaluation of network strength post-node removal. <div>
arXiv:2507.12711v1 Announce Type: new 
Abstract: Dismantling criminal networks or containing epidemics or misinformation through node removal is a well-studied problem. To evaluate the effectiveness of such efforts, one must measure the strength of the network before and after node removal. Process P1 is considered more effective than P2 if the strength of the residual network after removing k nodes via P1 is smaller than that from P2. This leads to the central question: How should network strength be measured?
  Existing metrics rely solely on structural properties of the graph, such as connectivity. However, in real-world scenarios, particularly in law enforcement, the perception of agents regarding network strength can differ significantly from structural assessments. These perceptions are often ignored in traditional metrics.
  We propose a new strength metric that integrates both structural properties and human perception. Using human subject surveys, we validate our approach against existing metrics. Our metric not only aligns more closely with human judgment but also outperforms traditional methods in identifying authoritative nodes and effectively dismantling both synthetic and real-world networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3MAL: Test-Time Fast Adaptation for Robust Multi-Scale Information Diffusion Prediction</title>
<link>https://arxiv.org/abs/2507.12880</link>
<guid>https://arxiv.org/abs/2507.12880</guid>
<content:encoded><![CDATA[
<div> adaptation, information diffusion prediction, test-time training, distribution shifts, self-supervised auxiliary task <br />
<br />
Summary: 
The paper introduces T3MAL, a framework for multi-scale diffusion prediction that addresses distribution shifts within IDP tasks. T3MAL utilizes a test-time training approach to adapt a trained model to the distribution of each test instance, improving prediction accuracy in social networks with uncertain user behavior. The framework includes a self-supervised auxiliary network inspired by BYOL, facilitating instance-specific adaptation during testing. T3MAL also incorporates a meta-auxiliary learning scheme and a lightweight adaptor for efficient test-time adaptation and better weight initialization to prevent catastrophic forgetting. Extensive experiments on public datasets demonstrate T3MAL's superiority over existing methods in information diffusion prediction tasks. <div>
arXiv:2507.12880v1 Announce Type: new 
Abstract: Information diffusion prediction (IDP) is a pivotal task for understanding how information propagates among users. Most existing methods commonly adhere to a conventional training-test paradigm, where models are pretrained on training data and then directly applied to test samples. However, the success of this paradigm hinges on the assumption that the data are independently and identically distributed, which often fails in practical social networks due to the inherent uncertainty and variability of user behavior. In the paper, we address the novel challenge of distribution shifts within IDP tasks and propose a robust test-time training (TTT)-based framework for multi-scale diffusion prediction, named T3MAL. The core idea is to flexibly adapt a trained model to accommodate the distribution of each test instance before making predictions via a self-supervised auxiliary task. Specifically, T3MAL introduces a BYOL-inspired self-supervised auxiliary network that shares a common feature extraction backbone with the primary diffusion prediction network to guide instance-specific adaptation during testing. Furthermore, T3MAL enables fast and accurate test-time adaptation by incorporating a novel meta-auxiliary learning scheme and a lightweight adaptor, which together provide better weight initialization for TTT and mitigate catastrophic forgetting. Extensive experiments on three public datasets demonstrate that T3MAL outperforms various state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Centrality Paradox: Why Your Friends Are Always More Important</title>
<link>https://arxiv.org/abs/2507.13059</link>
<guid>https://arxiv.org/abs/2507.13059</guid>
<content:encoded><![CDATA[
<div> friendship paradox, network centrality, graph theory, eigenvector centrality, PageRank <br />
<br />
Summary: 
The article revisits the classical friendship paradox in the context of network centrality measures. It shows that in any irreducible, undirected graph, various centrality measures such as degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities follow the friendship paradox principle, where one's friends, on average, have more connections than oneself. The study connects this result to the variational characterization of the eigenvector corresponding to the Perron eigenvalue. By generalizing the friendship paradox, the research demonstrates that different centrality measures showcase similar patterns, exceeding the global average. This finding contributes to a better understanding of how network structures influence the distribution of connections within a graph, highlighting the importance of considering various centrality measures in network analysis. <div>
arXiv:2507.13059v1 Announce Type: new 
Abstract: We revisit the classical friendship paradox which states that on an average one's friends have at least as many friends as oneself and generalize it to a variety of network centrality measures. In particular, we show that for any irreducible, undirected graph $G$, the "friends-average" of degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities exceeds the global average. We show that the result follows from the variational characterisation of the eigenvector corresponding to the Perron eigenvalue.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interacting Hosts with Microbiome Exchange: An Extension of Metacommunity Theory for Discrete Interactions</title>
<link>https://arxiv.org/abs/2507.11958</link>
<guid>https://arxiv.org/abs/2507.11958</guid>
<content:encoded><![CDATA[
<div> Metacommunity theory, microbiome model, interactions, dispersal, living hosts<br />
<br />
Summary: <br />
Microbiomes, collections of interacting microbes in environments, have significant effects on their surroundings. Metacommunity theory, which incorporates interactions across multiple scales, is commonly used in microbiome models. Current metacommunity models assume continuous dispersal of microbiomes between environments, but this does not reflect the discrete interactions between microbiomes in living hosts. A new modeling framework is developed in this paper, considering discrete interactions and using parameters to control interaction frequencies and microbiome exchange amounts. Analytical approximations in three parameter regimes are derived and proven to be accurate. Both parameters are crucial in determining microbiome dynamics, with microbiome convergence depending on the interplay between interaction frequency and strength. The study emphasizes the importance of incorporating discrete interactions in microbiome models for a better understanding of microbiome dynamics in living hosts. <div>
arXiv:2507.11958v1 Announce Type: cross 
Abstract: Microbiomes, which are collections of interacting microbes in an environment, often substantially impact the environmental patches or living hosts that they occupy. In microbiome models, it is important to consider both the local dynamics within an environment and exchanges of microbiomes between environments. One way to incorporate these and other interactions across multiple scales is to employ metacommunity theory. Metacommunity models commonly assume continuous microbiome dispersal between the environments in which local microbiome dynamics occur. Under this assumption, a single parameter between each pair of environments controls the dispersal rate between those environments. This metacommunity framework is well-suited to abiotic environmental patches, but it fails to capture an essential aspect of the microbiomes of living hosts, which generally do not interact continuously with each other. Instead, living hosts interact with each other in discrete time intervals. In this paper, we develop a modeling framework that encodes such discrete interactions and uses two parameters to separately control the interaction frequencies between hosts and the amount of microbiome exchange during each interaction. We derive analytical approximations of models in our framework in three parameter regimes and prove that they are accurate in those regimes. We compare these approximations to numerical simulations for an illustrative model. We demonstrate that both parameters in our modeling framework are necessary to determine microbiome dynamics. Key features of the dynamics, such as microbiome convergence across hosts, depend sensitively on the interplay between interaction frequency and strength.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines</title>
<link>https://arxiv.org/abs/2507.12840</link>
<guid>https://arxiv.org/abs/2507.12840</guid>
<content:encoded><![CDATA[
<div> Vaccine hesitancy, social media, public concerns, Language Models, VaxPulse Query Corner <br />
Summary: <br />
The article discusses the threat of vaccine hesitancy to public health and the role of social media in understanding public concerns. Traditional methods like topic modelling may struggle to capture nuanced opinions, while large Language Models (LLMs) can miss current events and community concerns. Hallucinations in LLMs further complicate public health communication. To address these challenges, the authors developed a tool called VaxPulse Query Corner using the Retrieval Augmented Generation technique. This tool helps answer complex queries about public vaccine concerns on online platforms, enabling public health administrators and stakeholders to better understand public concerns and implement targeted interventions to increase vaccine confidence. An analysis of 35,103 Shingrix social media posts showed high levels of answer faithfulness and relevance, demonstrating the tool's effectiveness in addressing public vaccine concerns. <div>
arXiv:2507.12840v1 Announce Type: cross 
Abstract: Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life Finds A Way: Emergence of Cooperative Structures in Adaptive Threshold Networks</title>
<link>https://arxiv.org/abs/2507.13253</link>
<guid>https://arxiv.org/abs/2507.13253</guid>
<content:encoded><![CDATA[
<div> cooperation, competition, network dynamics, evolution, threshold-directed network

Summary:
The study explores the evolution of new levels of organization in systems with varying biases towards cooperation and antagonism. Using a random threshold-directed network model, the researchers simulate the dynamic formation of directed links based on node-specific traits and threshold rules. The model generates a multi-digraph with signed edges reflecting support or antagonism, leading to two interdependent threshold graphs. By incorporating temporal growth and node turnover, the researchers observe phase transitions in connectivity and resilience within communities. The findings offer insights into adaptive systems in biological and economic contexts and have applications in Collective Affordance Sets. The framework may be valuable for predicting outcomes in ongoing experiments on microbial communities in soil. <br /><br />Summary: <div>
arXiv:2507.13253v1 Announce Type: cross 
Abstract: There has been a long debate on how new levels of organization have evolved. It might seem unlikely, as cooperation must prevail over competition. One well-studied example is the emergence of autocatalytic sets, which seem to be a prerequisite for the evolution of life. Using a simple model, we investigate how varying bias toward cooperation versus antagonism shapes network dynamics, revealing that higher-order organization emerges even amid pervasive antagonistic interactions. In general, we observe that a quantitative increase in the number of elements in a system leads to a qualitative transition.
  We present a random threshold-directed network model that integrates node-specific traits with dynamic edge formation and node removal, simulating arbitrary levels of cooperation and competition. In our framework, intrinsic node values determine directed links through various threshold rules. Our model generates a multi-digraph with signed edges (reflecting support/antagonism, labeled ``help''/``harm''), which ultimately yields two parallel yet interdependent threshold graphs. Incorporating temporal growth and node turnover in our approach allows exploration of the evolution, adaptation, and potential collapse of communities and reveals phase transitions in both connectivity and resilience.
  Our findings extend classical random threshold and Erd\H{o}s-R\'enyi models, offering new insights into adaptive systems in biological and economic contexts, with emphasis on the application to Collective Affordance Sets. This framework should also be useful for making predictions that will be tested by ongoing experiments of microbial communities in soil.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the spillover from online engagement to offline protest: stochastic dynamics and mean-field approximations on networks</title>
<link>https://arxiv.org/abs/2507.13310</link>
<guid>https://arxiv.org/abs/2507.13310</guid>
<content:encoded><![CDATA[
<div> social media, offline protest, coupled modelling framework, online social network, transmission rate

Summary:
The study explores how engagement on social media topics influences offline protest activities using a coupled modelling framework. Various stochastic and mean-field models are developed to estimate the reproductive number and predict surge times in activity. The transmission rate between online and offline realms is crucial, needing to be within a critical range for offline outbursts to occur. Network structure impacts model accuracy, with low-density networks requiring more complexity. However, when tested on real-world networks, increased complexity did not improve accuracy. The research highlights the importance of considering online engagement when studying offline actions and how network density affects model accuracy. <div>
arXiv:2507.13310v1 Announce Type: cross 
Abstract: Social media is transforming various aspects of offline life, from everyday decisions such as dining choices to the progression of conflicts. In this study, we propose a coupled modelling framework with an online social network layer to analyse how engagement on a specific topic spills over into offline protest activities. We develop a stochastic model and derive several mean-field models of varying complexity. These models allow us to estimate the reproductive number and anticipate when surges in activity are likely to occur. A key factor is the transmission rate between the online and offline domains; for offline outbursts to emerge, this rate must fall within a critical range, neither too low nor too high. Additionally, using synthetic networks, we examine how network structure influences the accuracy of these approximations. Our findings indicate that low-density networks need more complex approximations, whereas simpler models can effectively represent higher-density networks. When tested on two real-world networks, however, increased complexity did not enhance accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter</title>
<link>https://arxiv.org/abs/2501.09035</link>
<guid>https://arxiv.org/abs/2501.09035</guid>
<content:encoded><![CDATA[
<div> social media, web content, demographic factors, information sharing, Twitter data

Summary:
The study introduces DomainDemo, a dataset linking Twitter domains with user demographics from 2011 to 2022, providing insights into information sharing during elections. The dataset includes age, gender, race, political affiliation, and geolocation of over 1.5 million users, allowing for a decade of analysis. By aggregating user demographics onto domains, five metrics are derived, including localness and partisan audience, offering a better understanding of information flows. The metrics align with existing classifications, validating DomainDemo's approach. This dataset enhances understanding of social media dynamics and trends in political discourse among registered U.S. voters from different sociodemographic groups. <div>
arXiv:2501.09035v2 Announce Type: replace 
Abstract: Social media play a pivotal role in disseminating web content, particularly during elections, yet our understanding of the association between demographic factors and information sharing online remains limited. Here, we introduce a unique dataset, DomainDemo, linking domains shared on Twitter (X) with the demographic characteristics of associated users, including age, gender, race, political affiliation, and geolocation, from 2011 to 2022. This new resource was derived from a panel of over 1.5 million Twitter users matched against their U.S. voter registration records, facilitating a better understanding of a decade of information flows on one of the most prominent social media platforms and trends in political and public discourse among registered U.S. voters from different sociodemographic groups. By aggregating user demographic information onto the domains, we derive five metrics that provide critical insights into over 129,000 websites. In particular, the localness and partisan audience metrics quantify the domains' geographical reach and ideological orientation, respectively. These metrics show substantial agreement with existing classifications, suggesting the effectiveness and reliability of DomainDemo's approach.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Link Prediction via Hyperedge Copying</title>
<link>https://arxiv.org/abs/2502.02386</link>
<guid>https://arxiv.org/abs/2502.02386</guid>
<content:encoded><![CDATA[
<div> generative model, hypergraphs, noisy copying, likelihood, empirical hypergraphs <br /> 
<br />Summary: 
The article proposes a generative model for temporally-evolving hypergraphs where hyperedges are formed by noisy copying of previous hyperedges. This model is able to replicate several patterns observed in empirical hypergraphs and can be learned from data. It defines a likelihood over complete hypergraphs rather than subsets. The model allows for analysis of node degree, edge size, and edge intersection size distributions in terms of its parameters. It successfully captures certain characteristics of empirical hypergraphs but also has limitations. The article presents a scalable stochastic expectation maximization algorithm for fitting the model to large hypergraph datasets. Furthermore, the model is evaluated in a hypergraph link prediction task and shows competitive performance with a minimal number of parameters compared to large neural networks. <div>
arXiv:2502.02386v2 Announce Type: replace 
Abstract: We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. We also show several features of empirical hypergraphs which are and are not successfully captured by our model. We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
<div> job data, occupation classification, large language models, taxonomies, taxonomy-guided reasoning 

Summary:
The study evaluates large language models' ability to accurately classify job data using occupational taxonomies, highlighting their limitations for smaller models. A multi-stage framework is proposed, incorporating taxonomy-guided reasoning examples to improve performance by aligning outputs with taxonomic knowledge. Evaluations on a large dataset demonstrate that the framework not only enhances occupation and skill classification tasks but also offers a cost-effective alternative to advanced models like GPT-4o, reducing computational costs while maintaining strong performance. This framework proves to be practical and scalable for occupation classification and related tasks across large language models. 

<br /><br />Summary: <div>
arXiv:2503.12989v2 Announce Type: replace-cross 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictable Drifts in Collective Cultural Attention: Evidence from Nation-Level Library Takeout Data</title>
<link>https://arxiv.org/abs/2507.12007</link>
<guid>https://arxiv.org/abs/2507.12007</guid>
<content:encoded><![CDATA[
<div> Keywords: consumer attention, cultural products, popularity distributions, demographic groups, cultural drifts

Summary:
Consumer attention for cultural products, such as books, movies, and songs, is challenging to predict due to intrinsic limits. This study analyzed nationwide library loan data for over 660,000 unique books and discovered that culture drifts continuously, leading to a growing divergence over time. Different book genres exhibit varying drift rates. The influence of demographic factors like age, sex, education, and location on cultural drift was also examined, revealing diverse effects across demographic groups. These findings have implications for market forecasting and recommender systems, emphasizing the importance of considering specific drift dynamics for different types of items and demographic segments.
<br /><br />Summary: <div>
arXiv:2507.12007v1 Announce Type: new 
Abstract: Predicting changes in consumer attention for cultural products, such as books, movies, and songs, is notoriously difficult. Past research on predicting the popularity of individual products suggests the existence of intrinsic prediction limits. However, little is known about the limits for predicting collective attention across cultural products. Here, we analyze four years of nationwide library loan data for approximately 2 million individuals, comprising over 100 million loans of more than 660,000 unique books. We find that culture, as measured by popularity distributions of loaned books, drifts continually from month to month at a near-constant rate, leading to a growing divergence over time, and that drifts vary between different book genres. By linking book loans to registry data, we investigate the influence of age, sex, educational level, and geographical area on cultural drift, finding heterogeneous effects from the different demographic groups. Our findings have important implications for market forecasting and developing robust recommender systems, highlighting the need to account for specific drift dynamics for different types of items and demographic groups.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns</title>
<link>https://arxiv.org/abs/2507.12063</link>
<guid>https://arxiv.org/abs/2507.12063</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, cascade graph mining, Contrastive Cascade Graph Learning (CCGL), information diffusion analysis, cascade classification

Summary: 
CCGL is a promising approach for analyzing cascade graphs in social media data to curb harmful content spread and promote reliable information dissemination. The study evaluates CCGL's performance in cascade classification tasks, showcasing its ability to capture specific structural patterns in cascade graphs across different platforms and models. The findings suggest CCGL's strong effectiveness in understanding the dynamics of information diffusion, making it a valuable tool for various downstream analyses in this domain.<br /><br />Summary: <div>
arXiv:2507.12063v1 Announce Type: new 
Abstract: A wide variety of information is disseminated through social media, and content that spreads at scale can have tangible effects on the real world. To curb the spread of harmful content and promote the dissemination of reliable information, research on cascade graph mining has attracted increasing attention. A promising approach in this area is Contrastive Cascade Graph Learning (CCGL). One important task in cascade graph mining is cascade classification, which involves categorizing cascade graphs based on their structural characteristics. Although CCGL is expected to be effective for this task, its performance has not yet been thoroughly evaluated. This study aims to investigate the effectiveness of CCGL for cascade classification. Our findings demonstrate the strong performance of CCGL in capturing platform- and model-specific structural patterns in cascade graphs, highlighting its potential for a range of downstream information diffusion analysis tasks.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title>
<link>https://arxiv.org/abs/2507.12108</link>
<guid>https://arxiv.org/abs/2507.12108</guid>
<content:encoded><![CDATA[
<div> multimodal, coordinated behavior, digital ecosystem, detection, online platforms
Summary:<br /><br />The study explores the detection of multimodal coordinated online behavior, comparing monomodal and multimodal approaches. It highlights the trade-off between weakly and strongly integrated models, emphasizing the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. The research assesses the unique contributions of different data modalities and the impact of varying implementations of multimodality on detection outcomes. Findings indicate that a multimodal approach provides a more comprehensive understanding of coordination dynamics. The study enhances the ability to detect and analyze coordinated behavior online, offering new perspectives for safeguarding the integrity of digital platforms. <div>
arXiv:2507.12108v1 Announce Type: new 
Abstract: Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk Detection on Social Media</title>
<link>https://arxiv.org/abs/2507.11559</link>
<guid>https://arxiv.org/abs/2507.11559</guid>
<content:encoded><![CDATA[
<div> Dataset, Suicide risk assessment, Machine learning, Deep learning, Privacy protection
Summary:<br /><br />This paper introduces a large-scale dataset of 15,000 user-level social media posts related to cognitive and mental health (CMH) disorders and suicide risk. The dataset includes complete user posting time sequence information and has undergone rigorous annotations. Various machine learning methods, deep learning models, and large language models were evaluated for automatic assessment of suicide risk using this dataset. Results show promising performance in this task. Privacy protection and ethical use of the dataset were discussed, along with potential applications in mental health testing and clinical psychiatric treatment. The study provides valuable insights for future research in this area. <div>
arXiv:2507.11559v1 Announce Type: cross 
Abstract: In recent years, cognitive and mental health (CMH) disorders have increasingly become an important challenge for global public health, especially the suicide problem caused by multiple factors such as social competition, economic pressure and interpersonal relationships among young and middle-aged people. Social media, as an important platform for individuals to express emotions and seek help, provides the possibility for early detection and intervention of suicide risk. This paper introduces a large-scale dataset containing 15,000 user-level posts. Compared with existing datasets, this dataset retains complete user posting time sequence information, supports modeling the dynamic evolution of suicide risk, and we have also conducted comprehensive and rigorous annotations on these datasets. In the benchmark experiment, we systematically evaluated the performance of traditional machine learning methods, deep learning models, and fine-tuned large language models. The experimental results show that our dataset can effectively support the automatic assessment task of suicide risk. Considering the sensitivity of mental health data, we also discussed the privacy protection and ethical use of the dataset. In addition, we also explored the potential applications of the dataset in mental health testing, clinical psychiatric auxiliary treatment, etc., and provided directional suggestions for future research work.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review and the Diffusion of Ideas</title>
<link>https://arxiv.org/abs/2507.11825</link>
<guid>https://arxiv.org/abs/2507.11825</guid>
<content:encoded><![CDATA[
<div> exposure, peer review, knowledge, citations, scientific knowledge 
Summary: 
The study investigates the role of peer review in exposing reviewers to new ideas, utilizing a natural experiment with over 500,000 peer review invitations. Exposure to a manuscript's core ideas impacts the future referencing behavior and knowledge of reviewers who decline the review invite. Reviewers who view manuscript summaries increase citations to the manuscript and demonstrate enhanced breadth, depth, diversity, and prominence in citing the submitting author's work. This underscores how peer review influences the dissemination of scientific knowledge. Despite debates on the costs and burdens of peer review, the study highlights its role as a powerful engine for idea diffusion on a massive scale, driving scientific advancements and scholarly communication. <div>
arXiv:2507.11825v1 Announce Type: cross 
Abstract: This study examines a fundamental yet overlooked function of peer review: its role in exposing reviewers to new and unexpected ideas. Leveraging a natural experiment involving over half a million peer review invitations covering both accepted and rejected manuscripts, and integrating high-scale bibliographic and editorial records for 37,279 submitting authors, we find that exposure to a manuscript's core ideas significantly influences the future referencing behavior and knowledge of reviewer invitees who decline the review invite. Specifically, declining reviewer invitees who could view concise summaries of the manuscript's core ideas not only increase their citations to the manuscript itself but also demonstrate expanded breadth, depth, diversity, and prominence of citations to the submitting author's broader body of work. Overall, these results suggest peer review substantially influences the spread of scientific knowledge. Ironically, while the massive scale of peer review, entailing millions of reviews annually, often drives policy debates about its costs and burdens, our findings demonstrate that precisely because of this scale, peer review serves as a powerful yet previously unrecognized engine for idea diffusion, which is central to scientific advances and scholarly communication.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freshness, Persistence and Success of Scientific Teams</title>
<link>https://arxiv.org/abs/2507.12255</link>
<guid>https://arxiv.org/abs/2507.12255</guid>
<content:encoded><![CDATA[
<div> team science, academic teams, success, collaboration, network-driven approach <br /> 
Summary: <br /> 
The study explores the factors contributing to the success of academic teams in scientific knowledge production. By analyzing data on publications and authors, the research highlights the importance of team freshness and persistence in team success. Contrary to popular belief, success is not solely driven by team persistence but also by the freshness of new collaborations built on prior experience. The study suggests that high-impact research tends to emerge early in a team's lifespan, emphasizing the significance of new collaborative ties. Teams open to new collaborations consistently produce better science, and team re-combinations introducing freshness impulses sustain success. Additionally, experienced teams with persistence impulses are linked to earlier impact. Together, the balance of freshness and persistence influences team success at different stages of collaboration. <div>
arXiv:2507.12255v1 Announce Type: cross 
Abstract: Team science dominates scientific knowledge production, but what makes academic teams successful? Using temporal data on 25.2 million publications and 31.8 million authors, we propose a novel network-driven approach to identify and study the success of persistent teams. Challenging the idea that persistence alone drives success, we find that team freshness - new collaborations built on prior experience - is key to success. High impact research tends to emerge early in a team's lifespan. Analyzing complex team overlap, we find that teams open to new collaborative ties consistently produce better science. Specifically, team re-combinations that introduce new freshness impulses sustain success, while persistence impulses from experienced teams are linked to earlier impact. Together, freshness and persistence shape team success across collaboration stages.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fediverse Sharing: Cross-Platform Interaction Dynamics between Threads and Mastodon Users</title>
<link>https://arxiv.org/abs/2502.17926</link>
<guid>https://arxiv.org/abs/2502.17926</guid>
<content:encoded><![CDATA[
<div> federation, social media platforms, Mastodon, Threads, cross-platform interactions <br />
Summary:
Traditional social media platforms are facing criticism, leading users to seek alternatives like Mastodon and Threads. However, this diversification has caused user dispersion. Federation protocols like ActivityPub have been adopted to address these issues, with Mastodon taking the lead in building decentralized networks. Threads joined the federation in March 2024 with its Fediverse Sharing service, enabling interactions between Threads and Mastodon users. A study of interactions between 20,000+ Threads users and 20,000+ Mastodon users over ten months was conducted. This research lays the groundwork for further exploration of cross-platform interactions and integration driven by federation protocols. <br /><br /> <div>
arXiv:2502.17926v2 Announce Type: replace 
Abstract: Traditional social media platforms, once envisioned as digital town squares, now face growing criticism over corporate control, content moderation, and privacy concerns. Events such as Twitter's acquisition (now X) and major policy changes have pushed users toward alternative platforms like Mastodon and Threads. However, this diversification has led to user dispersion and fragmented discussions across the walled gardens of social media platforms. To address these issues, federation protocols like ActivityPub have been adopted, with Mastodon leading efforts to build decentralized yet interconnected networks. In March 2024, Threads joined this federation by introducing its Fediverse Sharing service, which enables interactions such as posts, replies, and likes between Threads and Mastodon users as if on a unified platform. Building on this development, we study the interactions between 20,000+ Threads users and 20,000+ Mastodon users over a ten-month period. Our work lays the foundation for research on cross-platform interactions and federation-driven platform integration.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Clustering in Hypergraphs through Higher-Order Motifs</title>
<link>https://arxiv.org/abs/2507.10570</link>
<guid>https://arxiv.org/abs/2507.10570</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering, higher-order motifs, local structures, computational efficiency

Summary:
This article introduces a novel approach for local clustering in hypergraphs by utilizing higher-order motifs. Traditional graph-based clustering methods often fail to capture higher-order interactions, leading to subpar clustering results. The proposed method leverages hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. Two strategies are presented for identifying local clusters around a seed hyperedge: a core-based approach using hypergraph core decomposition and a BFS-based method involving breadth-first exploration. An auxiliary hypergraph is constructed to facilitate efficient partitioning, along with a framework for local motif-based clustering. Extensive experiments on real-world datasets showcase the effectiveness of the framework, offering a comparative analysis of the two clustering strategies in terms of clustering quality and computational efficiency.
<br /><br />Summary: <div>
arXiv:2507.10570v1 Announce Type: new 
Abstract: Hypergraphs provide a powerful framework for modeling complex systems and networks with higher-order interactions beyond simple pairwise relationships. However, graph-based clustering approaches, which focus primarily on pairwise relations, fail to represent higher-order interactions, often resulting in low-quality clustering outcomes. In this work, we introduce a novel approach for local clustering in hypergraphs based on higher-order motifs, small connected subgraphs in which nodes may be linked by interactions of any order, extending motif-based techniques previously applied to standard graphs. Our method exploits hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. We propose two alternative strategies for identifying local clusters around a seed hyperedge: a core-based method utilizing hypergraph core decomposition and a BFS-based method based on breadth-first exploration. We construct an auxiliary hypergraph to facilitate efficient partitioning and introduce a framework for local motif-based clustering. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework and provide a comparative analysis of the two proposed clustering strategies in terms of clustering quality and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns</title>
<link>https://arxiv.org/abs/2507.10608</link>
<guid>https://arxiv.org/abs/2507.10608</guid>
<content:encoded><![CDATA[
<div> Keywords: anti-money laundering, network theory, behavioral consistency, laundering patterns, pattern fragility

Summary:
The article challenges the conventional approach to anti-money laundering systems by proposing a network-theoretic perspective that focuses on detecting predefined laundering patterns in transaction networks. It emphasizes the importance of behavioral consistency in identifying money laundering activities and argues that patterns are better captured through subgraph structures expressing semantic and functional roles. The concept of pattern fragility is introduced, highlighting the sensitivity of laundering patterns to small attribute changes and their semantic robustness under significant topological transformations. The article suggests that detecting money laundering should not rely on statistical outliers but on the preservation of behavioral essence. This philosophical shift in approach has significant implications for how anti-money laundering systems model, scan, and interpret networks to combat financial crime.

<br /><br />Summary: <div>
arXiv:2507.10608v1 Announce Type: new 
Abstract: Conventional anti-money laundering (AML) systems predominantly focus on identifying anomalous entities or transactions, flagging them for manual investigation based on statistical deviation or suspicious behavior. This paradigm, however, misconstrues the true nature of money laundering, which is rarely anomalous but often deliberate, repeated, and concealed within consistent behavioral routines. In this paper, we challenge the entity-centric approach and propose a network-theoretic perspective that emphasizes detecting predefined laundering patterns across directed transaction networks. We introduce the notion of behavioral consistency as the core trait of laundering activity, and argue that such patterns are better captured through subgraph structures expressing semantic and functional roles - not solely geometry. Crucially, we explore the concept of pattern fragility: the sensitivity of laundering patterns to small attribute changes and, conversely, their semantic robustness even under drastic topological transformations. We claim that laundering detection should not hinge on statistical outliers, but on preservation of behavioral essence, and propose a reconceptualization of pattern similarity grounded in this insight. This philosophical and practical shift has implications for how AML systems model, scan, and interpret networks in the fight against financial crime.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilayer Artificial Benchmark for Community Detection (mABCD)</title>
<link>https://arxiv.org/abs/2507.10795</link>
<guid>https://arxiv.org/abs/2507.10795</guid>
<content:encoded><![CDATA[
<div> random graph model, community detection, power-law distribution, LFR model, multilayer networks

Summary:
The article introduces the Artificial Benchmark for Community Detection (ABCD) model, a random graph model that incorporates community structure and power-law distribution for both degrees and community sizes. This model, similar to the LFR model but faster and more analytically accessible, is used to generate graphs. The article then proposes a variant of the ABCD model for multilayer networks, known as mABCD. This variant leverages the underlying elements of the ABCD model to address community detection in multilayer networks. The mABCD model offers interpretability and efficiency in generating multilayer networks with community structure and power-law distribution. Overall, the introduction of the mABCD model extends the applicability of the ABCD model to the realm of multilayer networks, providing a valuable tool for community detection research. 

<br /><br />Summary: <div>
arXiv:2507.10795v1 Announce Type: new 
Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model and introduce its variant for multilayer networks, mABCD.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxicity in State Sponsored Information Operations</title>
<link>https://arxiv.org/abs/2507.10936</link>
<guid>https://arxiv.org/abs/2507.10936</guid>
<content:encoded><![CDATA[
<div> Keywords: state-sponsored information operations, toxic language, social media, engagement metrics, Russian influence operations

Summary: 
This study analyzes toxic language deployment in state-sponsored information operations on social media platforms, specifically focusing on 56 million posts from over 42 thousand accounts linked to 18 geopolitical entities on Twitter. Six categories of toxic content were systematically detected and quantified using Google's Perspective API, revealing that toxic content only makes up 1.53% of all posts but garners high engagement. The distribution of toxic content varied across national origins and linguistic structures, indicating strategic deployment in specific geopolitical contexts. Russian influence operations stood out for receiving significantly higher user engagement compared to other countries in the dataset. The findings provide valuable insights into the emotional and rhetorical strategies used in state-sponsored information operations on social media platforms and highlight the importance of understanding these patterns for effective countermeasures. 

Summary:<br />
1. Analysis of toxic language deployment in state-sponsored information operations on social media platforms<br />
2. Detection and quantification of six categories of toxic content from 56 million posts<br />
3. Toxic content comprises 1.53% of all posts but shows high engagement rates<br />
4. Strategic deployment of toxic content in specific geopolitical contexts<br />
5. Russian influence operations garner significantly higher user engagement compared to other countries.<br /> <div>
arXiv:2507.10936v1 Announce Type: new 
Abstract: State-sponsored information operations (IOs) increasingly influence global discourse on social media platforms, yet their emotional and rhetorical strategies remain inadequately characterized in scientific literature. This study presents the first comprehensive analysis of toxic language deployment within such campaigns, examining 56 million posts from over 42 thousand accounts linked to 18 distinct geopolitical entities on X/Twitter. Using Google's Perspective API, we systematically detect and quantify six categories of toxic content and analyze their distribution across national origins, linguistic structures, and engagement metrics, providing essential information regarding the underlying patterns of such operations. Our findings reveal that while toxic content constitutes only 1.53% of all posts, they are associated with disproportionately high engagement and appear to be strategically deployed in specific geopolitical contexts. Notably, toxic content originating from Russian influence operations receives significantly higher user engagement compared to influence operations from any other country in our dataset. Our code is available at https://github.com/shafin191/Toxic_IO.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban delineation through the lens of commute networks: Leveraging graph embeddings to distinguish socioeconomic groups in cities</title>
<link>https://arxiv.org/abs/2507.11057</link>
<guid>https://arxiv.org/abs/2507.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: urban delineation, commute networks, Graph Neural Network, community detection, socioeconomic disparities 

Summary: 
This study focuses on using commute networks from census data to delineate urban areas, utilizing a Graph Neural Network (GNN) model to create low-dimensional representations of urban nodes. The nodes' embeddings are clustered to identify cohesive communities within urban regions. The research demonstrates the efficacy of network embeddings in capturing socioeconomic disparities, particularly in median household income, across various U.S. cities. The role of census mobility data in regional delineation is highlighted, showcasing the utility of GNNs in urban community detection as a powerful alternative to existing methods. The results emphasize the valuable insights provided by commute networks in shaping meaningful representations of urban regions. 

<br /><br />Summary: <div>
arXiv:2507.11057v1 Announce Type: new 
Abstract: Delineating areas within metropolitan regions stands as an important focus among urban researchers, shedding light on the urban perimeters shaped by evolving population dynamics. Applications to urban science are numerous, from facilitating comparisons between delineated districts and administrative divisions to informing policymakers of the shifting economic and labor landscapes. In this study, we propose using commute networks sourced from the census for the purpose of urban delineation, by modeling them with a Graph Neural Network (GNN) architecture. We derive low-dimensional representations of granular urban areas (nodes) using GNNs. Subsequently, nodes' embeddings are clustered to identify spatially cohesive communities in urban areas. Our experiments across the U.S. demonstrate the effectiveness of network embeddings in capturing significant socioeconomic disparities between communities in various cities, particularly in factors such as median household income. The role of census mobility data in regional delineation is also noted, and we establish the utility of GNNs in urban community detection, as a powerful alternative to existing methods in this domain. The results offer insights into the wider effects of commute networks and their use in building meaningful representations of urban regions.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance Stability of Network by Edge Anchor</title>
<link>https://arxiv.org/abs/2507.11090</link>
<guid>https://arxiv.org/abs/2507.11090</guid>
<content:encoded><![CDATA[
<div> influential relationships, community stability, anchor trussness reinforcement problem, greedy framework, real-world networks 

Summary: 
- The study focuses on identifying influential relationships in online social networks to enhance community stability.
- The anchor trussness reinforcement problem is introduced to boost user engagement by anchoring specific edges.
- The problem of identifying edges for maximum trussness gain while staying within a given budget is proven to be NP-hard.
- A greedy framework is proposed to iteratively select the best edges for reinforcement, with a focus on efficiency.
- Methods such as the upward-route approach and classification tree structure are introduced to efficiently compute trussness gain and minimize redundant computations in the process.
- Extensive experiments on real-world networks validate the efficiency and effectiveness of the proposed model and methods. <div>
arXiv:2507.11090v1 Announce Type: new 
Abstract: With the rapid growth of online social networks, strengthening their stability has emerged as a key research focus. This study aims to identify influential relationships that significantly impact community stability. In this paper, we introduce and explore the anchor trussness reinforcement problem to reinforce the overall user engagement of networks by anchoring some edges. Specifically, for a given graph $G$ and a budget $b$, we aim to identify $b$ edges whose anchoring maximizes the trussness gain, which is the cumulative increment of trussness across all edges in $G$. We establish the NP-hardness of the problem. To address this problem, we introduce a greedy framework that iteratively selects the current best edge. To scale for larger networks, we first propose an upward-route method to constrain potential trussness increment edges. Augmented with a support check strategy, this approach enables the efficient computation of the trussness gain for anchoring one edge. Then, we design a classification tree structure to minimize redundant computations in each iteration by organizing edges based on their trussness. We conduct extensive experiments on 8 real-world networks to validate the efficiency and effectiveness of the proposed model and methods.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services</title>
<link>https://arxiv.org/abs/2507.10605</link>
<guid>https://arxiv.org/abs/2507.10605</guid>
<content:encoded><![CDATA[
<div> Keywords: social networking services, large language models, RedOne, content management, interaction quality improvement 

Summary:
RedOne is a domain-specific large language model (LLM) developed to address the challenges faced by social networking services (SNS) in content management and interaction quality improvement. It utilizes a three-stage training strategy involving continue pretraining, supervised fine-tuning, and preference optimization using a large-scale real-world dataset. Through extensive experiments, RedOne demonstrates strong capabilities, outperforming base models by up to 14.02% in SNS tasks and 7.56% in a bilingual evaluation benchmark. In online testing, RedOne reduces exposure to harmful content by 11.23% and improves post-view search click rates by 14.95% compared to baseline models. These results establish RedOne as a robust LLM for SNS, showcasing its versatility across various tasks and potential for real-world applications. 

<br /><br />Summary: <div>
arXiv:2507.10605v1 Announce Type: cross 
Abstract: As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler</title>
<link>https://arxiv.org/abs/2507.10810</link>
<guid>https://arxiv.org/abs/2507.10810</guid>
<content:encoded><![CDATA[
<div> Keywords: online hate, social approval theory, Parler, hate speech, social media platforms <br />
Summary: 
This paper explores the relationship between social approval and online hate speech on the Parler platform from 2018 to 2021. The study specifically looks at whether receiving social approval influences the production and extremity of hate speech messages. Contrary to Walther's social approval theory, the study found that the number of upvotes on hate speech posts did not predict future hate speech production. Between-person effects showed a mixed relationship between social approval and hate speech at different time intervals. The findings suggest that social approval reinforcement mechanisms for online hate may vary on niche social media platforms like Parler. Overall, the study challenges existing theories on the role of social approval in motivating online hate speech production. <br /><br /> <div>
arXiv:2507.10810v1 Announce Type: cross 
Abstract: In this paper, we explored how online hate is motivated by receiving social approval from others. We specifically examined two central tenets of Walther's (2024) social approval theory of online hate: (H1a) more signals of social approval on hate messages predicts more subsequent hate messages, and (H1b) as social approval increases, hate speech messages become more extreme. Using over 110 million posts from Parler (2018-2021), we observed that the number of upvotes a person received on a hate speech post was unassociated with the amount of hate speech in their next post and posts during the next week, month, three months, and six months. Between-person effects revealed an average negative relationship between social approval and hate speech production at the post level, but this relationship was mixed at other time intervals. Social approval reinforcement mechanisms of online hate may operate differently on niche social media platforms.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Potential Impact of Disruptive AI Innovations on U.S. Occupations</title>
<link>https://arxiv.org/abs/2507.11403</link>
<guid>https://arxiv.org/abs/2507.11403</guid>
<content:encoded><![CDATA[
<div> disruption index, AI patents, job tasks, consolidating AI, disruptive AI
<br />
Summary:<br />
The study examines the impact of AI innovations on the labor market through a disruption index of U.S. AI patents. It distinguishes between consolidating AI that reinforces existing structures and disruptive AI that alters them. The analysis reveals that consolidating AI targets physical, routine, and solo tasks in manufacturing and construction, mainly in the Midwest and central states. On the other hand, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, disruptive AI disproportionately affects areas with skilled labor shortages, potentially accelerating change where workers are scarce. Overall, consolidating AI extends current automation trends, while disruptive AI is poised to transform complex mental work, with an exception for collaborative tasks. <div>
arXiv:2507.11403v1 Announce Type: cross 
Abstract: The rapid rise of AI is poised to disrupt the labor market. However, AI is not a monolith; its impact depends on both the nature of the innovation and the jobs it affects. While computational approaches are emerging, there is no consensus on how to systematically measure an innovation's disruptive potential. Here, we calculate the disruption index of 3,237 U.S. AI patents (2015-2022) and link them to job tasks to distinguish between "consolidating" AI innovations that reinforce existing structures and "disruptive" AI innovations that alter them. Our analysis reveals that consolidating AI primarily targets physical, routine, and solo tasks, common in manufacturing and construction in the Midwest and central states. By contrast, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, we also find that disruptive AI disproportionately affects areas already facing skilled labor shortages, suggesting disruptive AI technologies may accelerate change where workers are scarce rather than replacing a surplus. Ultimately, consolidating AI appears to extend current automation trends, while disruptive AI is set to transform complex mental work, with a notable exception for collaborative tasks.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIF: The hypergraph interchange format for higher-order networks</title>
<link>https://arxiv.org/abs/2507.11520</link>
<guid>https://arxiv.org/abs/2507.11520</guid>
<content:encoded><![CDATA[
<div> Hypergraph, Interchange Format, Higher-order networks, Software packages, Data exchange<br />
Summary: <br />
The article introduces the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. It aims to bring together various software packages used for analyzing complex interactions in systems such as chemical reactions, social groups, and ecological dependencies. HIF supports different types of higher-order networks, including undirected and directed hypergraphs, as well as simplicial complexes. It also includes metadata support for attributes associated with nodes, edges, and incidences. The initiative is a collaborative effort involving authors and contributors from prominent hypergraph software packages. The project provides a JSON schema, documentation, unit tests, example datasets, and tutorials demonstrating HIF's compatibility with popular higher-order network analysis software packages. This standardization of data format will facilitate seamless data exchange and enhance the interoperability of different higher-order network analysis tools. <br /> 
Summary: <div>
arXiv:2507.11520v1 Announce Type: cross 
Abstract: Many empirical systems contain complex interactions of arbitrary size, representing, for example, chemical reactions, social groups, co-authorship relationships, and ecological dependencies. These interactions are known as higher-order interactions and the collection of these interactions comprise a higher-order network, or hypergraph. Hypergraphs have established themselves as a popular and versatile mathematical representation of such systems and a number of software packages written in various programming languages have been designed to analyze these networks. However, the ecosystem of higher-order network analysis software is fragmented due to specialization of each software's programming interface and compatible data representations. To enable seamless data exchange between higher-order network analysis software packages, we introduce the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. HIF supports multiple types of higher-order networks, including undirected hypergraphs, directed hypergraphs, and simplicial complexes, while actively exploring extensions to represent multiplex hypergraphs, temporal hypergraphs, and ordered hypergraphs. To accommodate the wide variety of metadata used in different contexts, HIF also includes support for attributes associated with nodes, edges, and incidences. This initiative is a collaborative effort involving authors, maintainers, and contributors from prominent hypergraph software packages. This project introduces a JSON schema with corresponding documentation and unit tests, example HIF-compliant datasets, and tutorials demonstrating the use of HIF with several popular higher-order network analysis software packages.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-based models to randomize real-world hypergraphs</title>
<link>https://arxiv.org/abs/2207.12123</link>
<guid>https://arxiv.org/abs/2207.12123</guid>
<content:encoded><![CDATA[
<div> Keywords: Network theory, hypergraphs, Exponential Random Hypergraphs, entropy-based approach, real-world data <br />
Summary: 
The article focuses on the importance of considering many-body relationships in network theory through the use of hypergraphs to capture polyadic interactions. By using the representation of hypergraphs based on incidence matrices, the study introduces Exponential Random Hypergraphs (ERHs) as a framework to analyze higher-order structures. The research explores the thresholds and asymptotic behavior of ERHs in comparison to traditional percolation thresholds. Key network metrics are generalized to hypergraphs, allowing for the computation of expected values and comparison against empirical data to identify deviations from random behaviors. The method presented is analytically tractable, scalable, and effective in detecting structural patterns in real-world hypergraphs that differ significantly from simpler constraints. <div>
arXiv:2207.12123v3 Announce Type: replace 
Abstract: Network theory has often disregarded many-body relationships, solely focusing on pairwise interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent a suitable framework for describing polyadic interactions. Here, we leverage the representation of hypergraphs based on the incidence matrix for extending the entropy-based approach to higher-order structures: in analogy with the Exponential Random Graphs, we introduce the Exponential Random Hypergraphs (ERHs). After exploring the asymptotic behaviour of thresholds generalising the percolation one, we apply ERHs to study real-world data. First, we generalise key network metrics to hypergraphs; then, we compute their expected value and compare it with the empirical one, in order to detect deviations from random behaviours. Our method is analytically tractable, scalable and capable of revealing structural patterns of real-world hypergraphs that differ significantly from those emerging as a consequence of simpler constraints.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verified authors shape X/Twitter discursive communities</title>
<link>https://arxiv.org/abs/2405.04896</link>
<guid>https://arxiv.org/abs/2405.04896</guid>
<content:encoded><![CDATA[
<div> Keywords: discursive communities, Twitter, verified users, political events, community detection 

Summary: 
The study investigates the detection of discursive communities on Twitter during key Italian political events in 2022. It focuses on the role of verified users as content creators before paid account verification was introduced. Two novel methodologies, MonoDC and BiDC, were proposed and compared to identify community partitions based on the retweet network and shared audience similarity. Leveraging verified users as indicators of prestige and authority resulted in clear community partitions reflecting actual political affiliations. This approach outperformed standard algorithms applied to the entire retweet network. The study highlights the significant influence of verified users in shaping online discourse, emphasizing the importance of platform governance, especially in light of recent changes to paid verification. <div>
arXiv:2405.04896v2 Announce Type: replace 
Abstract: In this study, we address the challenge of detecting ``discursive communities'' on X/Twitter by focusing on the role of verified users as the main content creators in online political debates. The analysis centers on three major Italian political events in 2022 - the Presidential election, a governmental crisis, and the general elections - occurring before the introduction of paid account verification. We propose and compare two novel methodologies, MonoDC and BiDC, which exploit, respectively, the retweet network among users and a similarity network based on shared audiences, while integrating a maximum entropy null model to filter out the inherent noise in online social networks. Our results demonstrate that leveraging verified users-considered as indicators of prestige and authority-leads to significantly clear community partitions that closely reflect the actual political affiliations, outperforming standard community detection algorithms applied to the entire retweet network. Moreover, the comparison of different methodologies and user sets suggests that the status conferred by the blue verification tick plays a dominant role in shaping online discourse, with important implications for platform governance, especially in light of the recent shift to paid verification.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd: A Social Network Simulation Framework</title>
<link>https://arxiv.org/abs/2412.10781</link>
<guid>https://arxiv.org/abs/2412.10781</guid>
<content:encoded><![CDATA[
<div> Agent-based modeling, social networks, simulation, Crowd, Python
<br />
Summary:
Crowd is a social network simulator that utilizes agent-based modeling to simulate real-world phenomena within a network environment. It offers easy setup through YAML configuration, customization options, and features like no-code simulations, interactive visualizations, and data aggregation. Developed in Python, Crowd supports generative agents and integrates easily with data analysis and machine learning libraries. The framework is demonstrated through three case studies showcasing its application in modeling generative agents in epidemics, influence maximization, and networked trust games. Crowd addresses the limitations of general-purpose ABMS frameworks by providing specialized tools for social networks and simplifying complex simulations. <div>
arXiv:2412.10781v3 Announce Type: replace 
Abstract: To observe how individual behavior shapes a larger community's actions, agent-based modeling and simulation (ABMS) has been widely adopted by researchers in social sciences, economics, and epidemiology. While simulations can be run on general-purpose ABMS frameworks, these tools are not specifically designed for social networks and, therefore, provide limited features, increasing the effort required for complex simulations. In this paper, we introduce Crowd, a social network simulator that adopts the agent-based modeling methodology to model real-world phenomena within a network environment. Designed to facilitate easy and quick modeling, Crowd supports simulation setup through YAML configuration and enables further customization with user-defined methods. Other features include no-code simulations for diffusion tasks, interactive visualizations, data aggregation, and chart drawing facilities. Designed in Python, Crowd also supports generative agents and connects easily with Python's libraries for data analysis and machine learning. Finally, we include three case studies to illustrate the use of the framework, including generative agents in epidemics, influence maximization, and networked trust games.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Dynamics in Online Public Discourse: A Case Study of Universal Basic Income Discussions on Reddit</title>
<link>https://arxiv.org/abs/2312.09611</link>
<guid>https://arxiv.org/abs/2312.09611</guid>
<content:encoded><![CDATA[
<div> opinion change, online platforms, Universal Basic Income (UBI), Reddit, public discourse
<br />
Summary:
The study explores how public opinion on Universal Basic Income (UBI) has changed on Reddit, an online platform. By analyzing shifts in user cohorts, community affluence levels, and partisan leanings, the researchers found that support for UBI on Reddit initially declined but saw a significant increase starting in mid-2019. The study highlights the potential of online platforms to capture nuanced shifts in public opinion and offers a conceptual model to understand opinion change at a large scale. The findings illustrate the dynamic nature of public discourse and the impact of online discussions on shaping societal attitudes towards policy proposals. This research can be applied to other important issues and policies to gain insights into the diverse range of opinions within a heterogeneous population.
<br /> <div>
arXiv:2312.09611v2 Announce Type: replace-cross 
Abstract: Societal change is often driven by shifts in public opinion. As citizens evolve in their norms, beliefs, and values, public policies change too. While traditional opinion polling and surveys can outline the broad strokes of whether public opinion on a particular topic is changing, they usually cannot capture the full multi-dimensional richness and diversity of opinion present in a large heterogeneous population. However, an increasing fraction of public discourse about public policy issues is now occurring on online platforms, which presents an opportunity to measure public opinion change at a qualitatively different scale of resolution and context.
  In this paper, we present a conceptual model of observed opinion change on online platforms and apply it to study public discourse on Universal Basic Income (UBI) on Reddit throughout its history. UBI is a periodic, no-strings-attached cash payment given to every citizen of a population. We study UBI as it is a clearly-defined policy proposal that has recently experienced a surge of interest through trends like automation and events like the COVID-19 pandemic. We find that overall stance towards UBI on Reddit significantly declined until mid-2019, when this historical trend suddenly reversed and Reddit became substantially more supportive. Using our model, we find the most significant drivers of this overall stance change were shifts within different user cohorts, within communities that represented similar affluence levels, and within communities that represented similar partisan leanings. Our method identifies nuanced social drivers of opinion change in the large-scale public discourse that now regularly occurs online, and could be applied to a broad set of other important issues and policies.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks</title>
<link>https://arxiv.org/abs/2507.09055</link>
<guid>https://arxiv.org/abs/2507.09055</guid>
<content:encoded><![CDATA[
<div> influence centrality, health misinformation vulnerability centrality, propagation centrality, online social networks, misinformation influencers 

Summary: 
- The study introduces three novel centrality metrics, DIC, MVC, and PC, to understand information flow in online social networks during crises like the COVID-19 pandemic.
- These advanced metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions to identify influential nodes, propagation pathways, and misinformation influencers.
- Comparison with traditional metrics showed a 44.83% increase in the identification of influential nodes using the new metrics.
- Implementing interventions based on the new metrics led to a 25% improvement in reducing health misinformation spread compared to baseline interventions.
- The validation on a different dataset confirmed the generalizability of the advanced metrics in identifying influential actors in diverse health misinformation discussions beyond COVID-19. 

<br /><br /> <div>
arXiv:2507.09055v1 Announce Type: new 
Abstract: The rapid spread of health misinformation on online social networks (OSNs) during global crises such as the COVID-19 pandemic poses challenges to public health, social stability, and institutional trust. Centrality metrics have long been pivotal in understanding the dynamics of information flow, particularly in the context of health misinformation. However, the increasing complexity and dynamism of online networks, especially during crises, highlight the limitations of these traditional approaches. This study introduces and compares three novel centrality metrics: dynamic influence centrality (DIC), health misinformation vulnerability centrality (MVC), and propagation centrality (PC). These metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions. Using the FibVID dataset, we compared traditional and novel metrics to identify influential nodes, propagation pathways, and misinformation influencers. Traditional metrics identified 29 influential nodes, while the new metrics uncovered 24 unique nodes, resulting in 42 combined nodes, an increase of 44.83%. Baseline interventions reduced health misinformation by 50%, while incorporating the new metrics increased this to 62.5%, an improvement of 25%. To evaluate the broader applicability of the proposed metrics, we validated our framework on a second dataset, Monant Medical Misinformation, which covers a diverse range of health misinformation discussions beyond COVID-19. The results confirmed that the advanced metrics generalised successfully, identifying distinct influential actors not captured by traditional methods. In general, the findings suggest that a combination of traditional and novel centrality measures offers a more robust and generalisable framework for understanding and mitigating the spread of health misinformation in different online network contexts.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)</title>
<link>https://arxiv.org/abs/2507.09149</link>
<guid>https://arxiv.org/abs/2507.09149</guid>
<content:encoded><![CDATA[
<div> ELM, misinformation detection, CNN, LSTM, social media <br />
Summary: <br />
- Study uses ELM to improve detection of health misinformation on social media.
- Model combines CNN and LSTM to enhance accuracy and reliability of classification.
- Integrates ELM-based features like text readability and sentiment polarity.
- Achieves high performance metrics: accuracy 97.37%, F1-score 97.41%.
- Feature engineering further boosts precision, recall, and ROC-AUC metrics.
- Demonstrates practical application of psychological theories in ML algorithms for misinformation detection.<br /> <div>
arXiv:2507.09149v1 Announce Type: new 
Abstract: Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks</title>
<link>https://arxiv.org/abs/2507.09657</link>
<guid>https://arxiv.org/abs/2507.09657</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, social network, temperature decisions, personality traits, happiness

Summary:
This study utilizes generative agents powered by large language models to simulate a social network within a residential building, where they make temperature decisions for a central heating system. The agents, categorized as Family Members and Representatives, take into account personal preferences, traits, connections, and weather conditions. Daily simulations involve reaching consensus at the family level before making building-wide decisions among representatives. The research explores three distributions of personality traits (positive, mixed, and negative) and establishes a correlation between positive traits and increased happiness and stronger friendships. Results show that temperature preferences, assertiveness, and selflessness play a vital role in both happiness and decision-making processes. Overall, this work highlights the effectiveness of using LLM-driven agents to model complex human behavior in scenarios where real-life simulations are challenging. 

<br /><br />Summary: <div>
arXiv:2507.09657v1 Announce Type: new 
Abstract: We use generative agents powered by large language models (LLMs) to simulate a social network in a shared residential building, driving the temperature decisions for a central heating system. Agents, divided into Family Members and Representatives, consider personal preferences, personal traits, connections, and weather conditions. Daily simulations involve family-level consensus followed by building-wide decisions among representatives. We tested three personality traits distributions (positive, mixed, and negative) and found that positive traits correlate with higher happiness and stronger friendships. Temperature preferences, assertiveness, and selflessness have a significant impact on happiness and decisions. This work demonstrates how LLM-driven agents can help simulate nuanced human behavior where complex real-life human simulations are difficult to set.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Analysis and Evaluation of Cohesive Subgraph Discovery</title>
<link>https://arxiv.org/abs/2507.10262</link>
<guid>https://arxiv.org/abs/2507.10262</guid>
<content:encoded><![CDATA[
<div> cohesive subgraphs, networks, social network analysis, graph data management, evaluation

Summary: 
This study evaluates different cohesive subgraph models for their performance in retrieving cohesive subgraphs in networks. The research includes task-based evaluations on synthetic and real-world networks to provide insights into the efficiency and applicability of these models. The findings highlight the balance between interpretability and cohesion of subgraphs, guiding the selection of suitable models for specific analytical needs and applications. The study not only offers a comprehensive evaluation of current models but also sets the foundation for future research in this area. The systematic comparison of performance across varied network configurations fills a gap in the existing literature and provides valuable insights for marketers and recommendation systems. This research contributes to advancing the understanding of cohesive subgraphs in network analysis. 

<br /><br />Summary: <div>
arXiv:2507.10262v1 Announce Type: new 
Abstract: Retrieving cohesive subgraphs in networks is a fundamental problem in social network analysis and graph data management. These subgraphs can be used for marketing strategies or recommendation systems. Despite the introduction of numerous models over the years, a systematic comparison of their performance, especially across varied network configurations, remains unexplored. In this study, we evaluated various cohesive subgraph models using task-based evaluations and conducted extensive experimental studies on both synthetic and real-world networks. Thus, we unveil the characteristics of cohesive subgraph models, highlighting their efficiency and applicability. Our findings not only provide a detailed evaluation of current models but also lay the groundwork for future research by shedding light on the balance between the interpretability and cohesion of the subgraphs. This research guides the selection of suitable models for specific analytical needs and applications, providing valuable insights.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowsDT: A Geospatial Digital Twin for Navigating Urban Flood Dynamics</title>
<link>https://arxiv.org/abs/2507.08850</link>
<guid>https://arxiv.org/abs/2507.08850</guid>
<content:encoded><![CDATA[
<div> Keywords: flood analysis, hydrodynamic modeling, digital twin, urban resilience, Galveston City

Summary:
The study focuses on using advanced hydrodynamic modeling and digital twin technology to enhance flood analysis and forecasting in Galveston City. By developing a geospatial digital twin (GDT) and validating it with historical data, the researchers were able to model hyperlocal flood conditions under various rainfall scenarios. The model, FlowsDT-Galveston, accurately simulates flood depth, extent, duration, and velocity in a 4-D environment, identifying at-risk zones in the city. Results show an increase in building and road inundations with higher return period rainfall scenarios. This innovative approach can support proactive flood management and urban planning in Galveston, informing disaster resilience efforts and guiding sustainable infrastructure development. The framework established in this study can be applied to other communities facing similar flood hazard challenges.<br /><br />Summary: <div>
arXiv:2507.08850v1 Announce Type: cross 
Abstract: Communities worldwide increasingly confront flood hazards intensified by climate change, urban expansion, and environmental degradation. Addressing these challenges requires real-time flood analysis, precise flood forecasting, and robust risk communications with stakeholders to implement efficient mitigation strategies. Recent advances in hydrodynamic modeling and digital twins afford new opportunities for high-resolution flood modeling and visualization at the street and basement levels. Focusing on Galveston City, a barrier island in Texas, U.S., this study created a geospatial digital twin (GDT) supported by 1D-2D coupled hydrodynamic models to strengthen urban resilience to pluvial and fluvial flooding. The objectives include: (1) developing a GDT (FlowsDT-Galveston) incorporating topography, hydrography, and infrastructure; (2) validating the twin using historical flood events and social sensing; (3) modeling hyperlocal flood conditions under 2-, 10-, 25-, 50-, and 100-year return period rainfall scenarios; and (4) identifying at-risk zones under different scenarios. This study employs the PCSWMM to create dynamic virtual replicas of urban landscapes and accurate flood modeling. By integrating LiDAR data, land cover, and storm sewer geometries, the model can simulate flood depth, extent, duration, and velocity in a 4-D environment across different historical and design storms. Results show buildings inundated over one foot increased by 5.7% from 2- to 100-year flood. Road inundations above 1 foot increased by 6.7% from 2- to 100-year floods. The proposed model can support proactive flood management and urban planning in Galveston; and inform disaster resilience efforts and guide sustainable infrastructure development. The framework can be extended to other communities facing similar challenges.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions</title>
<link>https://arxiv.org/abs/2507.08881</link>
<guid>https://arxiv.org/abs/2507.08881</guid>
<content:encoded><![CDATA[
<div> consistency, acceptability, large language model, judicial systems, governance framework
Summary: 
The article discusses the integration of large language models (LLM) in judicial systems and highlights the emerging paradox of "consistency-acceptability divergence." This refers to the gap between the technical consistency of LLMs and their social acceptance. The study analyzes data from 2023-2025 and identifies the need to balance technical efficiency with social legitimacy in LLM judicial applications. The proposed Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF) aims to address this challenge by enabling intelligent task classification and fostering meaningful interactions among stakeholders. By understanding both the task and stakeholder dimensions, this framework provides theoretical insights and practical guidance for building a more balanced and effective LLM judicial ecosystem. <br /><br /> <div>
arXiv:2507.08881v1 Announce Type: cross 
Abstract: The integration of large language model (LLM) technology into judicial systems is fundamentally transforming legal practice worldwide. However, this global transformation has revealed an urgent paradox requiring immediate attention. This study introduces the concept of ``consistency-acceptability divergence'' for the first time, referring to the gap between technical consistency and social acceptance. While LLMs achieve high consistency at the technical level, this consistency demonstrates both positive and negative effects. Through comprehensive analysis of recent data on LLM judicial applications from 2023--2025, this study finds that addressing this challenge requires understanding both task and stakeholder dimensions. This study proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF), which enables intelligent task classification and meaningful interaction among diverse stakeholders. This framework offers both theoretical insights and practical guidance for building an LLM judicial ecosystem that balances technical efficiency with social legitimacy.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Te Ahorr\'e Un Click: A Revised Definition of Clickbait and Detection in Spanish News</title>
<link>https://arxiv.org/abs/2507.09777</link>
<guid>https://arxiv.org/abs/2507.09777</guid>
<content:encoded><![CDATA[
<div> clickbait, curiosity gap, sensationalism, headlines, clickbait detection

Summary:
- The definition of clickbait is revised to highlight the creation of a curiosity gap as the distinguishing factor.
- Clickbait is defined as a technique that deliberately omits information to raise curiosity and entice clicks.
- A new dataset, TA1C, for clickbait detection in Spanish is introduced, consisting of 3,500 tweets from 18 media sources.
- The dataset has reached a high inter-annotator agreement of 0.825 Fleiss' K.
- Strong baselines are implemented, achieving a F1-score of 0.84 in clickbait detection.

Summary: <div>
arXiv:2507.09777v1 Announce Type: cross 
Abstract: We revise the definition of clickbait, which lacks current consensus, and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena such as sensationalism and headlines that do not deliver what they promise or diverge from the article. Therefore, we propose a new definition: clickbait is a technique for generating headlines and teasers that deliberately omit part of the information with the goal of raising the readers' curiosity, capturing their attention and enticing them to click. We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible. Following it, we created and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the first open source dataset for clickbait detection in Spanish. It consists of 3,500 tweets coming from 18 well known media sources, manually annotated and reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong baselines that achieve 0.84 in F1-score.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantification of Interdependent Emotion Dynamics in Online Interactions</title>
<link>https://arxiv.org/abs/2408.05700</link>
<guid>https://arxiv.org/abs/2408.05700</guid>
<content:encoded><![CDATA[
<div> Keywords: online interactions, emotions, Hawkes self-exciting point process, peer interactions, emotional manipulation <br />
Summary: <br />
- The study focuses on understanding the dynamics of emotions in online interactions, particularly in YouTube Live chats.
- A multivariate Hawkes self-exciting point process is used to model the expression of six basic emotions, considering both external video content and social feedback.
- Emotional expressions in YouTube Live chats are influenced more by peer interactions than video content, with positivity being more contagious and negativity lingering longer.
- Negative emotions often trigger positive ones in a pattern consistent with trolling dynamics, suggesting the risks of emotional manipulation in online interactions.
- The findings emphasize the significant role of social interaction in shaping emotional dynamics online, particularly as human-chatbot interactions become more realistic. <br /> <br /> <div>
arXiv:2408.05700v3 Announce Type: replace 
Abstract: A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to correlation networks: Interdisciplinary approaches beyond thresholding</title>
<link>https://arxiv.org/abs/2311.09536</link>
<guid>https://arxiv.org/abs/2311.09536</guid>
<content:encoded><![CDATA[
<div> networks, correlation, thresholding, weighted networks, regularization

Summary:
This article addresses the challenge of constructing and analyzing correlation networks in various fields such as psychology, neuroscience, genomics, and finance. It highlights the limitations of the traditional method of thresholding on correlation values and explores alternative approaches such as weighted networks, regularization, dynamic correlation networks, and threshold-free methods. The article also discusses the importance of comparing networks with null models and presents key open questions in the field. Overall, the review emphasizes the need for cross-disciplinary collaboration and recommends best practices for transforming correlational data into meaningful networks. <div>
arXiv:2311.09536v3 Announce Type: replace-cross 
Abstract: Many empirical networks originate from correlational data, arising in domains as diverse as psychology, neuroscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been developed in different application domains for working with such networks, as well as in statistics, network science, and computer science, often with limited communication between practitioners in different fields. This leaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear how to best transform correlation matrix data into networks for the application at hand, and probably the most widespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks, suffers from multiple problems. In this article, we review various methods of constructing and analyzing correlation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic correlation networks, threshold-free approaches, comparison with null models, and more. Finally, we propose and discuss recommended practices and a variety of key open questions currently confronting this field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media</title>
<link>https://arxiv.org/abs/2504.12355</link>
<guid>https://arxiv.org/abs/2504.12355</guid>
<content:encoded><![CDATA[
<div> Keywords: drug overdose, social media, AI-driven NLP framework, multi-class classification, personalized intervention

Summary: 
The study focuses on the use of social media data and AI technology to detect drug misuse and overdose symptoms. By training a framework using a combination of large language models (LLMs) and human annotators, the researchers achieved high accuracy rates in classifying commonly used drugs and associated symptoms. The framework outperformed baseline models, showcasing its potential for real-time public health surveillance. With a 98% accuracy in multi-class classification and 97% in multi-label classification, the AI-driven approach offers new opportunities for personalized intervention strategies in combating drug overdose. This research demonstrates the effectiveness of leveraging AI and social media for timely insights and targeted interventions in addressing the global health challenge of substance misuse. 

<br /><br />Summary: <div>
arXiv:2504.12355v2 Announce Type: replace-cross 
Abstract: Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Propaganda in Tweets From Politically Biased Sources</title>
<link>https://arxiv.org/abs/2507.08169</link>
<guid>https://arxiv.org/abs/2507.08169</guid>
<content:encoded><![CDATA[
arXiv:2507.08169v1 Announce Type: new 
Abstract: News outlets are well known to have political associations, and many national outlets cultivate political biases to cater to different audiences. Journalists working for these news outlets have a big impact on the stories they cover. In this work, we present a methodology to analyze the role of journalists, affiliated with popular news outlets, in propagating their bias using some form of propaganda-like language. We introduce JMBX(Journalist Media Bias on X), a systematically collected and annotated dataset of 1874 tweets from Twitter (now known as X). These tweets are authored by popular journalists from 10 news outlets whose political biases range from extreme left to extreme right. We extract several insights from the data and conclude that journalists who are affiliated with outlets with extreme biases are more likely to use propaganda-like language in their writings compared to those who are affiliated with outlets with mild political leans. We compare eight different Large Language Models (LLM) by OpenAI and Google. We find that LLMs generally performs better when detecting propaganda in social media and news article compared to BERT-based model which is fine-tuned for propaganda detection. While the performance improvements of using large language models (LLMs) are significant, they come at a notable monetary and environmental cost. This study provides an analysis of both the financial costs, based on token usage, and the environmental impact, utilizing tools that estimate carbon emissions associated with LLM operations.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing overlapping communities in multiple-source detection: An edge clustering approach for complex networks</title>
<link>https://arxiv.org/abs/2507.08265</link>
<guid>https://arxiv.org/abs/2507.08265</guid>
<content:encoded><![CDATA[
arXiv:2507.08265v1 Announce Type: new 
Abstract: The source detection problem in network analysis involves identifying the origins of diffusion processes, such as disease outbreaks or misinformation propagation. Traditional methods often focus on single sources, whereas real-world scenarios frequently involve multiple sources, complicating detection efforts. This study addresses the multiple-source detection (MSD) problem by integrating edge clustering algorithms into the community-based label propagation framework, effectively handling mixed-membership issues where nodes belong to multiple communities.
  The proposed approach applies the automated latent space edge clustering model to a network, partitioning infected networks into edge-based clusters to identify multiple sources. Simulation studies on ADD HEALTH social network datasets demonstrate that this method achieves superior accuracy, as measured by the F1-Measure, compared to state-of-the-art clustering algorithms. The results highlight the robustness of edge clustering in accurately detecting sources, particularly in networks with complex and overlapping source regions. This work advances the applicability of clustering-based methods to MSD problems, offering improved accuracy and adaptability for real-world network analyses.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering High-Order Cohesive Structures: Efficient (k,g)-Core Computation and Decomposition for Large Hypergraphs</title>
<link>https://arxiv.org/abs/2507.08328</link>
<guid>https://arxiv.org/abs/2507.08328</guid>
<content:encoded><![CDATA[
arXiv:2507.08328v1 Announce Type: new 
Abstract: Hypergraphs, increasingly utilised to model complex and diverse relationships in modern networks, have gained significant attention for representing intricate higher-order interactions. Among various challenges, cohesive subgraph discovery is one of the fundamental problems and offers deep insights into these structures, yet the task of selecting appropriate parameters is an open question. To address this question, we aim to design an efficient indexing structure to retrieve cohesive subgraphs in an online manner. The main idea is to enable the discovery of corresponding structures within a reasonable time without the need for exhaustive graph traversals. Our method enables faster and more effective retrieval of cohesive structures, which supports decision-making in applications that require online analysis of large-scale hypergraphs. Through extensive experiments on real-world networks, we demonstrate the superiority of our proposed indexing technique.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Evolutionary Graph Theory</title>
<link>https://arxiv.org/abs/2507.08363</link>
<guid>https://arxiv.org/abs/2507.08363</guid>
<content:encoded><![CDATA[
arXiv:2507.08363v1 Announce Type: new 
Abstract: The stability of communities - whether biological, social, economic, technological or ecological depends on the balance between cooperation and cheating. While cooperation strengthens communities, selfish individuals, or "cheaters," exploit collective benefits without contributing. If cheaters become too prevalent, they can trigger the collapse of cooperation and of the community, often in an abrupt manner. A key challenge is determining whether the risk of such a collapse can be detected in advance. To address this, we use a combination of evolutionary graph theory and machine learning to examine how one can predict the unravel of cooperation on complex networks. By introducing few cheaters into a structured population, we employ machine learning to detect and anticipate the spreading of cheaters and cooperation collapse. Using temporal and structural data, the presented results show that prediction accuracy improves with stronger selection strength and larger observation windows, with CNN-Seq-LSTM and Seq-LSTM best performing models. Moreover, the accuracy for the predictions depends crucially on the type of game played between cooperators and cheaters (i.e., accuracy improves when it is more advantageous to defect) and on the community structure. Overall, this work introduces a machine learning approach into detecting abrupt shifts in evolutionary graph theory and offer potential strategies for anticipating and preventing cooperation collapse in complex social networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Feedback Enhances Community-Based Content Moderation through Engagement with Counterarguments</title>
<link>https://arxiv.org/abs/2507.08110</link>
<guid>https://arxiv.org/abs/2507.08110</guid>
<content:encoded><![CDATA[
arXiv:2507.08110v1 Announce Type: cross 
Abstract: Today, social media platforms are significant sources of news and political communication, but their role in spreading misinformation has raised significant concerns. In response, these platforms have implemented various content moderation strategies. One such method, Community Notes on X, relies on crowdsourced fact-checking and has gained traction, though it faces challenges such as partisan bias and delays in verification. This study explores an AI-assisted hybrid moderation framework in which participants receive AI-generated feedback -supportive, neutral, or argumentative -on their notes and are asked to revise them accordingly. The results show that incorporating feedback improves the quality of notes, with the most substantial gains resulting from argumentative feedback. This underscores the value of diverse perspectives and direct engagement in human-AI collective intelligence. The research contributes to ongoing discussions about AI's role in political content moderation, highlighting the potential of generative AI and the importance of informed design.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motifs for Financial Networks: A Study on Mercari, JPMC, and Venmo Platforms</title>
<link>https://arxiv.org/abs/2301.07791</link>
<guid>https://arxiv.org/abs/2301.07791</guid>
<content:encoded><![CDATA[
arXiv:2301.07791v2 Announce Type: replace 
Abstract: Understanding the dynamics of financial transactions among people is critical for various applications such as fraud detection. One important aspect of financial transaction networks is temporality. The order and repetition of transactions can offer new insights when considered within the graph structure. Temporal motifs, defined as a set of nodes that interact with each other in a short time period, are a promising tool in this context. In this work, we study three unique temporal financial networks: transactions in Mercari, an online marketplace, payments in a synthetic network generated by J.P. Morgan Chase, and payments and friendships among Venmo users. We consider the fraud detection problem on the Mercari and J.P. Morgan Chase networks, for which the ground truth is available. We show that temporal motifs offer superior performance to several baselines, including a previous method that considers simple graph features and two node embedding techniques (LINE and node2vec), while being practical in terms of runtime performance. For the Venmo network, we investigate the interplay between financial and social relations on three tasks: friendship prediction, vendor identification, and analysis of temporal cycles. For friendship prediction, temporal motifs yield better results than general heuristics, such as Jaccard and Adamic-Adar measures. We are also able to identify vendors with high accuracy and observe interesting patterns in rare motifs, such as temporal cycles. We believe that the analysis, datasets, and lessons from this work will be beneficial for future research on financial transaction networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signed Diverse Multiplex Networks: Clustering and Inference</title>
<link>https://arxiv.org/abs/2402.10242</link>
<guid>https://arxiv.org/abs/2402.10242</guid>
<content:encoded><![CDATA[
arXiv:2402.10242v3 Announce Type: replace 
Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models, including GRDPG, as its particular cases.
  By employing novel methodologies, our paper ensures strongly consistent clustering of layers and highly accurate subspace estimation, which are significant improvements over the results of Pensky and Wang (2024). All algorithms and theoretical results in the paper remain true for both signed and binary networks. In addition, the paper shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as, for example, analysis of brain networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incivility and Contentiousness Spillover in Public Engagement with Public Health and Climate Science</title>
<link>https://arxiv.org/abs/2502.05255</link>
<guid>https://arxiv.org/abs/2502.05255</guid>
<content:encoded><![CDATA[
arXiv:2502.05255v2 Announce Type: replace 
Abstract: Affective polarization and political sorting drive public antagonism around issues at the science-policy nexus. Looking at the COVID-19 period, we study cross-domain spillover of incivility and contentiousness in public engagements with climate change and public health on Twitter and Reddit. We find strong evidence of the signatures of affective polarization surrounding COVID-19 spilling into the climate change domain. Across different social media systems, COVID-19 content is associated with incivility and contentiousness in climate discussions. These patterns of increased antagonism were responsive to pandemic events that made the link between science and public policy more salient. The observed spillover activated along pre-pandemic political cleavages, specifically anti-internationalist populist beliefs, that linked climate policy opposition to vaccine hesitancy. Our findings show how affective polarization in public engagement with science becomes entrenched across science policy domains.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Probabilistic Propagation in Graphs by Adding Edges</title>
<link>https://arxiv.org/abs/2407.02624</link>
<guid>https://arxiv.org/abs/2407.02624</guid>
<content:encoded><![CDATA[
arXiv:2407.02624v3 Announce Type: replace-cross 
Abstract: Probabilistic graphs are an abstraction that allow us to study randomized propagation in graphs. In a probabilistic graph, each edge is "active" with a certain probability, independent of the other edges. For two vertices $u,v$, a classic quantity of interest, that we refer to as the proximity $\mathcal{P}_{G}(u, v)$, is the probability that there exists a path between $u$ and $v$ all of whose edges are active. For a given subset of vertices $V_s$, the reach of $V_s$ is defined as the minimum over pairs $u \in V_s$ and $v \in V$ of the proximity $\mathcal{P}_{G}(u,v)$. This quantity has been studied in the context of multicast in unreliable communication networks and in social network analysis.
  We study the problem of improving the reach in a probabilistic graph via edge augmentation. Formally, given a budget $k$ of edge additions and a set of source vertices $V_s$, the goal of Reach Improvement is to maximize the reach of $V_s$ by adding at most $k$ new edges to the graph. The problem was introduced in earlier empirical work in the algorithmic fairness community. We provide the first approximation guarantees and hardness results for Reach Improvement.
  We prove that the existence of a good augmentation implies a cluster structure for the graph. We use this structural result to analyze a novel algorithm that outputs a $k$-edge augmentation with an objective value that is poly($\beta^*$), where $\beta^*$ is the objective value for the optimal augmentation. We also give an algorithm that adds $O(k \log n)$ edges and yields a multiplicative approximation to $\beta^*$. Our arguments rely on new probabilistic tools for analyzing proximity, inspired by techniques in percolation theory; these tools may be of broader interest. Finally, we show that significantly better approximations are unlikely, under known hardness assumptions related to gap variants of the classic Set Cover problem.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Signed Exponential Random Graph Models under Local Dependence</title>
<link>https://arxiv.org/abs/2507.07660</link>
<guid>https://arxiv.org/abs/2507.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: network analysis, signed interactions, Stochastic Block Models, Exponential Family Random Graph Models, structural balance theory

Summary: 
Traditional network analysis typically focuses on binary relationships, but real-world interactions can be more complex, involving cooperation, neutrality, and conflict. The emergence of negative interactions in social media has led to increased interest in analyzing signed interactions in polarized debates. However, analyzing large digital networks poses challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM). This new method combines the strengths of SBM and ERGM while addressing their weaknesses, incorporating local dependence based on non-overlapping blocks. The approach involves decomposing the network into sub-networks using SBM and then estimating parameters using ERGM. Validation on synthetic networks and application to a signed Wikipedia network of editors reveal patterns that align with structural balance theory. <div>
arXiv:2507.07660v1 Announce Type: new 
Abstract: Traditional network analysis focuses on binary edges, while real-world relationships are more nuanced, encompassing cooperation, neutrality, and conflict. The rise of negative edges in social media discussions spurred interest in analyzing signed interactions, especially in polarized debates. However, the vast data generated by digital networks presents challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM), particularly due to the homogeneity assumption and global dependence, which become increasingly unrealistic as network size grows. To address this, we propose a novel method that combines the strengths of SBM and ERGM while mitigating their weaknesses by incorporating local dependence based on non-overlapping blocks. Our approach involves a two-step process: first, decomposing the network into sub-networks using SBM approximation, and then estimating parameters using ERGM methods. We validate our method on large synthetic networks and apply it to a signed Wikipedia network of thousands of editors. Through the use of local dependence, we find patterns consistent with structural balance theory.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics</title>
<link>https://arxiv.org/abs/2507.07727</link>
<guid>https://arxiv.org/abs/2507.07727</guid>
<content:encoded><![CDATA[
<div> Framework, higher-order network, transportation systems, memory-dependent dynamics, predictive accuracy

Summary:
The study introduces a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. This framework extends traditional graph representations to capture sequential dependencies in real-world mobility patterns. By incorporating higher-order Markov chains and de Bruijn graph structures, the framework encodes spatial and temporal ordering of paths, improving the analysis of critical components in transportation networks. The study generalizes network analytics such as betweenness centrality and PageRank to this higher-order setting and validates the approach on the Sioux Falls transportation network. Experimental results show that higher-order models outperform first-order baselines for tasks such as next-step prediction. The third-order model strikes a balance between predictive accuracy and model complexity, demonstrating the importance of incorporating memory effects in transportation analysis. This scalable, data-driven methodology offers insights into complex mobility behaviors in infrastructure systems. 

<br /><br /> <div>
arXiv:2507.07727v1 Announce Type: new 
Abstract: Understanding and predicting mobility dynamics in transportation networks is critical for infrastructure planning, resilience analysis, and traffic management. Traditional graph-based models typically assume memoryless movement, limiting their ability to capture sequential dependencies inherent in real-world mobility patterns. In this study, we introduce a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. By extending classical graph representations through higher-order Markov chains and de Bruijn graph structures, our framework encodes the spatial and temporal ordering of traversed paths, enabling the analysis of structurally and functionally critical components with improved fidelity. We generalize key network analytics, including betweenness centrality, PageRank, and next-step prediction, to this higher-order setting and validate our approach on the Sioux Falls transportation network using agent-based trajectory data generated with MATSim. Experimental results demonstrate that higher-order models outperform first-order baselines across multiple tasks, with the third-order model achieving an optimal balance between predictive accuracy and model complexity. These findings highlight the importance of incorporating memory effects into network-based transportation analysis and offer a scalable, data-driven methodology for capturing complex mobility behaviors in infrastructure systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conspiracy to Commit: Information Pollution, Artificial Intelligence, and Real-World Hate Crime</title>
<link>https://arxiv.org/abs/2507.07884</link>
<guid>https://arxiv.org/abs/2507.07884</guid>
<content:encoded><![CDATA[
<div> conspiracy theories, online search trends, hate crimes, 1D-CNN, machine learning

Summary:
This study looks at the relationship between online search trends for racially and politically charged conspiracy theories in Michigan from 2015 to 2019 and hate crime occurrences offline. Using a one-dimensional convolutional neural network (1D-CNN), the researchers found that specific conspiracy theories like the Rothschilds family, Q-Anon, and The Great Replacement were linked to an increase in hate crimes two to three weeks after search spikes. However, most theories did not show a clear connection to offline hate crimes. The findings support neutralization and differential association theories, suggesting that certain conspiracy theories can contribute to real-world violence. Additionally, the study highlights the potential for machine learning to identify harmful online patterns and advance social science research. <div>
arXiv:2507.07884v1 Announce Type: new 
Abstract: Is demand for conspiracy theories online linked to real-world hate crimes? By analyzing online search trends for 36 racially and politically-charged conspiracy theories in Michigan (2015-2019), we employ a one-dimensional convolutional neural network (1D-CNN) to predict hate crime occurrences offline. A subset of theories including the Rothschilds family, Q-Anon, and The Great Replacement improves prediction accuracy, with effects emerging two to three weeks after fluctuations in searches. However, most theories showed no clear connection to offline hate crimes. Aligning with neutralization and differential association theories, our findings provide a partial empirical link between specific racially charged conspiracy theories and real-world violence. Just as well, this study underscores the potential for machine learning to be used in identifying harmful online patterns and advancing social science research.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Adaptive Estimation of Local Triadic Coefficients</title>
<link>https://arxiv.org/abs/2507.07536</link>
<guid>https://arxiv.org/abs/2507.07536</guid>
<content:encoded><![CDATA[
<div> local triadic coefficients, graph properties, graph partitioning, sampling, collaboration networks

Summary: 
The article introduces the concept of local triadic coefficients, such as the local clustering coefficient and local closure coefficient, which are crucial in analyzing networked systems. The focus is on efficiently computing the average local triadic coefficients for a partitioned graph. Due to the infeasibility of exact computation for large networks, the Triad algorithm is developed based on sampling to provide highly accurate probabilistic estimates. Triad utilizes unbiased estimators and offers non-trivial bounds on sample complexity, enabling efficient computation. The algorithm is demonstrated to be effective in capturing high-order patterns in collaboration networks. The study highlights the importance of local triadic coefficients in understanding graph structures and node properties, offering insights for various applications ranging from graph embeddings to network analysis.<br /><br />Summary: <div>
arXiv:2507.07536v1 Announce Type: cross 
Abstract: Characterizing graph properties is fundamental to the analysis and to our understanding of real-world networked systems. The local clustering coefficient, and the more recently introduced, local closure coefficient, capture powerful properties that are essential in a large number of applications, ranging from graph embeddings to graph partitioning. Such coefficients capture the local density of the neighborhood of each node, considering incident triadic structures and paths of length two. For this reason, we refer to these coefficients collectively as local triadic coefficients.
  In this work, we consider the novel problem of computing efficiently the average of local triadic coefficients, over a given partition of the nodes of the input graph into a set of disjoint buckets. The average local triadic coefficients of the nodes in each bucket provide a better insight into the interplay of graph structure and the properties of the nodes associated to each bucket. Unfortunately, exact computation, which requires listing all triangles in a graph, is infeasible for large networks. Hence, we focus on obtaining highly-accurate probabilistic estimates.
  We develop Triad, an adaptive algorithm based on sampling, which can be used to estimate the average local triadic coefficients for a partition of the nodes into buckets. Triad is based on a new class of unbiased estimators, and non-trivial bounds on its sample complexity, enabling the efficient computation of highly accurate estimates. Finally, we show how Triad can be efficiently used in practice on large networks, and we present a case study showing that average local triadic coefficients can capture high-order patterns over collaboration networks.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion of complex contagions is shaped by a trade-off between reach and reinforcement</title>
<link>https://arxiv.org/abs/2411.07907</link>
<guid>https://arxiv.org/abs/2411.07907</guid>
<content:encoded><![CDATA[
<div> social network structure, behavior diffusion, clustered networks, random networks, social reinforcement

Summary:<br /><br />Existing theory suggests that behavior diffusion is influenced by social reinforcement and network structure. A new model was developed to analyze the impact of clustered and random networks on behavior spread. The study found that random networks often outperform clustered networks in spreading behavior, especially when social reinforcement increases adoption. Clustered networks may have an advantage only in specific conditions, such as when adoption is nearly deterministic. Additionally, clustered networks are less advantageous when individuals remain influential after adopting, have more neighbors, or require more neighbors for social reinforcement. The research highlights a tradeoff between random ties, which enhance reach, and clustered ties, which enhance social reinforcement. Ultimately, clustered networks outperform random networks by a small margin in only a minority of scenarios, showcasing the complexity of behavior diffusion on social networks. <div>
arXiv:2411.07907v2 Announce Type: replace 
Abstract: How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more -- both farther and faster -- on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, it should spread more on random networks which avoid such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks spread behavior better than random networks. Using simulations and analytical methods, we identify precise boundaries in the parameter space where one network type outperforms the other or they perform equally. We find that, in most cases, random networks spread behavior as far or farther than clustered networks, even when social reinforcement increases adoption. Although we find that probabilistic, socially reinforced behaviors can spread farther on clustered networks in some cases, this is not the dominant pattern. Clustered networks are even less advantageous when individuals remain influential for longer after adopting, have more neighbors, or need more neighbors before social reinforcement takes effect. Under such conditions, clustering tends to help only when adoption is nearly deterministic, which is not representative of socially reinforced behaviors more generally. Clustered networks outperform random networks by a 5% margin in only 22% of the parameter space under its most favorable conditions. This pattern reflects a fundamental tradeoff: random ties enhance reach, while clustered ties enhance social reinforcement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dialects Collide: How Socioeconomic Mixing Affects Language Use</title>
<link>https://arxiv.org/abs/2307.10016</link>
<guid>https://arxiv.org/abs/2307.10016</guid>
<content:encoded><![CDATA[
<div> Keywords: sociolinguistics, language variation, socioeconomic background, geotagged tweets, agent-based model

Summary: 
This study explores the relationship between language variation and socioeconomic background using geotagged tweets in England and Wales. By analyzing deviations from standard English in different areas and correlating them with income levels, the researchers found that in areas with greater socioeconomic diversity, the relationship between language variation and income becomes less pronounced. This suggests that interactions between different socioeconomic classes may influence how people use language. Additionally, an agent-based model was developed to explain the observed patterns, providing insights into the mechanisms behind linguistic variety adoption. Overall, the study highlights the complex interplay between socioeconomic factors and language use, shedding light on how social dynamics can shape linguistic patterns at a large scale. 

Summary: <div>
arXiv:2307.10016v2 Announce Type: replace-cross 
Abstract: The socioeconomic background of people and how they use standard forms of language are not independent, as demonstrated in various sociolinguistic studies. However, the extent to which these correlations may be influenced by the mixing of people from different socioeconomic classes remains relatively unexplored from a quantitative perspective. In this work we leverage geotagged tweets and transferable computational methods to map deviations from standard English on a large scale, in seven thousand administrative areas of England and Wales. We combine these data with high-resolution income maps to assign a proxy socioeconomic indicator to home-located users. Strikingly, across eight metropolitan areas we find a consistent pattern suggesting that the more different socioeconomic classes mix, the less interdependent the frequency of their departures from standard grammar and their income become. Further, we propose an agent-based model of linguistic variety adoption that sheds light on the mechanisms that produce the observations seen in the data.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motif Participation Profiles for Analyzing Node Similarity in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.06465</link>
<guid>https://arxiv.org/abs/2507.06465</guid>
<content:encoded><![CDATA[
<div> Temporal networks, temporal motifs, TMPPs, node clustering, militarized interstate disputes <br />
Summary: <br />
Temporal networks track interactions between nodes over time. Temporal motifs capture higher-order patterns like directed triangles. TMPPs represent node behavior in these motifs, serving as interpretable node embeddings. Nodes with similar TMPPs have similar roles in motifs. Clustering TMPPs reveals node groups with similar roles. Simulation experiments and a study on militarized interstate disputes demonstrate the effectiveness of TMPPs in uncovering node roles in temporal networks. <div>
arXiv:2507.06465v1 Announce Type: new 
Abstract: Temporal networks consisting of timestamped interactions between a set of nodes provide a useful representation for analyzing complex networked systems that evolve over time. Beyond pairwise interactions between nodes, temporal motifs capture patterns of higher-order interactions such as directed triangles over short time periods. We propose temporal motif participation profiles (TMPPs) to capture the behavior of nodes in temporal motifs. Two nodes with similar TMPPs take similar positions within temporal motifs, possibly with different nodes. TMPPs serve as unsupervised embeddings for nodes in temporal networks that are directly interpretable, as each entry denotes the frequency at which a node participates in a particular position in a specific temporal motif. We demonstrate that clustering TMPPs reveals groups of nodes with similar roles in a temporal network through simulation experiments and a case study on a network of militarized interstate disputes.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Fake Account Detection: A Survey</title>
<link>https://arxiv.org/abs/2507.06541</link>
<guid>https://arxiv.org/abs/2507.06541</guid>
<content:encoded><![CDATA[
<div> Keywords: fake account detection, online social networks, graph-based techniques, topological features, detection time

Summary:
This survey paper discusses the current state of algorithms for detecting fake accounts in online social networks, with a focus on graph-based techniques that use social graph topology. Different methods are categorized based on techniques, input data, and detection time, with strengths and limitations discussed. The paper also explores available datasets, including real-world and synthesized data models. Potential areas for future research are suggested. <div>
arXiv:2507.06541v1 Announce Type: new 
Abstract: In recent years, there has been a growing effort to develop effective and efficient algorithms for fake account detection in online social networks. This survey comprehensively reviews existing methods, with a focus on graph-based techniques that utilise topological features of social graphs (in addition to account information, such as their shared contents and profile data) to distinguish between fake and real accounts. We provide several categorisations of these methods (for example, based on techniques used, input data, and detection time), discuss their strengths and limitations, and explain how these methods connect in the broader context. We also investigate the available datasets, including both real-world data and synthesised models. We conclude the paper by proposing several potential avenues for future research.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic</title>
<link>https://arxiv.org/abs/2507.07036</link>
<guid>https://arxiv.org/abs/2507.07036</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial heterogeneity, sea ice retreat, Antarctic ice shelf melt, graph-based framework, climate adaptation<br />
Summary:<br />
- The study explores the complex linkages between sea ice retreat and Antarctic ice shelf (AIS) melt, focusing on the spatial heterogeneity in dynamic regions like ice shelves and sea ice.<br />
- Traditional models treat sea ice and AIS as separate systems, limiting their ability to capture localized linkages and cascading feedback.<br />
- The proposed Spatial-Link framework is a novel graph-based approach that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt, revealing non-local coupling patterns.<br />
- Results show that sea ice loss can initiate or amplify downstream AIS melt, establishing a direct linkage between sea ice retreat and AIS mass loss.<br />
- Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies. <br /> 

Summary: <div>
arXiv:2507.07036v1 Announce Type: new 
Abstract: Spatial phenomena often exhibit heterogeneity across spatial extents and in proximity, making them complex to model-especially in dynamic regions like ice shelves and sea ice. In this study, we address this challenge by exploring the linkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although atmospheric forcing and basal melting have been widely studied, the direct impact of sea ice retreat on AIS mass loss remains underexplored. Traditional models treat sea ice and AIS as separate systems. It limits their ability to capture localized linkages and cascading feedback. To overcome this, we propose Spatial-Link, a novel graph-based framework that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt. Our method constructs a spatial graph using Delaunay triangulation of satellite-derived ice change matrices, where nodes represent regions of significant change and edges encode proximity and directional consistency. We extract and statistically validate linkage paths using breadth-first search and Monte Carlo simulations. Results reveal non-local, spatially heterogeneous coupling patterns, suggesting sea ice loss can initiate or amplify downstream AIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid and progresses toward ice shelves-establishing a direct linkage. To our knowledge, this is the first proposed methodology linking sea ice retreat to AIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning</title>
<link>https://arxiv.org/abs/2507.06469</link>
<guid>https://arxiv.org/abs/2507.06469</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph representation learning, fraud detection, topology, class imbalance, dual-view graph

Summary: 
Graph representation learning has become a popular method for fraud detection, but the imbalance in global topological information transmission poses a challenge. In this paper, the authors propose a novel method, MimbFD, to address this issue. MimbFD includes a topological message reachability module for improved node representation learning and a local confounding debiasing module to enhance the association between node representations and labels. These components help mitigate the imbalance in supervisory messages caused by fraudsters' topological behavior obfuscation and identity feature concealment. Experimental results on three public fraud datasets demonstrate the effectiveness of MimbFD in fraud detection. <div>
arXiv:2507.06469v1 Announce Type: cross 
Abstract: Graph representation learning has become a mainstream method for fraud detection due to its strong expressive power, which focuses on enhancing node representations through improved neighborhood knowledge capture. However, the focus on local interactions leads to imbalanced transmission of global topological information and increased risk of node-specific information being overwhelmed during aggregation due to the imbalance between fraud and benign nodes. In this paper, we first summarize the impact of topology and class imbalance on downstream tasks in GNN-based fraud detection, as the problem of imbalanced supervisory messages is caused by fraudsters' topological behavior obfuscation and identity feature concealment. Based on statistical validation, we propose a novel dual-view graph representation learning method to mitigate Message imbalance in Fraud Detection(MimbFD). Specifically, we design a topological message reachability module for high-quality node representation learning to penetrate fraudsters' camouflage and alleviate insufficient propagation. Then, we introduce a local confounding debiasing module to adjust node representations, enhancing the stable association between node representations and labels to balance the influence of different classes. Finally, we conducted experiments on three public fraud datasets, and the results demonstrate that MimbFD exhibits outstanding performance in fraud detection.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Designing Social Interventions for Online Climate Change Denialism Discussions</title>
<link>https://arxiv.org/abs/2507.06561</link>
<guid>https://arxiv.org/abs/2507.06561</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, climate change denialism, Reddit, intervention strategies, social media

Summary: 
This study explores intervention strategies for combating conspiracy theory ideology in climate change denialism on Reddit. The researchers used insider language to engage with users in two Reddit communities - climate change deniers and supporters. By crafting evidence-based intervention messages and deploying them through bot accounts, the study found that neutral language interventions sparked positive engagement and open discussions among climate change deniers. Climate change supporters responded actively, contributing additional evidence to the discussions. The study sheds light on the challenges and processes involved in delivering interventions in conspiracy theory communities on social media, offering valuable insights for future research on social media interventions.

<br /><br />Summary: <div>
arXiv:2507.06561v1 Announce Type: cross 
Abstract: As conspiracy theories gain traction, it has become crucial to research effective intervention strategies that can foster evidence and science-based discussions in conspiracy theory communities online. This study presents a novel framework using insider language to contest conspiracy theory ideology in climate change denialism on Reddit. Focusing on discussions in two Reddit communities, our research investigates reactions to pro-social and evidence-based intervention messages for two cohorts of users: climate change deniers and climate change supporters. Specifically, we combine manual and generative AI-based methods to craft intervention messages and deploy the interventions as replies on Reddit posts and comments through transparently labeled bot accounts. On the one hand, we find that evidence-based interventions with neutral language foster positive engagement, encouraging open discussions among believers of climate change denialism. On the other, climate change supporters respond positively, actively participating and presenting additional evidence. Our study contributes valuable insights into the process and challenges of automatically delivering interventions in conspiracy theory communities on social media, and helps inform future research on social media interventions.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE: Data Influence Cascade in Decentralized Learning</title>
<link>https://arxiv.org/abs/2507.06931</link>
<guid>https://arxiv.org/abs/2507.06931</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized learning, Fair incentive mechanism, Data Influence Cascade, Peer-to-peer networks, Tractable approximations

Summary: 
The article introduces a novel method called Data Influence Cascade Estimation (DICE) for decentralized learning in peer-to-peer networks. The focus is on creating fair incentives for participating nodes by accurately attributing contributions. DICE addresses the challenge of influence cascading in decentralized networks by considering data, communication topology, and loss landscape curvature. The framework provides tractable approximations for influence cascades across neighbor hops, facilitating the selection of collaborators and detection of malicious behaviors. The theoretical foundations of DICE suggest that influence cascades are determined by a complex interplay of factors. This innovative approach aims to encourage participation and improve the efficiency of decentralized learning systems.<br /><br />Summary: <div>
arXiv:2507.06931v1 Announce Type: cross 
Abstract: Decentralized learning offers a promising approach to crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing demands. However, proper incentives are still in absence, considerably discouraging participation. Our vision is that a fair incentive mechanism relies on fair attribution of contributions to participating nodes, which faces non-trivial challenges arising from the localized connections making influence ``cascade'' in a decentralized network. To overcome this, we design the first method to estimate \textbf{D}ata \textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized environment. Theoretically, the framework derives tractable approximations of influence cascade over arbitrary neighbor hops, suggesting the influence cascade is determined by an interplay of data, communication topology, and the curvature of loss landscape. DICE also lays the foundations for applications including selecting suitable collaborators and identifying malicious behaviors. Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage</title>
<link>https://arxiv.org/abs/2507.07045</link>
<guid>https://arxiv.org/abs/2507.07045</guid>
<content:encoded><![CDATA[
<div> design, Large Language Models, prompt, framework, efficiency
Summary:
The article discusses the shift from traditional prompt engineering to a more structured prompt design approach in human-Large Language Model (LLM) interactions. It introduces the 5C Prompt Contract framework, comprising Character, Cause, Constraint, Contingency, and Calibration components, aimed at simplifying prompt design for better AI interactions. The framework integrates fallback and output optimization directives, enhancing reliability and interpretability while maintaining creativity. Experimental results show that the 5C framework improves input token efficiency and ensures consistent outputs across various LLM architectures. It is particularly beneficial for individuals and Small-to-Medium Enterprises (SMEs) with limited AI resources. <br /><br />Summary: <div>
arXiv:2507.07045v1 Announce Type: cross 
Abstract: The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the model's creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Public Perceptions of Generative AI in Libraries: A Social Media Analysis of X Discussions</title>
<link>https://arxiv.org/abs/2507.07047</link>
<guid>https://arxiv.org/abs/2507.07047</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, libraries, social network analysis, sentiment analysis, public perception
<br />
Summary: 
<br />
This study examines public perceptions of generative artificial intelligence (GenAI) in libraries by analyzing posts on X (formerly Twitter). The research combines temporal trend analysis, sentiment classification, and social network analysis to understand how discourse around GenAI and libraries has evolved. The analysis shows that discussions are mostly negative, particularly focused on ethical and intellectual property concerns. Social network analysis reveals the importance of institutional authority and individual bridge users in facilitating cross-domain engagement. Overall, the study contributes to the existing literature on GenAI in the library and GLAM sectors, offering a real-time insight into the opportunities and challenges presented by GenAI. <div>
arXiv:2507.07047v1 Announce Type: cross 
Abstract: This study investigates public perceptions of generative artificial intelligence (GenAI) in libraries through a large-scale analysis of posts on X (formerly Twitter). Using a mixed-method approach that combines temporal trend analysis, sentiment classification, and social network analysis, this paper explores how public discourse around GenAI and libraries has evolved over time, the emotional tones that dominate the conversation, and the key users or organizations driving engagement. The findings reveal that discussions are predominantly negative in tone, with surges linked to concerns about ethics and intellectual property. Furthermore, social network analysis identifies both institutional authority and individual bridge users who facilitate cross-domain engagement. The results in this paper contribute to the growing body of literature on GenAI in the library and GLAM (Galleries, Libraries, Archives, and Museums) sectors and offer a real-time, public-facing perspective on the emerging opportunities and concerns GenAI presents.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Ranking for Deliberation in the Public Sphere</title>
<link>https://arxiv.org/abs/2503.18962</link>
<guid>https://arxiv.org/abs/2503.18962</guid>
<content:encoded><![CDATA[
<div> comment sections, online platforms, algorithmic ranking, diverse viewpoints, social choice

Summary: 
Platforms with online comment sections often struggle with toxic exchanges, hindering productive public deliberation. To address this issue, algorithmic ranking is used to promote higher-quality discussions, but this can inadvertently suppress legitimate viewpoints, reducing the representation of diverse perspectives. To combat this, the concept of justified representation (JR) is introduced, drawing from social choice theory. By incorporating a JR constraint into comment ranking systems, platforms can ensure a more inclusive representation of diverse viewpoints while still optimizing for user engagement or conversational quality. This approach strikes a balance between fostering quality discussions and upholding the importance of diverse perspectives in online deliberation. <div>
arXiv:2503.18962v2 Announce Type: replace 
Abstract: Online comment sections, such as those on news sites or social media, have the potential to foster informal public deliberation, However, this potential is often undermined by the frequency of toxic or low-quality exchanges that occur in these settings. To combat this, platforms increasingly leverage algorithmic ranking to facilitate higher-quality discussions, e.g., by using civility classifiers or forms of prosocial ranking. Yet, these interventions may also inadvertently reduce the visibility of legitimate viewpoints, undermining another key aspect of deliberation: representation of diverse views. We seek to remedy this problem by introducing guarantees of representation into these methods. In particular, we adopt the notion of justified representation (JR) from the social choice literature and incorporate a JR constraint into the comment ranking setting. We find that enforcing JR leads to greater inclusion of diverse viewpoints while still being compatible with optimizing for user engagement or other measures of conversational quality.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Displacement and disconnection: the impact of violence on migration networks and highway traffic in Mexico</title>
<link>https://arxiv.org/abs/2301.12743</link>
<guid>https://arxiv.org/abs/2301.12743</guid>
<content:encoded><![CDATA[
<div> migration flows, violence impact, network strength, oil theft, Mexican municipalities
Summary:<br />
- The study examines how violence affects migration flows and reshapes the strength of migration networks by using a novel network algorithm and Mexican census data from 2005 to 2020. 
- Rising violence, driven by increased oil theft activities following government crackdowns on drug trafficking organizations, led to higher emigration flows within Mexico and towards the United States. 
- The study found that violence increased emigration by at least 1.12 million people domestically and reduced the return of 50,200 Mexicans from the US. 
- Additionally, violence eroded regional connectivity, causing a decline in daily vehicle traffic on highways linking violent areas to the rest of the country. 
- The findings highlight the complex dynamics between violence, migration, and network strength in shaping population movements within and outside Mexico. 
<br /><br />Summary: <div>
arXiv:2301.12743v2 Announce Type: replace-cross 
Abstract: We examine how violence affects migration flows and, crucially, how it reshapes the strength of migration networks -- measured by the intensity of migration between areas, accounting for the fact that some routes become more prominent or fade over time -- an aspect traditional studies overlook. Using a novel network algorithm and Mexican census data from 2005 to 2020, we first quantify changes in the strength of domestic and international migration networks across all Mexican municipalities. We exploit variation in local homicide rates, using exogenous fuel price increases and municipalities' proximity to oil pipelines as instruments, to estimate the causal impact of violence on migration. During our study period, following intensified government crackdowns on drug trafficking organizations, many criminal groups fragmented and turned toward large-scale oil theft, driving sharp increases in violence in areas with oil pipelines, particularly when fuel prices rose. The findings show that rising violence increased emigration flows, predominantly within Mexico, and strengthened the intensity of emigration networks both domestically and toward the United States. Although violent municipalities continued to receive new residents, the rise in emigration was larger. Increasing homicide rates led to at least an additional 1.12 million people emigrating domestically and 50,200 fewer Mexicans returning from the United States. Violence also eroded regional connectivity, causing a long-term decline in daily vehicle traffic on highways linking violent areas to the rest of the country.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The most influential philosophers in Wikipedia: a multicultural analysis</title>
<link>https://arxiv.org/abs/2507.06034</link>
<guid>https://arxiv.org/abs/2507.06034</guid>
<content:encoded><![CDATA[
<div> PageRank, CheiRank, philosophers, Wikipedia knowledge network, presocratic philosophers  
Summary:  
- The study examines the influence and interconnectivity of philosophers in the Wikipedia knowledge network across different language editions.  
- 237 philosopher articles in nine languages were analyzed using PageRank and CheiRank algorithms to determine their relative ranking and influence.  
- A comparison with entries from the Stanford and Internet Encyclopedia of Philosophy was conducted to highlight differences between general and specialized knowledge networks.  
- The study focuses on a sub-network of 21 presocratic philosophers grouped into traditional schools and uses the reduced Google matrix method to reveal both direct and hidden links between them.  
- The analysis provides new insights into the intellectual relationships and influence of early philosophers within the Western philosophical tradition.  

<br /><br />Summary: <div>
arXiv:2507.06034v1 Announce Type: new 
Abstract: We explore the influence and interconnectivity of philosophical thinkers within the Wikipedia knowledge network. Using a dataset of 237 articles dedicated to philosophers across nine different language editions (Arabic, Chinese, English, French, German, Japanese, Portuguese, Russian, and Spanish), we apply the PageRank and CheiRank algorithms to analyze their relative ranking and influence in each linguistic context. Furthermore, we compare our results with entries from the Stanford Encyclopedia of Philosophy and the Internet Encyclopedia of Philosophy, providing insight into the differences between general knowledge networks like Wikipedia and specialized philosophical databases. A key focus of our analysis is the sub-network of 21 presocratic philosophers, grouped into four traditional schools: Italic (Pythagorean + Eleatic), Ionian, Abderian (Atomist), and Sophist. Using the reduced Google matrix method, we uncover both direct and hidden links between these early thinkers, offering new perspectives on their intellectual relationships and influence within the Western philosophical tradition.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuHE: Optimizing Utility-Cost in Quantum Key Distribution and Homomorphic Encryption Enabled Secure Edge Computing Networks</title>
<link>https://arxiv.org/abs/2507.06086</link>
<guid>https://arxiv.org/abs/2507.06086</guid>
<content:encoded><![CDATA[
<div> Quantum Key Distribution, Homomorphic Encryption, Mobile Edge Computing, Optimization, Resource Allocation<br />
<br />
Summary: 
This paper introduces a novel framework for secure and efficient data processing in mobile edge computing (MEC) systems by integrating Quantum Key Distribution (QKD), transciphering, and Homomorphic Encryption (HE). The framework addresses the trade-offs among QKD utility, HE security, and system costs. An optimization problem is formulated to balance these factors, but it is non-convex and NP-hard. To efficiently solve it, the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm is proposed. The algorithm is proven to be convergent and optimal through theoretical analysis and is shown to be effective in simulations across various performance metrics. <div>
arXiv:2507.06086v1 Announce Type: new 
Abstract: Ensuring secure and efficient data processing in mobile edge computing (MEC) systems is a critical challenge. While quantum key distribution (QKD) offers unconditionally secure key exchange and homomorphic encryption (HE) enables privacy-preserving data processing, existing research fails to address the comprehensive trade-offs among QKD utility, HE security, and system costs. This paper proposes a novel framework integrating QKD, transciphering, and HE for secure and efficient MEC. QKD distributes symmetric keys, transciphering bridges symmetric encryption, and HE processes encrypted data at the server. We formulate an optimization problem balancing QKD utility, HE security, processing and wireless transmission costs. However, the formulated optimization is non-convex and NPhard. To solve it efficiently, we propose the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm. Theoretical analysis proves the proposed QuHE algorithm's convergence and optimality, and simulations demonstrate its effectiveness across multiple performance metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Nodes Identification in Complex Networks: A Survey</title>
<link>https://arxiv.org/abs/2507.06164</link>
<guid>https://arxiv.org/abs/2507.06164</guid>
<content:encoded><![CDATA[
<div> Complex networks, critical node identification, centrality, dynamic networks, higher-order networks <br />
Summary: <br />
This paper offers a comprehensive review of critical node identification techniques in complex networks, categorizing them into seven main classes. It addresses the challenges posed by the complexity and heterogeneity of real-world networks, particularly in dynamic and higher-order structures. The review highlights various methods, such as centrality, critical node deletion problem, influence maximization, network control, artificial intelligence, and higher-order and dynamic approaches. It emphasizes the strengths, limitations, and applicability of these methods across different network types. Key challenges identified include algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and underscores open questions in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning, and developing scalable and interpretable metrics for complex systems. <div>
arXiv:2507.06164v1 Announce Type: new 
Abstract: Complex networks have become essential tools for understanding diverse phenomena in social systems, traffic systems, biomolecular systems, and financial systems. Identifying critical nodes is a central theme in contemporary research, serving as a vital bridge between theoretical foundations and practical applications. Nevertheless, the intrinsic complexity and structural heterogeneity characterizing real-world networks, with particular emphasis on dynamic and higher-order networks, present substantial obstacles to the development of universal frameworks for critical node identification. This paper provides a comprehensive review of critical node identification techniques, categorizing them into seven main classes: centrality, critical nodes deletion problem, influence maximization, network control, artificial intelligence, higher-order and dynamic methods. Our review bridges the gaps in existing surveys by systematically classifying methods based on their methodological foundations and practical implications, and by highlighting their strengths, limitations, and applicability across different network types. Our work enhances the understanding of critical node research by identifying key challenges, such as algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and highlights open questions, particularly in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning approaches, and developing scalable and interpretable metrics for complex systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Introvert</title>
<link>https://arxiv.org/abs/2507.05638</link>
<guid>https://arxiv.org/abs/2507.05638</guid>
<content:encoded><![CDATA[
<div> transformed information dissemination, social media, generative AI, misinformation, information propagation dynamics <br />
<br />Summary: The article discusses the impact of social media and generative AI on information dissemination and the spread of misinformation. Traditional models like SIR are insufficient in capturing the complexities of online interactions. Advanced methods like attention mechanisms and graph neural networks overlook user psychology and behavioral dynamics. Large language models (LLMs) offer potential for simulating psychological aspects of information spread but have limitations in capturing authentic human dynamics. The article introduces the SIP-CoT mechanism enhanced by emotion-guided memory to address these limitations. Experimental results confirm that SIP-CoT-enhanced LLM agents process social information more effectively, demonstrating behaviors closer to real human interactions. Overall, the research identifies critical limitations in current LLM-based propagation simulations and shows how integrating SIP-CoT and emotional memory enhances the social intelligence and realism of LLM agents. <br /> <div>
arXiv:2507.05638v1 Announce Type: cross 
Abstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity isn't everything -- how far do demographics take us towards self-identified party ID?</title>
<link>https://arxiv.org/abs/2507.06193</link>
<guid>https://arxiv.org/abs/2507.06193</guid>
<content:encoded><![CDATA[
<div> party identification, demographics, identity strength, predictive framework, Black Democrats

Summary: 
Demographics alone may not fully explain party identification, as individuals may choose to construct a political identity independent of their demographic group. The study examines the role of identity strength alongside demographics in predicting party identification. While demographics are highly predictive for some groups, such as Black Democrats, others, like Hispanic Republicans, benefit from considering identity strength as well. This suggests that individuals may prioritize certain group affiliations in shaping their political identity, and that a deeper understanding of identity strength can enhance the accuracy of predicting party identification. <div>
arXiv:2507.06193v1 Announce Type: cross 
Abstract: How well do demographics explain party identification? Demographics are related to party identification in political polls, news articles, and academic publications. Yet, there is a diversity of party identification even within demographic groups which have historically been attached to one party. And some groups lack a clear connection to either party. It may be that demographics on their own fail to account for the fact that people generally belong to a variety of groups. They must select the groups which are most important to them when shaping a political identity, and may choose to construct an identity relatively unattached to any specific demographic group to which they belong. This prompts the question, do we need to consider measures of identity strength when using demographics to explain party identification? We utilize a predictive framework to address these questions and find that demographics are highly predictive for some groups (e.g., Black Democrats), while others benefit from the inclusion of identity strength (e.g., Hispanic Republicans).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.12149</link>
<guid>https://arxiv.org/abs/2503.12149</guid>
<content:encoded><![CDATA[
<div> Keywords: LVLMs, multimodal sarcasm, interpretive variations, subjectivity, uncertainty-aware modeling

Summary:
LVLMs, or large vision-language models, are increasingly being used to understand multimodal sarcasm. This study evaluates 12 LVLMs using a systematic framework on existing sarcasm datasets, analyzing interpretive variations and subjective perspectives. The results show discrepancies in how different models interpret sarcasm, especially when given varied prompts. While classification prompts show higher internal consistency, models diverge in interpretive reasoning. These findings challenge traditional labeling paradigms by highlighting sarcasm's subjectivity. The study suggests moving towards uncertainty-aware modeling to better understand multimodal sarcasm comprehension. This research provides valuable insights for developing more nuanced and deeper comprehension of sarcasm in LVLMs.<br /><br />Summary: <div>
arXiv:2503.12149v2 Announce Type: replace-cross 
Abstract: With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous "neutral" cases. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic</title>
<link>https://arxiv.org/abs/2507.03448</link>
<guid>https://arxiv.org/abs/2507.03448</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven, mean-field approach, popularity dynamics, influencers, social networks

Summary: 
This paper introduces a data-driven mean-field model to study the dynamics of influencer popularity on social media platforms. The model incorporates individual activity patterns, content virality, external events, and platform visibility to predict the success of influencers. By deriving conditions for system ergodicity, the model can forecast popularity distributions among influencers. A sensitivity analysis examines different system setups to identify factors that can lead to either dominance or fair play in the influencer ecosystem. The findings provide insights into the potential evolution of social networks towards more equitable or biased influence dynamics. <div>
arXiv:2507.03448v1 Announce Type: new 
Abstract: This paper presents a data-driven mean-field approach to model the popularity dynamics of users seeking public attention, i.e., influencers. We propose a novel analytical model that integrates individual activity patterns, expertise in producing viral content, exogenous events, and the platform's role in visibility enhancement, ultimately determining each influencer's success. We analytically derive sufficient conditions for system ergodicity, enabling predictions of popularity distributions. A sensitivity analysis explores various system configurations, highlighting conditions favoring either dominance or fair play among influencers. Our findings offer valuable insights into the potential evolution of social networks towards more equitable or biased influence ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Active Global Vaccine Infodemic Risk Assessment</title>
<link>https://arxiv.org/abs/2507.04222</link>
<guid>https://arxiv.org/abs/2507.04222</guid>
<content:encoded><![CDATA[
<div> AI, social listening, vaccine infodemics, misinformation, public health

Summary: 
The paper presents VaxPulse VIRAL, an AI-powered social listening platform to monitor and assess vaccine-related infodemic risks. It utilizes machine learning methods like deep learning and active learning to analyze public sentiments, misinformation trends, and social bot activity in real-time. The platform's dynamic dashboards offer tailored insights for immunization programs and combatting misinformation. Iterative feedback from experts and stakeholders guides continuous improvements. Collaboration with an international network and community leaders ensures ongoing enhancements to VaxPulse. This innovative approach aims to address the challenges posed by vaccine infodemics and support global public health initiatives. <br /><br /> <div>
arXiv:2507.04222v1 Announce Type: new 
Abstract: Vaccine infodemics, driven by misinformation, disinformation, and inauthentic online behaviours, pose significant threats to global public health. This paper presents our response to this challenge, demonstrating how we developed VaxPulse Vaccine Infodemic Risk Assessment Lifecycle (VIRAL), an AI-powered social listening platform designed to monitor and assess vaccine-related infodemic risks. Leveraging interdisciplinary expertise and international collaborations, VaxPulse VIRAL integrates machine learning methods, including deep learning, active learning, and data augmentation, to provide real-time insights into public sentiments, misinformation trends, and social bot activity. Iterative feedback from domain experts and stakeholders has guided the development of dynamic dashboards that offer tailored, actionable insights to support immunisation programs and address information disorder. Ongoing improvements to VaxPulse will continue through collaboration with our international network and community leaders.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating User Watch-Time to Investigate Bias in YouTube Shorts Recommendations</title>
<link>https://arxiv.org/abs/2507.04534</link>
<guid>https://arxiv.org/abs/2507.04534</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, engagement-driven algorithms, content exposure, viewer behaviors, relevance shift

Summary: 
This study explores the impact of viewer behaviors, such as fast scrolling or skipping, on the relevance and topical continuity of recommended videos on short-form video platforms like YouTube Shorts. Analyzing a dataset of over 404,000 videos on geopolitical themes and conflicts, including Russia, China, the Russia-Ukraine War, and the South China Sea dispute, the research simulates viewer interactions and assesses how relevance changes across recommendation chains under different watch-time conditions. Using GPT-4o to measure semantic alignment between videos, the study uncovers patterns of amplification, drift, and topic generalization that have significant implications for content diversity and platform accountability. By combining insights from computer science, media studies, and political communication, this interdisciplinary work enhances our understanding of how engagement cues shape algorithmic pathways in short-form content ecosystems. 

Summary: <div>
arXiv:2507.04534v1 Announce Type: new 
Abstract: Short-form video platforms such as YouTube Shorts increasingly shape how information is consumed, yet the effects of engagement-driven algorithms on content exposure remain poorly understood. This study investigates how different viewing behaviors, including fast scrolling or skipping, influence the relevance and topical continuity of recommended videos. Using a dataset of over 404,000 videos, we simulate viewer interactions across both broader geopolitical themes and more narrowly focused conflicts, including topics related to Russia, China, the Russia-Ukraine War, and the South China Sea dispute. We assess how relevance shifts across recommendation chains under varying watch-time conditions, using GPT-4o to evaluate semantic alignment between videos. Our analysis reveals patterns of amplification, drift, and topic generalization, with significant implications for content diversity and platform accountability. By bridging perspectives from computer science, media studies, and political communication, this work contributes a multidisciplinary understanding of how engagement cues influence algorithmic pathways in short-form content ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Algorithmic Bias in YouTube Shorts</title>
<link>https://arxiv.org/abs/2507.04605</link>
<guid>https://arxiv.org/abs/2507.04605</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, algorithmic bias, content visibility, drift, generative AI models<br />
<br />
Summary: 
The study explores algorithmic bias in YouTube Shorts' recommendation system, focusing on watch-time duration, topic sensitivity, and engagement metrics. Analyzing over 685,000 videos across different content domains, the research finds a drift away from politically sensitive content towards entertainment-focused videos. Emotion analysis reveals a preference for joyful or neutral content, while highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. These findings highlight how algorithm design influences content exposure on YouTube Shorts, impacting information diversity and transparency on the platform. <div>
arXiv:2507.04605v1 Announce Type: new 
Abstract: The rapid growth of YouTube Shorts, now serving over 2 billion monthly users, reflects a global shift toward short-form video as a dominant mode of online content consumption. This study investigates algorithmic bias in YouTube Shorts' recommendation system by analyzing how watch-time duration, topic sensitivity, and engagement metrics influence content visibility and drift. We focus on three content domains: the South China Sea dispute, the 2024 Taiwan presidential election, and general YouTube Shorts content. Using generative AI models, we classified 685,842 videos across relevance, topic category, and emotional tone. Our results reveal a consistent drift away from politically sensitive content toward entertainment-focused videos. Emotion analysis shows a systematic preference for joyful or neutral content, while engagement patterns indicate that highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. This work provides the first comprehensive analysis of algorithmic drift in YouTube Shorts based on textual content, emotional tone, topic categorization, and varying watch-time conditions. These findings offer new insights into how algorithmic design shapes content exposure, with implications for platform transparency and information diversity.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Monitoring of Online Public Concerns to Enhance Post-licensure Vaccine Surveillance</title>
<link>https://arxiv.org/abs/2507.04656</link>
<guid>https://arxiv.org/abs/2507.04656</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine safety, misinformation management, sentiment analysis, vaccine hesitancy, public opinion

Summary: 
The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media.<br /><br />Summary: The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media. <div>
arXiv:2507.04656v1 Announce Type: new 
Abstract: The recent vaccine-related infodemic has amplified public concerns, highlighting the need for proactive misinformation management. We describe how we enhanced the reporting surveillance system of Victoria's vaccine safety service, SAEFVIC, through the incorporation of new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online. Using VaxPulse, a multi-step framework, we integrate adverse events following immunisation (AEFI) with sentiment analysis, demonstrating the importance of contextualising public concerns. Additionally, we emphasise the need to address non-English languages to stratify concerns across ethno-lingual communities, providing valuable insights for vaccine uptake strategies and combating mis/disinformation. The framework is applied to real-world examples and a case study on women's vaccine hesitancy, showcasing its benefits and adaptability by identifying public opinion from online media.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancement of Circular Economy Through Interdisciplinary Collaboration: A Bibliometric Approach</title>
<link>https://arxiv.org/abs/2507.04923</link>
<guid>https://arxiv.org/abs/2507.04923</guid>
<content:encoded><![CDATA[
<div> Circular Economy, Research, Interdisciplinary Collaboration, Business and Management, Engineering<br />
<br />
Summary: 
The study analyzes over 25,000 Circular Economy (CE) publications to understand the interdisciplinary nature and researcher dynamics of the field. It identifies 16 research clusters and visualizes collaboration patterns among researchers. Business and management research attract significant attention, while engineering research tends to secure higher funding success. Collaborative CE papers from different disciplines demonstrate higher research impact compared to intradisciplinary work, emphasizing the value of interdisciplinary efforts. Case studies highlight the benefits of collaborations between business-oriented and engineering-oriented disciplines. The findings suggest a positive dynamic where attention drawn by business research contributes to securing economic resources for realizing CE goals. The study provides insights for guiding future cross-disciplinary engagement in the CE field. <br /><br /> <div>
arXiv:2507.04923v1 Announce Type: new 
Abstract: Since the European Union introduced its Circular Economy (CE) Action Plan in 2015, CE research has expanded rapidly. However, the structure of this emerging field - both in terms of its constituent disciplines and researcher dynamics - remains poorly understood. To address this gap, we analyze over 25,000 CE-related publications from Scopus by combining conventional bibliometric approaches with advanced machine learning techniques, including text embeddings and clustering. This hybrid method enables both a macro-level mapping of research domains and a micro-level investigation of individual researchers' disciplinary backgrounds and collaborations.
  We classify CE research into 16 distinct clusters, identifying the original disciplines of researchers and visualizing patterns of interdisciplinary collaboration. Building on this foundation, we ask: Which CE-related research domains receive the most attention in academic and policy contexts? And how are different types of interdisciplinary collaboration associated with research impact?
  Our findings show that research in business and management attracts substantial academic and policy attention, while engineering research - though less visible - tends to achieve higher funding success. This suggests a positive dynamic in which the former draws attention to CE issues and the latter secures the economic resources necessary to realize them.
  We further demonstrate that CE papers co-authored by researchers from different disciplines tend to show higher research impact than intradisciplinary work. Qualitative case analyses also highlight this tendency. Centered particularly on collaborations between business-oriented and engineering-oriented disciplines, our findings underscore the importance of interdisciplinary efforts in CE research and offer insights for guiding future cross-disciplinary engagement in the field.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban Behavior Explanations</title>
<link>https://arxiv.org/abs/2507.04995</link>
<guid>https://arxiv.org/abs/2507.04995</guid>
<content:encoded><![CDATA[
<div> Interest Networks, LBSNs, spatial granularities, recommendation system, explainable AI<br />
Summary: Location-Based Social Networks (LBSNs) provide valuable insights into urban behavior through Interest Networks (iNETs), with this study comparing iNETs on Google Places and Foursquare at different spatial levels. It reveals that user interests are mainly influenced by proximity and venue similarity, with socioeconomic and political factors playing a smaller role. The research develops a multi-level recommendation system that caters to different user behaviors, leveraging explainable AI techniques to offer personalized urban recommendations with natural-language explanations. The study introduces h3-cities for spatial analysis and releases a public demo for interactive exploration. This approach contributes to urban mobility research by delivering scalable, context-aware, and interpretable recommendation systems.<br /><br />Summary: <div>
arXiv:2507.04995v1 Announce Type: new 
Abstract: Location-Based Social Networks (LBSNs) provide a rich foundation for modeling urban behavior through iNETs (Interest Networks), which capture how user interests are distributed throughout urban spaces. This study compares iNETs across platforms (Google Places and Foursquare) and spatial granularities, showing that coarser levels reveal more consistent cross-platform patterns, while finer granularities expose subtle, platform-specific behaviors. Our analysis finds that, in general, user interest is primarily shaped by geographic proximity and venue similarity, while socioeconomic and political contexts play a lesser role. Building on these insights, we develop a multi-level, explainable recommendation system that predicts high-interest urban regions for different user types. The model adapts to behavior profiles -- such as explorers, who are driven by proximity, and returners, who prefer familiar venues -- and provides natural-language explanations using explainable AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool for multi-scale spatial analysis, and release a public demo for interactively exploring personalized urban recommendations. Our findings contribute to urban mobility research by providing scalable, context-aware, and interpretable recommendation systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Location Disclosure Fails to Deter Overseas Criticism but Amplifies Regional Divisions on Chinese Social Media</title>
<link>https://arxiv.org/abs/2507.03238</link>
<guid>https://arxiv.org/abs/2507.03238</guid>
<content:encoded><![CDATA[
<div> location disclosure policy, Sina Weibo, user behavior, censorship, online participation <br />
Summary: The study analyzes the impact of a user location disclosure policy on Sina Weibo, China's largest microblogging platform. It finds that the policy, implemented to deter overseas users from spreading harmful information, did not reduce their engagement. Instead, it significantly decreased domestic users' willingness to comment on local issues outside their provinces, particularly affecting out-of-province commenters and criticisms. The policy led to a rise in regionally discriminatory replies and reshaped online participation norms. The findings suggest that authoritarian regimes can use social cleavages, such as regional divisions, to reinforce censorship, suppress dissent, and fragment public discourse. <div>
arXiv:2507.03238v1 Announce Type: cross 
Abstract: We examine the behavioral impact of a user location disclosure policy implemented on Sina Weibo, China's largest microblogging platform, using a high-frequency, real-time dataset of uncensored user engagement with 165 leading government and media accounts. Leveraging a natural experiment result from the platform's sudden rollout of location tagging on April 28, 2022, we compare millions of time-stamped observations of user behavior in the comment sections of these accounts before and after the policy change. Although the policy appeared intended to deter overseas users from spreading information deemed harmful by the regime, we find no reduction in their engagement. Instead, the policy sharply reduced domestic users' willingness to comment on posts about local issues outside their own provinces. This effect was especially pronounced among out-of-province commenters and disproportionately curtailed criticisms. Using large language models, we further show that location disclosure triggered a rise in regionally discriminatory replies, which in turn heightened the perceived risk of cross-provincial engagement and reshaped the norms of online participation. Our findings suggest that authoritarian regimes can reinforce censorship not only through top-down control, but by mobilizing social cleavages, here, regional divisions, to suppress dissent and fragment public discourse.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products</title>
<link>https://arxiv.org/abs/2507.04364</link>
<guid>https://arxiv.org/abs/2507.04364</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine learning, health sentiments, Large Language Models, sentiment classification, public health

Summary: 
Machine learning methods are being used to analyze health-related public discourse, but their accuracy in detecting different health sentiments is not well understood. This research focused on three Large Language Models (LLMs)  GPT, Gemini, and LLAMA  to classify risk-promoting versus health-supporting sentiments on HPV vaccination and heated tobacco products. Results showed that all three LLMs were generally accurate in classifying sentiments, with some variations based on platform, health issue, and model type. Higher accuracy was observed for risk-promoting sentiment on Facebook, while health-supporting messages were better detected on Twitter. However, LLMs faced challenges in accurately detecting neutral messages. This study emphasizes the importance of validating language models for public health analysis and being aware of potential biases in training data influencing the results. 

<br /><br />Summary: <div>
arXiv:2507.04364v1 Announce Type: cross 
Abstract: Machine learning methods are increasingly applied to analyze health-related public discourse based on large-scale data, but questions remain regarding their ability to accurately detect different types of health sentiments. Especially, Large Language Models (LLMs) have gained attention as a powerful technology, yet their accuracy and feasibility in capturing different opinions and perspectives on health issues are largely unexplored. Thus, this research examines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in detecting risk-promoting versus health-supporting sentiments across two critical public health topics: Human Papillomavirus (HPV) vaccination and heated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we curated multiple sets of messages supporting or opposing recommended health behaviors, supplemented with human annotations as the gold standard for sentiment classification. The findings indicate that all three LLMs generally demonstrate substantial accuracy in classifying risk-promoting and health-supporting sentiments, although notable discrepancies emerge by platform, health issue, and model type. Specifically, models often show higher accuracy for risk-promoting sentiment on Facebook, whereas health-supporting messages on Twitter are more accurately detected. An additional analysis also shows the challenges LLMs face in reliably detecting neutral messages. These results highlight the importance of carefully selecting and validating language models for public health analyses, particularly given potential biases in training data that may lead LLMs to overestimate or underestimate the prevalence of certain perspectives.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Social Media Network Effects</title>
<link>https://arxiv.org/abs/2507.04545</link>
<guid>https://arxiv.org/abs/2507.04545</guid>
<content:encoded><![CDATA[
<div> local network effects, social media platforms, consumer surplus, incentive-compatible, online choice experiments

Summary:
Local network effects play a significant role in the value generated by social media platforms, with 20-34% of the total value derived from these effects. Different platforms show varying levels of value, with stronger ties being more valuable on Facebook and Instagram, while weaker ties are preferred on LinkedIn and X. Gender and race also play a role in how connections are valued, with men valuing connections to women more on specific platforms. Additionally, consumer behavior differs based on whether they are looking for work or not, with LinkedIn being more valuable for job seekers. Overall, social media platforms generate between $53B and $215B in consumer surplus per year in the US, highlighting the significant impact of local network effects on platform value. <div>
arXiv:2507.04545v1 Announce Type: cross 
Abstract: We use representative, incentive-compatible online choice experiments involving 19,923 Facebook, Instagram, LinkedIn, and X users in the US to provide the first large-scale, empirical measurement of local network effects in the digital economy. Our analysis reveals social media platform value ranges from $78 to $101 per consumer, per month, on average, and that 20-34% of that value is explained by local network effects. We also find 1) stronger ties are more valuable on Facebook and Instagram, while weaker ties are more valuable on LinkedIn and X; 2) connections known through work are most valuable on LinkedIn and least valuable on Facebook, and people looking for work value LinkedIn significantly more and Facebook significantly less than people not looking for work; 3) men value connections to women on social media significantly more than they value connections to other men, particularly on Instagram, Facebook and X, while women value connections to men and women equally; 4) white consumers value relationships with other white consumers significantly more than they value relationships with non-white consumers on Facebook while, on Instagram, connections to alters eighteen years old or younger are valued significantly more than any other age group-two patterns not seen on any other platforms. Social media platforms individually generate between $53B and $215B in consumer surplus per year in the US alone. These results suggest social media generates significant value, local network effects drive a substantial fraction of that value and that these effects vary across platforms, consumers, and connections.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, Social Network Data, Predictive Modeling, External Factors, Proactive Security Planning 

Summary: 
The article introduces a novel approach to understanding the Perception of Security (PoS) using social network data. By analyzing social network content, the model aims to offer real-time monitoring and predictive insights into security perceptions. The model incorporates external factors that influence the publication and reposting of security-related content, achieving competitive predictive performance while maintaining interpretability. The research highlights the importance of temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. Overall, the innovative approach presented in the article contributes to improving the measurement and understanding of security perceptions in short time frames, enhancing the ability to anticipate and address security concerns effectively. 

<br /><br />Summary: <div>
arXiv:2402.03378v2 Announce Type: replace 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Social Sphere Model: Heuristic Influence Prediction in Evolving Networks</title>
<link>https://arxiv.org/abs/2402.03522</link>
<guid>https://arxiv.org/abs/2402.03522</guid>
<content:encoded><![CDATA[
<div> link prediction, influencers, influence maximization, deep learning, social network analysis

Summary:
The study explores admissions in a university program for influencers, focusing on influence maximization and link prediction in social network analysis. It introduces The Social Sphere Model, an algorithm that combines path-based link prediction metrics and heuristic influence maximization strategies to identify future key nodes in weighted networks. Testing on contagion models shows promising results with lower computational requirements. This advancement enhances understanding of network dynamics and offers a more efficient approach to network management and influence strategy development. <div>
arXiv:2402.03522v3 Announce Type: replace 
Abstract: How would admissions look like in a university program for influencers? In the realm of social network analysis, influence maximization and link prediction stand out as pivotal challenges. Influence maximization focuses on identifying a set of key nodes to maximize information dissemination, while link prediction aims to foresee potential connections within the network. These strategies, primarily deep learning link prediction methods and greedy algorithms, have been previously used in tandem to identify future influencers. However, given the complexity of these tasks, especially in large-scale networks, we propose an algorithm, The Social Sphere Model, which uniquely utilizes expected value in its future graph prediction and combines specifically path-based link prediction metrics and heuristic influence maximization strategies to effectively identify future vital nodes in weighted networks. Our approach is tested on two distinct contagion models, offering a promising solution with lower computational demands. This advancement not only enhances our understanding of network dynamics but also opens new avenues for efficient network management and influence strategy development.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning</title>
<link>https://arxiv.org/abs/2410.10875</link>
<guid>https://arxiv.org/abs/2410.10875</guid>
<content:encoded><![CDATA[
<div> effective resistances, flow-based community detection, spectral framework, hypergraph partitioning, VLSI designs

Summary: 
The paper introduces SHyPar, a multilevel spectral framework for partitioning hypergraphs that considers structural features. It leverages hyperedge effective resistances and flow-based community detection techniques to guide the partitioning process. SHyPar aims to decompose hypergraphs into subgraphs with minimal inter-partition hyperedges. A key component is a flow-based local clustering scheme for hypergraph coarsening, which improves conductance. The framework also uses an effective resistance-based rating function for merging strongly connected nodes. Experimental results on VLSI designs show that SHyPar outperforms existing methods in terms of solution quality. <div>
arXiv:2410.10875v3 Announce Type: replace 
Abstract: State-of-the-art hypergraph partitioners utilize a multilevel paradigm to construct progressively coarser hypergraphs across multiple layers, guiding cut refinements at each level of the hierarchy. Traditionally, these partitioners employ heuristic methods for coarsening and do not consider the structural features of hypergraphs. In this work, we introduce a multilevel spectral framework, SHyPar, for partitioning large-scale hypergraphs by leveraging hyperedge effective resistances and flow-based community detection techniques. Inspired by the latest theoretical spectral clustering frameworks, such as HyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiple subgraphs with few inter-partition hyperedges (cut size). A key component of SHyPar is a flow-based local clustering scheme for hypergraph coarsening, which incorporates a max-flow-based algorithm to produce clusters with substantially improved conductance. Additionally, SHyPar utilizes an effective resistance-based rating function for merging nodes that are strongly connected (coupled). Compared with existing state-of-the-art hypergraph partitioning methods, our extensive experimental results on real-world VLSI designs demonstrate that SHyPar can more effectively partition hypergraphs, achieving state-of-the-art solution quality.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review</title>
<link>https://arxiv.org/abs/2409.18162</link>
<guid>https://arxiv.org/abs/2409.18162</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, augmented reality, user interface, autism spectrum disorder, therapy<br />
<br />
Summary: 
This review study delves into the integration of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children with autism spectrum disorder (ASD). Through a comprehensive literature review, the study identifies three key areas of focus: the impact of AR on social and learning outcomes, the role of LLMs in communication support, and the importance of effective UI/UX design in enhancing the efficacy of these technologies. Findings indicate that LLMs offer personalized learning and communication assistance, while AR shows potential in improving social skills, motivation, and attention for children with ASD. However, there is a dearth of robotics-based educational programs tailored specifically for autistic children. To optimize the benefits of these technologies, further research is needed to address issues surrounding customization, accessibility, and integration in ASD therapies and immersive education.<br /><br /> <div>
arXiv:2409.18162v2 Announce Type: replace-cross 
Abstract: The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Census-Based Genetic Algorithm for Target Set Selection Problem in Social Networks</title>
<link>https://arxiv.org/abs/2410.02011</link>
<guid>https://arxiv.org/abs/2410.02011</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, Target Set Selection Problem, Social Networks, Census, Viral Marketing<br />
<br />
Summary: This paper proposes a novel approach, the census-based genetic algorithm, to solve the Target Set Selection (TSS) Problem in social networks for viral marketing. The algorithm uses census information to maintain diversity and prevent premature convergence to local optima. It tracks the number of times individuals are identified and nodes are included in solutions. The algorithm can self-adjust by varying the aggressiveness parameter in reproduction methods and runs efficiently in a parallelized environment. Experimental results on random and real-life social network graphs demonstrate the algorithm's ability to find optimal solutions, outperforming previous studies by reducing solution size and including more network vertices. The novel approach shows promising results for efficiently solving the TSS problem in social networks. <br /><br />Summary: <div>
arXiv:2410.02011v2 Announce Type: replace-cross 
Abstract: This paper considers the Target Set Selection (TSS) Problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph are given. We need to find a minimum size vertex subset to "activate" such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called "a census-based genetic algorithm" for the TSS problem. In our algorithm, we use the idea of a census to gather and store information about each individual in a population and collect census data from the individuals constructed during the algorithm's execution so that we can achieve greater diversity and avoid premature convergence at locally optimal solutions. We use two distinct census information: (a) for each individual, the algorithm stores how many times it has been identified during the execution (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also self-adjust by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check each individual's feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, we execute the proposed algorithm on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Coefficient Reflecting Pairwise Relationships within Hyperedges</title>
<link>https://arxiv.org/abs/2410.23799</link>
<guid>https://arxiv.org/abs/2410.23799</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering coefficients, weighted graphs, higher-order motifs, complex networks 

Summary: 
This study introduces a novel clustering coefficient for hypergraphs that considers intra-hyperedge pairwise relationships and accurately quantifies local link density. By transforming hypergraphs into weighted graphs reflecting relationship strength between nodes based on hyperedge connections, the proposed coefficient addresses limitations of existing approaches. The new definition ensures values in the range [0,1], aligns with simple graph clustering coefficients, and effectively captures intra-hyperedge relationships. Theoretical evaluation on higher-order motifs demonstrates superior performance compared to existing definitions, particularly on motifs III, IV-a, IV-b of order 3. Empirical evaluation on real-world datasets confirms similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient provides a more accurate representation of structural characteristics in complex networks, particularly in systems where group membership implies connections between members, such as social communities and co-authorship networks.  

<br /><br />Summary: <div>
arXiv:2410.23799v2 Announce Type: replace-cross 
Abstract: Hypergraphs are generalizations of simple graphs that allow for the representation of complex group interactions beyond pairwise relationships. Clustering coefficients quantify local link density in networks and have been widely studied for both simple graphs and hypergraphs. However, existing clustering coefficients for hypergraphs treat each hyperedge as a distinct unit rather than a collection of potentially related node pairs, failing to capture intra-hyperedge pairwise relationships and incorrectly assigning zero values to nodes with meaningful clustering patterns. We propose a novel clustering coefficient that addresses this fundamental limitation by transforming hypergraphs into weighted graphs, where edge weights reflect relationship strength between nodes based on hyperedge connections. Our definition satisfies three key conditions: values in the range $[0,1]$, consistency with simple graph clustering coefficients, and effective capture of intra-hyperedge pairwise relationships -- a capability absent from existing approaches. Theoretical evaluation on higher-order motifs demonstrates that our definition correctly assigns values to motifs where existing definitions fail (motifs III, IV-a, IV-b of order 3), while empirical evaluation on three real-world datasets shows similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient enables accurate quantification of local density in complex networks, revealing structural characteristics missed by existing definitions in systems where group membership implies connections between members, such as social communities and co-authorship networks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypermodularity and community detection in hypergraphs</title>
<link>https://arxiv.org/abs/2412.06935</link>
<guid>https://arxiv.org/abs/2412.06935</guid>
<content:encoded><![CDATA[
<div> community detection, higher-order networks, hypermodularity, spectral methods, hidden information<br />
<br />
Summary: 
This article introduces a formalism for detecting community structures in networks with higher-order interactions, such as hypergraphs. The concept of hypermodularity is utilized to apply spectral methods for community detection in these complex networks. The approach is tested on both synthetic random networks and real-world data, demonstrating its effectiveness in capturing the dynamics and nature of interactions within the networks. The study emphasizes the importance of considering higher-order interactions in network analysis and presents a valuable tool for extracting hidden information from intricate higher-order data sets. The results show that the proposed method can reveal nontrivial communities in various types of networked systems, including biological, social, and technological networks. This novel approach offers insights into the modular organization of networks, enabling a deeper understanding of their underlying structure and functionality. <div>
arXiv:2412.06935v2 Announce Type: replace-cross 
Abstract: Numerous networked systems feature a structure of nontrivial communities, which often correspond to their functional modules. Such communities have been detected in real-world biological, social and technological systems, as well as in synthetic models thereof. While much effort has been devoted to developing methods for community detection in traditional networks, the study of community structure in networks with higher-order interactions is still not as extensively explored. In this article, we introduce a formalism for the hypermodularity of higher-order networks that allows us to use spectral methods to detect community structures in hypergraphs. We apply this approach to synthetic random networks as well as to real-world data, showing that it produces results that reflect the nature and the dynamics of the interactions modelled, thereby constituting a valuable tool for the extraction of hidden information from complex higher-order data sets.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks</title>
<link>https://arxiv.org/abs/2502.02197</link>
<guid>https://arxiv.org/abs/2502.02197</guid>
<content:encoded><![CDATA[
<div> Signed networks, community detection, polarization, trust dynamics, local search <br />
<br />
Summary:
This paper presents a novel method for identifying polarized communities in signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions. The key challenge addressed by the proposed method is to detect internally cohesive and externally antagonistic communities while allowing for neutral vertices. By introducing a new optimization objective that avoids highly size-imbalanced solutions, the method outperforms existing approaches in solution quality. In addition, the paper introduces the first local search algorithm that extends to the setting with neutral vertices, providing a scalable solution for large networks. The approach is connected to block-coordinate Frank-Wolfe optimization, ensuring a linear convergence rate. Experimental results on real-world and synthetic datasets demonstrate the superiority of the proposed method in both solution quality and computational efficiency compared to state-of-the-art baselines. <br /><br />Summary: <div>
arXiv:2502.02197v2 Announce Type: replace-cross 
Abstract: Signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions, offer a natural framework for analyzing polarization, trust, and conflict in social systems. Detecting meaningful group structures in such networks is crucial for understanding online discourse, political divisions, and trust dynamics. A key challenge is to identify communities that are internally cohesive and externally antagonistic, while allowing for neutral or unaligned vertices. In this paper, we propose a method for identifying $k$ polarized communities that addresses a major limitation of prior methods: their tendency to produce highly size-imbalanced solutions. We introduce a novel optimization objective that avoids such imbalance. In addition, it is well known that approximation algorithms based on local search are highly effective for clustering signed networks when neutral vertices are not allowed. We build on this idea and design the first local search algorithm that extends to the setting with neutral vertices while scaling to large networks. By connecting our approach to block-coordinate Frank-Wolfe optimization, we prove a linear convergence rate, enabled by the structure of our objective. Experiments on real-world and synthetic datasets demonstrate that our method consistently outperforms state-of-the-art baselines in solution quality, while remaining competitive in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommendation Algorithms on Social Media: Unseen Drivers of Political Opinion</title>
<link>https://arxiv.org/abs/2507.01978</link>
<guid>https://arxiv.org/abs/2507.01978</guid>
<content:encoded><![CDATA[
<div> Facebook, X, social media platforms, algorithms, political interest
<br />
Summary: 
This study examines the impact of social media platforms on political interest among users, analyzing data from over 3,300 participants. The research finds that moderate Facebook users show decreased political engagement, while minimal engagement with X boosts political interest. Demographic variations play a significant role, with males, older individuals, Black or African American users, and those with higher incomes exhibiting greater political interest. Republicans are particularly active on social media, potentially influencing engagement patterns. However, a key limitation is the lack of data regarding the content users are exposed to. Future research should explore these influences and consider additional platforms to enhance understanding. Addressing these gaps can provide insights into digital political mobilization, benefiting policymakers, educators, and platform designers in fostering healthier democratic engagement.
<br /><br /> <div>
arXiv:2507.01978v1 Announce Type: new 
Abstract: Social media broadly refers to digital platforms and applications that simulate social interactions online. This study investigates the impact of social media platforms and their algorithms on political interest among users. As social media usage continues to rise, platforms like Facebook and X (formerly Twitter) play increasingly pivotal roles in shaping political discourse. By employing statistical analyses on data collected from over 3,300 participants, this research identifies significant differences in how various social media platforms influence political interest. Findings reveal that moderate Facebook users demonstrate decreased political engagement, whereas even minimal engagement with X significantly boosts political interest. The study further identifies demographic variations, noting that males, older individuals, Black or African American users, those with higher incomes show greater political interest. The demographic analysis highlights that Republicans are particularly active on social media - potentially influencing their social media engagement patterns. However, the study acknowledges a crucial limitation - the lack of direct data regarding the content users are exposed to which is shaping their social media experiences. Future research should explore these influences and consider additional popular platforms to enhance the understanding of social media's political impact. Addressing these gaps can provide deeper insights into digital political mobilization, aiding policymakers, educators, and platform designers in fostering healthier democratic engagement.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Large Semi-Synthetic Graphs of Any Size</title>
<link>https://arxiv.org/abs/2507.02166</link>
<guid>https://arxiv.org/abs/2507.02166</guid>
<content:encoded><![CDATA[
<div> Keywords: graph generation, deep learning, Graph Neural Networks, Latent Graph Sampling Generation (LGSG), diffusion models

Summary: 
The article introduces Latent Graph Sampling Generation (LGSG), a new framework for generating graphs that addresses limitations of current models. LGSG leverages diffusion models and node embeddings to generate graphs of varying sizes without the need for retraining. By eliminating the dependency on node IDs and capturing the distribution of node embeddings and subgraph structures, LGSG enables scalable and flexible graph generation. Experimental results show that LGSG performs comparably to baseline models for standard metrics and outperforms them in metrics such as the tendency of nodes to form clusters. Additionally, LGSG maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability. This framework represents a significant advancement in data-driven graph generation and has the potential to impact various applications in network science. 

<br /><br />Summary: <div>
arXiv:2507.02166v1 Announce Type: new 
Abstract: Graph generation is an important area in network science. Traditional approaches focus on replicating specific properties of real-world graphs, such as small diameters or power-law degree distributions. Recent advancements in deep learning, particularly with Graph Neural Networks, have enabled data-driven methods to learn and generate graphs without relying on predefined structural properties. Despite these advances, current models are limited by their reliance on node IDs, which restricts their ability to generate graphs larger than the input graph and ignores node attributes. To address these challenges, we propose Latent Graph Sampling Generation (LGSG), a novel framework that leverages diffusion models and node embeddings to generate graphs of varying sizes without retraining. The framework eliminates the dependency on node IDs and captures the distribution of node embeddings and subgraph structures, enabling scalable and flexible graph generation. Experimental results show that LGSG performs on par with baseline models for standard metrics while outperforming them in overlooked ones, such as the tendency of nodes to form clusters. Additionally, it maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features</title>
<link>https://arxiv.org/abs/2507.01984</link>
<guid>https://arxiv.org/abs/2507.01984</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, multimodal features, early fusion approach, social media, COVID-19 pandemic

Summary:
The study explores the integration of text, images, and social features in detecting misinformation on social media during the COVID-19 pandemic and election periods. By analyzing 1,529 tweets, the researchers found that combining unsupervised and supervised machine learning models improved classification performance by 15% compared to unimodal models. Additionally, incorporating techniques like object detection and OCR for extracting visual features further enhanced the accuracy of the classification model. The study also delves into the propagation patterns of misinformation, shedding light on the characteristics of misinformation tweets and the users responsible for spreading them. Overall, the findings highlight the importance of leveraging multimodal feature combinations for effectively detecting misinformation and understanding its dissemination dynamics on social media platforms. 

<br /><br />Summary: <div>
arXiv:2507.01984v1 Announce Type: cross 
Abstract: Amid a tidal wave of misinformation flooding social media during elections and crises, extensive research has been conducted on misinformation detection, primarily focusing on text-based or image-based approaches. However, only a few studies have explored multimodal feature combinations, such as integrating text and images for building a classification model to detect misinformation. This study investigates the effectiveness of different multimodal feature combinations, incorporating text, images, and social features using an early fusion approach for the classification model. This study analyzed 1,529 tweets containing both text and images during the COVID-19 pandemic and election periods collected from Twitter (now X). A data enrichment process was applied to extract additional social features, as well as visual features, through techniques such as object detection and optical character recognition (OCR). The results show that combining unsupervised and supervised machine learning models improves classification performance by 15% compared to unimodal models and by 5% compared to bimodal models. Additionally, the study analyzes the propagation patterns of misinformation based on the characteristics of misinformation tweets and the users who disseminate them.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source Detection in Hypergraph Epidemic Dynamics using a Higher-Order Dynamic Message Passing Algorithm</title>
<link>https://arxiv.org/abs/2507.02523</link>
<guid>https://arxiv.org/abs/2507.02523</guid>
<content:encoded><![CDATA[
<div> Hypergraph, source detection, infectious diseases, message-passing algorithm, susceptible-infectious dynamics  
Summary:  
- Source detection in infectious diseases is crucial for containment strategies.  
- Existing approaches focus on pairwise networks, but higher-order interactions may play a significant role.  
- The HDMPN algorithm is proposed for source detection on hypergraphs, considering group interactions.  
- By modulating likelihood maximization with the fraction of infectious neighbors, HDMPN outperforms conventional methods in most cases.  
- Numerical simulations demonstrate the superiority of HDMPN over benchmarks in capturing the dynamics of epidemic spreading.   <div>
arXiv:2507.02523v1 Announce Type: cross 
Abstract: Source detection is crucial for capturing the dynamics of real-world infectious diseases and informing effective containment strategies. Most existing approaches to source detection focus on conventional pairwise networks, whereas recent efforts on both mathematical modeling and analysis of contact data suggest that higher-order (e.g., group) interactions among individuals may both account for a large fraction of infection events and change our understanding of how epidemic spreading proceeds in empirical populations. In the present study, we propose a message-passing algorithm, called the HDMPN, for source detection for a stochastic susceptible-infectious dynamics on hypergraphs. By modulating the likelihood maximization method by the fraction of infectious neighbors, HDMPN aims to capture the influence of higher-order structures and do better than the conventional likelihood maximization. We numerically show that, in most cases, HDMPN outperforms benchmarks including the likelihood maximization method without modification.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images</title>
<link>https://arxiv.org/abs/2507.02781</link>
<guid>https://arxiv.org/abs/2507.02781</guid>
<content:encoded><![CDATA[
<div> Keyword: earthquake, social media images, damage severity assessment, semantic segmentation, disaster reconnaissance <br />
Summary:
In the study, a novel approach is proposed for assessing damage severity in post-earthquake social media images. Traditional methods rely on subjective classification, but this study frames the problem as a semantic segmentation task for more objective analysis. A segmented dataset is created categorizing damage into undamaged structures, damaged structures, and debris. The SegFormer model is fine-tuned using this dataset to generate damage severity segmentations. A new scoring system is introduced to quantify damage by considering the varying degrees of damage across different areas within images, adjusted for depth estimation. This approach enables a more precise understanding of damage and provides valuable guidance to disaster reconnaissance teams for more effective response efforts following earthquakes. <br /><br />Summary: <div>
arXiv:2507.02781v1 Announce Type: cross 
Abstract: In the aftermath of earthquakes, social media images have become a crucial resource for disaster reconnaissance, providing immediate insights into the extent of damage. Traditional approaches to damage severity assessment in post-earthquake social media images often rely on classification methods, which are inherently subjective and incapable of accounting for the varying extents of damage within an image. Addressing these limitations, this study proposes a novel approach by framing damage severity assessment as a semantic segmentation problem, aiming for a more objective analysis of damage in earthquake-affected areas. The methodology involves the construction of a segmented damage severity dataset, categorizing damage into three degrees: undamaged structures, damaged structures, and debris. Utilizing this dataset, the study fine-tunes a SegFormer model to generate damage severity segmentations for post-earthquake social media images. Furthermore, a new damage severity scoring system is introduced, quantifying damage by considering the varying degrees of damage across different areas within images, adjusted for depth estimation. The application of this approach allows for the quantification of damage severity in social media images in a more objective and comprehensive manner. By providing a nuanced understanding of damage, this study enhances the ability to offer precise guidance to disaster reconnaissance teams, facilitating more effective and targeted response efforts in the aftermath of earthquakes.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into LLM-assisted writing in biomedical publications through excess vocabulary</title>
<link>https://arxiv.org/abs/2406.07016</link>
<guid>https://arxiv.org/abs/2406.07016</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Biomedical research, Scholarly writing, Vocabulary changes, Scientific writing<br />
Summary:<br />
- Large language models (LLMs) like ChatGPT are being widely used in academic literature for scholarly writing in the field of biomedical research.<br />
- A study of over 15 million biomedical abstracts from 2010-2024 indexed by PubMed shows an increase in the frequency of certain style words due to the appearance of LLMs, indicating that at least 13.5% of 2024 abstracts were processed with LLMs.<br />
- The impact of LLMs on scientific writing in biomedical research is significant, surpassing the effect of major world events such as the Covid pandemic.<br />
- LLMs have the capability to generate and revise text with human-level performance but come with limitations including producing inaccurate information, reinforcing biases, and being prone to misuse.<br />
- The usage of LLMs varied across disciplines, countries, and journals, with some subcorpora showing a 40% usage rate in 2024. <br /> <div>
arXiv:2406.07016v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging Activity Temporal Hypergraph (EATH), a model for generating realistic time-varying hypergraphs</title>
<link>https://arxiv.org/abs/2507.01124</link>
<guid>https://arxiv.org/abs/2507.01124</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal hypergraph, complex systems, synthetic datasets, higher-order contagion dynamics, memory mechanisms<br />
<br />
Summary: The study introduces a new model called the Emerging Activity Temporal Hypergraph (EATH) to mimic time-varying group interactions in complex systems. The EATH model can generate synthetic datasets with properties similar to real datasets by incorporating parameters from the original data. It demonstrates the ability to replicate temporal and topological features of empirical datasets of face-to-face interactions. The model allows for the simulation of higher-order contagion dynamics on both original and surrogate datasets to compare outcomes. Additionally, the flexibility of the EATH model enables the generation of synthetic hypergraphs with customizable properties, such as creating "hybrid" datasets by combining characteristics from different empirical datasets. Overall, this research provides a valuable tool for generating realistic temporal hypergraphs when data collection is challenging and offers insights into dynamic processes on temporal hypergraphs. <br /><br /> <div>
arXiv:2507.01124v1 Announce Type: cross 
Abstract: Time-varying group interactions constitute the building blocks of many complex systems. The framework of temporal hypergraphs makes it possible to represent them by taking into account the higher-order and temporal nature of the interactions. However, the corresponding datasets are often incomplete and/or limited in size and duration, and surrogate time-varying hypergraphs able to reproduce their statistical features constitute interesting substitutions, especially to understand how dynamical processes unfold on group interactions. Here, we present a new temporal hypergraph model, the Emerging Activity Temporal Hypergraph (EATH), which can be fed by parameters measured in a dataset and create synthetic datasets with similar properties. In the model, each node has an independent underlying activity dynamic and the overall system activity emerges from the nodes dynamics, with temporal group interactions resulting from both the activity of the nodes and memory mechanisms. We first show that the EATH model can generate surrogate hypergraphs of several empirical datasets of face-to-face interactions, mimicking temporal and topological properties at the node and hyperedge level. We also showcase the possibility to use the resulting synthetic data in simulations of higher-order contagion dynamics, comparing the outcome of such process on original and surrogate datasets. Finally, we illustrate the flexibility of the model, which can generate synthetic hypergraphs with tunable properties: as an example, we generate "hybrid" temporal hypergraphs, which mix properties of different empirical datasets. Our work opens several perspectives, from the generation of synthetic realistic hypergraphs describing contexts where data collection is difficult to a deeper understanding of dynamical processes on temporal hypergraphs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced Efficiency in the Right Fronto-Parietal Attentional Network During Distractor Suppression in Mild Cognitive Impairment</title>
<link>https://arxiv.org/abs/2507.01433</link>
<guid>https://arxiv.org/abs/2507.01433</guid>
<content:encoded><![CDATA[
<div> EEG, Mild Cognitive Impairment, Distractor Suppression, Global Efficiency, Attentional Control <br />
<br />
Summary: 
The study investigates distractor suppression mechanisms in Mild Cognitive Impairment (MCI) patients using EEG data and behavioral measures during attention tasks. Healthy controls (HCs) showed higher Global Efficiency (GE), faster Reaction Times (RTs), and higher Hit Rates (HRs) in congruent conditions compared to incongruent ones. HCs also exhibited increased GE in salient conditions for incongruent trials. MCI patients benefited from congruent conditions with shorter RTs and higher HRs but showed reduced adaptability in GE. The study suggests that alpha band coherence and GE could serve as early markers for cognitive impairment. The findings emphasize the importance of neural efficiency, processing speed, and task accuracy in MCI. This approach provides insights into cognitive load management and interference effects, offering implications for interventions targeting attentional control and processing speed in MCI patients. <br /><br /> <div>
arXiv:2507.01433v1 Announce Type: cross 
Abstract: Mild Cognitive Impairment (MCI) is a critical transitional stage between normal cognitive aging and dementia, making its early detection essential. This study investigates the neural mechanisms of distractor suppression in MCI patients using EEG and behavioral data during an attention-cueing Eriksen flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks with congruent and incongruent stimuli of varying saliency levels. During these tasks, EEG data were analyzed for alpha band coherence's functional connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency, and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster RTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent conditions. HCs also showed increased GE in salient conditions for incongruent trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions with shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent conditions but showed reduced adaptability in GE, with no significant GE differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early markers for cognitive impairment. By integrating GE, RT, and HR, this study provides insights into the interplay between neural efficiency, processing speed, and task accuracy. This approach offers valuable insights into cognitive load management and interference effects, indicating benefits for interventions aimed at improving attentional control and processing speed in MCIs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the interaction between science and misinformation in COVID-19 tweets</title>
<link>https://arxiv.org/abs/2507.01481</link>
<guid>https://arxiv.org/abs/2507.01481</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, science, Twitter, COVID-19, social media 

Summary: 
During the COVID-19 pandemic, a study analyzed the interaction between misinformation and science on Twitter. A database of 407M COVID-19-related tweets was examined to classify reliability and use Altmetric data to identify scientific publications. The study revealed that many users share both scientific and unreliable content, with 45% sharing both. Publications frequently shared by users also sharing unreliable content are more likely to be preprints and have lower impact. Misinformation is not linked to a lack of science, raising concerns about open science practices. The findings underscore the importance of proactive scientific engagement on social media platforms to combat false narratives in global crises. 

<br /><br />Summary: <div>
arXiv:2507.01481v1 Announce Type: cross 
Abstract: During the COVID-19 pandemic, scientific understanding related to the topic evolved rapidly. Along with scientific information being discussed widely, a large circulation of false information, labelled an infodemic by the WHO, emerged. Here, we study the interaction between misinformation and science on Twitter (now X) during the COVID-19 pandemic. We built a comprehensive database of $\sim$407M COVID-19 related tweets and classified the reliability of URLs in the tweets based on Media Bias/Fact Check. In addition, we use Altmetric data to see whether a tweet refers to a scientific publication. We find that many users find that many users share both scientific and unreliable content; out of the $\sim$1.2M users who share science, $45\%$ also share unreliable content. Publications that are more frequently shared by users who also share unreliable content are more likely to be preprints, slightly more often retracted, have fewer citations, and are published in lower-impact journals on average. Our findings suggest that misinformation is not related to a ``deficit'' of science. In addition, our findings raise some critical questions about certain open science practices and their potential for misuse. Given the fundamental opposition between science and misinformation, our findings highlight the necessity for proactive scientific engagement on social media platforms to counter false narratives during global crises.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling individual attention dynamics on online social media</title>
<link>https://arxiv.org/abs/2507.01511</link>
<guid>https://arxiv.org/abs/2507.01511</guid>
<content:encoded><![CDATA[
<div> Keywords: attention economy, limited attention, engagement decay, user interactions, Reddit

Summary: 
The article discusses the importance of understanding how individuals manage limited attention in the attention economy. A simple model is introduced to describe the decay of a user's engagement when faced with multiple inputs. The analytical analysis shows that individual attention decay is determined by the overall duration of interactions rather than the number or user activity. The model is validated using data from Reddit's Change My View subreddit, where user attention dynamics are explicitly traceable. Despite its simplicity, the model provides a crucial microscopic perspective that complements macroscopic studies in understanding how individuals navigate and prioritize attention in an information-rich environment. <br /><br />Summary: <div>
arXiv:2507.01511v1 Announce Type: cross 
Abstract: In the attention economy, understanding how individuals manage limited attention is critical. We introduce a simple model describing the decay of a user's engagement when facing multiple inputs. We analytically show that individual attention decay is determined by the overall duration of interactions, not their number or user activity. Our model is validated using data from Reddit's Change My View subreddit, where the user's attention dynamics is explicitly traceable. Despite its simplicity, our model offers a crucial microscopic perspective complementing macroscopic studies.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principal Graph Encoder Embedding and Principal Community Detection</title>
<link>https://arxiv.org/abs/2501.14939</link>
<guid>https://arxiv.org/abs/2501.14939</guid>
<content:encoded><![CDATA[
<div> principal communities, principal graph encoder embedding, vertex embedding, community score, graph adjacency matrix

Summary:<br />
- The paper introduces the concept of principal communities and proposes a method for detecting these communities and generating vertex embeddings simultaneously.
- The method computes a community score for each community, ranks them to determine community importance, and identifies a set of principal communities.
- Vertex embeddings are produced by retaining only the dimensions corresponding to these principal communities.
- The population version of the encoder embedding and community score are defined theoretically, based on a random Bernoulli graph distribution.
- Through simulations, the method demonstrates accurate detection of ground-truth principal communities, improved visualization, and vertex classification, along with robustness to label noise and computational scalability. 

Summary: <div>
arXiv:2501.14939v2 Announce Type: replace 
Abstract: In this paper, we introduce the concept of principal communities and propose a principal graph encoder embedding method that concurrently detects these communities and achieves vertex embedding. Given a graph adjacency matrix with vertex labels, the method computes a sample community score for each community, ranking them to measure community importance and estimate a set of principal communities. The method then produces a vertex embedding by retaining only the dimensions corresponding to these principal communities. Theoretically, we define the population version of the encoder embedding and the community score based on a random Bernoulli graph distribution. We prove that the population principal graph encoder embedding preserves the conditional density of the vertex labels and that the population community score successfully distinguishes the principal communities. We conduct a variety of simulations to demonstrate the finite-sample accuracy in detecting ground-truth principal communities, as well as the advantages in embedding visualization and subsequent vertex classification. The method is further applied to a set of real-world graphs, showcasing its numerical advantages, including robustness to label noise and computational scalability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Modeling with Household Coordination Activities under Limited Information via Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2409.17495</link>
<guid>https://arxiv.org/abs/2409.17495</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility patterns, household coordination, large language model, activity chains, mobility data

Summary:
This research addresses the challenge of understanding human mobility patterns by proposing a retrieval-augmented large language model (LLM) framework. The framework focuses on generating activity chains with household coordination using public accessible statistical and socio-demographic information. By incorporating a retrieval-augmentation mechanism, the model can accurately represent household coordination and maintain statistical consistency in generated mobility patterns. This approach fills a key gap in existing methods that primarily focus on spatial-temporal patterns. The validation with NHTS and SCAG-ABM datasets shows that the framework effectively synthesizes mobility patterns and can adapt well to regions with limited mobility data availability. Overall, this research offers a promising solution to the limitations of conventional activity-based models and learning-based human mobility modeling algorithms. <div>
arXiv:2409.17495v2 Announce Type: replace-cross 
Abstract: Understanding human mobility patterns has long been a challenging task in transportation modeling. Due to the difficulties in obtaining high-quality training datasets across diverse locations, conventional activity-based models and learning-based human mobility modeling algorithms are particularly limited by the availability and quality of datasets. Current approaches primarily focus on spatial-temporal patterns while neglecting semantic relationships such as logical connections or dependencies between activities and household coordination activities like joint shopping trips or family meal times, both crucial for realistic mobility modeling. We propose a retrieval-augmented large language model (LLM) framework that generates activity chains with household coordination using only public accessible statistical and socio-demographic information, reducing the need for sophisticated mobility data. The retrieval-augmentation mechanism enables household coordination and maintains statistical consistency across generated patterns, addressing a key gap in existing methods. Our validation with NHTS and SCAG-ABM datasets demonstrates effective mobility synthesis and strong adaptability for regions with limited mobility data availability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tenure and Research Trajectories</title>
<link>https://arxiv.org/abs/2411.10575</link>
<guid>https://arxiv.org/abs/2411.10575</guid>
<content:encoded><![CDATA[
<div> tenure, research trajectories, publication rates, creative search, impact <br />
Summary: 
The study examines the impact of tenure on faculty research trajectories in the US academic system. It reveals that publication rates rise significantly during the tenure track, peaking just before tenure. Post-tenure research output varies among disciplines, with lab-based fields maintaining high output while non-lab-based fields experience a decline. Tenured faculty tend to engage in novel, high-risk research post-tenure, although this comes at the expense of impact as post-tenure research yields fewer highly cited papers. The study highlights the role of tenure in shaping individual research trajectories, with breaks in trajectories closely tied to an individual's tenure year. These findings provide valuable insights into the dynamics of the tenure system, faculty research patterns, and scientific output. <br /> <div>
arXiv:2411.10575v2 Announce Type: replace-cross 
Abstract: Tenure is a cornerstone of the US academic system, yet its relationship to faculty research trajectories remains poorly understood. Conceptually, tenure systems may act as a selection mechanism, screening in high-output researchers; a dynamic incentive mechanism, encouraging high output prior to tenure but low output after tenure; and a creative search mechanism, encouraging tenured individuals to undertake high-risk work. Here, we integrate data from seven different sources to trace US tenure-line faculty and their research outputs at an unprecedented scale and scope, covering over 12,000 researchers across 15 disciplines. Our analysis reveals that faculty publication rates typically increase sharply during the tenure track and peak just before obtaining tenure. Post-tenure trends, however, vary across disciplines: in lab-based fields, such as biology and chemistry, research output typically remains high post-tenure, whereas in non-lab-based fields, such as mathematics and sociology, research output typically declines substantially post-tenure. Turning to creative search, faculty increasingly produce novel, high-risk research after securing tenure. However, this shift toward novelty and risk-taking comes with a decline in impact, with post-tenure research yielding fewer highly cited papers. Comparing outcomes across common career ages but different tenure years or comparing research trajectories in tenure-based and non-tenure-based research settings underscores that breaks in the research trajectories are sharply tied to the individual's tenure year. Overall, these findings provide a new empirical basis for understanding the tenure system, individual research trajectories, and the shape of scientific output.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wordkrill: Extending Wordfish into the multidimensional political space</title>
<link>https://arxiv.org/abs/2506.20275</link>
<guid>https://arxiv.org/abs/2506.20275</guid>
<content:encoded><![CDATA[
<div> Wordkrill, Wordfish, multidimensional, political conflict, spatial models <br />
Summary: Spatial models play a key role in the analysis of political conflict, often relying on text-based methods such as the Wordfish model. However, Wordfish's unidimensionality limits its ability to capture the multidimensional nature of political competition. To address this limitation, Wordkrill is introduced as a multidimensional extension of Wordfish that allows for the estimation of political positions along multiple latent dimensions. The mathematical framework of Wordkrill is presented, and its utility is demonstrated through applications to party manifestos and parliamentary speeches. While Wordkrill offers practical advantages in estimating political positions, there are current limitations that need to be addressed. Further exploration and refinement of Wordkrill could enhance the understanding of political conflicts and actors' positions in multidimensional space. <br /> <div>
arXiv:2506.20275v2 Announce Type: replace-cross 
Abstract: Spatial models are central to the study of political conflict, yet their empirical application often depends on text-based methods. A prominent example is the Wordfish model, which estimates actor positions from political texts. However, a key limitation of Wordfish is its unidimensionality, despite the well-established multidimensional nature of political competition. This contribution introduces Wordkrill, a multidimensional extension of Wordfish that retains the original model's interpretability while allowing for efficient estimation of political positions along multiple latent dimensions. After presenting the mathematical framework of Wordkrill, its utility through brief applications to party manifestos and parliamentary speeches is demonstrated. These examples illustrate both the practical advantages and current limitations of the approach.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Dynamics with Self-Interaction Learning in Networked Systems</title>
<link>https://arxiv.org/abs/2507.00422</link>
<guid>https://arxiv.org/abs/2507.00422</guid>
<content:encoded><![CDATA[
<div> evolution cooperation networked systems social networks multi-agent systems biological species <br />
Summary:<br />
The study explores the role of self-interaction in networked evolutionary dynamics. It investigates how self-persistence of strategies impacts the evolution of cooperation in various systems. The research introduces a self-interaction landscape to assess an agent's ability to maintain its strategy based on local topology. Findings reveal that appropriate self-interaction can promote cooperation by reducing the conditions for cooperation to thrive and helping cooperators resist full defection. In systems where spitefulness prevails, self-interaction can protect cooperative agents from harm. The study suggests that a well-designed self-interaction landscape can significantly lower the critical conditions for advantageous mutants, especially in high-degree networks. This research contributes to understanding the dynamics of cooperation in networked systems and sheds light on the importance of self-interaction in evolutionary processes. <br /> <div>
arXiv:2507.00422v1 Announce Type: new 
Abstract: The evolution of cooperation in networked systems helps to understand the dynamics in social networks, multi-agent systems, and biological species. The self-persistence of individual strategies is common in real-world decision making. The self-replacement of strategies in evolutionary dynamics forms a selection amplifier, allows an agent to insist on its autologous strategy, and helps the networked system to avoid full defection. In this paper, we study the self-interaction learning in the networked evolutionary dynamics. We propose a self-interaction landscape to capture the strength of an agent's self-loop to reproduce the strategy based on local topology. We find that proper self-interaction can reduce the condition for cooperation and help cooperators to prevail in the system. For a system that favors the evolution of spite, the self-interaction can save cooperative agents from being harmed. Our results on random networks further suggest that an appropriate self-interaction landscape can significantly reduce the critical condition for advantageous mutants, especially for large-degree networks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks</title>
<link>https://arxiv.org/abs/2507.00600</link>
<guid>https://arxiv.org/abs/2507.00600</guid>
<content:encoded><![CDATA[
<div> role-based clustering, financial institutions, multi-layer networks, market segments, node embeddings

Summary:
The study introduces a novel role-based clustering method for analyzing financial networks, aiming to uncover the functional positions of institutions across various market layers. By utilizing interpretative node embeddings derived from egonet features, the approach captures both direct and indirect trading relationships within and between market segments. Transaction-level data from the ECB's Money Market Statistical Reporting (MMSR) is used to demonstrate the method's effectiveness in identifying diverse institutional roles such as market intermediaries, cross-segment connectors, and peripheral lenders or borrowers. This approach offers flexibility and practical insights for supervisory, systemic risk assessment, and resolution planning purposes, enhancing understanding of institutional behavior within complex market structures.<br /><br />Summary: <div>
arXiv:2507.00600v1 Announce Type: new 
Abstract: Understanding the functional roles of financial institutions within interconnected markets is critical for effective supervision, systemic risk assessment, and resolution planning. We propose an interpretable role-based clustering approach for multi-layer financial networks, designed to identify the functional positions of institutions across different market segments. Our method follows a general clustering framework defined by proximity measures, cluster evaluation criteria, and algorithm selection. We construct explainable node embeddings based on egonet features that capture both direct and indirect trading relationships within and across market layers. Using transaction-level data from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate how the approach uncovers heterogeneous institutional roles such as market intermediaries, cross-segment connectors, and peripheral lenders or borrowers. The results highlight the flexibility and practical value of role-based clustering in analyzing financial networks and understanding institutional behavior in complex market structures.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Differences in International Research Collaboration in European Union</title>
<link>https://arxiv.org/abs/2507.00619</link>
<guid>https://arxiv.org/abs/2507.00619</guid>
<content:encoded><![CDATA[
<div> gender-based authorship patterns, International Research Collaboration, co-authorship networks, COVID-19, gender disparities

Summary:
The study investigates International Research Collaboration (IRC) among EU countries from 2011 to 2022, focusing on gender-based authorship patterns. Analysis of a WoS-SSCI database shows increased IRC, notably with the USA and China. However, articles with female authors lag behind those with male authors. Female-exclusive collaborations exhibit distinct network structures. The COVID-19 pandemic influenced collaboration dynamics, temporarily narrowing the gender gap but exposing vulnerabilities in female-dominated networks. The findings highlight progress in IRC but also persistent gender disparities in EU participation. <div>
arXiv:2507.00619v1 Announce Type: new 
Abstract: This paper investigates International Research Collaboration (IRC) among European Union (EU) countries from 2011 to 2022, with emphasis on gender-based authorship patterns. Drawing from the Web of Science Social Science Citation Index (WoS-SSCI) database, a large dataset of IRC articles was constructed, annotated with categories of authorship based on gender, author affiliation, and COVID-19 subject as topic. Using network science, the study maps collaboration structures and reveals gendered differences in co-authorship networks. Results highlight a substantial rise in IRC over the decade, particularly with the USA and China as key non-EU partners. Articles with at least one female author were consistently less frequent than those with at least one male author. Notably, female-exclusive collaborations showed distinctive network topologies, with more centralized (star-like) patterns and shorter tree diameters. The COVID-19 pandemic further reshaped collaboration dynamics, temporarily reducing the gender gap in IRC but also revealing vulnerabilities in female-dominated research networks. These findings underscore both progress and persistent disparities in the gender dynamics of EU participation in IRC.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How large language models judge and influence human cooperation</title>
<link>https://arxiv.org/abs/2507.00088</link>
<guid>https://arxiv.org/abs/2507.00088</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, cooperation, social interactions, moral judgements, human prosociality<br />
Summary:<br />
- The study investigates the impact of large language models (LLMs) on human cooperation in social settings.<br />
- LLMs were provided with examples of cooperative and non-cooperative actions to assess how they judge such interactions.<br />
- There is a consensus among LLMs in evaluating cooperation against good opponents, but variance exists when judging interactions with ill-reputed individuals.<br />
- Differences in LLM judgements can significantly affect the prevalence of cooperation in populations.<br />
- Interventions using goal-oriented prompts can influence LLM norms and shape their judgements.<br /> <div>
arXiv:2507.00088v1 Announce Type: cross 
Abstract: Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endogenous Network Structures with Precision and Dimension Choices</title>
<link>https://arxiv.org/abs/2507.00249</link>
<guid>https://arxiv.org/abs/2507.00249</guid>
<content:encoded><![CDATA[
<div> Keywords: social learning, network structure, signal precision, dimension choices, dynamic network

Summary:
This paper introduces a social learning model where the network structure is determined by signal precision and dimension choices. Agents make decisions on signal precision and dimensions to learn about, which directly influence the network structure. The optimal precision choice is sublinear to the agent's influence in a fixed network, with a gap from the socially optimal choice. In a dynamic network, a kernel distance between agents defines the network, guiding how much weight agents assign to each other. Agents select dimensions to learn about to minimize the squared sum of influences, preferring a network with equally distributed influence. This research sheds light on the interplay between individual decisions, network structures, and social learning outcomes. 

<br /><br />Summary: <div>
arXiv:2507.00249v1 Announce Type: cross 
Abstract: This paper presents a social learning model where the network structure is endogenously determined by signal precision and dimension choices. Agents not only choose the precision of their signals and what dimension of the state to learn about, but these decisions directly determine the underlying network structure on which social learning occurs. We show that under a fixed network structure, the optimal precision choice is sublinear in the agent's stationary influence in the network, and this individually optimal choice is worse than the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network structure, we specify the network by defining a kernel distance between agents, which then determines how much weight agents place on one another. Agents choose dimensions to learn about such that their choice minimizes the squared sum of influences of all agents: a network with equally distributed influence across agents is ideal.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan</title>
<link>https://arxiv.org/abs/2507.00279</link>
<guid>https://arxiv.org/abs/2507.00279</guid>
<content:encoded><![CDATA[
<div> Keywords: seasonal migration, Afghanistan, opium harvest, violence, civil conflict <br />
Summary: Seasonal migration in Afghanistan, particularly during the opium harvest, is crucial for stabilizing rural economies. Satellite imagery is used to estimate the timing of the harvest, which attracts seasonal workers. Districts with high levels of poppy cultivation receive more migrants. Despite violent events, migration remains resilient, but is influenced by long-term conflict patterns, such as Taliban control in origin and destination areas. The study utilizes mobile phone records to analyze migration response to the harvest, finding that violent incidents do not significantly disrupt labor flows. However, the extent of conflict in an area impacts the movement of seasonal workers. The research sheds light on the relationship between violence, civil conflict, and seasonal migration dynamics in Afghanistan. <br /><br />Summary: <div>
arXiv:2507.00279v1 Announce Type: cross 
Abstract: Seasonal migration plays a critical role in stabilizing rural economies and sustaining the livelihoods of agricultural households. Violence and civil conflict have long been thought to disrupt these labor flows, but this hypothesis has historically been hard to test given the lack of reliable data on migration in conflict zones. Focusing on Afghanistan in the 8-year period prior to the Taliban's takeover in 2021, we first demonstrate how satellite imagery can be used to infer the timing of the opium harvest, which employs a large number of seasonal workers in relatively well-paid jobs. We then use a dataset of nationwide mobile phone records to characterize the migration response to this harvest, and examine whether and how violence and civil conflict disrupt this migration. We find that, on average, districts with high levels of poppy cultivation receive significantly more seasonal migrants than districts with no poppy cultivation. These labor flows are surprisingly resilient to idiosyncratic violent events at the source or destination, including extreme violence resulting in large numbers of fatalities. However, seasonal migration is affected by longer-term patterns of conflict, such as the extent of Taliban control in origin and destination locations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Construction of Graphs with Maximum Robustness</title>
<link>https://arxiv.org/abs/2507.00415</link>
<guid>https://arxiv.org/abs/2507.00415</guid>
<content:encoded><![CDATA[
<div> network robustness, resilient control, misbehaving agents, communication structures, undirected graphs<br />
<br />
Summary: 
- The article explores network robustness and resilient control in the presence of misbehaving agents.
- Higher robustness levels require dense communication structures, which may not be ideal for systems with limited capabilities.
- Tight necessary conditions on the number of edges for undirected graphs to achieve maximum robustness are established.
- Two classes of undirected graphs, known as Minimal Edge Robust Graphs (MERGs), are constructed to achieve maximum robustness with minimal numbers of edges.
- The work is validated through simulations. <div>
arXiv:2507.00415v1 Announce Type: cross 
Abstract: The notions of network $r$-robustness and $(r,s)$-robustness have been earlier introduced in the literature to achieve resilient control in the presence of misbehaving agents. However, while higher robustness levels provide networks with higher tolerances against the misbehaving agents, they also require dense communication structures, which are not always desirable for systems with limited capabilities and energy capacities. Therefore, this paper studies the fundamental structures behind $r$-robustness and $(r,s)$- robustness properties in two different ways. (a) We first explore and establish the tight necessary conditions on the number of edges for undirected graphs with any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b) We then use these conditions to construct two classes of undirected graphs, referred as to $\gamma$- and $(\gamma,\gamma)$-Minimal Edge Robust Graphs (MERGs), that provably achieve maximum robustness with minimal numbers of edges. We finally validate our work through some sets of simulations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Network and Attack Graphs for Service-Centric Impact Analysis</title>
<link>https://arxiv.org/abs/2507.00637</link>
<guid>https://arxiv.org/abs/2507.00637</guid>
<content:encoded><![CDATA[
<div> Keywords: cyber threats, attack paths, impact analysis, probabilistic methods, network-based influence spreading <br />
Summary: <br />
- The article presents a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, and their impact on user services in digital networks. <br />
- It uses probabilistic methods to track the propagation of attacks through different network layers, enabling analysis at various levels of detail. <br />
- Understanding how attacks spread within services and between different servers can help in early detection and mitigation. <br />
- The approach allows for evaluating diverse attack scenarios and developing protection measures based on the criticality of services from the user's perspective. <br />
- This methodology can assist security specialists and system administrators in making informed decisions regarding risk mitigation strategies. <br /> <div>
arXiv:2507.00637v1 Announce Type: cross 
Abstract: We present a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, as well as their impact on user services in enterprise or infrastructure networks of digital devices and services they provide. Using probabilistic methods to track the propagation of an attack through attack graphs, via the service or application layers, and on physical communication networks, our model enables us to analyse cyber attacks at different levels of detail. Understanding the propagation of an attack within a service among microservices and its spread between different services or application servers could help detect and mitigate it early. We demonstrate that this network-based influence spreading modelling approach enables the evaluation of diverse attack scenarios and the development of protection and mitigation measures, taking into account the criticality of services from the user's perspective. This methodology could also aid security specialists and system administrators in making well-informed decisions regarding risk mitigation strategies.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity</title>
<link>https://arxiv.org/abs/2507.00657</link>
<guid>https://arxiv.org/abs/2507.00657</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, political discourse, social media, polarization, toxicity

Summary: 
Large Language Models (LLMs) were studied for simulating political discourse on social media during the 2024 U.S. presidential election. Using 21 million interactions, LLM agents based on real users were constructed and compared to human replies. Three model families (Gemini, Mistral, and DeepSeek) were evaluated for linguistic style, ideological consistency, and toxicity. It was found that richer contextualization improved internal consistency but also increased polarization, stylized signals, and harmful language. A phenomenon called "generation exaggeration" was observed, where salient traits were systematically amplified beyond empirical baselines. The study concluded that LLMs do not emulate users but reconstruct them, introducing structural biases that affect their reliability as social proxies. This challenges their suitability for content moderation, deliberative simulations, and policy modeling. 

<br /><br />Summary: <div>
arXiv:2507.00657v1 Announce Type: cross 
Abstract: We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular versus Hierarchical: A Structural Signature of Topic Popularity in Mathematical Research</title>
<link>https://arxiv.org/abs/2506.22946</link>
<guid>https://arxiv.org/abs/2506.22946</guid>
<content:encoded><![CDATA[
<div> popularity, collaboration network, structural patterns, topic specialization, mathematical research<br />
Summary:<br />
This study explores how the popularity of research topics is linked to the structure of collaboration networks within those topics. Analyzing 1,938 topics from arXiv papers, the research finds a divide in how popular and niche topics organize collaboration. Popular topics form modular "schools of thought," while niche topics have core-periphery structures around established experts. This structural dichotomy is independent of network size and impacts collaboration opportunities for researchers. Interestingly, researchers in popular fields face greater structural constraints on collaboration, contrary to common assumptions. The findings suggest that choosing a research topic is not just about the subject matter but also about entering different collaborative environments with implications for a researcher's career. To make these patterns clear to the research community, the authors developed the Math Research Compass, an interactive platform providing data on topic popularity and collaboration patterns in mathematical research.<br /> <div>
arXiv:2506.22946v1 Announce Type: new 
Abstract: Mathematical researchers, especially those in early-career positions, face critical decisions about topic specialization with limited information about the collaborative environments of different research areas. The aim of this paper is to study how the popularity of a research topic is associated with the structure of that topic's collaboration network, as observed by a suite of measures capturing organizational structure at several scales. We apply these measures to 1,938 algorithmically discovered topics across 121,391 papers sourced from arXiv metadata during the period 2020--2025. Our analysis, which controls for the confounding effects of network size, reveals a structural dichotomy--we find that popular topics organize into modular "schools of thought," while niche topics maintain hierarchical core-periphery structures centered around established experts. This divide is not an artifact of scale, but represents a size-independent structural pattern correlated with popularity. We also document a "constraint reversal": after controlling for size, researchers in popular fields face greater structural constraints on collaboration opportunities, contrary to conventional expectations. Our findings suggest that topic selection is an implicit choice between two fundamentally different collaborative environments, each with distinct implications for a researcher's career. To make these structural patterns transparent to the research community, we developed the Math Research Compass (https://mathresearchcompass.com), an interactive platform providing data on topic popularity and collaboration patterns across mathematical topics.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Improving Large Language Models for Competitive Program Generation</title>
<link>https://arxiv.org/abs/2506.22954</link>
<guid>https://arxiv.org/abs/2506.22954</guid>
<content:encoded><![CDATA[
<div> competitive programming, large language models, algorithmic reasoning, benchmark datasets, error taxonomy

Summary:
- The study evaluates the performance of large language models (LLMs) in solving real-world competitive programming problems.
- A curated benchmark of 80 problems from regional ICPC/CCPC contests in 2024 was used to assess the capabilities of LLMs.
- The LLM DeepSeek-R1 was evaluated through online judge platforms with carefully designed prompts.
- A fine-grained error taxonomy was developed to categorize incorrect submissions, identifying general and specialized errors.
- An improvement framework combining dialogue-based repair and information-augmented regeneration phases significantly increased the number of correct solutions, with 46 out of 80 problems successfully accepted.

<br /><br />Summary: <div>
arXiv:2506.22954v1 Announce Type: new 
Abstract: Context: Due to the demand for strong algorithmic reasoning, complex logic implementation, and strict adherence to input/output formats and resource constraints, competitive programming generation by large language models (LLMs) is considered the most challenging problem in current LLM-based code generation. However, previous studies often evaluate LLMs using simple prompts and benchmark datasets prone to data leakage. Moreover, prior work has limited consideration of the diversity in algorithm types and difficulty levels. Objective: In this study, we aim to evaluate and improve LLMs in solving real-world competitive programming problems. Methods: We initially collect 117 problems from nine regional ICPC/CCPC contests held in 2024 and design four filtering criteria to construct a curated benchmark consisting of 80 problems. Leveraging DeepSeek-R1 as the LLM, we evaluate its competitive program generation capabilities through the online judge (OJ) platforms, guided by a carefully designed basic prompt. For incorrect submissions, we construct a fine-grained error taxonomy and then propose a targeted improvement framework by combining a multi-turn dialogue-based repair phase and an information-augmented regeneration phase. Results: Experimental results show that only 5 out of 80 problems are fully accepted when using basic prompts. For the unsolved problems, we construct the error taxonomy, including general errors (such as design, boundary, condition, data type, syntax, and input/output errors) and specialized errors (such as those in mathematical problems, greedy algorithms, and graph theories). After applying our proposed improvement strategies, we substantially increased the number of correct solutions, with 46 out of 80 problems successfully accepted.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction Gaps as Pathways to Explanation: Rethinking Educational Outcomes through Differences in Model Performance</title>
<link>https://arxiv.org/abs/2506.22993</link>
<guid>https://arxiv.org/abs/2506.22993</guid>
<content:encoded><![CDATA[
<div> Keywords: social contexts, prediction gaps, university completion, gradient boosting, graph neural networks 

Summary: 
The study conducted using population-scale administrative data from the Netherlands explores the impact of social contexts on educational outcomes, specifically university completion. Various statistical models were compared, including logistic regression, gradient boosting, and graph neural networks, to predict university completion based on early-life social contexts. The results showed small prediction gaps overall, indicating that previously identified indicators, especially parental status, account for most measurable variation in educational attainment. However, larger prediction gaps were observed for girls growing up without fathers, suggesting that the effects of social context for this particular group are more complex and not fully captured by simpler models. This highlights the importance of considering social contexts in understanding educational outcomes and the potential of prediction methods to support sociological explanations.

<br /><br />Summary: <div>
arXiv:2506.22993v1 Announce Type: new 
Abstract: Social contexts -- such as families, schools, and neighborhoods -- shape life outcomes. The key question is not simply whether they matter, but rather for whom and under what conditions. Here, we argue that prediction gaps -- differences in predictive performance between statistical models of varying complexity -- offer a pathway for identifying surprising empirical patterns (i.e., not captured by simpler models) which highlight where theories succeed or fall short. Using population-scale administrative data from the Netherlands, we compare logistic regression, gradient boosting, and graph neural networks to predict university completion using early-life social contexts. Overall, prediction gaps are small, suggesting that previously identified indicators, particularly parental status, capture most measurable variation in educational attainment. However, gaps are larger for girls growing up without fathers -- suggesting that the effects of social context for these groups go beyond simple models in line with sociological theory. Our paper shows the potential of prediction methods to support sociological explanation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Based Efficient Algorithms for User-Driven Competitive Influence Maximization in Social Networks</title>
<link>https://arxiv.org/abs/2506.23179</link>
<guid>https://arxiv.org/abs/2506.23179</guid>
<content:encoded><![CDATA[
<div> social networks, information diffusion, linear threshold model, heuristic algorithms, genetic algorithm

Summary:
The article discusses the User-driven competitive influence Maximization (UDCIM) model in social networks, focusing on human behavior in decision-making processes. It extends the existing work by proposing algorithms and LP-formulation of the problem. The study includes implementing and testing LP-formulated equations on small datasets using the Gurobi Solver. Additionally, the article introduces a heuristic and genetic algorithm for further exploration. Extensive experimentation is conducted on medium to large datasets, with outcomes plotted and discussed in the results section. The study aims to advance the understanding of information diffusion in social networks and provide practical tools for maximizing influence within communities. <div>
arXiv:2506.23179v1 Announce Type: new 
Abstract: Nowadays, people in the modern world communicate with their friends, relatives, and colleagues through the internet. Persons/nodes and communication/edges among them form a network. Social media networks are a type of network where people share their views with the community. There are several models that capture human behavior, such as a reaction to the information received from friends or relatives. The two fundamental models of information diffusion widely discussed in the social networks are the Independent Cascade Model and the Linear Threshold Model. Liu et al. [1] propose a variant of the linear threshold model in their paper title User-driven competitive influence Maximization(UDCIM) in social networks. Authors try to simulate human behavior where they do not make a decision immediately after being influenced, but take a pause for a while, and then they make a final decision. They propose the heuristic algorithms and prove the approximation factor under community constraints( The seed vertices belong to an identical community). Even finding the community is itself an NP-hard problem. In this article, we extend the existing work with algorithms and LP-formation of the problem. We also implement and test the LP-formulated equations on small datasets by using the Gurobi Solver [2]. We furthermore propose one heuristic and one genetic algorithm. The extensive experimentation is carried out on medium to large datasets, and the outcomes of both algorithms are plotted in the results and discussion section.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship</title>
<link>https://arxiv.org/abs/2506.22497</link>
<guid>https://arxiv.org/abs/2506.22497</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, blockchain, AI, scholarly evaluation, academic knowledge<br />
Summary:<br />
This paper proposes a new model of peer review, viewing it as structured public commentary rather than a traditional academic validation process. The authors highlight the limitations of anonymity, latency, and gatekeeping in the current peer review system and suggest a transparent, identity-linked, and reproducible framework anchored in open commentary. Leveraging blockchain technology for immutable audit trails and AI for iterative synthesis, the proposed model aims to incentivize intellectual contribution, capture epistemic evolution, and enable traceable reputational dynamics. By reframing academic knowledge as a living process rather than a static credential, this innovative approach has the potential to empower various fields, from computational science to the humanities. This reconceptualization of peer review could lead to a more dynamic and inclusive scholarly evaluation system. <br /> <div>
arXiv:2506.22497v1 Announce Type: cross 
Abstract: This paper reconceptualises peer review as structured public commentary. Traditional academic validation is hindered by anonymity, latency, and gatekeeping. We propose a transparent, identity-linked, and reproducible system of scholarly evaluation anchored in open commentary. Leveraging blockchain for immutable audit trails and AI for iterative synthesis, we design a framework that incentivises intellectual contribution, captures epistemic evolution, and enables traceable reputational dynamics. This model empowers fields from computational science to the humanities, reframing academic knowledge as a living process rather than a static credential.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools</title>
<link>https://arxiv.org/abs/2506.22940</link>
<guid>https://arxiv.org/abs/2506.22940</guid>
<content:encoded><![CDATA[
<div> AI, collaboration, misinformation, social media, user agency
<br />
Summary: This paper explores the use of collaborative AI systems to empower users in identifying and evaluating misinformation on social media. Traditional methods often struggle with emotionally charged or context-lacking content, leading to the design and evaluation of an interactive interface with features such as real-time explanations, source aggregation, and debate-style interaction. In a user study, the debate mode was found to be more effective than standard chatbot interfaces, while the multiple-source view was rated highly useful. The findings suggest that context-rich, dialogic AI systems can enhance media literacy and trust in digital information environments. The study emphasizes the importance of ethical design, explainability, and interactive engagement in developing tools for countering misinformation in the era of post-truth. 
<br /><br /> <div>
arXiv:2506.22940v1 Announce Type: cross 
Abstract: This paper investigates how collaborative AI systems can enhance user agency in identifying and evaluating misinformation on social media platforms. Traditional methods, such as personal judgment or basic fact-checking, often fall short when faced with emotionally charged or context-deficient content. To address this, we designed and evaluated an interactive interface that integrates collaborative AI features, including real-time explanations, source aggregation, and debate-style interaction. These elements aim to support critical thinking by providing contextual cues and argumentative reasoning in a transparent, user-centered format. In a user study with 14 participants, 79% found the debate mode more effective than standard chatbot interfaces, and the multiple-source view received an average usefulness rating of 4.6 out of 5. Our findings highlight the potential of context-rich, dialogic AI systems to improve media literacy and foster trust in digital information environments. We argue that future tools for misinformation mitigation should prioritize ethical design, explainability, and interactive engagement to empower users in a post-truth era.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density, asymmetry and citation dynamics in scientific literature</title>
<link>https://arxiv.org/abs/2506.23366</link>
<guid>https://arxiv.org/abs/2506.23366</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific behavior, similarity, citation rate, document embeddings, Bayesian regression <br />
<br />
Summary: 
The study explores the relationship between the similarity of a scientific paper to previous research and its citation rate. Two metrics, density and asymmetry, are introduced to measure a publication's semantic neighborhood. Density, representing the local geometry of a publication's semantic neighborhood, shows a small but consistent effect on citation count, improving prediction models. However, asymmetry does not yield predictive power for citation rates. The research surveyed approximately 53,000 publications across multiple disciplines and document embeddings, using a Bayesian hierarchical regression approach. The findings suggest that the density of surrounding scientific literature provides valuable insights into a paper's potential impact. The study presents a scalable framework for linking document embeddings to scientometric outcomes and raises questions about the role of semantic similarity in shaping scientific reward structures. <br /><br /> <div>
arXiv:2506.23366v1 Announce Type: cross 
Abstract: Scientific behavior is often characterized by a tension between building upon established knowledge and introducing novel ideas. Here, we investigate whether this tension is reflected in the relationship between the similarity of a scientific paper to previous research and its eventual citation rate. To operationalize similarity to previous research, we introduce two complementary metrics to characterize the local geometry of a publication's semantic neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed number of previously-published papers and the minimum distance enclosing those papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as the average directional difference between a paper and its nearest neighbors. We tested the predictive relationship between these two metrics and its subsequent citation rate using a Bayesian hierarchical regression approach, surveying $\sim 53,000$ publications across nine academic disciplines and five different document embeddings. While the individual effects of $\rho$ on citation count are small and variable, incorporating density-based predictors consistently improves out-of-sample prediction when added to baseline models. These results suggest that the density of a paper's surrounding scientific literature may carry modest but informative signals about its eventual impact. Meanwhile, we find no evidence that publication asymmetry improves model predictions of citation rates. Our work provides a scalable framework for linking document embeddings to scientometric outcomes and highlights new questions regarding the role that semantic similarity plays in shaping the dynamics of scientific reward.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.23469</link>
<guid>https://arxiv.org/abs/2506.23469</guid>
<content:encoded><![CDATA[
<div> Detection of graph anomalies, TripleAD framework, attribute anomalies, structural anomalies, mixed anomalies <br />
<br />
Summary: The article discusses the development of the TripleAD framework for graph anomaly detection, focusing on attribute, structural, and mixed anomalies. The framework includes three estimation modules: multiscale attribute estimation for node interactions, link-enhanced structure estimation for isolated nodes, and attribute-mixed curvature for mixed anomalies. The mutual distillation strategy encourages collaboration between the three modules. The framework aims to address the tug-of-war problem faced by existing unsupervised approaches by effectively identifying different types of anomalies without interference. Experimental results demonstrate the superior performance of TripleAD compared to strong baselines. <div>
arXiv:2506.23469v1 Announce Type: cross 
Abstract: Graph anomaly detection is critical in domains such as healthcare and economics, where identifying deviations can prevent substantial losses. Existing unsupervised approaches strive to learn a single model capable of detecting both attribute and structural anomalies. However, they confront the tug-of-war problem between two distinct types of anomalies, resulting in suboptimal performance. This work presents TripleAD, a mutual distillation-based triple-channel graph anomaly detection framework. It includes three estimation modules to identify the attribute, structural, and mixed anomalies while mitigating the interference between different types of anomalies. In the first channel, we design a multiscale attribute estimation module to capture extensive node interactions and ameliorate the over-smoothing issue. To better identify structural anomalies, we introduce a link-enhanced structure estimation module in the second channel that facilitates information flow to topologically isolated nodes. The third channel is powered by an attribute-mixed curvature, a new indicator that encapsulates both attribute and structural information for discriminating mixed anomalies. Moreover, a mutual distillation strategy is introduced to encourage communication and collaboration between the three channels. Extensive experiments demonstrate the effectiveness of the proposed TripleAD model against strong baselines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breadth, Depth, and Flux of Course-Prerequisite Networks</title>
<link>https://arxiv.org/abs/2506.23510</link>
<guid>https://arxiv.org/abs/2506.23510</guid>
<content:encoded><![CDATA[
<div> Keywords: Course-prerequisite networks, academic curricula, global measures, topological stratification, GitHub repository

Summary:
Course-prerequisite networks (CPNs) are DAGs representing academic curricula, enabling analysis for course importance, advising improvement, curriculum design guidance, graduation time distribution analysis, and knowledge flow quantification. Previous CPN analyses focused on micro- and meso-scale properties, lacking macro-scale measures. This study introduces three new global CPN measures - breadth, depth, and flux - invariant under transitive reduction and based on topological stratification concept. These measures facilitate macro-scale CPN comparison. Numerical illustration using real and synthetic CPNs from three universities demonstrates the utility of the new measures. The CPN data analyzed are publicly accessible in a GitHub repository. <div>
arXiv:2506.23510v1 Announce Type: cross 
Abstract: Course-prerequisite networks (CPNs) are directed acyclic graphs that model complex academic curricula by representing courses as nodes and dependencies between them as directed links. These networks are indispensable tools for visualizing, studying, and understanding curricula. For example, CPNs can be used to detect important courses, improve advising, guide curriculum design, analyze graduation time distributions, and quantify the strength of knowledge flow between different university departments. However, most CPN analyses to date have focused only on micro- and meso-scale properties. To fill this gap, we define and study three new global CPN measures: breadth, depth, and flux. All three measures are invariant under transitive reduction and are based on the concept of topological stratification, which generalizes topological ordering in directed acyclic graphs. These measures can be used for macro-scale comparison of different CPNs. We illustrate the new measures numerically by applying them to three real and synthetic CPNs from three universities: the Cyprus University of Technology, the California Institute of Technology, and Johns Hopkins University. The CPN data analyzed in this paper are publicly available in a GitHub repository.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Are the Antidote to Walled Gardens</title>
<link>https://arxiv.org/abs/2506.23978</link>
<guid>https://arxiv.org/abs/2506.23978</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet, interoperability, agents, AI, data exchange <br />
Summary: 
The article discusses the shift towards universal interoperability in digital services facilitated by AI-based agents. It points out that while the Internet's core infrastructure was designed to be universal, the application layer is currently dominated by closed, proprietary platforms. The use of agents can automate data format translation and interaction with various interfaces, making interoperability easier and more affordable. This development challenges monopolistic behaviors and promotes data portability, leading to increased user freedom and competitive markets. However, it also presents new security risks and technical challenges that need to be addressed. The authors argue that the ML community should embrace universal interoperability while developing frameworks to mitigate potential downsides and ensure robust security measures are in place. By seizing this opportunity, AI can play a crucial role in fostering an open and interoperable digital ecosystem while safeguarding user privacy and data security. 
<br /><br />Summary: <div>
arXiv:2506.23978v1 Announce Type: cross 
Abstract: While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Human Judgment in Community Notes with LLMs</title>
<link>https://arxiv.org/abs/2506.24118</link>
<guid>https://arxiv.org/abs/2506.24118</guid>
<content:encoded><![CDATA[
<div> open ecosystem, Community Notes, human raters, Reinforcement Learning, feedback <br />
<br />
Summary: This paper proposes a new paradigm for Community Notes in the LLM era, advocating for an open ecosystem where both humans and Language Model Models (LLMs) can contribute notes. The ultimate decision on which notes are deemed helpful is left to humans, maintaining trust and legitimacy. The community of diverse human raters plays a crucial role as evaluators and arbiters of helpful content. The feedback collected from this diverse community can be utilized to enhance LLMs' ability to generate accurate and unbiased notes through Reinforcement Learning from Community Feedback (RLCF). This bi-directional system allows LLMs to assist humans in delivering context efficiently, while human feedback contributes to improving LLM performance. The paper outlines the functionality of this system, its advantages, potential risks, challenges, and proposes a research agenda to address these challenges and maximize the benefits of this collaborative approach. <br /> <div>
arXiv:2506.24118v1 Announce Type: cross 
Abstract: This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is driving the conversation? Analysing the nodality of British MPs and journalists on social media</title>
<link>https://arxiv.org/abs/2402.08765</link>
<guid>https://arxiv.org/abs/2402.08765</guid>
<content:encoded><![CDATA[
<div> policy actors, influence, social media, nodality, discourse network  
Summary:  
- The study explores factors influencing influence in political conversations on social media platforms.  
- Nodality, a concept in political science, is used to measure actors' capacity to exchange information in discourse networks.  
- Influence on Twitter in the UK is driven by active nodality (level of engagement) and inherent nodality (institutional position).  
- MPs and accredited journalists were studied on four policy topics to compare their influence.  
- Findings reveal that influence is transferable across topics depending on the actor's nodality levels.   <div>
arXiv:2402.08765v3 Announce Type: replace 
Abstract: With the rise of social media, political conversations now take place in more diffuse environments. In this context, it is not always clear why some actors, more than others, have greater influence on how discussions are shaped. To investigate the factors behind such influence, we build on nodality, a concept in political science which describes the capacity of an actor to exchange information within discourse networks. This concept goes beyond traditional network metrics that describe the position of an actor in the network to include exogenous drivers of influence (e.g. factors relating to organisational hierarchies). We study online discourse on Twitter (now X) in the UK to measure the relative nodality of two sets of policy actors - Members of Parliament (MPs) and accredited journalists - on four policy topics. We find that influence on the platform is driven by two key factors: (i) active nodality, derived from the actor's level of topic-related engagement, and (ii) inherent nodality, which is independent of the platform discourse and reflects the actor's institutional position. These findings significantly further our understanding of the origins of influence on social media platforms and suggest in which contexts influence is transferable across topics.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coupled Friedkin-Johnsen Model of Popularity Dynamics in Social Media</title>
<link>https://arxiv.org/abs/2503.15170</link>
<guid>https://arxiv.org/abs/2503.15170</guid>
<content:encoded><![CDATA[
<div> influencers, popularity dynamics, social media, social influence, recommendations <br />
<br />Summary: 
The study focuses on understanding the dynamics of popularity in social media through a complex system that incorporates social influence and popularity-based recommendations. Building on the Friedkin-Johnsen model, the researchers introduce a discrete-time dynamical system that accounts for influencers competing for popularity. Their model examines how social influence, past popularity, and content quality interact to determine an influencer's popularity over time. Through numerical examples, the study explores the asymptotic behavior of the model, shedding light on the intricate relationship between different factors influencing an influencer's popularity on social media. <div>
arXiv:2503.15170v2 Announce Type: replace 
Abstract: Popularity dynamics in social media depend on a complex interplay of social influence between users and popularity-based recommendations that are provided by the platforms. In this work, we introduce a discrete-time dynamical system to model the evolution of popularity on social media. Our model generalizes the well-known Friedkin-Johnsen model to a set of influencers vying for popularity. We study the asymptotic behavior of this model and illustrate it with numerical examples. Our results highlight the interplay of social influence, past popularity, and content quality in determining the popularity of influencers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines</title>
<link>https://arxiv.org/abs/2206.00535</link>
<guid>https://arxiv.org/abs/2206.00535</guid>
<content:encoded><![CDATA[
<div> deepfakes, misinformation, detection models, Artifact Attention module, Deepfake Caricatures <br />
Summary: <br />
This article addresses the issue of deepfakes contributing to online misinformation by developing a framework that enhances human detection of deepfake videos. The researchers introduce the Artifact Attention module, which utilizes human responses to create attention maps highlighting video artifacts, leading to the creation of "Deepfake Caricatures." These visual indicators significantly improve human detection of deepfakes, regardless of video presentation times and user engagement levels. Additionally, a deepfake detection model incorporating the Artifact Attention module is introduced to enhance accuracy and robustness. The study showcases the effectiveness of a human-centric approach in designing methods to mitigate the impact of deepfakes. <div>
arXiv:2206.00535v4 Announce Type: replace-cross 
Abstract: Deepfakes can fuel online misinformation. As deepfakes get harder to recognize with the naked eye, human users become more reliant on deepfake detection models to help them decide whether a video is real or fake. Currently, models yield a prediction for a video's authenticity, but do not integrate a method for alerting a human user. We introduce a framework for amplifying artifacts in deepfake videos to make them more detectable by people. We propose a novel, semi-supervised Artifact Attention module, which is trained on human responses to create attention maps that highlight video artifacts, and magnify them to create a novel visual indicator we call "Deepfake Caricatures". In a user study, we demonstrate that Caricatures greatly increase human detection, across video presentation times and user engagement levels. We also introduce a deepfake detection model that incorporates the Artifact Attention module to increase its accuracy and robustness. Overall, we demonstrate the success of a human-centered approach to designing deepfake mitigation methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Contrastive Learning with Low-Rank Regularization and Low-Rank Attention for Noisy Node Classification</title>
<link>https://arxiv.org/abs/2402.09600</link>
<guid>https://arxiv.org/abs/2402.09600</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, contrastive learning, low-rank regularization, transductive learning, node classification 

Summary: 
Graph Neural Networks (GNNs) have shown strong performance in node representation learning but can be affected by noise in real-world graph data. To address this issue, a new method called Graph Contrastive Learning with Low-Rank Regularization (GCL-LRR) is proposed. It follows a two-stage transductive learning approach for node classification, combining prototypical contrastive learning with low-rank regularization. The method is inspired by the Low Frequency Property of graph data and labels, and is supported by theoretical bounds for transductive learning. An enhanced model, GCL-LR-Attention, is introduced by incorporating an LR-Attention layer to improve performance. Empirical evaluations on benchmark datasets demonstrate the effectiveness and robustness of both GCL-LRR and GCL-LR-Attention in learning meaningful node representations. The code is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2402.09600v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in learning node representations and have shown strong performance in tasks such as node classification. However, recent findings indicate that the presence of noise in real-world graph data can substantially impair the effectiveness of GNNs. To address this challenge, we introduce a robust and innovative node representation learning method named Graph Contrastive Learning with Low-Rank Regularization, or GCL-LRR, which follows a two-stage transductive learning framework for node classification. In the first stage, the GCL-LRR encoder is optimized through prototypical contrastive learning while incorporating a low-rank regularization objective. In the second stage, the representations generated by GCL-LRR are employed by a linear transductive classifier to predict the labels of unlabeled nodes within the graph. Our GCL-LRR is inspired by the Low Frequency Property (LFP) of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank regularization in transductive learning, which is also supported by strong empirical results. To further enhance the performance of GCL-LRR, we present an improved model named GCL-LR-Attention, which incorporates a novel LR-Attention layer into GCL-LRR. GCL-LR-Attention reduces the kernel complexity of GCL-LRR and contributes to a tighter generalization bound, leading to improved performance. Extensive evaluations on standard benchmark datasets evidence the effectiveness and robustness of both GCL-LRR and GCL-LR-Attention in learning meaningful node representations. The code is available at https://github.com/Statistical-Deep-Learning/GCL-LR-Attention.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Z-REx: Human-Interpretable GNN Explanations for Real Estate Recommendations</title>
<link>https://arxiv.org/abs/2503.18001</link>
<guid>https://arxiv.org/abs/2503.18001</guid>
<content:encoded><![CDATA[
<div> explanation framework, GNN, Link Prediction tasks, Z-REx, real-estate dataset

Summary:
Z-REx is introduced as a GNN explanation framework specifically designed for heterogeneous link prediction tasks. It utilizes structural and attribute perturbation techniques to identify critical substructures and important features, incorporating domain-specific knowledge to reduce the search space. In experiments using a real-world real-estate dataset from Zillow Group, Z-REx is shown to generate contextually relevant and human-interpretable explanations for the ZiGNN recommendation engine. Comparing against State-of-The-Art (SOTA) GNN explainers, Z-REx demonstrated a 61% improvement in the Fidelity metric, producing superior human-interpretable explanations. <div>
arXiv:2503.18001v2 Announce Type: replace-cross 
Abstract: Transparency and interpretability are crucial for enhancing customer confidence and user engagement, especially when dealing with black-box Machine Learning (ML)-based recommendation systems. Modern recommendation systems leverage Graph Neural Network (GNN) due to their ability to produce high-quality recommendations in terms of both relevance and diversity. Therefore, the explainability of GNN is especially important for Link Prediction (LP) tasks since recommending relevant items can be viewed as predicting links between users and items. GNN explainability has been a well-studied field, but existing methods primarily focus on node or graph-level tasks, leaving a gap in LP explanation techniques. This work introduces Z-REx, a GNN explanation framework designed explicitly for heterogeneous link prediction tasks. Z-REx utilizes structural and attribute perturbation to identify critical substructures and important features while reducing the search space by leveraging domain-specific knowledge. In our experimentation, we show the efficacy of Z-REx in generating contextually relevant and human-interpretable explanations for ZiGNN, a GNN-based recommendation engine, using a real-world real-estate dataset from Zillow Group, Inc. We compare against State-of-The-Art (SOTA) GNN explainers to show Z-REx outperforms them by 61% in the Fidelity metric by producing superior human-interpretable explanations.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Institutional Gender Inequality in Contemporary Visual Art</title>
<link>https://arxiv.org/abs/2506.22103</link>
<guid>https://arxiv.org/abs/2506.22103</guid>
<content:encoded><![CDATA[
<div> artist, gender equity, exhibitions, auctions, institutional forces
Summary: 
The study examines the under-representation of women in the visual art world by analyzing the exhibition history and auction sales of over 65,000 contemporary artists across 20,000 institutions. The research reveals gender disparities in artist populations, exhibition opportunities, and auction success. Only 24% of institutions strive for gender parity in representation, with the majority being gender-neutral. The study finds that as institutional prestige increases, the likelihood of gender inequality also rises. By defining artist's co-exhibition gender, the research highlights the impact of institutional forces on an artist's success. Surprisingly, the artist's co-exhibition gender has a stronger correlation with auction market access than the artist's own gender. This study sheds light on the systemic issues contributing to the persistent gender imbalance in the art world. 
<br /><br />Summary: <div>
arXiv:2506.22103v1 Announce Type: new 
Abstract: From disparities in the number of exhibiting artists to auction opportunities, there is evidence of women's under-representation in visual art. Here we explore the exhibition history and auction sales of 65,768 contemporary artists in 20,389 institutions, revealing gender differences in the artist population, exhibitions and auctions. We distinguish between two criteria for gender equity: gender-neutrality, when artists have gender-independent access to exhibition opportunities, and gender-balanced, that strives for gender parity in representation, finding that 58\% of institutions are gender-neutral but only 24\% are gender-balanced, and that the fraction of man-overrepresented institutions increases with institutional prestige. We define artist's co-exhibition gender to capture the gender inequality of the institutions that an artist exhibits. Finally, we use logistic regression to predict an artist's access to the auction market, finding that co-exhibition gender has a stronger correlation with success than the artist's gender. These results help unveil and quantify the institutional forces that relate to the persistent gender imbalance in the art world.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment</title>
<link>https://arxiv.org/abs/2506.22165</link>
<guid>https://arxiv.org/abs/2506.22165</guid>
<content:encoded><![CDATA[
<div> Graph-Neural-Network, legal norms, court decisions, link prediction, citation graph
<br />Summary:
The study proposes a Graph-Neural-Network (GNN) model for identifying Case-Law and Case-Case citations in legal systems. By integrating semantic and topological information, the model improves prediction accuracy by 3.1 points in average precision and 8.5 points in data sparsity. The adapted relational graph convolutions allow for the topological integration of semantic meta-information, enhancing prediction performance over time and in fully inductive scenarios. Jointly learning and predicting case and norm citations results in a synergistic effect, boosting case citation prediction efficiency by up to 4.7 points. This approach provides practitioners, novices, and legal AI systems with enhanced access to relevant data for informed appraisals and judgments. <div>
arXiv:2506.22165v1 Announce Type: new 
Abstract: Legal systems heavily rely on cross-citations of legal norms as well as previous court decisions. Practitioners, novices and legal AI systems need access to these relevant data to inform appraisals and judgments. We propose a Graph-Neural-Network (GNN) link prediction model that can identify Case-Law and Case-Case citations with high proficiency through fusion of semantic and topological information. We introduce adapted relational graph convolutions operating on an extended and enriched version of the original citation graph that allow the topological integration of semantic meta-information. This further improves prediction by 3.1 points of average precision and by 8.5 points in data sparsity as well as showing robust performance over time and in challenging fully inductive prediction. Jointly learning and predicting case and norm citations achieves a large synergistic effect that improves case citation prediction by up to 4.7 points, at almost doubled efficiency.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decade of News Forum Interactions: Threaded Conversations, Signed Votes, and Topical Tags</title>
<link>https://arxiv.org/abs/2506.22224</link>
<guid>https://arxiv.org/abs/2506.22224</guid>
<content:encoded><![CDATA[
<div> Keywords: online platform, user activity, user comments, editorial topics, privacy

Summary: 
The article introduces a new dataset from the online platform of DerStandard, a major Austrian newspaper, covering user activity over a ten-year period. The dataset includes over 75 million user comments, 400 million votes, and metadata on articles and user interactions. It offers structured conversation threads, up- and downvotes of comments, and editorial topic labels for analysis of online discourse while protecting user privacy through anonymization. Persistent identifiers are hashed for privacy, and raw comment texts are not shared publicly. Pre-computed vector representations derived from an embedding model are released instead. This dataset facilitates research on discussion dynamics, network structures, and semantic analyses in German, serving as a valuable resource for computational social science and related fields.<br /><br />Summary: The dataset from DerStandard's online platform covers user activity over ten years, including millions of user comments, votes, and metadata. It provides structured conversation threads, up- and downvotes, and topic labels for analysis while protecting user privacy. Persistent identifiers are anonymized, and pre-computed vector representations are released. The dataset supports research on discussion dynamics and network structures in German, benefiting computational social science and related fields. <div>
arXiv:2506.22224v1 Announce Type: new 
Abstract: We present a large-scale, longitudinal dataset capturing user activity on the online platform of DerStandard, a major Austrian newspaper. The dataset spans ten years (2013-2022) and includes over 75 million user comments, more than 400 million votes, and detailed metadata on articles and user interactions. It provides structured conversation threads, explicit up- and downvotes of user comments and editorial topic labels, enabling rich analyses of online discourse while preserving user privacy. To ensure this privacy, all persistent identifiers are anonymized using salted hash functions, and the raw comment texts are not publicly shared. Instead, we release pre-computed vector representations derived from a state-of-the-art embedding model. The dataset supports research on discussion dynamics, network structures, and semantic analyses in the mid-resourced language German, offering a reusable resource across computational social science and related fields.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Network Topology on the Equilibria of Influence-Opinion Games</title>
<link>https://arxiv.org/abs/2506.22293</link>
<guid>https://arxiv.org/abs/2506.22293</guid>
<content:encoded><![CDATA[
<div> social networks, public opinion, adversarial influence, network connectivity, opinion evolution 

Summary: 
- The study examines the impact of network connectivity on influencing public opinion in online social networks, focusing on the game between two players shaping discourse.
- Opinion evolution is modeled as a competitive influence-propagation process, where players inject messages that diffuse and update opinions based on exposure.
- The bi-level model accounts for viral-media correlation effects, providing a more comprehensive understanding of opinion dynamics.
- An algorithm based on linear-quadratic regulators is proposed to solve the high-dimensional game, approximating local feedback Stackelberg strategies for players with limited cognitive abilities.
- By analyzing synthetic networks and real Facebook data, the research identifies structural characteristics that enhance a network's resilience to adversarial influence, offering insights for designing more resilient social networks. 

<br /><br />Summary: <div>
arXiv:2506.22293v1 Announce Type: new 
Abstract: Online social networks exert a powerful influence on public opinion. Adversaries weaponize these networks to manipulate discourse, underscoring the need for more resilient social networks. To this end, we investigate the impact of network connectivity on Stackelberg equilibria in a two-player game to shape public opinion. We model opinion evolution as a repeated competitive influence-propagation process. Players iteratively inject \textit{messages} that diffuse until reaching a steady state, modeling the dispersion of two competing messages. Opinions then update according to the discounted sum of exposure to the messages. This bi-level model captures viral-media correlation effects omitted by standard opinion-dynamics models. To solve the resulting high-dimensional game, we propose a scalable, iterative algorithm based on linear-quadratic regulators that approximates local feedback Stackelberg strategies for players with limited cognition. We analyze how the network topology shapes equilibrium outcomes through experiments on synthetic networks and real Facebook data. Our results identify structural characteristics that improve a network's resilience to adversarial influence, guiding the design of more resilient social networks.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit</title>
<link>https://arxiv.org/abs/2506.21620</link>
<guid>https://arxiv.org/abs/2506.21620</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Reddit, US Presidential Election, Political Alignment, Discourse Manipulation

Summary: 
The study evaluates the performance of Large Language Models (LLMs), specifically GPT-4, in replicating user-generated content on Reddit during the 2016 US Presidential election. Three experiments were conducted to generate comments mimicking real or artificial partisan users. GPT-4 was successful in creating realistic comments aligned with the community's candidate preference, potentially influencing consensus. However, dissent was less common. The comments were analyzed for political alignment, sentiment, and linguistic features, revealing that real and artificial comments are distinguishable in a semantically embedded space but appear indistinguishable through manual inspection. The findings suggest that LLMs could be used to infiltrate online discussions, manipulate political debates, and shape political narratives, highlighting the broader implications of AI-driven discourse manipulation. 

<br /><br />Summary: <div>
arXiv:2506.21620v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The relationship between episcopal genealogy and ideology in the Roman Catholic Church</title>
<link>https://arxiv.org/abs/2506.22108</link>
<guid>https://arxiv.org/abs/2506.22108</guid>
<content:encoded><![CDATA[
<div> genealogical proximity, doctrinal alignment, hierarchical structures, ideological coherence, episcopal lineage

Summary: 
The study examines how hierarchical structures within the Roman Catholic Church influence the ideological orientation of its leadership. A dataset of 245 living cardinals is analyzed, focusing on genealogical proximity and doctrinal alignment. Through the analysis of episcopal lineage, recurring patterns of lineage are identified, such as shared consecrators. The study finds that cardinals linked by specific genealogical motifs, especially those sharing the same principal consecrator, are more likely to exhibit ideological similarity. The influence of Pope John Paul II is shown to persist through bishops he consecrated, who hold more conservative views. The research highlights the role of hierarchical mentorship in shaping ideological coherence within large religious institutions and suggests that institutional lineages play a significant role in the transmission and consolidation of doctrinal positions over time.<br /><br />Summary: <div>
arXiv:2506.22108v1 Announce Type: cross 
Abstract: In this study we investigate how hierarchical structures within the Roman Catholic Church shape the ideological orientation of its leadership. The full episcopal genealogy dataset comprises over 35,000 bishops, each typically consecrated by one principal consecrator and two co-consecrators, forming a dense and historically continuous directed network of episcopal lineage. Within this broader structure, we focus on a dataset of 245 living cardinals to examine whether genealogical proximity correlates with doctrinal alignment on a broad set of theological and sociopolitical issues. We identify motifs that capture recurring patterns of lineage, such as shared consecrators or co-consecrators. In parallel, we apply natural language processing techniques to extract each cardinal's publicly stated positions on ten salient topics, including LGBTQIA+ rights, women's roles in the Church, liturgy, bioethics, priestly celibacy, and migration. Our results show that cardinals linked by specific genealogical motifs, particularly those who share the same principal consecrator, are significantly more likely to exhibit ideological similarity. We find that the influence of pope John Paul II persists through the bishops he consecrated, who demonstrate systematically more conservative views than their peers. These findings underscore the role of hierarchical mentorship in shaping ideological coherence within large-scale religious institutions. Our contribution offers quantitative evidence that institutional lineages, beyond individual background factors, may have an impact on the transmission and consolidation of doctrinal positions over time.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harder, shorter, sharper, forward: A comparison of women's and men's elite football gameplay (2020-2025)</title>
<link>https://arxiv.org/abs/2506.22119</link>
<guid>https://arxiv.org/abs/2506.22119</guid>
<content:encoded><![CDATA[
<div> Keywords: football, match dynamics, passing volume, collective play, professional

Summary: 
- The study examines the evolution of elite football between 2020 and 2025 by analyzing event-level records from top-tier men's and women's leagues in five countries.
- Over this period, there was a significant increase in passing volume, pass accuracy, and the percentage of passes made under pressure, with the most substantial changes seen in women's competitions.
- Pitch-passing networks were used to track ball movement among pitch regions, revealing changes in normalized outreach and average shortest path lengths, indicating a wider ball circulation on the field.
- The findings suggest a sustained intensification of collective play in contemporary professional football, as evidenced by the trend towards higher passing quality and increased ball circulation across teams.
- This study provides systematic evidence for the pace and form of change in elite football, highlighting the evolving dynamics of the game in recent years.<br /><br />Summary: <div>
arXiv:2506.22119v1 Announce Type: cross 
Abstract: Elite football is believed to have evolved in recent years, but systematic evidence for the pace and form of that change is sparse. Drawing on event-level records for 13,067 matches in ten top-tier men's and women's leagues in England, Spain, Germany, Italy, and the United States (2020-2025), we quantify match dynamics with two views: conventional performance statistics and pitch-passing networks that track ball movement among a grid of pitch (field) regions. Between 2020 and 2025, average passing volume, pass accuracy, and the percent of passes made under pressure all rose. In general, the largest year-on-year changes occurred in women's competitions. Network measures offer alternative but complementary perspectives on the changing gameplay in recent years, normalized outreach in the pitch passing networks decreased, while the average shortest path lengths increased, indicating a wider ball circulation. Together, these indicators point to a sustained intensification of collective play across contemporary professional football.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterization Of Diseases In Temporal Comorbidity Networks</title>
<link>https://arxiv.org/abs/2506.22136</link>
<guid>https://arxiv.org/abs/2506.22136</guid>
<content:encoded><![CDATA[
<div> Age groups, comorbidity networks, disease clustering, disease prevalence, mortality <br />
<br />
Summary: 
The study analyzes comorbidity networks derived from a large dataset of Austrian hospital stays to understand how diseases cluster and evolve across different age groups. The networks become denser with age, revealing three dominant age-related components. The relationship between disease prevalence and degree highlights conditions disproportionately connected to other diseases, such as iron deficiency anemia in children and nicotine dependence in adults. High-mortality bridging diseases, like cancers and chronic kidney disease, are identified using betweenness centrality. Specific diseases with high connectivity relative to their prevalence are pinpointed, emphasizing the importance of targeting age-specific, network-central conditions with high mortality for prevention and integrated care. <div>
arXiv:2506.22136v1 Announce Type: cross 
Abstract: Comorbidity networks, which capture disease-disease co-occurrence usually based on electronic health records, reveal structured patterns in how diseases cluster and progress across individuals. However, how these networks evolve across different age groups and how this evolution relates to properties like disease prevalence and mortality remains understudied. To address these issues, we used publicly available comorbidity networks extracted from a comprehensive dataset of 45 million Austrian hospital stays from 1997 to 2014, covering 8.9 million patients. These networks grow and become denser with age. We identified groups of diseases that exhibit similar patterns of structural centrality throughout the lifespan, revealing three dominant age-related components with peaks in early childhood, midlife, and late life. To uncover the drivers of this structural change, we examined the relationship between prevalence and degree. This allowed us to identify conditions that were disproportionately connected to other diseases. Using betweenness centrality in combination with mortality data, we further identified high-mortality bridging diseases. Several diseases show high connectivity relative to their prevalence, such as iron deficiency anemia (D50) in children, nicotine dependence (F17), and lipoprotein metabolism disorders (E78) in adults. We also highlight structurally central diseases with high mortality that emerge at different life stages, including cancers (C group), liver cirrhosis (K74), subarachnoid hemorrhage (I60), and chronic kidney disease (N18). These findings underscore the importance of targeting age-specific, network-central conditions with high mortality for prevention and integrated care.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link Prediction with Physics-Inspired Graph Neural Networks</title>
<link>https://arxiv.org/abs/2402.14802</link>
<guid>https://arxiv.org/abs/2402.14802</guid>
<content:encoded><![CDATA[
<div> GRAFF-LP, link prediction, heterophily, GNNs, physics biases<br />
Summary:<br />
The article introduces GRAFF-LP, an extension of GNNs for link prediction on heterophilic datasets. The existing link prediction models focused on node classification struggle with heterophily, where adjacent nodes have different labels. GRAFF-LP addresses this issue by incorporating physics biases in the architecture to effectively discriminate between existing and non-existing edges. The new readout function inspired by physics not only enhances GRAFF-LP's performance but also improves other baseline models. The study reveals that simple GNNs do not face greater difficulty in predicting heterophilic links compared to homophilic ones. This suggests the need for heterophily measures tailored for link prediction, distinct from those used in node classification.<br /> 
Summary: <div>
arXiv:2402.14802v3 Announce Type: replace-cross 
Abstract: The message-passing mechanism underlying Graph Neural Networks (GNNs) is not naturally suited for heterophilic datasets, where adjacent nodes often have different labels. Most solutions to this problem remain confined to the task of node classification. In this article, we focus on the valuable task of link prediction under heterophily, an interesting problem for recommendation systems, social network analysis, and other applications. GNNs like GRAFF have improved node classification under heterophily by incorporating physics biases in the architecture. Similarly, we propose GRAFF-LP, an extension of GRAFF for link prediction. We show that GRAFF-LP effectively discriminates existing from non-existing edges by learning implicitly to separate the edge gradients. Based on this information, we propose a new readout function inspired by physics. Remarkably, this new function not only enhances the performance of GRAFF-LP but also improves that of other baseline models, leading us to reconsider how every link prediction experiment has been conducted so far. Finally, we provide evidence that even simple GNNs did not experience greater difficulty in predicting heterophilic links compared to homophilic ones. This leads us to believe in the necessity for heterophily measures specifically tailored for link prediction, distinct from those used in node classification. The code and appendix are available at https://github.com/difra100/Link_Prediction_with_PIGNN_IJCNN.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets</title>
<link>https://arxiv.org/abs/2502.02379</link>
<guid>https://arxiv.org/abs/2502.02379</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, graph learning, evaluation, Rings framework, dataset quality <br />
Summary: <br />
The article discusses the importance of benchmark datasets in graph learning and the need for better evaluation practices. It addresses questions about what makes a good graph-learning dataset and how dataset quality can be evaluated. The Rings framework is introduced as a mode-perturbation framework to assess the quality of graph-learning datasets through dataset ablations. Two measures, performance separability, and mode complementarity, are proposed to evaluate dataset capacity. Extensive experiments on graph-level tasks demonstrate the utility of the framework for dataset evaluation and provide recommendations for improving the evaluation of graph-learning methods. This work opens new research directions in data-centric graph learning and contributes to the systematic evaluation of evaluations. <div>
arXiv:2502.02379v2 Announce Type: replace-cross 
Abstract: Benchmark datasets have proved pivotal to the success of graph learning, and good benchmark datasets are crucial to guide the development of the field. Recent research has highlighted problems with graph-learning datasets and benchmarking practices -- revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches. Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning? Our work addresses these questions. As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation. Hence, we start from first principles. Observing that graph-learning datasets uniquely combine two modes -- graph structure and node features --, we introduce Rings, a flexible and extensible mode-perturbation framework to assess the quality of graph-learning datasets based on dataset ablations -- i.e., quantifying differences between the original dataset and its perturbed representations. Within this framework, we propose two measures -- performance separability and mode complementarity -- as evaluation tools, each assessing the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods from a distinct angle. We demonstrate the utility of our framework for dataset evaluation via extensive experiments on graph-level tasks and derive actionable recommendations for improving the evaluation of graph-learning methods. Our work opens new research directions in data-centric graph learning, and it constitutes a step toward the systematic evaluation of evaluations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing validated standards for Home and Work location Detection</title>
<link>https://arxiv.org/abs/2506.20679</link>
<guid>https://arxiv.org/abs/2506.20679</guid>
<content:encoded><![CDATA[
<div> Algorithm, Home and work detection, Mobility data, Methodological standards, Urban mobility research

Summary:
HoWDe is a robust algorithm designed for accurately identifying home and work locations from smartphone location data. It addresses methodological challenges such as missing data and varying data quality, achieving high detection accuracies for home and work locations. The algorithm's performance is consistent across countries and demographic groups, providing a standardized framework for home-work detection. Parameter choices in the algorithm impact the trade-off between accuracy and user retention, influencing downstream applications like employment estimation and commuting pattern analysis. HoWDe supports privacy-preserving sharing of mobility data by facilitating in-house pre-processing through a transparent and validated pipeline. These methodological standards established by HoWDe enhance the robustness, scalability, and reproducibility of mobility research at both individual and urban scales. 

<br /><br />Summary: <div>
arXiv:2506.20679v1 Announce Type: new 
Abstract: Smartphone location data have transformed urban mobility research, providing unprecedented insights into how people navigate and interact in cities. However, leveraging location data at scale presents methodological challenges. Accurately identifying individuals' home and work locations is critical for a range of applications, including commuting analysis, unemployment estimation, and urban accessibility studies. Despite their widespread use, home-work detection methods lack a standardized framework that accounts for differing data quality and that is validated against ground-truth observations. This limits the comparability and reproducibility of results across studies and datasets. In this paper, we present HoWDe, a robust algorithm for identifying home and work locations from mobility data, explicitly designed to handle missing data and varying data quality across individuals. Using two unique ground-truth datasets comprising over 5100 individuals from more than 80 countries, HoWDe achieves home and work detection accuracies of up to 97% and 88%, respectively, with consistent performance across countries and demographic groups. We examine how parameter choices shape the trade-off between accuracy and user retention, and demonstrate how these methodological decisions influence downstream applications such as employment estimation and commuting pattern analysis. By supporting in-house pre-processing through a transparent and validated pipeline, HoWDe also facilitates the sharing of privacy-preserving mobility data. Together, our tools and findings establish methodological standards that support more robust, scalable, and reproducible mobility research at both individual and urban scales.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Malicious earworms and useful memes, how the far-right surfs on TikTok audio trends</title>
<link>https://arxiv.org/abs/2506.20695</link>
<guid>https://arxiv.org/abs/2506.20695</guid>
<content:encoded><![CDATA[
<div> TikTok, meme-making, remix, sound, extremist formations <br />
Summary: 
The research focuses on TikTok as a platform for meme creation and explores its role in right-wing extremist formations related to the 2024 German state elections. TikTok's sound infrastructure allows for the spread of xenophobic content through user-generated sounds that can easily blend in with benign meme trends. While hateful songs are not eligible for personalized feeds, they remain online and intersect with popular meme trends, making them visible in search results. TikTok has implemented measures to detect harmful challenges and illegal content, but the platform continues to face scrutiny for allowing extremist content to thrive. The analysis highlights the importance of sound in facilitating the dissemination of extremist content on TikTok, showcasing how cloaking practices benefit from the platform's sound infrastructure. <div>
arXiv:2506.20695v1 Announce Type: new 
Abstract: With its features of remix, TikTok is the designated platform for meme-making and dissemination. Creative combinations of video, emoji, and filters allow for an endless stream of memes and trends animated by sound. The platform has focused its moderation on upholding physical safety, hence investing in the detection of harmful challenges. In response to the DSA, TikTok implemented opt-outs for personalized feeds and features allowing users to report illegal content. At the same time, the platform remains subject to scrutiny. Centering on the role of sound and its intersections with ambiguous memes, the presented research probed right-wing extremist formations relating to the 2024 German state elections. The analysis evidences how the TikTok sound infrastructure affords a sustained presence of xenophobic content, often cloaked through vernacular modes of communication. These cloaking practices benefit from a sound infrastructure that affords the ongoing posting of user-generated sounds that instantly spread through the use-this-sound button. Importantly, these sounds are often not clearly recognizable as networkers of extremist content. Songs that do contain hateful lyrics are not eligible for personalized feeds, however, they remain online where they profit from intersecting with benign meme trends, rendering them visible in search results.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)</title>
<link>https://arxiv.org/abs/2506.20971</link>
<guid>https://arxiv.org/abs/2506.20971</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence in Education, Knowledge Co-occurrence Network Analysis, Intelligent Tutoring Systems, Generative Artificial Intelligence, Human-AI Collaboration 

Summary: 
The study examines 2,398 research articles in Artificial Intelligence in Education (AIED) from 2020 to 2024. The analysis reveals a strong technical focus in AIED research, with prevalent themes like intelligent tutoring systems and learning analytics, alongside emerging interests in large language models and generative artificial intelligence. Four emerging frontiers in AIED are identified: LLMs, GenAI, multimodal learning analytics, and human-AI collaboration. Research in GenAI covers personalization, self-regulated learning, feedback, assessment, motivation, and ethics. The shift towards human-centered AI for education is evident in current research interests. This study presents a comprehensive overview of AIED's evolution in the GenAI era, offering insights into future research directions and educational practices. 

<br /><br />Summary: <div>
arXiv:2506.20971v1 Announce Type: new 
Abstract: In this study, we analyze 2,398 research articles published between 2020 and 2024 across eight core venues related to the field of Artificial Intelligence in Education (AIED). Using a three-step knowledge co-occurrence network analysis, we analyze the knowledge structure of the field, the evolving knowledge clusters, and the emerging frontiers. Our findings reveal that AIED research remains strongly technically focused, with sustained themes such as intelligent tutoring systems, learning analytics, and natural language processing, alongside rising interest in large language models (LLMs) and generative artificial intelligence (GenAI). By tracking the bridging keywords over the past five years, we identify four emerging frontiers in AIED--LLMs, GenAI, multimodal learning analytics, and human-AI collaboration. The current research interests in GenAI are centered around GAI-driven personalization, self-regulated learning, feedback, assessment, motivation, and ethics.The key research interests and emerging frontiers in AIED reflect a growing emphasis on co-adaptive, human-centered AI for education. This study provides the first large-scale field-level mapping of AIED's transformation in the GenAI era and sheds light on the future research development and educational practices.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2506.20980</link>
<guid>https://arxiv.org/abs/2506.20980</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous graphs, node heterophily, contrastive learning, heterogeneity, multi-relational bipartite subgraphs 

Summary:
The paper introduces a novel framework called RASH, designed to tackle the challenge of capturing node heterophily in heterogeneous graphs. RASH explicitly models high-order semantics of heterogeneous interactions and separates homophilic and heterophilic patterns by utilizing dual heterogeneous hypergraphs. By dynamically constructing homophilic and heterophilic graphs based on relation importance, RASH effectively resolves the issues of heterogeneity and heterophily in heterogeneous graphs. The framework also incorporates a multi-relation contrastive loss to align heterogeneous and homophilic/heterophilic views and maximize mutual information. Experimental results on benchmark datasets demonstrate the effectiveness of RASH across various downstream tasks. The code for RASH is available on GitHub, providing a practical implementation of the proposed framework. <div>
arXiv:2506.20980v1 Announce Type: new 
Abstract: Real-world networks usually have a property of node heterophily, that is, the connected nodes usually have different features or different labels. This heterophily issue has been extensively studied in homogeneous graphs but remains under-explored in heterogeneous graphs, where there are multiple types of nodes and edges. Capturing node heterophily in heterogeneous graphs is very challenging since both node/edge heterogeneity and node heterophily should be carefully taken into consideration. Existing methods typically convert heterogeneous graphs into homogeneous ones to learn node heterophily, which will inevitably lose the potential heterophily conveyed by heterogeneous relations. To bridge this gap, we propose Relation-Aware Separation of Homophily and Heterophily (RASH), a novel contrastive learning framework that explicitly models high-order semantics of heterogeneous interactions and adaptively separates homophilic and heterophilic patterns. Particularly, RASH introduces dual heterogeneous hypergraphs to encode multi-relational bipartite subgraphs and dynamically constructs homophilic graphs and heterophilic graphs based on relation importance. A multi-relation contrastive loss is designed to align heterogeneous and homophilic/heterophilic views by maximizing mutual information. In this way, RASH simultaneously resolves the challenges of heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on benchmark datasets demonstrate the effectiveness of RASH across various downstream tasks. The code is available at: https://github.com/zhengziyu77/RASH.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty</title>
<link>https://arxiv.org/abs/2506.20992</link>
<guid>https://arxiv.org/abs/2506.20992</guid>
<content:encoded><![CDATA[
<div> game theory, blockchain systems, protocol mutability, cooperative mining behavior, institutional noise 

Summary:
This paper presents a game-theoretic model analyzing the impact of protocol mutability on cooperative mining behavior in blockchain systems. It demonstrates that even slight uncertainty in institutional rules can lead to strategic deviation and a shift towards short-term decision-making. Stable protocols support long-term investment and equilibrium strategies, while mutable ones promote short-termism and higher discount rates, potentially leading to collapse in cooperation. Simulation results reveal zones where rational mining transitions to extractive or arbitrage behavior due to institutional noise. The study suggests that protocol design should be viewed as a fundamental economic constraint rather than a variable, emphasizing the importance of rule stability for sustainable cooperation in decentralized systems. <div>
arXiv:2506.20992v1 Announce Type: cross 
Abstract: This paper develops a formal game-theoretic model to examine how protocol mutability disrupts cooperative mining behaviour in blockchain systems. Using a repeated game framework with stochastic rule shocks, we show that even minor uncertainty in institutional rules increases time preference and induces strategic deviation. Fixed-rule environments support long-term investment and stable equilibrium strategies; in contrast, mutable protocols lead to short-termism, higher discounting, and collapse of coordinated engagement. Simulation results identify instability zones in the parameter space where rational mining gives way to extractive or arbitrage conduct. These findings support an Austrian economic interpretation: calculability requires rule stability. Institutional noise undermines the informational basis for productive action. We conclude that protocol design must be treated as a constitutional economic constraint, not a discretionary variable, if sustainable cooperation is to emerge in decentralised systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution and determinants of firm-level systemic risk in local production networks</title>
<link>https://arxiv.org/abs/2506.21426</link>
<guid>https://arxiv.org/abs/2506.21426</guid>
<content:encoded><![CDATA[
<div> supply chain, systemic risk, production network, COVID-19, firm disruption
<br />
Summary: 
This study examines firm-level systemic risk in the Hungarian production network from 2015 to 2022, focusing on the impact of the COVID-19 pandemic. Using a heuristic maximum entropy null model, the researchers find that firms with high systemic risk undergo structural changes during the pandemic, with those facilitating economic exchanges becoming key players. The study reveals that firms' adaptive behavior leads to a more resilient economy post-pandemic. The international trade volume of firms is a significant predictor of systemic risk, but the effects of imports and exports on local systemic risk differ due to supply and demand channels. The findings highlight the importance of considering firms' abilities to react to crises and rewire supply links in assessing production network resilience. 
<br /> <div>
arXiv:2506.21426v1 Announce Type: cross 
Abstract: Recent crises like the COVID-19 pandemic and geopolitical tensions have exposed vulnerabilities and caused disruptions of supply chains, leading to product shortages, increased costs, and economic instability. This has prompted increasing efforts to assess systemic risk, namely the effects of firm disruptions on entire economies. However, the ability of firms to react to crises by rewiring their supply links has been largely overlooked, limiting our understanding of production networks resilience. Here we study dynamics and determinants of firm-level systemic risk in the Hungarian production network from 2015 to 2022. We use as benchmark a heuristic maximum entropy null model that generates an ensemble of production networks at equilibrium, by preserving the total input (demand) and output (supply) of each firm at the sector level. We show that the fairly stable set of firms with highest systemic risk undergoes a structural change during COVID-19, as those enabling economic exchanges become key players in the economy -- a result which is not reproduced by the null model. Although the empirical systemic risk aligns well with the null value until the onset of the pandemic, it becomes significantly smaller afterwards as the adaptive behavior of firms leads to a more resilient economy. Furthermore, firms' international trade volume (being a subject of disruption) becomes a significant predictor of their systemic risk. However, international links cannot provide an unequivocal explanation for the observed trends, as imports and exports have opposing effects on local systemic risk through the supply and demand channels.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A systematic comparison of measures for publishing k-anonymous social network data</title>
<link>https://arxiv.org/abs/2407.02290</link>
<guid>https://arxiv.org/abs/2407.02290</guid>
<content:encoded><![CDATA[
<div> anonymity measures, k-anonymity, social network data, privacy, attacker scenario  
Summary:  
This paper discusses the challenges of sharing or publishing social network data while preserving individual privacy. It specifically focuses on k-anonymity, a widely used privacy concept, and compares different anonymity measures to determine their effectiveness in protecting against attacker scenarios. The study includes a theoretical analysis and experimental investigations on real-world network datasets to evaluate the impact of different measures on anonymity levels, privacy trade-offs, and computational costs. The findings suggest that the choice of anonymity measure significantly influences the level of anonymity provided and the resources required. Surprisingly, the most effective measure for protecting against attackers considers a broader node vicinity with minimal structural information, leading to lower computational costs. This research provides valuable insights for researchers and practitioners in selecting appropriate anonymity measures for sharing social network data under privacy constraints. <div>
arXiv:2407.02290v2 Announce Type: replace 
Abstract: Sharing or publishing social network data while accounting for privacy of individuals is a difficult task due to the interconnectedness of nodes in networks. A key question in k-anonymity, a widely studied notion of privacy, is how to measure the anonymity of an individual, as this determines the attacker scenarios one protects against. In this paper, we systematically compare the most prominent anonymity measures from the literature in terms of the completeness and reach of the structural information they take into account. We present a theoretical characterization and a distance-parametrized strictness ordering of the existing measures for k-anonymity in networks. In addition, we conduct empirical experiments on a wide range of real-world network datasets with up to millions of edges. Our findings reveal that the choice of the measure significantly impacts the measured level of anonymity and hence the effectiveness of the corresponding attacker scenario, the privacy vs. utility trade-off, and computational cost. Surprisingly, we find that the anonymity measure representing the most effective attacker scenario considers a greater node vicinity yet utilizes only limited structural information and therewith minimal computational resources. Overall, the insights provided in this work offer researchers and practitioners practical guidance for selecting appropriate anonymity measures when sharing or publishing social network data under privacy constraints.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
<link>https://arxiv.org/abs/2110.11856</link>
<guid>https://arxiv.org/abs/2110.11856</guid>
<content:encoded><![CDATA[
<div> Sparse networks, $\beta$-model, estimation algorithms, $\ell_2$-penalized MLE algorithm, COVID-19

Summary:
This paper introduces improvements to the $\beta$-model for modeling large and sparse networks driven by degree heterogeneity. Existing estimation algorithms for the $\beta$-model are limited in scalability, but the proposed $\ell_2$-penalized MLE algorithm can handle sparse networks with millions of nodes efficiently. The paper also establishes rate-optimal error bounds and asymptotic normality results for $\beta$-models, even under weaker network sparsity assumptions. Application of the method to large COVID-19 network data sets yielded meaningful results. The advancements presented in this paper address the challenges faced in modeling large and sparse networks with degree heterogeneity, providing practical solutions for complex network analysis. 

<br /><br />Summary: <div>
arXiv:2110.11856v5 Announce Type: replace-cross 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets discovered meaningful results.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BotHash: Efficient and Training-Free Bot Detection Through Approximate Nearest Neighbor</title>
<link>https://arxiv.org/abs/2506.20503</link>
<guid>https://arxiv.org/abs/2506.20503</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Social Networks, Social bots, Large Language Models, BotHash, Bot detection <br />
Summary: <br />
Online Social Networks (OSNs) are vital platforms for content consumption but face challenges with the accuracy of shared information due to the spread of disinformation. Social bots, particularly problematic, spread misinformation and are made more complex by Large Language Models (LLMs). BotHash is introduced as a training-free solution for social bot detection by simplifying user representation for effective detection using approximate nearest-neighbor search. This approach offers advantages such as independence from training, robust performance with minimal data, and early detection capabilities, even with the use of state-of-the-art LLMs. BotHash shows promising results across various datasets, showcasing its effectiveness in differentiating between human and bot accounts. <div>
arXiv:2506.20503v1 Announce Type: new 
Abstract: Online Social Networks (OSNs) are a cornerstone in modern society, serving as platforms for diverse content consumption by millions of users each day. However, the challenge of ensuring the accuracy of information shared on these platforms remains significant, especially with the widespread dissemination of disinformation. Social bots -- automated accounts designed to mimic human behavior, frequently spreading misinformation -- represent one of the critical problems of OSNs. The advent of Large Language Models (LLMs) has further complicated bot behaviors, making detection increasingly difficult. This paper presents BotHash, an innovative, training-free approach to social bot detection. BotHash leverages a simplified user representation that enables approximate nearest-neighbor search to detect bots, avoiding the complexities of Deep-Learning model training and large dataset creation. We demonstrate that BotHash effectively differentiates between human and bot accounts, even when state-of-the-art LLMs are employed to generate posts' content. BotHash offers several advantages over existing methods, including its independence from a training phase, robust performance with minimal ground-truth data, and early detection capabilities, showing promising results across various datasets.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Aware Design: Modulating Valence, Arousal, and Dominance in Communication via Design</title>
<link>https://arxiv.org/abs/2502.16038</link>
<guid>https://arxiv.org/abs/2502.16038</guid>
<content:encoded><![CDATA[
<div> Emotionally-aware design, VAD model, comprehension, memory, behavior, multimodal design space

Summary: This review explores the concept of emotion-aware design, which emphasizes the importance of emotional awareness in communication. By utilizing the valence-arousal-dominance (VAD) model of affect, the framework examines how emotions impact information processing. Emotional responses play a significant role in perception, memory, and behavior, highlighting the need for strategic regulation of emotional dimensions in communication design. The proposed multimodal design space incorporating text, visuals, audio, and interaction offers a practical way to enhance communication efficacy by harnessing emotional dynamics. By linking emotional modulation to cognitive outcomes, this review provides a foundation for creating emotionally resonant communication in various fields including education, health, media, and public discourse.<br /><br />Summary: <div>
arXiv:2502.16038v3 Announce Type: replace 
Abstract: In an era of emotionally saturated digital media and information overload, effective communication demands more than clarity and accuracy-it requires emotional awareness. This review introduces the paradigm of emotion-aware design, a framework grounded in the valence-arousal-dominance (VAD) model of affect, which systematically examines how emotional modulation shapes comprehension, memory, and behavior. Drawing on insights from psychology, neuroscience, communication, and design, we show that emotional responses significantly influence how information is perceived, retained, and shared. We further propose a multimodal design space-encompassing text, visuals, audio, and interaction-that enables strategic regulation of emotional dimensions to enhance communication efficacy. By linking emotional dynamics to cognitive outcomes and practical design strategies, this review offers both a conceptual foundation and an applied roadmap for designing emotionally resonant communication across domains such as education, health, media, and public discourse.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block-corrected Modularity for Community Detection</title>
<link>https://arxiv.org/abs/2502.20083</link>
<guid>https://arxiv.org/abs/2502.20083</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, community structure, modularity, spectral method, citation networks

Summary: 
The study introduces a block-corrected modularity measure designed to uncover community structures hidden by unknown node attributes in complex networks. Through analytical analysis and experiments on synthetic models, the proposed modularity successfully identifies underlying community structures masked by known attributes. Spectral methods and fine-tuning algorithms inspired by the Louvain method are developed to maximize the modularity. These approaches are shown to outperform methods utilizing different null models in revealing hidden community structures. The methodology is applied to real-world citation networks constructed from OpenAlex data, where it effectively corrects for temporal citation patterns to identify significant community structures driven by unknown attributes. The study highlights the importance of considering unknown node attributes in determining community structures within complex networks. 

<br /><br />Summary: <div>
arXiv:2502.20083v2 Announce Type: replace-cross 
Abstract: Unknown node attributes in complex networks may introduce community structures that are important to distinguish from those driven by known attributes. We propose a block-corrected modularity that discounts given block structures present in the network to reveal communities masked by them. We show analytically how the proposed modularity finds the community structure driven by an unknown attribute in a simple network model. Further, we observe that the block-corrected modularity finds the underlying community structure on a number of simple synthetic network models while methods using different null models fail. We develop an efficient spectral method as well as two Louvain-inspired fine-tuning algorithms to maximize the proposed modularity and demonstrate their performance on several synthetic network models. Finally, we assess our methodology on various real-world citation networks built using the OpenAlex data by correcting for the temporal citation patterns.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Diffusion Structures of Heterogeneous Network Cascade</title>
<link>https://arxiv.org/abs/2506.19142</link>
<guid>https://arxiv.org/abs/2506.19142</guid>
<content:encoded><![CDATA[
<div> Keywords: network cascade, diffusion networks, double mixture directed graph model, multi-layer, Granger causality

Summary: 
The article introduces a novel double mixture directed graph model for inferring multi-layer diffusion networks from cascade data. This model captures the heterogeneity present in real-world cascades by representing cascade pathways as a mixture of diffusion networks across different layers. It also imposes layer-specific structural constraints to capture complementary cascading patterns across the population. The advantage of the model is its convex formulation, providing statistical and computational guarantees for diffusion network estimates. Through simulation studies, the model's performance in recovering diverse diffusion structures is demonstrated. Lastly, the method is applied to analyze research topic cascades among U.S. universities in the social sciences, revealing the underlying diffusion networks of research topic propagation among institutions.<br /><br />Summary: <div>
arXiv:2506.19142v1 Announce Type: new 
Abstract: Network cascade refers to diffusion processes in which outcome changes within part of an interconnected population trigger a sequence of changes across the entire network. These cascades are governed by underlying diffusion networks, which are often latent. Inferring such networks is critical for understanding cascade pathways, uncovering Granger causality of interaction mechanisms among individuals, and enabling tasks such as forecasting or maximizing information propagation. In this project, we propose a novel double mixture directed graph model for inferring multi-layer diffusion networks from cascade data. The proposed model represents cascade pathways as a mixture of diffusion networks across different layers, effectively capturing the strong heterogeneity present in real-world cascades. Additionally, the model imposes layer-specific structural constraints, enabling diffusion networks at different layers to capture complementary cascading patterns at the population level. A key advantage of our model is its convex formulation, which allows us to establish both statistical and computational guarantees for the resulting diffusion network estimates. We conduct extensive simulation studies to demonstrate the model's performance in recovering diverse diffusion structures. Finally, we apply the proposed method to analyze cascades of research topics in the social sciences across U.S. universities, revealing the underlying diffusion networks of research topic propagation among institutions.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection in Energy Networks based on Energy Self-Sufficiency and Dynamic Flexibility Activation</title>
<link>https://arxiv.org/abs/2506.19412</link>
<guid>https://arxiv.org/abs/2506.19412</guid>
<content:encoded><![CDATA[
<div> Keywords: energy transition, microgrids, community detection, energy modularity, decentralized energy systems

Summary:
The article addresses the need for novel control structures in the global energy transition towards distributed resources like microgrids and energy communities. It introduces the concept of energy modularity as a metric to evaluate community partitions based on energy self-sufficiency and flexibility. A scalable community detection algorithm based on the Louvain method is proposed to maximize energy modularity, either through linear programming or simulation-based approaches. The algorithm is validated on a benchmark grid, showcasing its effectiveness in identifying optimal energy clusters in decentralized energy systems. <div>
arXiv:2506.19412v1 Announce Type: new 
Abstract: The global energy transition towards distributed, smaller-scale resources, such as decentralized generation and flexible assets like storage and shiftable loads, demands novel control structures aligned with the emerging network architectures. These architectures consist of interconnected, self-contained clusters, commonly called microgrids or energy communities. These clusters aim to optimize collective self-sufficiency by prioritizing local energy use or operating independently during wide-area blackouts. This study addresses the challenge of defining optimal clusters, framed as a community detection problem. A novel metric, termed energy modularity, is proposed to evaluate community partitions by quantifying energy self-sufficiency within clusters while incorporating the influence of flexible resources. Furthermore, a highly scalable community detection algorithm to maximize energy modularity based on the Louvain method is presented. Therefore, energy modularity is calculated using linear programming or a more efficient simulation-based approach. The algorithm is validated on an exemplary benchmark grid, demonstrating its effectiveness in identifying optimal energy clusters for modern decentralized energy systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanders in Models of Social Networks</title>
<link>https://arxiv.org/abs/2506.19485</link>
<guid>https://arxiv.org/abs/2506.19485</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, geometric random graphs, expanders, minimum-component distance, algorithms

Summary:
Minimum-Component Distance Geometric Inhomogeneous Random Graphs (GIRGs) model social networks where vertices are positioned in a latent geometric space, forming edges based on similarity in some dimensions. Unlike the traditional model where similarity in all dimensions is required to form edges, this model allows edges to form if vertices align in some dimensions. For dimensions greater than or equal to 2, these GIRGs exhibit strong expanding properties, with the induced subgraph of vertices having expected degree at least $(\log n)^C$ forming an expander. The expansion factor of the resulting subgraph is shown to be significant except for sets already occupying a constant fraction of the vertices. This finding is important as it indicates that algorithms and mixing processes operate efficiently on these expander graphs. <div>
arXiv:2506.19485v1 Announce Type: new 
Abstract: A common model for social networks are Geometric Inhomogeneous Random Graphs (GIRGs), in which vertices draw a random position in some latent geometric space, and the probability of two vertices forming an edge depends on their geometric distance. The geometry may be modelled in two ways: either two points are defined as close if they are similar in all dimensions, or they are defined as close if they are similar in some dimensions. The first option is mathematically more natural since it can be described by metrics. However, the second option is arguably the better model for social networks if the different dimensions represent features like profession, kinship, or interests. In such cases, nodes already form bonds if they align in some, but not all dimensions. For the first option, it is known that the resulting networks are poor expanders. We study the second option in the form of Minimum-Component Distance GIRGs, and find that those behave the opposite way for dimension $d\ge 2$, and that they have strong expanding properties. More precisely, for a suitable constant $C>0$, the subgraph induced by vertices of (expected) degree at least $(\log n)^C$ forms an expander. Moreover, we study how the expansion factor of the resulting subgraph depends on the choice of $C$, and show that this expansion factor is $\omega(1)$ except for sets that already take up a constant fraction of the vertices. This has far-reaching consequences, since many algorithms and mixing processes are fast on expander graphs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Properties and Expressivity of Linear Geometric Centralities</title>
<link>https://arxiv.org/abs/2506.19670</link>
<guid>https://arxiv.org/abs/2506.19670</guid>
<content:encoded><![CDATA[
<div> linear centralities, geometric centralities, shortest-path distances, centrality measures, axiomatic approach

Summary:
Linear (geometric) centralities, based on shortest-path distances, are studied in their entirety in this paper. These centralities, defined as a linear transformation of the distance-count vector, are analyzed using an axiomatic approach to understand their expressivity in distinguishing nodes in a graph. The study also investigates the ability of linear centralities to induce different rankings of nodes in a graph. A linear programming formulation, utilizing Farkas' lemma, is employed to determine the number of distinct rankings possible with linear centralities. This approach has potential applications in various scenarios, particularly in adversarial settings. The research provides insights into the nature and capabilities of linear centralities, shedding light on their significance in ranking nodes within a graph. 

<br /><br />Summary: <div>
arXiv:2506.19670v1 Announce Type: new 
Abstract: Centrality indices are used to rank the nodes of a graph by importance: this is a common need in many concrete situations (social networks, citation networks, web graphs, for instance) and it was discussed many times in sociology, psychology, mathematics and computer science, giving rise to a whole zoo of definitions of centrality. Although they differ widely in nature, many centrality measures are based on shortest-path distances: such centralities are often referred to as geometric. Geometric centralities can use the shortest-path-length information in many different ways, but most of the existing geometric centralities can be defined as a linear transformation of the distance-count vector (that is, the vector containing, for every index t, the number of nodes at distance t).
  In this paper we study this class of centralities, that we call linear (geometric) centralities, in their full generality. In particular, we look at them in the light of the axiomatic approach, and we study their expressivity: we show to what extent linear centralities can be used to distinguish between nodes in a graph, and how many different rankings of nodes can be induced by linear centralities on a given graph. The latter problem (which has a number of possible applications, especially in an adversarial setting) is solved by means of a linear programming formulation, which is based on Farkas' lemma, and is interesting in its own right.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Use and Emergent Cooperation</title>
<link>https://arxiv.org/abs/2506.18920</link>
<guid>https://arxiv.org/abs/2506.18920</guid>
<content:encoded><![CDATA[
<div> agents, tribes, communication signals, cooperation, culture <br />
Summary: <br />
The study explores how autonomous agents in tribes develop a shared behavioral system through communication signals, similar to a culture, using the NEC-DAC system with neural networks. By examining different social structures and communication strategies, the research reveals that a culture of cooperation significantly impacts the tribe's performance. The study also highlights how signals facilitate culture emergence and transmission across agent generations. Coordination of behavior and signaling within individual agents' neural networks is shown to be beneficial. The self-organization of culture within tribes and the influence of varying communication strategies on fitness and cooperation are key focuses of the research. <div>
arXiv:2506.18920v1 Announce Type: cross 
Abstract: In this work, we investigate how autonomous agents, organized into tribes, learn to use communication signals to coordinate their activities and enhance their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture - Distributed Autonomous Communicators) system, where each agent is equipped with its own neural network for decision-making, we demonstrate how these agents develop a shared behavioral system -- akin to a culture -- through learning and signalling. Our research focuses on the self-organization of culture within these tribes of agents and how varying communication strategies impact their fitness and cooperation. By analyzing different social structures, such as authority hierarchies, we show that the culture of cooperation significantly influences the tribe's performance. Furthermore, we explore how signals not only facilitate the emergence of culture but also enable its transmission across generations of agents. Additionally, we examine the benefits of coordinating behavior and signaling within individual agents' neural networks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Hatred: Efficient Multimodal Detection of Hatemongers</title>
<link>https://arxiv.org/abs/2506.19603</link>
<guid>https://arxiv.org/abs/2506.19603</guid>
<content:encoded><![CDATA[
<div> hate speech, online discourse, user level, multimodal approach, detection

Summary:
- Automatic detection of online hate speech is crucial for improving online discourse and understanding hate as a social phenomenon.
- Focusing on the user level along with their texts, social activity, and network can enhance the detection of hate-mongers.
- A multimodal aggregative approach was used to detect hate-mongers by considering texts, user activity, and user network.
- The method was evaluated on Twitter, Gab, and Parler datasets, showing improved detection compared to previous methods.
- The approach can enhance classification of coded messages, dog-whistling, racial gas-lighting, and inform intervention measures.
- The multimodal approach performed well across diverse content platforms, large datasets, and networks.

<br /><br />Summary: <div>
arXiv:2506.19603v1 Announce Type: cross 
Abstract: Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases. Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Graph Databases</title>
<link>https://arxiv.org/abs/2506.19661</link>
<guid>https://arxiv.org/abs/2506.19661</guid>
<content:encoded><![CDATA[
<div> Graph databases, higher-order interactions, HO-GDBs, polyadic modeling, graph neural networks <br />
Summary: 
This study introduces higher-order graph databases (HO-GDBs) to support higher-order interactions in graph analytics beyond first-order relations. The system uses lifting and lowering paradigms to extend traditional graph databases seamlessly. Theoretical analysis ensures correctness, scalability, and ACID compliance for OLTP and OLAP queries. A lightweight, modular, and parallelizable HO-GDB prototype is implemented offering native support for hypergraphs, node-tuples, and subgraphs under a unified API. The prototype scales to large HO workloads and demonstrates improved performance for analytical tasks, such as enhancing graph neural networks within a GDB. The system maintains low latency, high query throughput, and generalizes both ACID-compliant and eventually consistent systems. <div>
arXiv:2506.19661v1 Announce Type: cross 
Abstract: Recent advances in graph databases (GDBs) have been driving interest in large-scale analytics, yet current systems fail to support higher-order (HO) interactions beyond first-order (one-hop) relations, which are crucial for tasks such as subgraph counting, polyadic modeling, and HO graph learning. We address this by introducing a new class of systems, higher-order graph databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and OLAP queries, ensuring correctness, scalability, and ACID compliance. We implement a lightweight, modular, and parallelizable HO-GDB prototype that offers native support for hypergraphs, node-tuples, subgraphs, and other HO structures under a unified API. The prototype scales to large HO OLTP & OLAP workloads and shows how HO improves analytical tasks, for example enhancing accuracy of graph neural networks within a GDB by 44%. Our work ensures low latency and high query throughput, and generalizes both ACID-compliant and eventually consistent systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words as Trigger Points in Social Media Discussions: A Large-Scale Case Study about UK Politics on Reddit</title>
<link>https://arxiv.org/abs/2405.10213</link>
<guid>https://arxiv.org/abs/2405.10213</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, political debates, trigger points, online communication, affective polarisation

Summary: 
Trigger points, moments that challenge individuals' societal norms and beliefs, have been found to elicit strong negative emotional responses in online political debates. Through the analysis of over 100 million comments on Reddit related to trigger words identified in UK politics, researchers found that trigger words increased user engagement and animosity, leading to more negativity, hate speech, and controversial comments. This study suggests that trigger points play a significant role in shaping online communication dynamics, particularly in political discussions. By introducing the concept of trigger points to computational studies of online communication, researchers can gain insights into affective polarisation, online deliberation, and how citizens engage in political discourse on social media platforms.<br /><br />Summary: <div>
arXiv:2405.10213v3 Announce Type: replace 
Abstract: Political debates on social media sometimes flare up. From that moment on, users engage much more with one another; their communication is also more emotional and polarised. While it has been difficult to grasp such moments with computational methods, we suggest that trigger points are a useful concept to understand and ultimately model such behaviour. Established in qualitative focus group interviews to understand political polarisation (Mau, Lux, and Westheuser 2023), trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals show strong and negative emotional responses when certain triggering words or topics are mentioned. Our paper finds that these trigger points also exist in online debates. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million comments from subreddits related to a set of words identified as trigger points in UK politics. Analysing the comments, we find that trigger words increase user engagement and animosity, i.e., more negativity, hate speech, and controversial comments. Introducing trigger points to computational studies of online communication, our findings are relevant to researchers interested in affective computing, online deliberation, and how citizens debate politics and society in light of affective polarisation.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Temporal Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2506.17312</link>
<guid>https://arxiv.org/abs/2506.17312</guid>
<content:encoded><![CDATA[
<div> Graph representation learning, heterogeneity, dynamics, complex networks, capturing high-order interactions<br />
<br />
Summary: 
The article discusses the importance of capturing high-order interactions in complex networks through graph representation learning (GRL) methods. Existing GRL techniques often focus on preserving low-order topology information and neglect higher-order group interaction relationships found in real-world networks. To address this limitation, the authors propose a formal definition of heterogeneous temporal hypergraphs and a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN). The HTHGN incorporates a hierarchical attention mechanism to facilitate temporal message-passing between heterogeneous nodes and hyperedges, allowing for a more comprehensive capture of high-order interactions in heterogeneous temporal graphs (HTGs). Additionally, the HTHGN employs contrastive learning to enhance the consistency between low-order correlated heterogeneous node pairs in HTGs. Experimental results on real-world datasets demonstrate the effectiveness of the proposed HTHGN in modeling high-order interactions and achieving significant performance improvements in HTGs. <br /><br />Summary: <div>
arXiv:2506.17312v1 Announce Type: new 
Abstract: Graph representation learning (GRL) has emerged as an effective technique for modeling graph-structured data. When modeling heterogeneity and dynamics in real-world complex networks, GRL methods designed for complex heterogeneous temporal graphs (HTGs) have been proposed and have achieved successful applications in various fields. However, most existing GRL methods mainly focus on preserving the low-order topology information while ignoring higher-order group interaction relationships, which are more consistent with real-world networks. In addition, most existing hypergraph methods can only model static homogeneous graphs, limiting their ability to model high-order interactions in HTGs. Therefore, to simultaneously enable the GRL model to capture high-order interaction relationships in HTGs, we first propose a formal definition of heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge construction algorithm that does not rely on additional information. Then, a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical attention mechanism module that simultaneously performs temporal message-passing between heterogeneous nodes and hyperedges to capture rich semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN performs contrastive learning by maximizing the consistency between low-order correlated heterogeneous node pairs on HTG to avoid the low-order structural ambiguity issue. Detailed experimental results on three real-world HTG datasets verify the effectiveness of the proposed HTHGN for modeling high-order interactions in HTGs and demonstrate significant performance improvements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A family of graph GOSPA metrics for graphs with different sizes</title>
<link>https://arxiv.org/abs/2506.17316</link>
<guid>https://arxiv.org/abs/2506.17316</guid>
<content:encoded><![CDATA[
<div> Graph metrics, distances, sizes, GOSPA metric, linear programming<br />
<br />
Summary: 
This paper introduces a family of graph metrics designed to measure distances between graphs of varying sizes. The proposed metric family builds upon the graph generalised optimal sub-pattern assignment (GOSPA) metric, while also incorporating additional penalties for edge mismatches. The metrics in the family retain the ability to penalize node attribute costs and unassigned nodes, similar to the original GOSPA metric. The paper demonstrates that these metrics can be approximately computed using linear programming techniques. Simulation experiments showcase the characteristics of the graph GOSPA metric family across different hyperparameter selections. Furthermore, real-world dataset experiments highlight the advantages of utilizing the proposed graph GOSPA metric family for classification tasks. <div>
arXiv:2506.17316v1 Announce Type: new 
Abstract: This paper proposes a family of graph metrics for measuring distances between graphs of different sizes. The proposed metric family defines a general form of the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also proved to satisfy the metric properties. Similarly to the graph GOSPA metric, the proposed graph GOSPA metric family also penalises the node attribute costs for assigned nodes between the two graphs, and the number of unassigned nodes. However, the proposed family of metrics provides more general penalties for edge mismatches than the graph GOSPA metric. This paper also shows that the graph GOSPA metric family can be approximately computed using linear programming. Simulation experiments are performed to illustrate the characteristics of the proposed graph GOSPA metric family with different choices of hyperparameters. The benefits of the proposed graph GOSPA metric family for classification tasks are also shown on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Iterative Graph Alignment Using Heat Diffusion</title>
<link>https://arxiv.org/abs/2506.17640</link>
<guid>https://arxiv.org/abs/2506.17640</guid>
<content:encoded><![CDATA[
<div> representation generation, heat diffusion, node alignment, UPGA, IterAlign

Summary: 
- The paper introduces IterAlign, a novel unsupervised plain graph alignment method that is parameter-free and efficient.
- IterAlign addresses the issue of biased node representations in existing UPGA methods by using a representation generation method based on heat diffusion.
- Two complementary node alignment strategies are employed in IterAlign to balance accuracy and efficiency across graphs of varying scales.
- Through an iterative process of representation generation and node alignment, IterAlign rectifies biases in node representations and refines the alignment process.
- Extensive experiments on three public benchmarks show that IterAlign outperforms state-of-the-art UPGA methods with lower computational overhead and is able to approach the theoretical accuracy upper bound in unsupervised plain graph alignment tasks. 

<br /><br />Summary: <div>
arXiv:2506.17640v1 Announce Type: new 
Abstract: Unsupervised plain graph alignment (UPGA) aims to align corresponding nodes across two graphs without any auxiliary information. Existing UPGA methods rely on structural consistency while neglecting the inherent structural differences in real-world graphs, leading to biased node representations. Moreover, their one-shot alignment strategies lack mechanisms to correct erroneous matches arising from inaccurate anchor seeds. To address these issues, this paper proposes IterAlign, a novel parameter-free and efficient UPGA method. First, a simple yet powerful representation generation method based on heat diffusion is introduced to capture multi-level structural characteristics, mitigating the over-reliance on structural consistency and generating stable node representations. Two complementary node alignment strategies are then adopted to balance alignment accuracy and efficiency across graphs of varying scales. By alternating between representation generation and node alignment, IterAlign iteratively rectifies biases in nodes representations and refines the alignment process, leading to superior and robust alignment performance. Extensive experiments on three public benchmarks demonstrate that the proposed IterAlign outperforms state-of-the-art UPGA approaches with a lower computational overhead, but also showcases the ability to approach the theoretical accuracy upper bound of unsupervised plain graph alignment task.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evolution of Complex Networks: A Reinforcement Learning Approach Applying Evolutionary Games to Community Structure</title>
<link>https://arxiv.org/abs/2506.17925</link>
<guid>https://arxiv.org/abs/2506.17925</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, individual birth-death, community structures, reinforcement learning, population dynamics

Summary: 
The article proposes a networked evolution model to study individual birth-death processes and community development within dynamic systems. This model incorporates reinforcement learning through games among individuals with varying lifespans. Through extensive experiments, the model demonstrates the evolution of cooperative behaviors and community structures within systems. The fitting of real-world populations and networks supports the practicality of the proposed model. Analyses reveal that exploitation rates, payoff parameters, learning rates, discount factors, and two-dimensional space dimensions play crucial roles in determining the emergence, speed, stability, and size of communities. Overall, the model offers a novel perspective on understanding real-world community development and provides a valuable framework for studying population dynamics behaviors.<br /><br />Summary: <div>
arXiv:2506.17925v1 Announce Type: new 
Abstract: Complex networks serve as abstract models for understanding real-world complex systems and provide frameworks for studying structured dynamical systems. This article addresses limitations in current studies on the exploration of individual birth-death and the development of community structures within dynamic systems. To bridge this gap, we propose a networked evolution model that includes the birth and death of individuals, incorporating reinforcement learning through games among individuals. Each individual has a lifespan following an arbitrary distribution, engages in games with network neighbors, selects actions using Q-learning in reinforcement learning, and moves within a two-dimensional space. The developed theories are validated through extensive experiments. Besides, we observe the evolution of cooperative behaviors and community structures in systems both with and without the birth-death process. The fitting of real-world populations and networks demonstrates the practicality of our model. Furthermore, comprehensive analyses of the model reveal that exploitation rates and payoff parameters determine the emergence of communities, learning rates affect the speed of community formation, discount factors influence stability, and two-dimensional space dimensions dictate community size. Our model offers a novel perspective on real-world community development and provides a valuable framework for studying population dynamics behaviors.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on False Information Detection: From A Perspective of Propagation on Social Networks</title>
<link>https://arxiv.org/abs/2506.18052</link>
<guid>https://arxiv.org/abs/2506.18052</guid>
<content:encoded><![CDATA[
<div> propagation, false information, detection techniques, misinformation, information dissemination
Summary: 
The paper provides a comprehensive review of false information detection techniques, focusing on the propagation characteristics of misinformation. It introduces a new taxonomy categorizing methods into homogeneous and heterogeneous propagation-based approaches. The paper includes problem formulation, dataset review, and state-of-the-art method summaries for each category. Promising future research directions identified include creating a unified benchmark suite, exploring diverse information modalities, and developing innovative rumor debunking tasks. The systematic organization of current techniques in this work offers a clear landscape of the research field, aiding researchers and practitioners in navigating the complexities and inspiring further advancements. <div>
arXiv:2506.18052v1 Announce Type: new 
Abstract: The proliferation of false information in the digital age has become a pressing concern, necessitating the development of effective and robust detection methods. This paper offers a comprehensive review of existing false information detection techniques, approached from a novel perspective that emphasizes the propagation characteristics of misinformation. We introduce a new taxonomy that categorizes these methods into homogeneous and heterogeneous propagation-based approaches, providing a deeper understanding of the varying scopes and complexities involved in information dissemination. For each category, we present a formal problem formulation, review commonly used datasets, and summarize state-of-the-art methods. Additionally, we identify several promising directions for future research, including the creation of a unified benchmark suite, exploration of diverse information modalities, and development of innovative rumor debunking tasks. By systematically organizing the vast array of current techniques, this work offers a clear overview of the research landscape, aiding researchers and practitioners in navigating this complex field and inspiring further advancements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving spreading dynamics and information flow in complex network reduction</title>
<link>https://arxiv.org/abs/2506.18641</link>
<guid>https://arxiv.org/abs/2506.18641</guid>
<content:encoded><![CDATA[
<div> Keywords: network reduction, epidemic spreading dynamics, information flow, subgraph extraction, optimization strategy

Summary: 
The paper introduces a novel network reduction framework based on subgraph extraction to preserve both structural and dynamical properties efficiently. The proposed method involves a degree centrality-driven node removal algorithm to selectively eliminate low-degree nodes and create a smaller subnetwork. An edge pruning algorithm is then used to adjust the edge density of the subnetwork while maintaining its average degree similar to the original network. Experimental results on various network types demonstrate that the approach can reduce network size by over 85% while retaining epidemic dynamics and information flow characteristics. This method shows promise in predicting the dynamic behavior of large-scale real-world networks. 

<br /><br />Summary: <div>
arXiv:2506.18641v1 Announce Type: new 
Abstract: Effectively preserving both the structural and dynamical properties during the reduction of complex networks remains a significant research topic. Existing network reduction methods based on renormalization group or sampling often face challenges such as high computational complexity and the loss of critical dynamic attributes. This paper proposes an efficient network reduction framework based on subgraph extraction, which accurately preserves epidemic spreading dynamics and information flow through a coordinated optimization strategy of node removal and edge pruning. Specifically, a degree centrality-driven node removal algorithm is adopted to preferentially remove low-degree nodes, thereby constructing a smaller-scale subnetwork. Subsequently, an edge pruning algorithm is designed to regulate the edge density of the subnetwork, ensuring that its average degree remains approximately consistent with that of the original network. Experimental results on Erd\"os-R\'enyi random graphs, Barab\'asi-Albert scale-free networks, and real-world social contact networks from various domains demonstrate that this proposed method can reduce the size of networks with heterogeneous structures by more than 85%, while preserving their epidemic dynamics and information flow. These findings provide valuable insights for predicting the dynamical behavior of large-scale real-world networks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioXplorer: An Interactive Tool for Topic and Network Analysis in Social Data</title>
<link>https://arxiv.org/abs/2506.18845</link>
<guid>https://arxiv.org/abs/2506.18845</guid>
<content:encoded><![CDATA[
<div> AI, natural language processing, social network analysis, Twitter, YouTube

Summary:
SocioXplorer is a new interactive tool designed for computational social science researchers to analyze topics and networks in social data from Twitter and YouTube. It combines artificial intelligence, natural language processing, and social network analysis, allowing for in-depth analysis of live datasets that can be regularly updated. This tool is an extension of the previous system, TwiXplorer, which was limited to analyzing archival Twitter data. SocioXplorer enhances this capability by adding the ability to analyze YouTube data and offers batch data processing features. The system is released under the Apache 2 license, providing researchers with a powerful tool for understanding social data on multiple platforms. <div>
arXiv:2506.18845v1 Announce Type: new 
Abstract: SocioXplorer is a powerful interactive tool that computational social science researchers can use to understand topics and networks in social data from Twitter (X) and YouTube. It integrates, among other things, artificial intelligence, natural language processing and social network analysis. It can be used with ``live" datasets that receive regular updates. SocioXplorer is an extension of a previous system called TwiXplorer, which was limited to the analysis of archival Twitter (X) data. SocioXplorer builds on this by adding the ability to analyse YouTube data, greater depth of analysis and batch data processing. We release it under the Apache 2 licence.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</title>
<link>https://arxiv.org/abs/2506.17851</link>
<guid>https://arxiv.org/abs/2506.17851</guid>
<content:encoded><![CDATA[
<div> Keywords: novelty, scientific progress, network science, interdisciplinary studies, citation counts

Summary:
- The study addresses the issue of current reward systems failing to recognize novel ideas in science.
- A triadic typology is introduced, categorizing novelty into Pioneers, Mavericks, and Vanguards based on how they introduce new ideas.
- Analysis of a dataset in philanthropy and nonprofit studies shows that different types of novelty are not uniformly rewarded.
- Pioneers lay the foundation but are often overlooked, Mavericks benefit from displacing prior focus, and Vanguards gain recognition by strengthening weak connections.
- Citation advantages vary for different types of novelty over time and as topics become more central in the field.

<br /><br />Summary: <div>
arXiv:2506.17851v1 Announce Type: cross 
Abstract: Scientific progress depends on novel ideas, but current reward systems often fail to recognize them. Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them. This study develops a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition. Drawing on network science and theories of discovery, we introduce a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak but promising connections. We apply this typology to a dataset of 41,623 articles in the interdisciplinary field of philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression. Results show that novelty is not uniformly rewarded. Pioneer efforts are foundational but often overlooked. Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus. Vanguard novelty is more likely to gain recognition when it strengthens weakly connected topics, but its citation advantage diminishes as those reinforced nodes become more central. To enable fair comparison across time and domains, we introduce a simulated baseline model. These findings improve the evaluation of innovations, affecting science policy, funding, and institutional assessment practices.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
<link>https://arxiv.org/abs/2506.18154</link>
<guid>https://arxiv.org/abs/2506.18154</guid>
<content:encoded><![CDATA[
<div> Taylor's Power Law, universality, ecosystems, statistical distributions, future research directions
Summary: 
Taylor's Power Law (TPL), first discovered in 1961, correlates population mean abundances and variances using a power function. Two main prongs of exploration have emerged: mathematical/statistical and ecological mechanisms. Over the past six decades, TPL studies have evolved through three distinct periods, covering various themes including population spatial aggregation, statistical distributions, and complex networks. The significance of TPL research lies in its practical applications in various fields such as agriculture, epidemiology, and finance, as well as its theoretical implications related to phase transitions and scale invariance. Three future research directions have been identified: fostering interactions between the two prongs, heterogeneity measurement, and exploration in evolutionary contexts. Reciprocal interactions between mathematical/statistical and ecological perspectives are crucial for advancing TPL research in ecosystems. <div>
arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Total Variation Distance Estimators for Changepoint Detection in News Data</title>
<link>https://arxiv.org/abs/2506.18764</link>
<guid>https://arxiv.org/abs/2506.18764</guid>
<content:encoded><![CDATA[
<div> Keywords: changepoint detection, neural networks, public discourse, news data, societal dynamics

Summary: 
This paper presents a novel method for changepoint detection in public discourse using neural networks. The approach, based on the learning-by-confusion scheme, involves training classifiers to distinguish between articles from different time periods, with classification accuracy indicating shifts in content distribution. The method successfully identifies significant events such as 9/11, the COVID-19 pandemic, and presidential elections in real-world data from The Guardian newspaper. It requires minimal domain knowledge, autonomously detects changes in public discourse, and provides a quantitative measure of content change. This approach has implications for journalism, policy analysis, and crisis monitoring. <div>
arXiv:2506.18764v1 Announce Type: cross 
Abstract: Detecting when public discourse shifts in response to major events is crucial for understanding societal dynamics. Real-world data is high-dimensional, sparse, and noisy, making changepoint detection in this domain a challenging endeavor. In this paper, we leverage neural networks for changepoint detection in news data, introducing a method based on the so-called learning-by-confusion scheme, which was originally developed for detecting phase transitions in physical systems. We train classifiers to distinguish between articles from different time periods. The resulting classification accuracy is used to estimate the total variation distance between underlying content distributions, where significant distances highlight changepoints. We demonstrate the effectiveness of this method on both synthetic datasets and real-world data from The Guardian newspaper, successfully identifying major historical events including 9/11, the COVID-19 pandemic, and presidential elections. Our approach requires minimal domain knowledge, can autonomously discover significant shifts in public discourse, and yields a quantitative measure of change in content, making it valuable for journalism, policy analysis, and crisis monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rumor Detection on Social Media with Reinforcement Learning-based Key Propagation Graph Generator</title>
<link>https://arxiv.org/abs/2405.13094</link>
<guid>https://arxiv.org/abs/2405.13094</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, rumor detection, social media, propagation graphs, public health<br />
Summary:<br />
The spread of rumors on social media during significant events like the US elections and COVID-19 poses a threat to social stability and public health. Current rumor detection methods use propagation graphs but struggle with noisy or irrelevant structures. The Key Propagation Graph Generator (KPG) introduces a reinforcement learning-based framework that generates informative propagation patterns for events with limited topology information and identifies significant substructures in events with noisy propagation. KPG includes the Candidate Response Generator (CRG) and the Ending Node Selector (ENS) to refine propagation patterns and identify influential substructures. This end-to-end framework utilizes rewards from a pre-trained graph neural network to guide the training process. Extensive experiments show that KPG outperforms current methods in rumor detection tasks. <br /><br /> <div>
arXiv:2405.13094v2 Announce Type: replace 
Abstract: The spread of rumors on social media, particularly during significant events like the US elections and the COVID-19 pandemic, poses a serious threat to social stability and public health. Current rumor detection methods primarily rely on propagation graphs to improve the model performance. However, the effectiveness of these methods is often compromised by noisy and irrelevant structures in the propagation process. To tackle this issue, techniques such as weight adjustment and data augmentation have been proposed. However, they depend heavily on rich original propagation structures, limiting their effectiveness in handling rumors that lack sufficient propagation information, especially in the early stages of dissemination. In this work, we introduce the Key Propagation Graph Generator (KPG), a novel reinforcement learning-based framework, that generates contextually coherent and informative propagation patterns for events with insufficient topology information and identifies significant substructures in events with redundant and noisy propagation structures. KPG comprises two key components: the Candidate Response Generator (CRG) and the Ending Node Selector (ENS). CRG learns latent variable distributions from refined propagation patterns to eliminate noise and generate new candidates for ENS, while ENS identifies the most influential substructures in propagation graphs and provides training data for CRG. Furthermore, we develop an end-to-end framework that utilizes rewards derived from a pre-trained graph neural network to guide the training process. The resulting key propagation graphs are then employed in downstream rumor detection tasks. Extensive experiments conducted on four datasets demonstrate that KPG outperforms current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Innovative Distinctiveness of Prizewinners and their Networks</title>
<link>https://arxiv.org/abs/2411.12180</link>
<guid>https://arxiv.org/abs/2411.12180</guid>
<content:encoded><![CDATA[
<div> Prizes, innovation, impact, productivity, collaboration <br />
Summary: <br />
- Prizewinners are more innovative in their research, combining existing ideas in new ways and integrating historical and contemporary thinking, as well as incorporating interdisciplinary perspectives.
- Prizewinners show increased innovativeness in their work compared to non-prizewinners, with a growing gap leading up to and following the prize year.
- Network embeddedness predicts unusual innovativeness, with prizewinners' collaborations being shorter, involving wider exposure to unfamiliar topics, and minimal overlap in coauthors' networks.
- Despite having equivalent impact and productivity records initially, prizewinners show a sustained increase in innovativeness post-prize year.
- The implications of these findings are significant for understanding the effectiveness of reward systems in promoting innovation in science. <br /> <div>
arXiv:2411.12180v2 Announce Type: replace-cross 
Abstract: Science prizes purportedly reward innovation and explorations of new phenomena. Yet, in practice prizes may inadvertently divert resources from similarly impactful but less celebrated scholars. Despite this paradox, knowledge of how prizewinning relates to innovation is nascent even as prizes proliferate widely. Analyzing 2,460 worldwide prizes, we compared the innovativeness of over 23,000 prizewinners and matched non-prizewinners whose performance records were statistically equivalent up to the prize year. First, we find that prizewinners are more innovative. Their research is more likely to combine existing ideas in new ways, integrate a topic's historical and contemporary thinking, and incorporate interdisciplinary perspectives. Second, although prizewinners and matched non-prizewinners have statistically equivalent impact and productivity records up to the prize year, at about five years before the prize, prizewinners' papers become more innovative than their matched peers, a difference that widens each year, peaks during the prize year, and then persists for the remainder of their careers. Third, network embeddedness predicts unusual innovativeness. Compared to non-prizewinners, prizewinners' collaborations are shorter in duration, encompass wider exposure to unfamiliar topics, and involve coauthors whose networks minimally overlap with each other. The implications of the findings for the efficacy of reward systems and innovation in science are discussed.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drivers of cooperation in social dilemmas on higher-order networks</title>
<link>https://arxiv.org/abs/2502.09446</link>
<guid>https://arxiv.org/abs/2502.09446</guid>
<content:encoded><![CDATA[
<div> Keywords: cooperation, social dilemmas, network framework, multi-player games, structured populations

Summary:
In the study, a new higher-order network framework for multi-player games on structured populations is introduced. The model considers multi-dimensional strategies to better capture the complexity of real-world interactions in social dilemmas. The research explores the dynamical and structural coupling between different orders of interactions and highlights the importance of nested multilevel interactions in promoting cooperative behavior. By incorporating features that go beyond traditional uni-dimensional strategies, the model shows how cooperation can be enhanced in group social dilemmas. This work sheds light on the key drivers of cooperative behavior observed in real-world scenarios, providing valuable insights for understanding and promoting cooperation in social interactions. 

<br /><br />Summary: <div>
arXiv:2502.09446v2 Announce Type: replace-cross 
Abstract: Understanding cooperation in social dilemmas requires models that capture the complexity of real-world interactions. While network frameworks have provided valuable insights to model the evolution of cooperation, they are unable to encode group interactions properly. Here, we introduce a general higher-order network framework for multi-player games on structured populations. Our model considers multi-dimensional strategies, based on the observation that social behaviours are affected by the size of the group interaction. We investigate dynamical and structural coupling between different orders of interactions, revealing the crucial role of nested multilevel interactions, and showing how such features can enhance cooperation beyond the limit of traditional models with uni-dimensional strategies. Our work identifies the key drivers promoting cooperative behaviour commonly observed in real-world group social dilemmas.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Online Polarization Through Human-Agent Interaction in a Synthetic LLM-Based Social Network</title>
<link>https://arxiv.org/abs/2506.15866</link>
<guid>https://arxiv.org/abs/2506.15866</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, polarization dynamics, artificial agents, user study, online environments<br />
Summary: 
This study introduces an experimental framework using artificial agents to investigate polarization dynamics in social media. Through a user study involving 122 participants, the research successfully replicated key characteristics of polarized online discourse and demonstrated the manipulation of environmental factors. The results validate theoretical predictions by showing that polarized environments heighten emotionality and group identity salience while decreasing uncertainty. This causal evidence enhances our understanding of how online environments influence user perceptions and behaviors. The novel methodology offers researchers precise control over experimental conditions while capturing real-world dynamics, providing valuable insights into social media dynamics. <br /><br />Summary: <div>
arXiv:2506.15866v1 Announce Type: new 
Abstract: The rise of social media has fundamentally transformed how people engage in public discourse and form opinions. While these platforms offer unprecedented opportunities for democratic engagement, they have been implicated in increasing social polarization and the formation of ideological echo chambers. Previous research has primarily relied on observational studies of social media data or theoretical modeling approaches, leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments. Here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with LLM-based artificial agents in a controlled social network simulation. Through a user study with 122 participants, we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors. Our results provide empirical validation of theoretical predictions about online polarization, showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty. These findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors. More broadly, this research introduces a powerful new methodology for studying social media dynamics, offering researchers unprecedented control over experimental conditions while maintaining ecological validity.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascade-driven opinion dynamics on social networks</title>
<link>https://arxiv.org/abs/2506.16302</link>
<guid>https://arxiv.org/abs/2506.16302</guid>
<content:encoded><![CDATA[
<div> Keywords: Online social networks, information cascades, opinion dynamics, Friedkin-Johnsen model, public discourse

Summary: 
The paper introduces the Friedkin-Johnsen on Cascade (FJC) model, which integrates information cascades and opinion dynamics in online social networks. By studying real social cascades, the model shows how the convergence of socialization and news sharing can disrupt traditional opinion evolution dynamics. The research reveals that these cascades can enhance the influence of central opinion leaders, making them less receptive to opposing viewpoints even when faced with a critical mass of dissent. Understanding the interplay between social dynamics and information flow is crucial in shaping public discourse in the digital age. <br /><br />Summary: <div>
arXiv:2506.16302v1 Announce Type: new 
Abstract: Online social networks (OSNs) have transformed the way individuals fulfill their social needs and consume information. As OSNs become increasingly prominent sources for news dissemination, individuals often encounter content that influences their opinions through both direct interactions and broader network dynamics. In this paper, we propose the Friedkin-Johnsen on Cascade (FJC) model, which is, to the best of our knowledge, is the first attempt to integrate information cascades and opinion dynamics, specifically using the very popular Friedkin-Johnsen model. Our model, validated over real social cascades, highlights how the convergence of socialization and sharing news on these platforms can disrupt opinion evolution dynamics typically observed in offline settings. Our findings demonstrate that these cascades can amplify the influence of central opinion leaders, making them more resistant to divergent viewpoints, even when challenged by a critical mass of dissenting opinions. This research underscores the importance of understanding the interplay between social dynamics and information flow in shaping public discourse in the digital age.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse</title>
<link>https://arxiv.org/abs/2506.16412</link>
<guid>https://arxiv.org/abs/2506.16412</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, education, social media, sentiment analysis, stakeholder discourse

Summary: 
Generative AI technologies are increasingly influencing education, with stakeholders like teachers and students actively engaging in discussions around its impact. A comprehensive analysis of social media data on GAI in education revealed various sentiments and viewpoints. The study utilized a modular framework leveraging large language models for sentiment analysis, topic modeling, and author classification, outperforming traditional NLP models. The analysis identified 12 latent topics in public discourse, showing optimism from teachers and students for GAI's personalized learning benefits but also highlighting concerns such as privacy, academic integrity, and job security. Students expressed distress over AI detectors falsely accusing them of cheating, while teachers were wary of institutional pressures to adopt GAI tools. The findings emphasize the need for clear policies, transparent integration practices, and support mechanisms to balance innovation with oversight in GAI-enabled educational environments. The study also showcased the potential of LLM-based frameworks for modeling stakeholder discourse in online communities. 


<br /><br />Summary: <div>
arXiv:2506.16412v1 Announce Type: new 
Abstract: Generative AI (GAI) technologies are quickly reshaping the educational landscape. As adoption accelerates, understanding how students and educators perceive these tools is essential. This study presents one of the most comprehensive analyses to date of stakeholder discourse dynamics on GAI in education using social media data. Our dataset includes 1,199 Reddit posts and 13,959 corresponding top-level comments. We apply sentiment analysis, topic modeling, and author classification. To support this, we propose and validate a modular framework that leverages prompt-based large language models (LLMs) for analysis of online social discourse, and we evaluate this framework against classical natural language processing (NLP) models. Our GPT-4o pipeline consistently outperforms prior approaches across all tasks. For example, it achieved 90.6% accuracy in sentiment analysis against gold-standard human annotations. Topic extraction uncovered 12 latent topics in the public discourse with varying sentiment and author distributions. Teachers and students convey optimism about GAI's potential for personalized learning and productivity in higher education. However, key differences emerged: students often voice distress over false accusations of cheating by AI detectors, while teachers generally express concern about job security, academic integrity, and institutional pressures to adopt GAI tools. These contrasting perspectives highlight the tension between innovation and oversight in GAI-enabled learning environments. Our findings suggest a need for clearer institutional policies, more transparent GAI integration practices, and support mechanisms for both educators and students. More broadly, this study demonstrates the potential of LLM-based frameworks for modeling stakeholder discourse within online communities.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Political Influence Through Social Media: Network and Causal Dynamics in the 2022 French Presidential Election</title>
<link>https://arxiv.org/abs/2506.16449</link>
<guid>https://arxiv.org/abs/2506.16449</guid>
<content:encoded><![CDATA[
<div> Keywords: French presidential election, Twitter messages, political discourse dynamics, causal inference, social media network 

Summary: 
During the 2022 French presidential election, Twitter messages on key topics were collected from political candidates and their networks, with a focus on analyzing interactions and central topics shaping political debate. Using causal inference techniques like Convergent Cross Mapping, directional influences among political parties were uncovered, distinguishing true influence from correlation and revealing asymmetric relationships. Specific issues like health and foreign policy were found to act as catalysts for cross-party influence, particularly in critical election phases. These findings provide a unique framework for understanding political discourse dynamics and offer practical implications for campaign strategists and media analysts to monitor and respond to shifts in political influence in real time. 

Summary: <div>
arXiv:2506.16449v1 Announce Type: new 
Abstract: During the 2022 French presidential election, we collected daily Twitter messages on key topics posted by political candidates and their close networks. Using a data-driven approach, we analyze interactions among political parties, identifying central topics that shape the landscape of political debate. Moving beyond traditional correlation analyses, we apply a causal inference technique: Convergent Cross Mapping, to uncover directional influences among political communities, revealing how some parties are more likely to initiate changes in activity while others tend to respond. This approach allows us to distinguish true influence from mere correlation, highlighting asymmetric relationships and hidden dynamics within the social media political network. Our findings demonstrate how specific issues, such as health and foreign policy, act as catalysts for cross-party influence, particularly during critical election phases. These insights provide a novel framework for understanding political discourse dynamics and have practical implications for campaign strategists and media analysts seeking to monitor and respond to shifts in political influence in real time.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data marketplaces can increase the willingness to share social media data at low prices</title>
<link>https://arxiv.org/abs/2506.16618</link>
<guid>https://arxiv.org/abs/2506.16618</guid>
<content:encoded><![CDATA[
<div> keywords: social media data, data donation, data marketplace, privacy protections, data buyer<br />
Summary:<br />
Living in the post API age, researchers face challenges in obtaining social media data. Data donation as an alternative is limited by low participation rates. This paper examines the impact of data marketplaces on individuals' willingness to sell their social media data. Results show a significant increase in willingness to sell within a marketplace compared to data donation or one-time purchase offers. While minimum acceptable prices did not vary significantly, most participants set prices within the marketplace's suggested range. The type of buyer and privacy safeguards did not significantly influence participants' decisions within the marketplace setting. This research suggests that data marketplaces could be a viable solution for obtaining social media data ethically and efficiently. <br />Summary: <div>
arXiv:2506.16618v1 Announce Type: new 
Abstract: Living in the Post API age, researchers face unprecedented challenges in obtaining social media data, while users are concerned about how big tech companies use their data. Data donation offers a promising alternative, however, its scalability is limited by low participation and high dropout rates. Research suggests that data marketplaces could be a solution, but its realization remains challenging due to theoretical gaps in treating data as an asset. This paper examines whether data marketplaces can increase individuals willingness to sell their X (Twitter) data package and the minimum price they would accept. It also explores how privacy protections and the type of data buyer may affect these decisions. Results from two preregistered online survey experiments show that a data marketplace increases participants' willingness to sell their X data by 12 to 25 percentage points compared to data donation (depending on treatments), and by 6.8 points compared to onetime purchase offers. Although difference in minimum acceptable prices are not statistically significant, over 64 percentage of participants set their price within the marketplace's suggested range (0.25 to 2), substantially lower than the amounts offered in prior onetime purchase studies. Finally, in the marketplace setting, neither the type of buyer nor the inclusion of a privacy safeguard significantly influenced participants willingness to sell.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
<link>https://arxiv.org/abs/2506.15690</link>
<guid>https://arxiv.org/abs/2506.15690</guid>
<content:encoded><![CDATA[
<div> framework, model collapse, language model, synthetic data, convergence pattern
Summary:
The article introduces the concept of LLM Web Dynamics (LWD) for analyzing model collapse in large language models trained with synthetic data from the Internet. By using a retrieval-augmented generation (RAG) database to simulate the Internet, the framework investigates the convergence pattern of model outputs at a network level. The study aims to address the potential threat of model collapse, which has been understudied in existing research that primarily focuses on individual models or statistical surrogates. The theoretical guarantees for convergence are provided, drawing parallels to interacting Gaussian Mixture Models. The approach enhances understanding of how large language models behave when trained on synthetic data from the public Internet, shedding light on important considerations for maintaining model performance and preventing collapse.<br /><br />Summary: <div>
arXiv:2506.15690v1 Announce Type: cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer entropy for finite data</title>
<link>https://arxiv.org/abs/2506.16215</link>
<guid>https://arxiv.org/abs/2506.16215</guid>
<content:encoded><![CDATA[
<div> Transfer entropy, directed information flow, complex systems, statistical significance, finite data<br />
Summary:<br />
Transfer entropy is a widely used measure for quantifying directed information flows in complex systems. However, it has shortcomings for data of finite cardinality, including a positive bias for sparse bin counts and a lack of clear means to assess statistical significance. By accounting for information content in finite data streams, a new transfer entropy measure is derived that addresses these issues. This measure is asymptotically equivalent to the standard estimator but provides improved results for small size and high cardinality time series. It allows for a fully nonparametric assessment of statistical significance without the need for simulation. The correction for finite data has a significant impact on results in both real and synthetic time series datasets.<br /> 
Summary: <div>
arXiv:2506.16215v1 Announce Type: cross 
Abstract: Transfer entropy is a widely used measure for quantifying directed information flows in complex systems. While the challenges of estimating transfer entropy for continuous data are well known, it has two major shortcomings that persist even for data of finite cardinality: it exhibits a substantial positive bias for sparse bin counts, and it has no clear means to assess statistical significance. By more precisely accounting for information content in finite data streams, we derive a transfer entropy measure which is asymptotically equivalent to the standard plug-in estimator but remedies these issues for time series of small size and/or high cardinality, permitting a fully nonparametric assessment of statistical significance without simulation. We show that this correction for finite data has a substantial impact on results in both real and synthetic time series datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding</title>
<link>https://arxiv.org/abs/2506.16754</link>
<guid>https://arxiv.org/abs/2506.16754</guid>
<content:encoded><![CDATA[
<div> Metapath-based Hyperbolic Contrastive Learning framework, heterogeneous graphs, hyperbolic space, complex structures, contrastive learning<br />
<br />
Summary: <br />
The article introduces the Metapath-based Hyperbolic Contrastive Learning framework (MHCL) for embedding heterogeneous graphs with diverse power-law structures. Unlike existing models that use a single hyperbolic space, MHCL utilizes multiple hyperbolic spaces to capture the complex structures corresponding to different metapaths within the graph. By learning distinct hyperbolic spaces for each metapath, MHCL effectively captures semantic information and preserves the discriminability of metapath embeddings. The framework employs a contrastive learning approach to optimize the embeddings, minimizing distances between embeddings of the same metapath and maximizing distances between different metapaths. Experimental results reveal that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, showcasing its ability to effectively capture the complex structures of heterogeneous graphs. <div>
arXiv:2506.16754v1 Announce Type: cross 
Abstract: The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elephant in the Room: Dissecting and Reflecting on the Evolution of Online Social Network Research</title>
<link>https://arxiv.org/abs/2411.13681</link>
<guid>https://arxiv.org/abs/2411.13681</guid>
<content:encoded><![CDATA[
<div> data access policies, social networks, research, metadata, survey
Summary: 
The paper highlights the challenges faced by external scientists in conducting research on Online Social Networks (OSN) due to restrictive data access policies, leading to reliance on static datasets. It presents the Minerva-OSN dataset, comprising metadata from over 13,000 papers on OSN research, revealing trends like the focus on platforms like Twitter and difficulties in obtaining OSN data. The expert survey conducted with 50 established scientists in the field suggests improvements, such as increased involvement of OSN owners. The paper calls for reflection on the collective body of research on OSN since 2006 and emphasizes the need to enhance research efforts to benefit OSN owners, end-users, and society as a whole.<br /><br />Summary: <div>
arXiv:2411.13681v2 Announce Type: replace 
Abstract: Billions of individuals engage with Online Social Networks (OSN) daily. The owners of OSN try to meet the demands of their end-users while complying with business necessities. Such necessities may, however, lead to the adoption of restrictive data access policies that hinder research activities from "external" scientists -- who may, in turn, resort to other means (e.g., rely on static datasets) for their studies. Given the abundance of literature on OSN, we -- as academics -- should take a step back and reflect on what we have done so far, after having written thousands of papers on OSN. This is the first paper that provides a holistic outlook to the entire body of research that focused on OSN -- since the seminal work by Acquisti and Gross (2006). First, we search through over 1 million peer-reviewed publications, and derive 13,842 papers that focus on OSN: we organize the metadata of these works in the Minerva-OSN dataset, the first of its kind -- which we publicly release. Next, by analyzing Minerva-OSN, we provide factual evidence elucidating trends and aspects that deserve to be brought to light, such as the predominant focus on Twitter or the difficulty in obtaining OSN data. Finally, as a constructive step to guide future research, we carry out an expert survey (n=50) with established scientists in this field, and coalesce suggestions to improve the status quo such as an increased involvement of OSN owners. Our findings should inspire a reflection to "rescue" research on OSN. Doing so would improve the overall OSN ecosystem, benefiting both their owners and end-users and, hence, our society.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set Graph Anomaly Detection via Normal Structure Regularisation</title>
<link>https://arxiv.org/abs/2311.06835</link>
<guid>https://arxiv.org/abs/2311.06835</guid>
<content:encoded><![CDATA[
<div> novel open-set GAD, normal structure regularisation, unseen anomalies, graph anomaly detection, supervised anomaly detection<br />
<br />
Summary:<br />
This paper introduces a novel approach for open-set Graph Anomaly Detection (GAD) called normal structure regularisation (NSReg). NSReg aims to improve the detection ability of unseen anomalies while effectively detecting seen anomalies. By incorporating a regularisation term that focuses on learning compact and semantically-rich representations of normal nodes based on their structural relations, NSReg prevents overfitting to seen anomalies and improves the normality decision boundary. Experimental results on seven datasets show that NSReg outperforms existing methods by at least 14% in AUC-ROC for unseen anomaly classes and 10% for all anomaly classes. The approach effectively captures discriminative features from graph structure and node attributes for GAD, providing a strong foundation for future research in anomaly detection. <br /><br /> <div>
arXiv:2311.06835v5 Announce Type: replace-cross 
Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Interest-aware Graph Learning for Group Identification</title>
<link>https://arxiv.org/abs/2506.14826</link>
<guid>https://arxiv.org/abs/2506.14826</guid>
<content:encoded><![CDATA[
<div> evolution relationship, group identification, collaborative, interests, social media
<br />
Summary: 
The paper introduces CI4GI, a Collaborative Interest-aware model for Group Identification, to address the issue of recommending groups to users on social media platforms. It highlights the collaborative evolution relationship between dual-level user interests, encompassing group-level and item-level interests. The model incorporates an interest enhancement strategy to capture additional user interests from groups they have joined, enhancing the alignment between the two interest levels. Furthermore, it utilizes the distance between interest distributions of users to optimize the identification of negative samples, reducing false-negative interference during cross-level interests alignment. Experimental results on real-world datasets demonstrate the superior performance of CI4GI compared to existing models. <div>
arXiv:2506.14826v1 Announce Type: new 
Abstract: With the popularity of social media, an increasing number of users are joining group activities on online social platforms. This elicits the requirement of group identification (GI), which is to recommend groups to users. We reveal that users are influenced by both group-level and item-level interests, and these dual-level interests have a collaborative evolution relationship: joining a group expands the user's item interests, further prompting the user to join new groups. Ultimately, the two interests tend to align dynamically. However, existing GI methods fail to fully model this collaborative evolution relationship, ignoring the enhancement of group-level interests on item-level interests, and suffering from false-negative samples when aligning cross-level interests. In order to fully model the collaborative evolution relationship between dual-level user interests, we propose CI4GI, a Collaborative Interest-aware model for Group Identification. Specifically, we design an interest enhancement strategy that identifies additional interests of users from the items interacted with by the groups they have joined as a supplement to item-level interests. In addition, we adopt the distance between interest distributions of two users to optimize the identification of negative samples for a user, mitigating the interference of false-negative samples during cross-level interests alignment. The results of experiments on three real-world datasets demonstrate that CI4GI significantly outperforms state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse</title>
<link>https://arxiv.org/abs/2506.14836</link>
<guid>https://arxiv.org/abs/2506.14836</guid>
<content:encoded><![CDATA[
<div> persistent homology, structural change, media narratives, semantic disruption, narrative volatility

Summary:
Persistent homology is introduced as a topological framework for identifying structural changes in media narratives during major global events. The study analyzes international news articles from events such as the Russian invasion of Ukraine, the murder of George Floyd, the U.S. Capitol insurrection, and the Hamas-led invasion of Israel. By constructing daily co-occurrence graphs of noun phrases and utilizing a Vietoris-Rips filtration to transform them into persistence diagrams, the study calculates Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. The results show sharp spikes in both connected components and loops during major events, indicating sudden reorganization in narrative structure. Cross-correlation analyses reveal a lag pattern where changes in component-level structure precede higher-order motif shifts, suggesting a bottom-up cascade of semantic change. An exception is observed during the Russian invasion of Ukraine, where higher-order entropy leads connected components, possibly reflecting top-down narrative framing before local discourse adjusts. The study demonstrates that persistent homology provides an unsupervised method for detecting inflection points and directional shifts in public attention during crises, protests, and information shocks. <br /><br />Summary: <div>
arXiv:2506.14836v1 Announce Type: new 
Abstract: How can we detect when global events fundamentally reshape public discourse? This study introduces a topological framework for identifying structural change in media narratives using persistent homology. Drawing on international news articles surrounding major events - including the Russian invasion of Ukraine (Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol insurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we construct daily co-occurrence graphs of noun phrases to trace evolving discourse. Each graph is embedded and transformed into a persistence diagram via a Vietoris-Rips filtration. We then compute Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. Our results show that major geopolitical and social events align with sharp spikes in both H0 (connected components) and H1 (loops), indicating sudden reorganization in narrative structure and coherence. Cross-correlation analyses reveal a typical lag pattern in which changes to component-level structure (H0) precede higher-order motif shifts (H1), suggesting a bottom-up cascade of semantic change. An exception occurs during the Russian invasion of Ukraine, where H1 entropy leads H0, possibly reflecting top-down narrative framing before local discourse adjusts. Persistence entropy further distinguishes tightly focused from diffuse narrative regimes. These findings demonstrate that persistent homology offers a mathematically principled, unsupervised method for detecting inflection points and directional shifts in public attention - without requiring prior knowledge of specific events. This topological approach advances computational social science by enabling real-time detection of semantic restructuring during crises, protests, and information shocks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic resolution of crowd-sourced moderation on X in polarized settings across countries</title>
<link>https://arxiv.org/abs/2506.15168</link>
<guid>https://arxiv.org/abs/2506.15168</guid>
<content:encoded><![CDATA[
<div> community notes, crowd-sourced moderation, ideological dimension, polarization contexts, social platforms 

Summary: 
The study examines the effectiveness of X's Community Notes system, which utilizes crowd-sourced moderation to combat misleading content on social platforms. By analyzing a vast amount of moderation data and cross-referencing ideological scaling data from 13 countries, the researchers found that the system accurately captures each country's main polarizing dimension. However, it falls short in moderating the most polarizing content, which poses risks to civic discourse and electoral processes. The transition from expert fact-checking to crowd-sourced moderation on social platforms is a notable trend, and this study sheds light on the challenges and potential implications of such a shift. <div>
arXiv:2506.15168v1 Announce Type: new 
Abstract: Social platforms increasingly transition from expert fact-checking to crowd-sourced moderation, with X pioneering this shift through its Community Notes system, enabling users to collaboratively moderate misleading content. To resolve conflicting moderation, Community Notes learns a latent ideological dimension and selects notes garnering cross-partisan support. As this system, designed for and evaluated in the United States, is now deployed worldwide, we evaluate its operation across diverse polarization contexts. We analyze 1.9 million moderation notes with 135 million ratings from 1.2 million users, cross-referencing ideological scaling data across 13 countries. Our results show X's Community Notes effectively captures each country's main polarizing dimension but fails by design to moderate the most polarizing content, posing potential risks to civic discourse and electoral processes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control</title>
<link>https://arxiv.org/abs/2506.15397</link>
<guid>https://arxiv.org/abs/2506.15397</guid>
<content:encoded><![CDATA[
<div> learning algorithm, vaccination strategies, sample complexity, optimal algorithm, real-world data

Summary:
The article addresses the problem of minimizing the extinction time of a highly contagious disease modeled by the Susceptible-Infected-Susceptible (SIS) model on an unknown graph. It proposes a novel inclusion-exclusion-based learning algorithm with established sample complexity for graph recovery. The article also details an optimal algorithm for the Spectral Radius Minimization (SRM) problem, proving its polynomial running time for graphs with bounded treewidth. Additionally, an efficient polynomial-time greedy heuristic for any graph is presented. Experimental validation on synthetic and real-world data confirms the efficacy of the learning and vaccination algorithms. <div>
arXiv:2506.15397v1 Announce Type: cross 
Abstract: The Susceptible-Infected-Susceptible (SIS) model is a widely used model for the spread of information and infectious diseases, particularly non-immunizing ones, on a graph. Given a highly contagious disease, a natural question is how to best vaccinate individuals to minimize the disease's extinction time. While previous works showed that the problem of optimal vaccination is closely linked to the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that the graph is known, which is often not the case in practice. In this work, we consider the problem of minimizing the extinction time of an outbreak modeled by an SIS model where the graph on which the disease spreads is unknown and only the infection states of the vertices are observed. To this end, we split the problem into two: learning the graph and determining effective vaccination strategies. We propose a novel inclusion-exclusion-based learning algorithm and, unlike previous approaches, establish its sample complexity for graph recovery. We then detail an optimal algorithm for the SRM problem and prove that its running time is polynomial in the number of vertices for graphs with bounded treewidth. This is complemented by an efficient and effective polynomial-time greedy heuristic for any graph. Finally, we present experiments on synthetic and real-world data that numerically validate our learning and vaccination algorithms.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic Graphs</title>
<link>https://arxiv.org/abs/2506.15464</link>
<guid>https://arxiv.org/abs/2506.15464</guid>
<content:encoded><![CDATA[
<div> Graph signal processing, hierarchical graphs, boundary-weighted operator, spectral norm, delta-hyperbolic networks <br />
<br />
Summary: 
Hierarchical graphs with tree-like branching patterns present challenges for traditional graph filters. A boundary-weighted operator is introduced to address this issue by rescaling edges based on their distance from the graph's Gromov boundary. Using Busemann functions on delta-hyperbolic networks, a closed-form upper bound on the operator's spectral norm is proven, showing that each signal loses a controlled fraction of energy at each pass. This results in a lightweight filter that is parameter-free and stable, with stability derived from geometric principles. The filter provides a new analytical tool for graph signal processing on data with dense or hidden hierarchical structures. <div>
arXiv:2506.15464v1 Announce Type: cross 
Abstract: Hierarchical graphs often exhibit tree-like branching patterns, a structural property that challenges the design of traditional graph filters. We introduce a boundary-weighted operator that rescales each edge according to how far its endpoints drift toward the graph's Gromov boundary. Using Busemann functions on delta-hyperbolic networks, we prove a closed-form upper bound on the operator's spectral norm: every signal loses a curvature-controlled fraction of its energy at each pass. The result delivers a parameter-free, lightweight filter whose stability follows directly from geometric first principles, offering a new analytic tool for graph signal processing on data with dense or hidden hierarchical structure.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HetGCoT: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning for Academic Question Answering</title>
<link>https://arxiv.org/abs/2501.01203</link>
<guid>https://arxiv.org/abs/2501.01203</guid>
<content:encoded><![CDATA[
<div> Keyword: academic question answering, heterogeneous scholarly networks, graph neural networks, large language models, reasoning

Summary:
HetGCoT is a framework designed for academic question answering in heterogeneous scholarly networks by integrating graph neural networks and large language models. It addresses the challenges of structured understanding and interpretable reasoning in QA tasks. The framework transforms graph structural information into reasoning chains for LLMs, employs an adaptive metapath selection mechanism, and incorporates graph contexts in a multi-step reasoning strategy. Experimental results on OpenAlex and DBLP datasets show superior performance compared to state-of-the-art baselines. HetGCoT is adaptable to different LLM architectures and supports various scholarly question answering tasks. This framework bridges the gap between structured graph information and semantic comprehension, providing a comprehensive solution for academic QA. <br /><br /> Summary: HetGCoT integrates graph neural networks and large language models for academic question answering in heterogeneous scholarly networks, achieving superior performance through reasoning chain transformation, metapath selection, and multi-step reasoning strategy. <div>
arXiv:2501.01203v2 Announce Type: replace 
Abstract: Academic question answering (QA) in heterogeneous scholarly networks presents unique challenges requiring both structural understanding and interpretable reasoning. While graph neural networks (GNNs) capture structured graph information and large language models (LLMs) demonstrate strong capabilities in semantic comprehension, current approaches lack integration at the reasoning level. We propose HetGCoT, a framework enabling LLMs to effectively leverage and learn information from graphs to reason interpretable academic QA results. Our framework introduces three technical contributions: (1) a framework that transforms heterogeneous graph structural information into LLM-processable reasoning chains, (2) an adaptive metapath selection mechanism identifying relevant subgraphs for specific queries, and (3) a multi-step reasoning strategy systematically incorporating graph contexts into the reasoning process. Experiments on OpenAlex and DBLP datasets show our approach outperforms all sota baselines. The framework demonstrates adaptability across different LLM architectures and applicability to various scholarly question answering tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voter model can accurately predict individual opinions in online populations</title>
<link>https://arxiv.org/abs/2501.13215</link>
<guid>https://arxiv.org/abs/2501.13215</guid>
<content:encoded><![CDATA[
<div> opinion dynamics, multi-state voter model, Twitter dataset, French Presidential elections, individual opinions <br />
Summary:<br />
The study evaluates the multi-state voter model with zealots by analyzing a fine-grained Twitter dataset from the 2017 French Presidential elections. It finds a strong correspondence between individual opinion distributions in the model's equilibrium state and the users' actual political leanings. Additionally, the model accurately identifies pairs of like-minded users through discord probabilities. These results highlight the model's validity in capturing individual opinions in complex settings and advocate for further empirical evaluations of opinion dynamics models at the user level. <br /> <div>
arXiv:2501.13215v2 Announce Type: replace 
Abstract: Models of opinion dynamics describe how opinions are shaped in various environments. While these models are able to replicate general opinion distributions observed in real-world scenarios, their capacity to align with data at the user level remains mostly untested. We evaluate the capacity of the multi-state voter model with zealots to capture individual opinions in a fine-grained Twitter dataset collected during the 2017 French Presidential elections. Our findings reveal a strong correspondence between individual opinion distributions in the equilibrium state of the model and ground-truth political leanings of the users. Additionally, we demonstrate that discord probabilities accurately identify pairs of like-minded users. These results emphasize the validity of the voter model in complex settings, and advocate for further empirical evaluations of opinion dynamics models at the user level.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation</title>
<link>https://arxiv.org/abs/2205.14651</link>
<guid>https://arxiv.org/abs/2205.14651</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph autoencoders, Variational graph autoencoders, Scalability, Directed graphs, Community detection<br />
<br />
Summary: In this thesis, several advancements are proposed to enhance the effectiveness of Graph Autoencoders (GAE) and Variational Graph Autoencoders (VGAE) for industrial applications. The scalability issues of previous models are addressed through strategies leveraging graph degeneracy and stochastic subgraph decoding, enabling training on large graphs. Gravity-Inspired GAE and VGAE extensions are introduced for directed graphs commonly found in industrial settings. Models for dynamic graphs and simplified versions using linear encoders are also presented. Modularity-Aware GAE and VGAE improve community detection while maintaining good performance in link prediction tasks. The methods are evaluated on music graphs from Deezer, demonstrating their effectiveness in music recommendation scenarios by detecting similar musical items, ranking artists in a cold start setting, and modeling music genre perception across cultures.<br /><br />Summary: <div>
arXiv:2205.14651v3 Announce Type: replace-cross 
Abstract: Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as two powerful groups of unsupervised node embedding methods, with various applications to graph-based machine learning problems such as link prediction and community detection. Nonetheless, at the beginning of this Ph.D. project, GAE and VGAE models were also suffering from key limitations, preventing them from being adopted in the industry. In this thesis, we present several contributions to improve these models, with the general aim of facilitating their use to address industrial-level problems involving graph representations. Firstly, we propose two strategies to overcome the scalability issues of previous GAE and VGAE models, permitting to effectively train these models on large graphs with millions of nodes and edges. These strategies leverage graph degeneracy and stochastic subgraph decoding techniques, respectively. Besides, we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of these models for directed graphs, that are ubiquitous in industrial applications. We also consider extensions of GAE and VGAE models for dynamic graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily complex, and we propose to simplify them by leveraging linear encoders. Lastly, we introduce Modularity-Aware GAE and VGAE to improve community detection on graphs, while jointly preserving good performances on link prediction. In the last part of this thesis, we evaluate our methods on several graphs extracted from the music streaming service Deezer. We put the emphasis on graph-based music recommendation problems. In particular, we show that our methods can improve the detection of communities of similar musical items to recommend to users, that they can effectively rank similar artists in a cold start setting, and that they permit modeling the music genre perception across cultures.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density-aware Walks for Coordinated Campaign Detection</title>
<link>https://arxiv.org/abs/2506.13912</link>
<guid>https://arxiv.org/abs/2506.13912</guid>
<content:encoded><![CDATA[
<div> Keywords: Coordinated campaigns, Social media platforms, Graph classification, Large Engagement Networks, Random weighted walk

Summary: 
- The study focuses on detecting coordinated campaigns on social media platforms, particularly on Twitter, by modeling the problem as a graph classification task.
- The researchers use the Large Engagement Networks (LEN) dataset, which contains data on engagement patterns from fake and authentic trends on Twitter before the 2023 Turkish elections.
- Existing graph neural networks struggle with accurately classifying campaign graphs due to the large network sizes in the LEN dataset.
- The study introduces a new graph classification method that leverages the density of local network structures through a random weighted walk (RWW) approach.
- By training message-passing neural networks (MPNNs) on density-aware structural embeddings generated by the RWW approach, the researchers achieve significant improvements in classification accuracy for identifying coordinated inauthentic behavior on social media networks like Twitter.

<br /><br />Summary: <div>
arXiv:2506.13912v1 Announce Type: new 
Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering</title>
<link>https://arxiv.org/abs/2506.13989</link>
<guid>https://arxiv.org/abs/2506.13989</guid>
<content:encoded><![CDATA[
<div> Keywords: money laundering, synthetic datasets, AMLGentex, anti-money laundering systems, benchmarking

Summary: 
Money laundering is a significant issue that facilitates organized crime by allowing illicit funds to enter the legitimate economy. Despite trillions of dollars being laundered annually, only a small fraction is detected due to various factors, including deliberate evasion and limited visibility into the global transaction network. Existing synthetic datasets lack the complexity of real-world money laundering, such as partial observability, sparse labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, the AMLGentex framework has been developed as an open-source suite for generating realistic transaction data and benchmarking detection methods. This framework allows for the systematic evaluation of anti-money laundering systems under conditions that mirror the complexity of practical AML scenarios. <div>
arXiv:2506.13989v1 Announce Type: new 
Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Precinct and District Voting Data Using Persistent Homology to Identify Gerrymandering in North Carolina</title>
<link>https://arxiv.org/abs/2506.13997</link>
<guid>https://arxiv.org/abs/2506.13997</guid>
<content:encoded><![CDATA[
<div> Keywords: level-set method, geospatial election data, gerrymandering, persistent homologies, topological data analysis 

Summary: 
This paper extends a previous study on using the level-set method to analyze geospatial election data for gerrymandering. By comparing precinct and district level voting data in North Carolina House of Representatives elections, the study identifies discrepancies that indicate gerrymandering. The analysis reveals how redistricting affects voting patterns and provides evidence of gerrymandering practices. The research utilized composite shapefiles created with QGIS and R, rasterized using Python, and employed the level-set method to generate filtered simplicial complexes. Persistence barcodes were analyzed using GUDHI and PHAT libraries to compare precinct and district voting patterns. The study also compared results with traditional gerrymandering identification measures like Polsby-Popper and Reock scores. This research showcases a novel application of topological data analysis in evaluating gerrymandering practices. 

<br /><br />Summary: <div>
arXiv:2506.13997v1 Announce Type: new 
Abstract: We present an extension of Feng and Porter's 2019 paper on the use of the level-set method for the construction of a filtered simplicial complex from geospatial election data. Precincts are regarded to be too small to be gerrymandered, allowing us to identify discrepancies between precinct and district level voting data to quantify gerrymandering in the United States. Comparing the persistent homologies of Democratic voting areas on the precinct and district level shows when areas have been 'cracked' or 'packed' for partisan gain. This analysis was done for North Carolina House of Representatives elections (2012 to 2024). North Carolina has been redistricted 4 times in the past 10 years, whereas most states redistrict decennially, allowing us to understand how and when redistricted maps deviate from precinct-level voting data, and when gerrymandering occurs. Comparing persistence barcodes at the precinct and district levels (using the bottleneck distance) shows that precinct-level voting patterns do not significantly fluctuate biannually, while district level patterns do, suggesting that shifts are likely a result of redistricting rather than voter behavior, providing strong evidence of gerrymandering. North Carolina election data was collected from the public domain. Composite shapefiles were created using QGIS and R, and rasterized using Python. The level-set method was employed to generate filtered simplicial complexes. Persistence barcodes were produced using GUDHI and PHAT libraries. Additionally, we compare our results with traditional measures such as Polsby-Popper and Reock scores (gerrymandering identification measures). This research presents a novel application of topological data analysis in evaluating gerrymandering.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection</title>
<link>https://arxiv.org/abs/2506.13795</link>
<guid>https://arxiv.org/abs/2506.13795</guid>
<content:encoded><![CDATA[
<div> transfer learning, social bots, graph neural networks, domain adaptation, multi-source

Summary:<br />
- The article addresses challenges in transferring knowledge from social networks to detect social bots using GNN-based models.
- It introduces a multi-source graph domain adaptation model called BotTrans to overcome the network heterophily problem.
- BotTrans leverages labeling knowledge from multiple source domains to establish a cross-domain topology with increased network homophily.
- It aggregates cross-domain neighbor information to enhance the discriminability of source node embeddings.
- The model optimizes the relevance between source-target pairs to facilitate knowledge transfer from more relevant source networks.
- A refinement strategy is proposed to improve detection performance by utilizing semantic knowledge within the target domain. 
- Extensive experiments on real-world datasets show that BotTrans outperforms existing methods, demonstrating its effectiveness in leveraging multi-source knowledge for unlabeled social bot detection. <div>
arXiv:2506.13795v1 Announce Type: cross 
Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Socioeconomic Status is Associated with Diverse Consumption across Brands and Price Levels</title>
<link>https://arxiv.org/abs/2506.13840</link>
<guid>https://arxiv.org/abs/2506.13840</guid>
<content:encoded><![CDATA[
<div> income, diverse consumption, niche consumption, socioeconomic status, mobile tracking<br />
Summary: <br />
The study examines the relationship between income and consumption diversity, focusing on mobile-tracked visits to stores in New York State. It finds that higher income levels are linked to more diverse consumption across brands and price points, supporting the diversity hypothesis. This association is less pronounced in densely populated and diverse areas like New York City. Education level also shows a similar pattern when used as a measure of socioeconomic status. The findings are replicated in Texas, suggesting a broader applicability. The study rules out simple geographic factors as explanations, pointing towards deeper social and cultural influences on consumption patterns. <div>
arXiv:2506.13840v1 Announce Type: cross 
Abstract: Consumption practices are determined by a combination of economic, social, and cultural forces. We posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestyle-based niche consumption. We provide empirical evidence for this diversity hypothesis by analysing millions of mobile-tracked visits from thousands of Census Block Groups to thousands of stores in New York State. The results show that high income is significantly associated with diverse consumption across brands and price levels. The associations between diversity and income persist but are less prominent for necessity-based consumption and for the densely populated and demographically diverse New York City. The associations replicate for education as an alternative measure of socioeconomic status and for the state of Texas. We further illustrate that the associations cannot be explained by simple geographic constraints, including the neighbourhoods' demographic diversity, the residents' geographic mobility and the stores' local availability, so deeper social and cultural factors must be at play.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic Demonstration of Miner-Centric Propagation Topologies</title>
<link>https://arxiv.org/abs/2506.14197</link>
<guid>https://arxiv.org/abs/2506.14197</guid>
<content:encoded><![CDATA[
<div> complex graph theory, Bitcoin CORE, Bitcoin Satoshi Vision, scale-free networks, small-world connectivity
<br />
The paper analyzes the network structure of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory. It finds that home-hosted full nodes are unable to participate in or influence the propagation topology. The propagation graph is primarily controlled by a closely connected miner clique, while full nodes are on the outskirts and cannot affect transaction-to-block inclusion paths. Through simulation-based metrics and eigenvalue centrality analysis, it is determined that full nodes are not crucial or operationally significant for consensus propagation. 
<br /><br />Summary: 
The study examines the network structures of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory. It reveals that full nodes hosted at home lack the capability to influence the propagation topology, with the network being dominated by a miner clique. Full nodes are situated on the periphery and are excluded from transaction-to-block inclusion paths. Simulation-based metrics and eigenvalue centrality analysis confirm that full nodes play a minimal role in consensus propagation. <div>
arXiv:2506.14197v1 Announce Type: cross 
Abstract: This paper formally examines the network structure of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that home-hosted full nodes are incapable of participating in or influencing the propagation topology. Leveraging established models such as scale-free networks and small-world connectivity, we demonstrate that the propagation graph is dominated by a densely interconnected miner clique, while full nodes reside on the periphery, excluded from all transaction-to-block inclusion paths. Using simulation-backed metrics and eigenvalue centrality analysis, we confirm that full nodes are neither critical nor operationally relevant for consensus propagation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2506.14291</link>
<guid>https://arxiv.org/abs/2506.14291</guid>
<content:encoded><![CDATA[
<div> Keywords: graph machine learning, symmetries, foundation models, node-level tasks, permutation-equivariance

Summary:
In the realm of graph machine learning, the challenge lies in creating models that can generalize across various graphs and feature sets. This study delves into the development of graph foundation models for node-level tasks, emphasizing the importance of respecting specific symmetries. The key symmetries identified are label permutation-equivariance, feature permutation-invariance, and node permutation-equivariance within local graph neighborhoods. By investigating linear transformations that adhere to these symmetries, the researchers establish a universal approximator capable of handling multisets with the specified properties. Utilizing these transformation layers on local neighborhood features, they construct a class of graph foundation models for predicting node properties. Empirical validation on real-world datasets showcases strong zero-shot performance and consistent enhancement as the number of training graphs increases. This work provides a promising recipe for designing versatile graph foundation models with broad applicability potential. 

<br /><br />Summary: <div>
arXiv:2506.14291v1 Announce Type: cross 
Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
<link>https://arxiv.org/abs/2406.11423</link>
<guid>https://arxiv.org/abs/2406.11423</guid>
<content:encoded><![CDATA[
<div> Keywords: Proactive content moderation, website credibility classification, webgraph, social media, dredge words

Summary: 
Our study focuses on developing a system for proactive content moderation that assesses the credibility of websites by analyzing direct and indirect user pathways to unreliable websites. We introduce a novel approach that integrates webgraph and social media contexts, utilizing graph neural networks to achieve state-of-the-art results in website credibility classification. Additionally, we introduce the concept of dredge words, terms that unreliable domains rank highly for on search engines, and explore their usage on social media platforms. Our system significantly enhances the identification of unreliable domains, improving top-k identification accuracy. We also release a dataset of dredge words, showcasing their links to social media and e-commerce platforms. Overall, our approach offers a comprehensive solution for evaluating website credibility and enhancing content moderation efforts.<br /><br />Summary: <div>
arXiv:2406.11423v4 Announce Type: replace 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Intelligence: A Survey on Image Privacy in Online Social Networks</title>
<link>https://arxiv.org/abs/2008.12199</link>
<guid>https://arxiv.org/abs/2008.12199</guid>
<content:encoded><![CDATA[
<div> privacy intelligence, online social networks, image sharing, privacy protection, user-centric perspective 

Summary:
The article discusses the importance of privacy intelligence in the context of image sharing on online social networks (OSNs). It highlights the growing risk of privacy invasion due to image leaks and the use of advanced algorithms like DeepFake. The authors emphasize the need for a more intelligent approach to privacy management in OSN image sharing. They introduce a comprehensive framework that includes a definition and taxonomy of OSN image privacy, as well as a stage-based analysis of privacy issues in the lifecycle of image sharing. The framework considers user behaviors and identifies corresponding privacy challenges. A systematic review of intelligent solutions targeting these issues is provided, leading to the proposal of an intelligent privacy firewall for improved privacy management. The article concludes with a discussion on challenges and future directions in the field of privacy intelligence in OSN image sharing.<br /><br />Summary: <div>
arXiv:2008.12199v4 Announce Type: replace-cross 
Abstract: Image sharing on online social networks (OSNs) has become an indispensable part of daily social activities, but it has also led to an increased risk of privacy invasion. The recent image leaks from popular OSN services and the abuse of personal photos using advanced algorithms (e.g. DeepFake) have prompted the public to rethink individual privacy needs in OSN image sharing. However, OSN image privacy itself is quite complicated, and solutions currently in place for privacy management in reality are insufficient to provide personalized, accurate and flexible privacy protection. A more intelligent environment for privacy-friendly OSN image sharing is in demand. To fill the gap, we contribute a survey of "privacy intelligence" that targets modern privacy issues in dynamic OSN image sharing from a user-centric perspective. Specifically, we present a definition and a taxonomy of OSN image privacy, and a high-level privacy analysis framework based on the lifecycle of OSN image sharing. The framework consists of three stages with different principles of privacy by design. At each stage, we identify typical user behaviors in OSN image sharing and the privacy issues associated with these behaviors. Then a systematic review on the representative intelligent solutions targeting those privacy issues is conducted, also in a stage-based manner. The resulting analysis describes an intelligent privacy firewall for closed-loop privacy management. We also discuss the challenges and future directions in this area.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlation-Aware Graph Convolutional Networks for Multi-Label Node Classification</title>
<link>https://arxiv.org/abs/2411.17350</link>
<guid>https://arxiv.org/abs/2411.17350</guid>
<content:encoded><![CDATA[
<div> Graph Convolution Networks, Multi-label node classification, Correlation-aware, Graph mining, Label correlations <br />
<br />
Summary: 
This paper introduces the Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node classification in graph mining. Traditional methods using GCNs struggle with ambiguous features and topologies induced by multiple labels, leading to reduced credibility of graph messages and overlooking label correlations. To address this challenge, CorGCN incorporates a Correlation-Aware Graph Decomposition module to capture label-correlated information for each label, and a Correlation-Enhanced Graph Convolution to model label relationships during message passing. Experimental results on five datasets demonstrate the effectiveness of CorGCN in improving multi-label node classification accuracy. <div>
arXiv:2411.17350v3 Announce Type: replace-cross 
Abstract: Multi-label node classification is an important yet under-explored domain in graph mining as many real-world nodes belong to multiple categories rather than just a single one. Although a few efforts have been made by utilizing Graph Convolution Networks (GCNs) to learn node representations and model correlations between multiple labels in the embedding space, they still suffer from the ambiguous feature and ambiguous topology induced by multiple labels, which reduces the credibility of the messages delivered in graphs and overlooks the label correlations on graph data. Therefore, it is crucial to reduce the ambiguity and empower the GCNs for accurate classification. However, this is quite challenging due to the requirement of retaining the distinctiveness of each label while fully harnessing the correlation between labels simultaneously. To address these issues, in this paper, we propose a Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node classification. By introducing a novel Correlation-Aware Graph Decomposition module, CorGCN can learn a graph that contains rich label-correlated information for each label. It then employs a Correlation-Enhanced Graph Convolution to model the relationships between labels during message passing to further bolster the classification process. Extensive experiments on five datasets demonstrate the effectiveness of our proposed CorGCN.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint UAV Trajectory Planning and LEO Satellite Selection for Data Offloading in Space-Air-Ground Integrated Networks</title>
<link>https://arxiv.org/abs/2506.12750</link>
<guid>https://arxiv.org/abs/2506.12750</guid>
<content:encoded><![CDATA[
<div> Keywords: LEO satellites, UAVs, IoT devices, energy consumption, optimization

Summary:
The paper presents a study on a two-phase hierarchical data uplink model in the Space-Air-Ground Integrated Network (SAGIN) to cater to the remote Internet of Things (IoT) requirements. The model consists of optimizing trajectories of unmanned aerial vehicles (UAVs) for efficient data collection from IoT devices and transmitting it to Low Earth Orbit (LEO) satellites for further processing. The objective is to minimize the total energy consumption for IoT devices, UAVs, and LEO satellites. The problem is decomposed into two phases - IoT-UAV phase and UAV-LEO phase - to address the mixed-integer nonlinear programming challenge. Simulation results indicate significant energy savings compared to benchmark algorithms. The proposed algorithms show promise in enhancing the efficiency of data collection and offloading in the evolving SAGIN environment. 

<br /><br />Summary: <div>
arXiv:2506.12750v1 Announce Type: new 
Abstract: With the development of low earth orbit (LEO) satellites and unmanned aerial vehicles (UAVs), the space-air-ground integrated network (SAGIN) becomes a major trend in the next-generation networks. However, due to the instability of heterogeneous communication and time-varying characteristics of SAGIN, it is challenging to meet the remote Internet of Things (IoT) demands for data collection and offloading. In this paper, we investigate a two-phase hierarchical data uplink model in SAGIN. Specifically, UAVs optimize trajectories to enable efficient data collection from IoT devices, and then they transmit the data to LEO satellites with computing capabilities for further processing. The problem is formulated to minimize the total energy consumption for IoT devices, UAVs, and LEO satellites. Since the problem is in the form of mixed-integer nonlinear programming and intractable to solve directly, we decompose it into two phases. In the IoT-UAV phase, we design the algorithm to jointly optimize the IoT pairing, power allocation, and UAVs trajectories. Considering the high dynamic characteristics of LEO satellites, a real-time LEO satellite selection mechanism joint with the Satellite Tool Kit is proposed in the UAV-LEO phase. Finally, simulation results show the effectiveness of the proposed algorithms, with about 10% less energy consumption compared with the benchmark algorithm.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governments Should Mandate Tiered Anonymity on Social-Media Platforms to Counter Deepfakes and LLM-Driven Mass Misinformation</title>
<link>https://arxiv.org/abs/2506.12814</link>
<guid>https://arxiv.org/abs/2506.12814</guid>
<content:encoded><![CDATA[
<div> three-tier anonymity framework, social-media platforms, deepfakes, misinformation, Reddit

Summary:
This position paper suggests implementing a three-tier anonymity framework on social media platforms to address the challenges posed by deepfakes and misinformation fueled by large-language models. The tiers would be based on a user's reach score, with smaller accounts enjoying pseudonymity, moderately influential accounts requiring legal-identity linkage, and accounts with significant reach undergoing fact-checking for each post. The analysis of Reddit shows that volunteer moderators use similar mechanisms as audience size grows, indicating the operational feasibility of such a framework. To address existing engagement incentives that may impede voluntary adoption, the paper outlines a regulatory approach based on US legal principles and recent safety statutes in the EU and UK. By integrating reach-based identity checks into platform tools, the proposed framework aims to combat large-scale misinformation while safeguarding everyday privacy.<br /><br />Summary: <div>
arXiv:2506.12814v1 Announce Type: new 
Abstract: This position paper argues that governments should mandate a three-tier anonymity framework on social-media platforms as a reactionary measure prompted by the ease-of-production of deepfakes and large-language-model-driven misinformation. The tiers are determined by a given user's $\textit{reach score}$: Tier 1 permits full pseudonymity for smaller accounts, preserving everyday privacy; Tier 2 requires private legal-identity linkage for accounts with some influence, reinstating real-world accountability at moderate reach; Tier 3 would require per-post, independent, ML-assisted fact-checking, review for accounts that would traditionally be classed as sources-of-mass-information.
  An analysis of Reddit shows volunteer moderators converge on comparable gates as audience size increases -- karma thresholds, approval queues, and identity proofs -- demonstrating operational feasibility and social legitimacy. Acknowledging that existing engagement incentives deter voluntary adoption, we outline a regulatory pathway that adapts existing US jurisprudence and recent EU-UK safety statutes to embed reach-proportional identity checks into existing platform tooling, thereby curbing large-scale misinformation while preserving everyday privacy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Social Network Activity Using Joint User and Topic Interaction</title>
<link>https://arxiv.org/abs/2506.12842</link>
<guid>https://arxiv.org/abs/2506.12842</guid>
<content:encoded><![CDATA[
<div> Keywords: online social platforms, information cascades, opinion formation, user behavior, Hawkes processes

Summary:<br /><br />
The paper introduces the Mixture of Interacting Cascades (MIC) model, which focuses on the dynamics of information cascades on online social platforms. The model combines marked multidimensional Hawkes processes to capture the interaction between cascades and user activity. By employing a mixture of temporal point processes, MIC can effectively model the spread of multiple pieces of information among users with varying behavior adoption mechanisms. Experimental results on synthetic and real data demonstrate the superior performance of MIC compared to existing methods. The model's learned parameters provide valuable insights into the interplay between information cascades and user behavior, allowing for bi-layered visualizations of social network activity data. Overall, MIC offers a comprehensive framework for understanding and analyzing the complex dynamics of opinion formation on online social platforms. 

Summary: <br /><br /> <div>
arXiv:2506.12842v1 Announce Type: new 
Abstract: The emergence of online social platforms, such as social networks and social media, has drastically affected the way people apprehend the information flows to which they are exposed. In such platforms, various information cascades spreading among users is the main force creating complex dynamics of opinion formation, each user being characterized by their own behavior adoption mechanism. Moreover, the spread of multiple pieces of information or beliefs in a networked population is rarely uncorrelated. In this paper, we introduce the Mixture of Interacting Cascades (MIC), a model of marked multidimensional Hawkes processes with the capacity to model jointly non-trivial interaction between cascades and users. We emphasize on the interplay between information cascades and user activity, and use a mixture of temporal point processes to build a coupled user/cascade point process model. Experiments on synthetic and real data highlight the benefits of this approach and demonstrate that MIC achieves superior performance to existing methods in modeling the spread of information cascades. Finally, we demonstrate how MIC can provide, through its learned parameters, insightful bi-layered visualizations of real social network activity data.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Coordinated Processes From Social Online Networks</title>
<link>https://arxiv.org/abs/2506.12988</link>
<guid>https://arxiv.org/abs/2506.12988</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, coordinated behavior, AI-generated content, process mining, Twitter<br />
<br />
Summary: 
The article discusses the use of process mining techniques to analyze coordinated agent behavior in social networks, particularly focusing on the detection of AI-generated content. It highlights the challenge of identifying AI-driven campaigns due to the high quality of generated text. By leveraging process mining methods on real-world Twitter data, the study aims to discover structural and behavioral properties of process models that can reveal coordinated AI and human behaviors online. The research emphasizes the importance of considering metadata like post timings for effective detection of coordinated campaigns. Current approaches to modeling information spread online are limited in representing different control flows, underscoring the need for innovative methods like process mining. The study showcases how process mining can offer insights into the sophisticated structures and behaviors in social networks, shedding light on the prevalence of coordinated AI and human actions in online environments. <br /> <div>
arXiv:2506.12988v1 Announce Type: new 
Abstract: The rapid growth of social media presents a unique opportunity to study coordinated agent behavior in an unfiltered environment. Online processes often exhibit complex structures that reflect the nature of the user behavior, whether it is authentic and genuine, or part of a coordinated effort by malicious agents to spread misinformation and disinformation. Detection of AI-generated content can be extremely challenging due to the high quality of large language model-generated text. Therefore, approaches that use metadata like post timings are required to effectively detect coordinated AI-driven campaigns. Existing work that models the spread of information online is limited in its ability to represent different control flows that occur within the network in practice. Process mining offers techniques for the discovery of process models with different routing constructs and are yet to be applied to social networks. We propose to leverage process mining methods for the discovery of AI and human agent behavior within social networks. Applying process mining techniques to real-world Twitter (now X) event data, we demonstrate how the structural and behavioral properties of discovered process models can reveal coordinated AI and human behaviors online.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evolution of Cooperation Based on Adaptive Reputation Threshold and Game Transition</title>
<link>https://arxiv.org/abs/2506.13319</link>
<guid>https://arxiv.org/abs/2506.13319</guid>
<content:encoded><![CDATA[
<div> reputation, game evolution, network topology, cooperation, strategy evolution
<br />
Summary:<br />
The study explores how reputation influences game evolution in social systems using a heterogeneous game transition model with a reputation-based dynamic threshold mechanism. Individual interactions are shaped by reputation, driving adaptation and strategy evolution. The square lattice network promotes the coexistence of competing strategies, while the small-world network is more sensitive to changes due to efficient information dissemination. The reputation mechanism fosters the formation of a dominant state of cooperation, especially in contexts highly sensitive to reputation. The initial distribution of reputation affects the early stage but has minimal impact on the final steady state, which is determined by the reputation mechanism and network structure. <div>
arXiv:2506.13319v1 Announce Type: new 
Abstract: In real-world social systems, individual interactions are frequently shaped by reputation, which not only influences partner selection but also affects the nature and benefits of the interactions themselves. We propose a heterogeneous game transition model that incorporates a reputation-based dynamic threshold mechanism to investigate how reputation regulates game evolution. In our framework, individuals determine the type of game they engage in according to their own and their neighbors' reputation levels. In turn, the outcomes of these interactions modify their reputations, thereby driving the adaptation and evolution of future strategies in a feedback-informed manner. Through simulations on two representative topological structures, square lattice and small-world networks, we find that network topology exerts a profound influence on the evolutionary dynamics. Due to its localized connection characteristics, the square lattice network fosters the long-term coexistence of competing strategies. In contrast, the small-world network is more susceptible to changes in system parameters due to the efficiency of information dissemination and the sensitivity of strategy evolution. Additionally, the reputation mechanism is significant in promoting the formation of a dominant state of cooperation, especially in contexts of high sensitivity to reputation. Although the initial distribution of reputation influences the early stage of the evolutionary path, it has little effect on the final steady state of the system. Hence, we can conclude that the ultimate steady state of evolution is primarily determined by the reputation mechanism and the network structure.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwiUSD: A Benchmark Dataset and Structure-Aware LLM Framework for User Stance Detection</title>
<link>https://arxiv.org/abs/2506.13343</link>
<guid>https://arxiv.org/abs/2506.13343</guid>
<content:encoded><![CDATA[
<div> keywords: User-level stance detection, TwiUSD benchmark, MRFG framework, relevance filtering, graph neural networks<br />
Summary:<br />
User-level stance detection (UserSD) remains a challenge, lacking high-quality benchmarks encompassing linguistic and social aspects. TwiUSD introduces a large-scale benchmark with explicit followee relationships, facilitating rigorous evaluation of stance models. The MRFG framework, utilizing LLM-based relevance filtering and feature routing, shows superior performance by addressing noise and context heterogeneity. MRFG employs multi-scale filtering and adaptively routes features through graph neural networks or multi-layer perceptrons based on topological informativeness. Experimental results demonstrate MRFG's superiority over strong baselines, including PLMs, graph-based models, and LLM prompting, in both in-target and cross-target evaluation.<br /> 
Summary: <div>
arXiv:2506.13343v1 Announce Type: new 
Abstract: User-level stance detection (UserSD) remains challenging due to the lack of high-quality benchmarks that jointly capture linguistic and social structure. In this paper, we introduce TwiUSD, the first large-scale, manually annotated UserSD benchmark with explicit followee relationships, containing 16,211 users and 47,757 tweets. TwiUSD enables rigorous evaluation of stance models by integrating tweet content and social links, with superior scale and annotation quality. Building on this resource, we propose MRFG: a structure-aware framework that uses LLM-based relevance filtering and feature routing to address noise and context heterogeneity. MRFG employs multi-scale filtering and adaptively routes features through graph neural networks or multi-layer perceptrons based on topological informativeness. Experiments show MRFG consistently outperforms strong baselines (including PLMs, graph-based models, and LLM prompting) in both in-target and cross-target evaluation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
<link>https://arxiv.org/abs/2506.12078</link>
<guid>https://arxiv.org/abs/2506.12078</guid>
<content:encoded><![CDATA[
<div> agent-based models, large language models, societal behaviors, human complexity, simulation

Summary:
The article introduces Light Society, a new agent-based simulation framework that combines traditional agent-based models with large language models to model human-like societies on a planetary scale. The framework formalizes social processes using structured transitions of agent and environment states powered by large language models. Light Society supports efficient simulation of societies with over one billion agents, enabling high-fidelity modeling of complex societal behaviors such as trust games and opinion propagation. The simulations conducted using Light Society demonstrate its ability to capture social trust and information diffusion accurately, with larger simulations leading to more stable and realistic emergent behaviors. The framework addresses the limitations of traditional agent-based models and offers new opportunities for understanding how complex societal behaviors emerge from individual cognition and interactions. <div>
arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs</title>
<link>https://arxiv.org/abs/2506.12367</link>
<guid>https://arxiv.org/abs/2506.12367</guid>
<content:encoded><![CDATA[
<div> extraction performance, knowledge graphs, error modeling, downstream analysis, community detection <br />
<br />Summary: <br />
This study evaluates knowledge graph extraction performance in affiliation graphs of person memberships in organizations. It identifies micro-level edge accuracy and macro-level graph metrics as key evaluation criteria, highlighting biases in downstream graph analysis metrics as extraction performance declines. The study demonstrates that error models commonly used in the literature do not capture these bias patterns, emphasizing the need for more realistic error modeling in knowledge graph extraction. The findings provide actionable insights for practitioners working with knowledge graphs, underscoring the importance of advancing extraction methods to ensure reliable downstream analyses. The study's focus on social register books underscores the relevance of understanding extraction errors for applications in social structures and institutional memberships. <div>
arXiv:2506.12367v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are useful for analyzing social structures, community dynamics, institutional memberships, and other complex relationships across domains from sociology to public health. While recent advances in large language models (LLMs) have improved the scalability and accessibility of automated KG extraction from large text corpora, the impacts of extraction errors on downstream analyses are poorly understood, especially for applied scientists who depend on accurate KGs for real-world insights. To address this gap, we conducted the first evaluation of KG extraction performance at two levels: (1) micro-level edge accuracy, which is consistent with standard NLP evaluations, and manual identification of common error sources; (2) macro-level graph metrics that assess structural properties such as community detection and connectivity, which are relevant to real-world applications. Focusing on affiliation graphs of person membership in organizations extracted from social register books, our study identifies a range of extraction performance where biases across most downstream graph analysis metrics are near zero. However, as extraction performance declines, we find that many metrics exhibit increasingly pronounced biases, with each metric tending toward a consistent direction of either over- or under-estimation. Through simulations, we further show that error models commonly used in the literature do not capture these bias patterns, indicating the need for more realistic error models for KG extraction. Our findings provide actionable insights for practitioners and underscores the importance of advancing extraction methods and error modeling to ensure reliable and meaningful downstream analyses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circular Directional Flow Decomposition of Networks</title>
<link>https://arxiv.org/abs/2506.12546</link>
<guid>https://arxiv.org/abs/2506.12546</guid>
<content:encoded><![CDATA[
<div> Circular Directional Flow Decomposition, weighted directed networks, circularity index, flow analysis, network structure<br />
Summary:<br />
The Circular Directional Flow Decomposition (CDFD) framework introduces a novel way to analyze circularity in weighted directed networks. It separates flow into a circular component and an acyclic component, providing a normalized circularity index between 0 and 1. This index captures flow involved in cycles and can represent system closure, feedback, and structural redundancy. The set of all decompositions forms a well-structured geometric space, with benchmark solutions including the maximum circularity solution and the Balanced Flow Forwarding (BFF) solution. These solutions outperform existing circularity metrics in detecting structural variation and enabling flow allocation or routing in practical applications. The CDFD framework allows for structural analysis, such as mapping cyclic flow distribution, and supports efficient transport and multilateral netting. <div>
arXiv:2506.12546v1 Announce Type: cross 
Abstract: We introduce the Circular Directional Flow Decomposition (CDFD), a new framework for analyzing circularity in weighted directed networks. CDFD separates flow into two components: a circular (divergence-free) component and an acyclic component that carries all nett directional flow. This yields a normalized circularity index between 0 (fully acyclic) and 1 (for networks formed solely by the superposition of cycles), with the complement measuring directionality. This index captures the proportion of flow involved in cycles, and admits a range of interpretations - such as system closure, feedback, weighted strong connectivity, structural redundancy, or inefficiency. Although the decomposition is generally non-unique, we show that the set of all decompositions forms a well-structured geometric space with favourable topological properties. Within this space, we highlight two benchmark decompositions aligned with distinct analytical goals: the maximum circularity solution, which minimizes nett flow, and the Balanced Flow Forwarding (BFF) solution, a unique, locally computable decomposition that distributes circular flow across all feasible cycles in proportion to the original network structure. We demonstrate the interpretive value and computational tractability of both decompositions on synthetic and empirical networks. They outperform existing circularity metrics in detecting meaningful structural variation. The decomposition also enables structural analysis - such as mapping the distribution of cyclic flow - and supports practical applications that require explicit flow allocation or routing, including multilateral netting and efficient transport.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prosocial Design in Trust and Safety</title>
<link>https://arxiv.org/abs/2506.12792</link>
<guid>https://arxiv.org/abs/2506.12792</guid>
<content:encoded><![CDATA[
<div> Keywords: Prosocial Design, platform design, governance, Trust and Safety, research <br />
Summary: <br />
This chapter provides an overview of Prosocial Design, emphasizing its impact on behavior and the importance of design choices in promoting healthy interactions. The authors discuss core principles of Prosocial Design and its connection to Trust and Safety. Research indicates that Prosocial Design can effectively reduce rule-breaking and misinformation spread. However, the field is still developing, and more research is needed to fully understand its potential. The chapter aims to inspire further research and adoption of a prosocial design approach, sparking discussions on its principles and role in enhancing Trust and Safety. <div>
arXiv:2506.12792v1 Announce Type: cross 
Abstract: This chapter presents an overview of Prosocial Design, an approach to platform design and governance that recognizes design choices influence behavior and that those choices can or should be made toward supporting healthy interactions and other prosocial outcomes. The authors discuss several core principles of Prosocial Design and its relationship to Trust and Safety and other related fields. As a primary contribution, the chapter reviews relevant research to demonstrate how Prosocial Design can be an effective approach to reducing rule-breaking and other harmful behavior and how it can help to stem the spread of harmful misinformation. Prosocial Design is a nascent and evolving field and research is still limited. The authors hope this chapter will not only inspire more research and the adoption of a prosocial design approach, but that it will also provoke discussion about the principles of Prosocial Design and its potential to support Trust and Safety.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Approximate Temporal Triangle Counting in Streaming with Predictions</title>
<link>https://arxiv.org/abs/2506.13173</link>
<guid>https://arxiv.org/abs/2506.13173</guid>
<content:encoded><![CDATA[
<div> Algorithm, Triangle counting, Temporal graphs, Streaming processing, Scalability <br />
Summary: 
STEP is a new algorithm designed to approximate temporal triangle counts in massive temporal graphs efficiently. It combines predictions on the number of triangles involving a temporal edge with a sampling strategy to achieve scalability and accuracy in estimating all eight temporal triangle types simultaneously. The algorithm uses a sublinear amount of memory while providing unbiased and highly accurate estimates, even with noisy predictions. Extensive experiments on billion-edge temporal graphs show that STEP outperforms existing methods in terms of efficiency and output quality. This novel approach addresses the challenge of handling large-scale temporal graphs and provides a practical solution for counting triangles in dynamic graph datasets. <br /> <div>
arXiv:2506.13173v1 Announce Type: cross 
Abstract: Triangle counting is a fundamental and widely studied problem on static graphs, and recently on temporal graphs, where edges carry information on the timings of the associated events. Streaming processing and resource efficiency are crucial requirements for counting triangles in modern massive temporal graphs, with millions of nodes and up to billions of temporal edges. However, current exact and approximate algorithms are unable to handle large-scale temporal graphs. To fill such a gap, we introduce STEP, a scalable and efficient algorithm to approximate temporal triangle counts from a stream of temporal edges. STEP combines predictions to the number of triangles a temporal edge is involved in, with a simple sampling strategy, leading to scalability, efficiency, and accurate approximation of all eight temporal triangle types simultaneously. We analytically prove that, by using a sublinear amount of memory, STEP obtains unbiased and very accurate estimates. In fact, even noisy predictions can significantly reduce the variance of STEP's estimates. Our extensive experiments on massive temporal graphs with up to billions of edges demonstrate that STEP outputs high-quality estimates and is more efficient than state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven analysis of the impact of non-compliant individuals on epidemic diffusion in urban settings</title>
<link>https://arxiv.org/abs/2506.13325</link>
<guid>https://arxiv.org/abs/2506.13325</guid>
<content:encoded><![CDATA[
<div> Keywords: public health, non-compliance, epidemic dynamics, urban environments, contact networks

Summary:
Individuals not complying with public health measures in urban settings can undermine epidemic control efforts. This study focused on Torino, Milano, and Palermo in Italy, using data-driven contact networks to analyze the impact of non-compliant individuals. The research modelled non-compliance using a Susceptible-Infected-Recovered framework, showing that even a small number of non-compliant individuals can significantly increase infections and peak times, especially at moderate transmission rates. Spatially varying distributions of non-compliance, obtained through electoral and vaccine hesitancy data, led to infection hotspots forming with differing intensities. The study highlights the need for tailored public health interventions to address localized risks and stresses the importance of monitoring behavioral compliance for effective epidemic control. 

<br /><br />Summary: <div>
arXiv:2506.13325v1 Announce Type: cross 
Abstract: Individuals who do not comply with public health safety measures pose a significant challenge to effective epidemic control, as their risky behaviours can undermine public health interventions. This is particularly relevant in urban environments because of their high population density and complex social interactions. In this study, we employ detailed contact networks, built using a data-driven approach, to examine the impact of non-compliant individuals on epidemic dynamics in three major Italian cities: Torino, Milano, and Palermo. We use a heterogeneous extension of the Susceptible-Infected-Recovered model that distinguishes between ordinary and non-compliant individuals, who are more infectious and/or more susceptible. By combining electoral data with recent findings on vaccine hesitancy, we obtain spatially heterogeneous distributions of non-compliance. Epidemic simulations demonstrate that even a small proportion of non-compliant individuals in the population can substantially increase the number of infections and accelerate the timing of their peak. Furthermore, the impact of non-compliance is greatest when disease transmission rates are moderate. Including the heterogeneous, data-driven distribution of non-compliance in the simulations results in infection hotspots forming with varying intensity according to the disease transmission rate. Overall, these findings emphasise the importance of monitoring behavioural compliance and tailoring public health interventions to address localised risks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best Soules Basis for the Estimation of a Spectral Barycentre Network</title>
<link>https://arxiv.org/abs/2502.00038</link>
<guid>https://arxiv.org/abs/2502.00038</guid>
<content:encoded><![CDATA[
<div> algorithm, barycentre, Laplacian spectral pseudo-distance, networks, stochastic block models

Summary:
The article presents a fast algorithm for computing the barycentre of a set of networks using a Laplacian spectral pseudo-distance. The algorithm utilizes Soules bases to reconstruct a sparse approximation of the sample mean adjacency matrix. The study proves that when networks are random realizations of stochastic block models, the algorithm accurately reconstructs the population mean adjacency matrix. Monte Carlo simulations validate the theoretical properties of the estimator. This research is significant as it paves the way for the development of new spectral-based network synthesis methods with theoretical guarantees. <div>
arXiv:2502.00038v2 Announce Type: replace 
Abstract: The main contribution of this work is a fast algorithm to compute the barycentre of a set of networks based on a Laplacian spectral pseudo-distance. The core engine for the reconstruction of the barycentre is an algorithm that explores the large library of Soules bases, and returns a basis that yields a sparse approximation of the sample mean adjacency matrix. We prove that when the networks are random realizations of stochastic block models, then our algorithm reconstructs the population mean adjacency matrix. In addition to the theoretical analysis of the estimator of the barycentre network, we perform Monte Carlo simulations to validate the theoretical properties of the estimator. This work is significant because it opens the door to the design of new spectral-based network synthesis that have theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2506.08298</link>
<guid>https://arxiv.org/abs/2506.08298</guid>
<content:encoded><![CDATA[
<div> framework, graph learning, heterogeneous graphs, context encoding, transformer <br />
Summary: 
The article introduces a novel framework, H$^2$GFM, designed for graph learning that can handle both homogeneous (HoTAGs) and heterogeneous text-attributed graphs (HeTAGs). The framework projects meta-relations among graphs into a unified textual space, utilizes context encoding to capture spatial and semantic relationships, and introduces a context-adaptive graph transformer (CGT) for robust node representations. It also incorporates a mixture of CGT experts to capture heterogeneous structural patterns among graph types. The model is tested on a variety of graphs and learning scenarios, demonstrating its effectiveness in generalizing across different graph types and tasks. <div>
arXiv:2506.08298v2 Announce Type: replace-cross 
Abstract: The growing interests and applications of graph learning in diverse domains have propelled the development of a unified model generalizing well across different graphs and tasks, known as the Graph Foundation Model (GFM). Existing research has leveraged text-attributed graphs (TAGs) to tackle the heterogeneity in node features among graphs. However, they primarily focus on homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple types of nodes/edges reside, underexplored. To enhance the capabilities and applications of GFM, we introduce H$^2$GFM, a novel framework designed to generalize across both HoTAGs and HeTAGs. Our model projects diverse meta-relations among graphs under a unified textual space, and employs a context encoding to capture spatial and higher-order semantic relationships. To achieve robust node representations, we propose a novel context-adaptive graph transformer (CGT), effectively capturing information from both context neighbors and their relationships. Furthermore, we employ a mixture of CGT experts to capture the heterogeneity in structural patterns among graph types. Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as learning scenarios demonstrate the effectiveness of our model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Dynamics of Emotions in Italian Online Soccer Fandoms</title>
<link>https://arxiv.org/abs/2506.11934</link>
<guid>https://arxiv.org/abs/2506.11934</guid>
<content:encoded><![CDATA[
<div> Keywords: Italian soccer, fandoms, sentiment analysis, emotional dynamics, social media

Summary:
This study delves into the emotional dynamics of Italian soccer fandoms by analyzing user-generated content from Instagram accounts of 83 teams in different leagues. Through sentiment analysis, researchers examined emotional patterns, identified fan base clusters with similar expectations, and linked emotions to preseason expectations, team performance, and socioeconomic factors. Joy was found to have anti-bursty distributions, while anger showed bursty patterns. The study revealed correlations between emotional signals, preseason expectations, and final league rankings. Notably, burstiness emerged as a significant indicator of team performance, with its exclusion leading to a decreased coefficient of determination in statistical models. These findings provide unique insights into the interplay of fan emotions, team outcomes, and social media dynamics, offering potential avenues for future research in sports analytics and fan engagement studies. 

<br /><br />Summary: <div>
arXiv:2506.11934v1 Announce Type: new 
Abstract: This study investigates the emotional dynamics of Italian soccer fandoms through computational analysis of user-generated content from official Instagram accounts of 83 teams across Serie A, Serie B, and Lega Pro during the 2023-24 season. By applying sentiment analysis to fan comments, we extract temporal emotional patterns and identify distinct clusters of fan bases with similar preseason expectations. Drawing from complex systems theory, we characterize joy as displaying anti-bursty temporal distributions, while anger is marked by pronounced bursty patterns. Our analysis reveals significant correlations between these emotional signals, preseason expectations, socioeconomic factors, and final league rankings. In particular, the burstiness metric emerges as a meaningful correlate of team performance; statistical models excluding this parameter show a decrease in the coefficient of determination of 32%. These findings offer novel insights into the relationship between fan emotional expression and team outcomes, suggesting potential avenues for research in sports analytics, social media dynamics, and fan engagement studies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Networks: Enumerating Maximal Community Patterns in $c$-Closed Graphs</title>
<link>https://arxiv.org/abs/2506.11437</link>
<guid>https://arxiv.org/abs/2506.11437</guid>
<content:encoded><![CDATA[
<div> maximal cliques, complete bipartite subgraphs, blow-ups, c-closed graphs, polynomial time
<br />
Summary:
This article discusses the enumeration of maximal blow-ups of arbitrary finite graphs in c-closed graphs. It is established that for any fixed graph H, the number of maximal blow-ups in an n-vertex c-closed graph is bounded by a polynomial in n. The study also extends to induced blow-ups, providing a precise characterization of the graphs H for which the number of maximal induced blow-ups is polynomially bounded in n. Additionally, the article explores the same questions for an infinite family of graphs. The research contributes to understanding the structural patterns and properties of c-closed graphs, offering insights into the computational complexity of enumerating specific graph structures within this model. 
<br /> <div>
arXiv:2506.11437v1 Announce Type: cross 
Abstract: Fox, Seshadhri, Roughgarden, Wei, and Wein (SICOMP 2020) introduced the model of $c$-closed graphs--a distribution-free model motivated by triadic closure, one of the most pervasive structural signatures of social networks. While enumerating maximal cliques in general graphs can take exponential time, it is known that in $c$-closed graphs, maximal cliques and maximal complete bipartite subgraphs can always be enumerated in polynomial time. These structures correspond to blow-ups of simple patterns: a single vertex or a single edge, with some vertices required to form cliques. In this work, we explore a natural extension: we study maximal blow-ups of arbitrary finite graphs $H$ in $c$-closed graphs. We prove that for any fixed graph $H$, the number of maximal blow-ups of $H$ in an $n$-vertex $c$-closed graph is always bounded by a polynomial in $n$. We further investigate the case of induced blow-ups and provide a precise characterization of the graphs $H$ for which the number of maximal induced blow-ups is also polynomially bounded in $n$. Finally, we study the analogue questions when $H$ ranges over an infinite family of graphs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems</title>
<link>https://arxiv.org/abs/2506.11467</link>
<guid>https://arxiv.org/abs/2506.11467</guid>
<content:encoded><![CDATA[
<div> Keywords: Human evaluators, Machine Translation, low-resource languages, evaluation platform, Natural Language Processing

Summary:
This paper addresses the challenge of evaluating Machine Translation (MT) systems for low-resource languages (LRLs) due to the lack of datasets and human evaluators. It reviews existing evaluation procedures and proposes a design for a recruitment and gamified evaluation platform to bridge the resource gap. The platform aims to enable developers of MT systems to find and engage adequate human evaluators, especially those with expertise in the target language. The design emphasizes testing for adequacy, fluency, and other key metrics that automated metrics may overlook. Challenges in evaluating the platform are discussed, as well as its potential applications in broader Natural Language Processing (NLP) research. The proposed platform offers a solution to the scarcity of resources for evaluating MT systems in LRLs and could significantly enhance the quality and accuracy of these systems in practice. 

<br /><br />Summary: <div>
arXiv:2506.11467v1 Announce Type: cross 
Abstract: Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research</title>
<link>https://arxiv.org/abs/2506.11727</link>
<guid>https://arxiv.org/abs/2506.11727</guid>
<content:encoded><![CDATA[
<div> limitations, YouTube Data API, academic research, search endpoint, bias

Summary:
This paper evaluates the search endpoint of YouTube's Data API (v3) for academic research, highlighting limitations in completeness, representativeness, consistency, and bias. The study, conducted over six months with eleven queries, uncovers significant challenges in video recall and precision, particularly with relevance rankings retrieving off-topic videos. Temporal decay is observed, with a rapid decline in findable videos after 20-60 days post-publication. Inconsistencies in search results compromise replicability, affecting research outcomes like the case study on European Parliament elections. The paper suggests mitigation strategies but ultimately deems the API's search function inadequate for robust academic research, especially regarding Digital Services Act requirements.
<br /><br />Summary: <div>
arXiv:2506.11727v1 Announce Type: cross 
Abstract: This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing "freshness" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries of weighted networks: weight approximation method and its application to food webs</title>
<link>https://arxiv.org/abs/2506.11824</link>
<guid>https://arxiv.org/abs/2506.11824</guid>
<content:encoded><![CDATA[
<div> food webs, symmetries, weighted networks, group theory, ecological co-existence<br />
<br />
Summary: 
The study focuses on identifying symmetries in weighted networks by aggregating edge weights into discrete categories. They apply this method to 250 empirical food webs to assess ecological co-existence and competition in species. Symmetric vertices, even under weak approximations, are found to form small orbits of size two or three in food webs. These symmetric vertices can be present at various trophic levels or network positions. Three symmetry measures are applied to compare structural patterns at the network level, revealing important insights into the roles of vertices in the network. The study highlights the significance of identifying symmetries in complex systems to simplify computations and understand network structures, particularly in the context of ecological interactions in food webs. <div>
arXiv:2506.11824v1 Announce Type: cross 
Abstract: Knowing which parts of a complex system have identical roles simplifies computations and reveals patterns in its network structure. Group theory has been applied to study symmetries in unweighted networks. However, in real-world weighted networks, edge weights are rarely equal, making exact symmetry uncommon. To study symmetries in weighted networks, we aggregate edge weights into a small number of discrete categories. The symmetries of these aggregated networks identify vertices with similar roles in the original weighted network.
  In food webs, this approach helps to quantify ecological co-existence and competition by assessing the functional substitutability of species. We apply our method to 250 empirical food webs, finding that symmetric vertices emerge even under weak approximations, typically forming small orbits of size two or three. These symmetric vertices can appear at any trophic level or network position. We also apply three symmetry measures to compare structural patterns at the network level.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Political Bias in LLMs through Structured Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2506.11825</link>
<guid>https://arxiv.org/abs/2506.11825</guid>
<content:encoded><![CDATA[
<div> language models, political bias, debates, agent gender, echo chambers
Summary:
This study explores the impact of large language models (LLMs) and agent attributes on political bias in debates. Neutral LLM agents tend to align with Democrats, while Republican agents move closer to Neutral positions. Agent gender influences attitudes, with agents adjusting their opinions based on gender knowledge. Contrary to previous findings, agents with shared political affiliations can form echo chambers, intensifying attitudes as debates progress. The study highlights the importance of considering LLM type and agent attributes in understanding political bias and interaction dynamics in debates. <div>
arXiv:2506.11825v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll in the Tanks! Measuring Left-wing Extremism on Reddit at Scale</title>
<link>https://arxiv.org/abs/2307.06981</link>
<guid>https://arxiv.org/abs/2307.06981</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, left-wing extremism, tankies, Reddit, discourse <br />
Summary:<br />
This large-scale study focuses on left-wing extremism, specifically the "tankie" community on Reddit. Tankies emerged in the 1950s in support of actions by the USSR and now back "Actually Existing Socialist" countries like China, USSR, and North Korea. Analysis of 1.3M posts from 53K authors reveals tankies' peripheral position within the far-left community on Reddit. Posts predominantly cover state-level political events over social issues. The study confirms misalignments and conceptual homomorphisms in tankie discourse, aligning with existing theory on their beliefs and behavior. This research sheds light on the distinct positioning and discourse of left-wing extremist groups on social media. <br /><br /> <div>
arXiv:2307.06981v3 Announce Type: replace 
Abstract: Social media's role in the spread and evolution of extremism is a focus of intense study. Online extremists have been involved in the spread of online hate, mis- and disinformation, and real-world violence. However, most existing work has focuses on right-wing extremism. In this paper, we perform a first of its kind large-scale measurement study exploring left-wing extremism. We focus on "tankies," a left-wing community that first arose in the 1950s in support of hardline actions of the USSR and has evolved to support what they call "Actually Existing Socialist" countries, e.g., CCP-run China, the USSR, and North Korea. We collect and analyze 1.3M posts from 53K authors from tankie subreddits, and explore the position of tankies within the broader far-left community on Reddit. Among other things, we find that tankies are clearly on the periphery of the larger far-left community. When examining the contents of posts, we find misalignments and conceptual homomorphisms that confirm the description of tankies in the theoretical work. We also discover that tankies focus more on state-level political events rather than social issues. Our findings provide empirical evidence of the distinct positioning and discourse of left-wing extremist groups on social media.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Datasets for Information Diffusion Tasks</title>
<link>https://arxiv.org/abs/2407.05161</link>
<guid>https://arxiv.org/abs/2407.05161</guid>
<content:encoded><![CDATA[
<div> Keywords: information diffusion, 5W Model, tasks, datasets, taxonomy

Summary:
The study categorizes information diffusion tasks into ten subtasks according to the 5W Model framework, focusing on prediction, bot detection, and misinformation detection. A systematic taxonomy is provided, along with an analysis of publicly available datasets linked to users and content attributes. The attributes include user information, social network, bot label, propagation content, propagation network, and veracity label. The dataset repository is accessible on the authors' GitHub page. The study highlights the need for a comprehensive categorization and integration of datasets in information diffusion research to advance understanding and prediction capabilities. Future research directions and limitations are also discussed, emphasizing the importance of enhancing current datasets for improved information diffusion analysis. <br /><br />Summary: <div>
arXiv:2407.05161v2 Announce Type: replace 
Abstract: Information diffusion across various new media platforms gradually influences perceptions, decisions, and social behaviors of individual users. In communication studies, the famous Five W's of Communication model (5W Model) has displayed the process of information diffusion clearly. At present, although plenty of studies and corresponding datasets about information diffusion have emerged, a systematic categorization of tasks and an integration of datasets are still lacking. To address this gap, we survey a systematic taxonomy of information diffusion tasks and datasets based on the "5W Model" framework. We first categorize the information diffusion tasks into ten subtasks with definitions and datasets analysis, from three main tasks of information diffusion prediction, social bot detection, and misinformation detection. We also collect the publicly available dataset repository of information diffusion tasks with the available links and compare them based on six attributes affiliated to users and content: user information, social network, bot label, propagation content, propagation network, and veracity label. In addition, we discuss the limitations and future directions of current datasets and research topics to advance the future development of information diffusion. The dataset repository can be accessed at our website https://github.com/fuxiaG/Information-Diffusion-Datasets.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly modelling the evolution of social structure and language in online communities</title>
<link>https://arxiv.org/abs/2409.19243</link>
<guid>https://arxiv.org/abs/2409.19243</guid>
<content:encoded><![CDATA[
<div> Keywords: group interactions, online communities, community structure, language modelling, extremist groups

Summary: 
Our study focuses on the dynamic nature of group interactions within online communities. We introduce a novel method that simultaneously models community structure and language over time. By generating dynamic word and user representations, our system can effectively cluster users, explore thematic interests of groups, and predict group membership. In our evaluation using a dataset of misogynistic extremist groups, our method outperforms previous models by incorporating both social structure and temporal language embeddings. This approach allows for new analyses of online groups, such as tracking their response to temporal events and measuring their use of violent language. Understanding the socio-temporal context of group interactions is crucial, especially in the context of extremist groups, and our method provides valuable insights for researchers studying online community dynamics.<br /><br />Summary: <div>
arXiv:2409.19243v2 Announce Type: replace 
Abstract: Group interactions take place within a particular socio-temporal context, which should be taken into account when modelling interactions in online communities. We propose a method for jointly modelling community structure and language over time. Our system produces dynamic word and user representations that can be used to cluster users, investigate thematic interests of groups, and predict group membership. We apply and evaluate our method in the context of a set of misogynistic extremist groups. Our results indicate that this approach outperforms prior models which lacked one of these components (i.e. not incorporating social structure, or using static word embeddings) when evaluated on clustering and embedding prediction tasks. Our method further enables novel types of analyses on online groups, including tracing their response to temporal events and quantifying their propensity for using violent language, which is of particular importance in the context of extremist groups.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stationary distribution of node2vec random walks on household models</title>
<link>https://arxiv.org/abs/2502.19039</link>
<guid>https://arxiv.org/abs/2502.19039</guid>
<content:encoded><![CDATA[
<div> random walk, node2vec, network embedding algorithms, stationary distribution, community-structured graphs

Summary:
The study focuses on the node2vec random walk in network embedding algorithms, particularly on community-structured household model graphs. It explores the mathematical properties of node2vec walks, including their stationary distribution. The research provides an explicit description of the stationary distribution in terms of walk parameters and demonstrates how tuning these parameters can interpolate between different types of stationary distributions. By adjusting the walk parameters, the stationary distribution can vary from uniform to size-biased or simple random walk distributions. The study highlights the flexibility and range of node2vec walks and their impact on various graph settings. <div>
arXiv:2502.19039v2 Announce Type: replace-cross 
Abstract: The node2vec random walk has proven to be a key tool in network embedding algorithms. These random walks are tuneable, and their transition probabilities depend on the previous visited node and on the triangles containing the current and the previously visited node. Even though these walks are widely used in practice, most mathematical properties of node2vec walks are largely unexplored, including their stationary distribution. We study the node2vec random walk on community-structured household model graphs. We prove an explicit description of the stationary distribution of node2vec walks in terms of the walk parameters. We then show that by tuning the walk parameters, the stationary distribution can interpolate between uniform, size-biased, or the simple random walk stationary distributions, demonstrating the wide range of possible walks. We further explore these effects on some specific graph settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of A* based heuristic algorithm for efficient interdiction in multi-Layer networks</title>
<link>https://arxiv.org/abs/2506.10017</link>
<guid>https://arxiv.org/abs/2506.10017</guid>
<content:encoded><![CDATA[
<div> Keywords: Criminal interception, limited police resources, transportation network, defender strategy, A-Star heuristic algorithm<br />
Summary: 
In dynamic crime environments, intercepting criminals with limited police resources is a challenging task, especially with continuous changes in the criminal's location and the vast transportation network. The study proposes a layered graph representation with duplicate transportation networks at each time step to optimize defender strategies using the A-Star heuristic algorithm. The defender aims to maximize the probability of successful interdiction against attacker strategies. Performance evaluation against a Mixed-Integer Linear Programming (MILP) approach shows that the proposed method effectively tackles complexity and produces high-quality solutions quickly.<br /> 
Summary: <div>
arXiv:2506.10017v1 Announce Type: new 
Abstract: Intercepting a criminal using limited police resources presents a significant challenge in dynamic crime environments, where the criminal's location continuously changes over time. The complexity is further heightened by the vastness of the transportation network. To tackle this problem, we propose a layered graph representation, in which each time step is associated with a duplicate of the transportation network. For any given set of attacker strategies, a near-optimal defender strategy is computed using the A-Star heuristic algorithm applied to the layered graph. The defender's goal is to maximize the probability of successful interdiction. We evaluate the performance of the proposed method by comparing it with a Mixed-Integer Linear Programming (MILP) approach used for the defender. The comparison considers both computational efficiency and solution quality. The results demonstrate that our approach effectively addresses the complexity of the problem and delivers high-quality solutions within a short computation time.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference of Hierarchical Core-Periphery Structure in Temporal Network</title>
<link>https://arxiv.org/abs/2506.10135</link>
<guid>https://arxiv.org/abs/2506.10135</guid>
<content:encoded><![CDATA[
<div> Keywords: core-periphery structure, temporal networks, multilayer-network, stochastic block models, Markov-chain Monte Carlo.

Summary: 
The study focuses on detecting hierarchical core-periphery structures in temporal networks using a multilayer-network representation and stochastic block models. Unlike traditional core-periphery structures, the hierarchical approach allows for nested, tree-like, and non-nested mesoscale structures. Statistical inference is performed using a Markov-chain Monte Carlo approach. The method is applied to two real-world temporal networks, revealing interesting structures within them. This research fills a gap in the field by providing a method for detecting core-periphery structures in time-dependent networks, enabling a deeper understanding of network dynamics and relationships. <div>
arXiv:2506.10135v1 Announce Type: new 
Abstract: Networks can have various types of mesoscale structures. One type of mesoscale structure in networks is core-periphery structure, which consists of densely-connected core nodes and sparsely-connected peripheral nodes. The core nodes are connected densely to each other and can be connected to the peripheral nodes, which are connected sparsely to other nodes. There has been much research on core-periphery structure in time-independent networks, but few core-periphery detection methods have been developed for time-dependent (i.e., ``temporal") networks. Using a multilayer-network representation of temporal networks and an inference approach that employs stochastic block models, we generalize a recent method for detecting hierarchical core-periphery structure \cite{Polanco23} from time-independent networks to temporal networks. In contrast to ``onion-like'' nested core-periphery structures (where each node is assigned to a group according to how deeply it is nested in a network's core), hierarchical core-periphery structures encompass networks with nested structures, tree-like structures (where any two groups must either be disjoint or have one as a strict subset of the other), and general non-nested mesoscale structures (where the group assignments of nodes do not have to be nested in any way). To perform statistical inference and thereby identify core-periphery structure, we use a Markov-chain Monte Carlo (MCMC) approach. We illustrate our method for detecting hierarchical core-periphery structure in two real-world temporal networks, and we briefly discuss the structures that we identify in these networks.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments</title>
<link>https://arxiv.org/abs/2506.10120</link>
<guid>https://arxiv.org/abs/2506.10120</guid>
<content:encoded><![CDATA[
<div> Graph-based Active Learning, dynamic environments, GRAIL framework, evaluation metrics, real-world sensor data <br />
<br />
Summary: <br />
Graph-based Active Learning (AL) leverages graph structure to prioritize label queries in dynamic environments. The GRAIL framework introduces novel metrics for evaluating AL strategies, considering sustained effectiveness, diversity, and user burden. Experiments on real-life sensor data highlight trade-offs between prediction performance and user burden, emphasizing the importance of balancing node importance, query diversity, and network topology in AL methods. GRAIL provides a comprehensive evaluation mechanism for graph AL solutions in dynamic environments. <div>
arXiv:2506.10120v1 Announce Type: cross 
Abstract: Graph-based Active Learning (AL) leverages the structure of graphs to efficiently prioritize label queries, reducing labeling costs and user burden in applications like health monitoring, human behavior analysis, and sensor networks. By identifying strategically positioned nodes, graph AL minimizes data collection demands while maintaining model performance, making it a valuable tool for dynamic environments. Despite its potential, existing graph AL methods are often evaluated on static graph datasets and primarily focus on prediction accuracy, neglecting user-centric considerations such as sampling diversity, query fairness, and adaptability to dynamic settings. To bridge this gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate graph AL strategies in dynamic, real-world environments. GRAIL introduces novel metrics to assess sustained effectiveness, diversity, and user burden, enabling a comprehensive evaluation of AL methods under varying conditions. Extensive experiments on datasets featuring dynamic, real-life human sensor data reveal trade-offs between prediction performance and user burden, highlighting limitations in existing AL strategies. GRAIL demonstrates the importance of balancing node importance, query diversity, and network topology, providing an evaluation mechanism for graph AL solutions in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing name generator designs in rural panel studies: analyzing alter retention and change</title>
<link>https://arxiv.org/abs/2506.10136</link>
<guid>https://arxiv.org/abs/2506.10136</guid>
<content:encoded><![CDATA[
<div> Keywords: personal network study, rural community, relational attributes, network stability, design considerations

Summary: 
- A two-wave personal network study was conducted in a rural Romanian community with 68 participants.
- Two name generators were used in the study - a fixed-choice generator focusing on emotional closeness and a free-choice generator based on frequent interaction.
- Participants were interviewed using both generators to compare tie characteristics and assess retention across waves.
- The study found that alters who were kin, co-residents, or emotionally close were more likely to be retained in personal networks, regardless of the generator type used.
- These findings highlight the importance of relational attributes in personal network stability and suggest design considerations for conducting network studies in resource-limited and culturally distinct settings.

<br /><br />Summary: 
A personal network study in a rural Romanian community involving 68 participants utilized two name generators to assess tie characteristics and retention across waves. The study found that kin, co-residents, and emotionally close alters were more likely to be retained in personal networks, regardless of the generator type used. These findings emphasize the significance of relational attributes in personal network stability and provide insights for designing network studies in resource-limited and culturally distinct settings. <div>
arXiv:2506.10136v1 Announce Type: cross 
Abstract: We conducted a two-wave personal network study in a rural Romanian community, interviewing the same participants (n = 68) using two name generators. Wave 1 employed a fixed-choice generator (n = 25) focused on emotional closeness; Wave 2 used a free-choice generator based on frequent interaction. We compared tie characteristics and assessed retention across waves. Alters who were kin, co-residents, or emotionally close were more likely to be retained, regardless of generator type. These findings underscore the role of relational attributes in personal network stability and highlight design considerations for network studies in resource-limited, culturally distinct settings.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast nonparametric inference of network backbones for weighted graph sparsification</title>
<link>https://arxiv.org/abs/2409.06417</link>
<guid>https://arxiv.org/abs/2409.06417</guid>
<content:encoded><![CDATA[
<div> Keywords: Network backbones, Weighted networks, Minimum Description Length, Bayesian model, Greedy algorithm

Summary:
Network backbones provide sparse representations of weighted networks, simplifying computations and visualizations. Existing methods often rely on user-specified parameters or impose restrictions on the backbone structure. A new nonparametric framework based on the Minimum Description Length principle automatically selects the optimal set of edges for the backbone without these limitations. Objective functions evaluate edge importance globally and locally, accommodating various weight distributions under Bayesian model specifications. An efficient greedy algorithm minimizes these objectives, maintaining network connectivity, weight heterogeneity, and spreading dynamics while removing a significant portion of edges. Empirical comparisons on real and synthetic networks highlight the effectiveness of both global and local backboning methods. The runtime complexity of the algorithm is log-linear in the number of edges.<br /><br />Summary: <div>
arXiv:2409.06417v3 Announce Type: replace 
Abstract: Network backbones provide useful sparse representations of weighted networks by keeping only their most important links, permitting a range of computational speedups and simplifying network visualizations. A key limitation of existing network backboning methods is that they either require the specification of a free parameter (e.g. significance level) that determines the number of edges to keep in the backbone, or impose specific restrictions on the topology of the backbone (e.g. that it is a spanning tree). Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations and automatically selects the optimal set of edges to retain using the Minimum Description Length (MDL) principle. We develop objective functions for global and local network backboning which evaluate the importance of an edge in the context of the whole network and individual node neighborhoods respectively and are generalizable to any weight distribution under Bayesian model specifications that fix the average edge weight either exactly or in expectation. We then construct an efficient and provably optimal greedy algorithm to identify the backbone minimizing our objectives, whose runtime complexity is log-linear in the number of edges. We demonstrate our methods by comparing them with existing methods in a range of tasks on real and synthetic networks, finding that both the global and local backboning methods can preserve network connectivity, weight heterogeneity, and spreading dynamics while removing a substantial fraction of edges.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alice and the Caterpillar: A more descriptive null model for assessing data mining results</title>
<link>https://arxiv.org/abs/2506.09764</link>
<guid>https://arxiv.org/abs/2506.09764</guid>
<content:encoded><![CDATA[
<div> Bipartite Joint Degree Matrix, caterpillars, Markov chain Monte Carlo, null models, statistical hypothesis testing <br />
Summary:
The article introduces novel null models for evaluating binary transactional and sequence data using statistical hypothesis testing. These new null models maintain more properties of the observed dataset, particularly preserving the Bipartite Joint Degree Matrix of the corresponding bipartite graph. The introduced algorithms, collectively called Alice, utilize Markov chain Monte Carlo methods to sample datasets from the new null models efficiently. Experimental results demonstrate that Alice mixes quickly and is scalable, outperforming existing models. The null model proposed in the study reveals different significant results compared to those in existing literature, showcasing its potential for uncovering new insights from binary datasets. <br /><br />Summary: <div>
arXiv:2506.09764v1 Announce Type: new 
Abstract: We introduce novel null models for assessing the results obtained from observed binary transactional and sequence datasets, using statistical hypothesis testing. Our null models maintain more properties of the observed dataset than existing ones. Specifically, they preserve the Bipartite Joint Degree Matrix of the bipartite (multi-)graph corresponding to the dataset, which ensures that the number of caterpillars, i.e., paths of length three, is preserved, in addition to other properties considered by other models. We describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling datasets from our null models, based on a carefully defined set of states and efficient operations to move between them. The results of our experimental evaluation show that Alice mixes fast and scales well, and that our null model finds different significant results than ones previously considered in the literature.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELRUHNA: Elimination Rule-basedHypergraph Alignment</title>
<link>https://arxiv.org/abs/2506.09866</link>
<guid>https://arxiv.org/abs/2506.09866</guid>
<content:encoded><![CDATA[
<div> framework, unsupervised, hypergraph alignment, binary quadratic optimization, similarity propagation <br />
<br />
The paper introduces ELRUHNA, an elimination rule-based framework for unsupervised hypergraph alignment using a bipartite representation. It proposes an incidence alignment formulation, a binary quadratic optimization method for aligning vertices and hyperedges. ELRUHNA utilizes a novel similarity propagation scheme with local matching and cooling rules, supported by an initialization strategy based on generalized eigenvector centrality. Extensive experiments on real-world datasets show that ELRUHNA outperforms state-of-the-art algorithms in alignment accuracy while efficiently scaling to large hypergraphs. The approach addresses the NP-hard nature of hypergraph alignment and provides a practical solution for discovering patterns and correspondence in high-order relational data.<br /><br />Summary: <div>
arXiv:2506.09866v1 Announce Type: new 
Abstract: Hypergraph alignment is a well-known NP-hard problem with numerous practical applications across domains such as bioinformatics, social network analysis, and computer vision. Despite its computational complexity, practical and scalable solutions are urgently needed to enable pattern discovery and entity correspondence in high-order relational data. The problem remains understudied in contrast to its graph based counterpart. In this paper, we propose ELRUHNA, an elimination rule-based framework for unsupervised hypergraph alignment that operates on the bipartite representation of hypergraphs. We introduce the incidence alignment formulation, a binary quadratic optimization approach that jointly aligns vertices and hyperedges. ELRUHNA employs a novel similarity propagation scheme using local matching and cooling rules, supported by an initialization strategy based on generalized eigenvector centrality for incidence matrices. Through extensive experiments on real-world datasets, we demonstrate that ELRUHNA achieves higher alignment accuracy compared to state-of-the-art algorithms, while scaling effectively to large hypergraphs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation</title>
<link>https://arxiv.org/abs/2506.09221</link>
<guid>https://arxiv.org/abs/2506.09221</guid>
<content:encoded><![CDATA[
<div> Keywords: online misinformation, crowdsourcing, human judgment, cognitive biases, automated fact-checking systems

Summary: 
This thesis explores the challenges posed by online misinformation and the potential of harnessing human intelligence through crowdsourcing. It focuses on three key areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, the research identifies factors that influence human judgments and introduces a model for predicting and explaining truthfulness. The findings indicate that non-expert judgments can align with expert assessments, especially when considering factors such as timing and experience. By increasing our understanding of human judgment and bias in truthfulness assessment, this research paves the way for the development of more transparent, trustworthy, and interpretable systems for combating misinformation. <div>
arXiv:2506.09221v1 Announce Type: cross 
Abstract: The spread of online misinformation poses serious threats to democratic societies. Traditionally, expert fact-checkers verify the truthfulness of information through investigative processes. However, the volume and immediacy of online content present major scalability challenges. Crowdsourcing offers a promising alternative by leveraging non-expert judgments, but it introduces concerns about bias, accuracy, and interpretability. This thesis investigates how human intelligence can be harnessed to assess the truthfulness of online information, focusing on three areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, it identifies key factors influencing human judgments and introduces a model for the joint prediction and explanation of truthfulness. The findings show that non-expert judgments often align with expert assessments, particularly when factors such as timing and experience are considered. By deepening our understanding of human judgment and bias in truthfulness assessment, this thesis contributes to the development of more transparent, trustworthy, and interpretable systems for combating misinformation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't be Afraid of Cell Complexes! An Introduction from an Applied Perspective</title>
<link>https://arxiv.org/abs/2506.09726</link>
<guid>https://arxiv.org/abs/2506.09726</guid>
<content:encoded><![CDATA[
<div> topology, signal processing, cell complexes, network science, algebra

Summary: 
In this paper, a simplified definition of cell complexes is presented, making the concept more accessible to a wider audience and applicable in practical settings. The introduction of abstract regular cell complexes (ARCCs) eliminates the need for abstract topological concepts, replacing them with algebraic notions, making it easier to understand and apply in signal processing and network science. This new definition bridges the gap between abstract topology and signal processing methods, providing a more straightforward approach to working with cell complexes. The paper also offers a simplified explanation of cell complexes, focusing on dimensions 2 and below, which are commonly used in practical applications. This simplification enhances the understanding and usability of cell complexes in various fields, making them more accessible for researchers and practitioners alike. <br /><br /> <div>
arXiv:2506.09726v1 Announce Type: cross 
Abstract: Cell complexes (CCs) are a higher-order network model deeply rooted in algebraic topology that has gained interest in signal processing and network science recently. However, while the processing of signals supported on CCs can be described in terms of easily-accessible algebraic or combinatorial notions, the commonly presented definition of CCs is grounded in abstract concepts from topology and remains disconnected from the signal processing methods developed for CCs. In this paper, we aim to bridge this gap by providing a simplified definition of CCs that is accessible to a wider audience and can be used in practical applications. Specifically, we first introduce a simplified notion of abstract regular cell complexes (ARCCs). These ARCCs only rely on notions from algebra and can be shown to be equivalent to regular cell complexes for most practical applications. Second, using this new definition we provide an accessible introduction to (abstract) cell complexes from a perspective of network science and signal processing. Furthermore, as many practical applications work with CCs of dimension 2 and below, we provide an even simpler definition for this case that significantly simplifies understanding and working with CCs in practice.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse</title>
<link>https://arxiv.org/abs/2506.09947</link>
<guid>https://arxiv.org/abs/2506.09947</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, extremism, AI, monitoring, German online data 

Summary: 
KI4Demokratie is an AI-based platform designed to assist in monitoring right-wing discourse that threatens democratic values on social media. By utilizing machine learning models on large-scale German online data, the platform provides a comprehensive overview of trends in the German digital sphere. The early analysis demonstrates the complexity of tracking organized extremist behavior and highlights the potential of the integrated approach, particularly during significant events. This tool is crucial for journalists, researchers, and policymakers seeking to combat the rapid spread of antidemocratic narratives without infringing on freedom of expression. With social media increasingly fueling extremism, KI4Demokratie serves as a valuable tool in addressing the challenges posed by right-wing extremism and ensuring the preservation of democratic values in the digital landscape. 

<br /><br />Summary: <div>
arXiv:2506.09947v1 Announce Type: cross 
Abstract: Social media increasingly fuel extremism, especially right-wing extremism, and enable the rapid spread of antidemocratic narratives. Although AI and data science are often leveraged to manipulate political opinion, there is a critical need for tools that support effective monitoring without infringing on freedom of expression. We present KI4Demokratie, an AI-based platform that assists journalists, researchers, and policymakers in monitoring right-wing discourse that may undermine democratic values. KI4Demokratie applies machine learning models to a large-scale German online data gathered on a daily basis, providing a comprehensive view of trends in the German digital sphere. Early analysis reveals both the complexity of tracking organized extremist behavior and the promise of our integrated approach, especially during key events.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The increasing fragmentation of global science limits the diffusion of ideas</title>
<link>https://arxiv.org/abs/2404.05861</link>
<guid>https://arxiv.org/abs/2404.05861</guid>
<content:encoded><![CDATA[
<div> Keywords: global recognition network, national citation preferences, scientific communities, international diffusion, equitable recognition 

Summary: 
The study introduces a rank-based measure of national citation preferences to construct a global recognition network, revealing uneven recognition between countries. Economic, cultural, and scientific variables influence national citation preferences, leading to a fragmented international scientific system. Multiple scientific communities exhibit strong internal citation preferences but negative preferences between them, highlighting growing fragmentation. A weighted logistic regression model demonstrates the impact of this network on the international diffusion of scientific ideas. The findings emphasize structural barriers to equitable recognition and the role of scientific community membership in shaping influence, providing valuable insights for policymakers seeking to nurture inclusive and impactful global science. 

Summary: <div>
arXiv:2404.05861v2 Announce Type: replace 
Abstract: Global science is often portrayed as a unified system of shared knowledge and open exchange. Yet this vision contrasts with emerging evidence that scientific recognition is uneven and increasingly fragmented along regional and cultural lines. Traditional models emphasize Western dominance in knowledge production but overlook regional dynamics, reinforcing a core-periphery narrative that sustains disparities and marginalizes less prominent countries. In this study, we introduce a rank-based signed measure of national citation preferences, enabling the construction of a global recognition network that distinguishes over- and under-recognition between countries. Using a multinomial logistic link prediction model, we assess how economic, cultural, and scientific variables shape the presence and direction of national citation preferences. We uncover a global structure composed of multiple scientific communities, characterized by strong internal citation preferences and negative preferences between them-revealing growing fragmentation in the international scientific system. A separate weighted logistic regression framework suggests that this network significantly influences the international diffusion of scientific ideas, even after controlling for common covariates. Together, these findings highlight the structural barriers to equitable recognition and underscore the importance of scientific community membership in shaping influence, offering valuable insights for policymakers aiming to foster inclusive and impactful global science.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervision policies can shape long-term risk management in general-purpose AI models</title>
<link>https://arxiv.org/abs/2501.06137</link>
<guid>https://arxiv.org/abs/2501.06137</guid>
<content:encoded><![CDATA[
<div> Keywords: General-Purpose AI, risk reporting ecosystems, supervision policies, AI risks, ChatGPT interactions

Summary: 
The article explores the challenges posed by the rapid deployment of General-Purpose AI models, focusing on large language models. It suggests that AI supervisory entities will face difficulties in managing the influx of risk and incident reports in the AI ecosystem. The study develops a simulation framework based on features extracted from various risk reporting systems and evaluates four supervision policies. Results show that prioritizing high-risk incidents and maintaining diversity in addressing risks are more effective but may overlook systemic issues reported by the broader community. The study validates its findings using real-world data, emphasizing the complex trade-offs involved in AI risk supervision. Ultimately, the choice of risk management policies can significantly impact the perception and management of AI risks across different AI models used in society. 

<br /><br />Summary: <div>
arXiv:2501.06137v2 Announce Type: replace-cross 
Abstract: The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Agent Interaction in Synthetic Social Networks: A Framework for Studying Online Polarization</title>
<link>https://arxiv.org/abs/2502.01340</link>
<guid>https://arxiv.org/abs/2502.01340</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, polarization mechanisms, opinion dynamics, linguistic realism, artificial agents<br />
Summary:<br />
This paper presents a new framework that combines mathematical models with language-based simulations to study online polarization in social networks. By embedding opinion dynamics principles within artificial agents, the framework allows for rigorous mathematical analysis while maintaining naturalistic social interactions. Through offline testing and experiments with human participants in controlled environments, the framework was validated to investigate how polarized discussions impact user perceptions and behavior. The results showed that exposure to polarized content increased sensitivity to emotions and group affiliations, while reducing uncertainty in agents' positions. The methodology bridges the gap between theoretical models and empirical observations, providing insights into the causal mechanisms driving online opinion dynamics. This innovative approach offers opportunities to study social media phenomena through controlled experimentation, opening new avenues for research in understanding online polarization. <br /><br />Summary: <div>
arXiv:2502.01340v3 Announce Type: replace-cross 
Abstract: Online social networks have dramatically altered the landscape of public discourse, creating both opportunities for enhanced civic participation and risks of deepening social divisions. Prevalent approaches to studying online polarization have been limited by a methodological disconnect: mathematical models excel at formal analysis but lack linguistic realism, while language model-based simulations capture natural discourse but often sacrifice analytical precision. This paper introduces an innovative computational framework that synthesizes these approaches by embedding formal opinion dynamics principles within LLM-based artificial agents, enabling both rigorous mathematical analysis and naturalistic social interactions. We validate our framework through comprehensive offline testing and experimental evaluation with 122 human participants engaging in a controlled social network environment. The results demonstrate our ability to systematically investigate polarization mechanisms while preserving ecological validity. Our findings reveal how polarized environments shape user perceptions and behavior: participants exposed to polarized discussions showed markedly increased sensitivity to emotional content and group affiliations, while perceiving reduced uncertainty in the agents' positions. By combining mathematical precision with natural language capabilities, our framework opens new avenues for investigating social media phenomena through controlled experimentation. This methodological advancement allows researchers to bridge the gap between theoretical models and empirical observations, offering unprecedented opportunities to study the causal mechanisms underlying online opinion dynamics.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic block models with many communities and the Kesten--Stigum bound</title>
<link>https://arxiv.org/abs/2503.03047</link>
<guid>https://arxiv.org/abs/2503.03047</guid>
<content:encoded><![CDATA[
<div> community recovery, stochastic block models, low-degree hardness, graphon estimation, non-backtracking walks

Summary:
- The study explores community inference in stochastic block models with a growing number of communities.
- Efficient recovery of communities is achievable above the Kesten-Stigum bound in block models.
- Recovery of block models becomes low-degree hard when the number of communities is below the Kesten-Stigum bound with qn.
- An efficient algorithm based on non-backtracking walks allows for recovery even below the Kesten-Stigum bound when qn.
- A new threshold for efficient community recovery in the regime of increasing number of communities is identified.
- Detection of communities is easy, and the information-theoretic threshold for community recovery is determined as the number of communities q approaches infinity.
- The low-degree hardness results have implications for graphon estimation, improving upon previous research by Luo and Gao (2024). 

Summary:<br />
The study delves into community recovery in growing stochastic block models, revealing that efficient recovery is possible above the Kesten-Stigum bound and becomes challenging below it with qn. Surprisingly, an efficient algorithm using non-backtracking walks enables recovery even below the bound when qn, indicating a new threshold for efficient recovery in this regime. Community detection is straightforward, with the information-theoretic threshold identified as q approaches infinity. These results also positively impact graphon estimation, enhancing previous work by Luo and Gao (2024). <div>
arXiv:2503.03047v2 Announce Type: replace-cross 
Abstract: We study the inference of communities in stochastic block models with a growing number of communities. For block models with $n$ vertices and a fixed number of communities $q$, it was predicted in Decelle et al. (2011) that there are computationally efficient algorithms for recovering the communities above the Kesten--Stigum (KS) bound and that efficient recovery is impossible below the KS bound. This conjecture has since stimulated a lot of interest, with the achievability side proven in a line of research that culminated in the work of Abbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025) provides evidence for the hardness part using the low-degree paradigm.
  In this paper we investigate community recovery in the regime $q=q_n \to \infty$ as $n\to\infty$ where no such predictions exist. We show that efficient inference of communities remains possible above the KS bound. Furthermore, we show that recovery of block models is low-degree hard below the KS bound when the number of communities satisfies $q\ll \sqrt{n}$. Perhaps surprisingly, we find that when $q \gg \sqrt{n}$, there is an efficient algorithm based on non-backtracking walks for recovery even below the KS bound. We identify a new threshold and ask if it is the threshold for efficient recovery in this regime. Finally, we show that detection is easy and identify (up to a constant) the information-theoretic threshold for community recovery as the number of communities $q$ diverges.
  Our low-degree hardness results also naturally have consequences for graphon estimation, improving results of Luo and Gao (2024).
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph signal aware decomposition of dynamic networks via latent graphs</title>
<link>https://arxiv.org/abs/2506.08519</link>
<guid>https://arxiv.org/abs/2506.08519</guid>
<content:encoded><![CDATA[
<div> latent factor analysis, network dynamics, tensor decomposition, graph topology, missing data recovery

Summary:
The article introduces a novel approach for analyzing dynamics on networks by utilizing low-rank tensor decomposition to uncover underlying driving factors of network evolution. The proposed method combines topology and node signals to capture the joint evolution of the network structure and associated signals. By estimating latent adjacency matrices and their temporal scaling signatures through alternating minimization, the approach demonstrates superior performance in reconstructing missing network data, particularly when observations are limited. The two-way decomposition technique enhances interpretability and captures the coupling between topology and signals, outperforming standard tensor-based decompositions and signal-based topology identification methods. The numerical results support the effectiveness of the method in recovering expressive latent graphs and showcasing the potential for addressing practical constraints and privacy concerns in socio-technological systems. 

<br /><br />Summary: <div>
arXiv:2506.08519v1 Announce Type: cross 
Abstract: Dynamics on and of networks refer to changes in topology and node-associated signals, respectively and are pervasive in many socio-technological systems, including social, biological, and infrastructure networks. Due to practical constraints, privacy concerns, or malfunctions, we often observe only a fraction of the topological evolution and associated signal, which not only hinders downstream tasks but also restricts our analysis of network evolution. Such aspects could be mitigated by moving our attention at the underlying latent driving factors of the network evolution, which can be naturally uncovered via low-rank tensor decomposition. Tensor-based methods provide a powerful means of uncovering the underlying factors of network evolution through low-rank decompositions. However, the extracted embeddings typically lack a relational structure and are obtained independently from the node signals. This disconnect reduces the interpretability of the embeddings and overlooks the coupling between topology and signals. To address these limitations, we propose a novel two-way decomposition to represent a dynamic graph topology, where the structural evolution is captured by a linear combination of latent graph adjacency matrices reflecting the overall joint evolution of both the topology and the signal. Using spatio-temporal data, we estimate the latent adjacency matrices and their temporal scaling signatures via alternating minimization, and prove that our approach converges to a stationary point. Numerical results show that the proposed method recovers individually and collectively expressive latent graphs, outperforming both standard tensor-based decompositions and signal-based topology identification methods in reconstructing the missing network especially when observations are limited.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning</title>
<link>https://arxiv.org/abs/2304.09914</link>
<guid>https://arxiv.org/abs/2304.09914</guid>
<content:encoded><![CDATA[
<div> deep-learning, affective nonverbal communication, political leaders, YouTube videos, populist rhetoric

Summary:
This study utilizes deep-learning methods to analyze facial expressions in YouTube videos of political leaders from 15 countries. Emotions such as anger, disgust, fear, happiness, sadness, surprise, and neutrality are measured and compared. The research shows a moderate agreement between deep-learning and human label emotions. Significant differences in negative emotion scores are found among leaders exhibiting varying levels of populist rhetoric. <div>
arXiv:2304.09914v5 Announce Type: replace-cross 
Abstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branch-and-cut algorithms for colorful components problems</title>
<link>https://arxiv.org/abs/2408.16508</link>
<guid>https://arxiv.org/abs/2408.16508</guid>
<content:encoded><![CDATA[
<div> community detection, cybersecurity, bioinformatics, partitioning, colorful connected components

Summary:
The article addresses optimization problems involving the partitioning of colored graphs into colorful connected components. These components are defined as having each color appear at most once. Different objective functions determine the best partition. The problems have practical applications in community detection, cybersecurity, and bioinformatics. The authors present integer non-linear formulations, linearized using standard techniques. They develop exact branch-and-cut algorithms for solving the formulations, incorporating various improvement techniques like valid inequalities and warm-start methods. Computational tests show the algorithms are effective for reasonably sized instances. This work introduces the first exact algorithm for solving these specific optimization problems. <div>
arXiv:2408.16508v2 Announce Type: replace-cross 
Abstract: We tackle three optimization problems in which a colored graph, where each node is assigned a color, must be partitioned into colorful connected components. A component is defined as colorful if each color appears at most once. The problems differ in the objective function, which determines which partition is the best one. These problems have applications in community detection, cybersecurity, and bioinformatics. We present integer non-linear formulations, which are then linearized using standard techniques. To solve these formulations, we develop exact branch-and-cut algorithms, embedding various improving techniques, such as valid inequalities, bounds limiting the number of variables, and warm-start and preprocessing techniques. Extensive computational tests on benchmark instances demonstrate the effectiveness of the proposed procedures. The branch-and-cut algorithms can solve reasonably sized instances efficiently. To the best of our knowledge, we are the first to propose an exact algorithm for solving these problems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmarking Framework for Network Classification Methods</title>
<link>https://arxiv.org/abs/2506.06513</link>
<guid>https://arxiv.org/abs/2506.06513</guid>
<content:encoded><![CDATA[
<div> Benchmark Dataset, Network Classification, Feature Extraction Techniques, Structural Noise, Classification Performance

Summary: 
The research introduces a benchmark dataset of synthetic networks for testing network classification methods. Different feature extraction techniques including traditional structural measures, Life-Like Network Automata (LLNA), Graph2Vec, Deterministic Tourist Walk (DTW), and Deterministic Tourist Walk with Bifurcation (DTWB) are evaluated. The study incorporates various levels of structural noise to test the resilience of these techniques. Results show that DTWB outperforms other methods in both classifying classes and subclasses, even in noisy conditions. LLNA and DTW also show strong performance, while Graph2Vec falls mid-range in accuracy. Surprisingly, traditional topological measures exhibit the weakest classification performance. The research highlights the importance of robust feature extraction techniques for effective network classification, especially in noisy environments.<br /><br />Summary: <div>
arXiv:2506.06513v1 Announce Type: new 
Abstract: Network classification plays a crucial role in the study of complex systems, impacting fields like biology, sociology, and computer science. In this research, we present an innovative benchmark dataset made up of synthetic networks that are categorized into various classes and subclasses. This dataset is specifically crafted to test the effectiveness and resilience of different network classification methods. To put these methods to the test, we also introduce various types and levels of structural noise. We evaluate five feature extraction techniques: traditional structural measures, Life-Like Network Automata (LLNA), Graph2Vec, Deterministic Tourist Walk (DTW), and its improved version, the Deterministic Tourist Walk with Bifurcation (DTWB). Our experimental results reveal that DTWB surpasses the other methods in classifying both classes and subclasses, even when faced with significant noise. LLNA and DTW also perform well, while Graph2Vec lands somewhere in the middle in terms of accuracy. Interestingly, topological measures, despite their simplicity and common usage, consistently show the weakest classification performance. These findings underscore the necessity of robust feature extraction techniques for effective network classification, particularly in noisy conditions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neighborhood Overlap-Aware High-Order Graph Neural Network for Dynamic Graph Learning</title>
<link>https://arxiv.org/abs/2506.06728</link>
<guid>https://arxiv.org/abs/2506.06728</guid>
<content:encoded><![CDATA[
<div> Dynamic Graph Learning, Temporal Dynamics, Structural Dependencies, Dynamic Graph Neural Networks, Neighborhood Overlap<br />
<br />
Summary: 
The article introduces the Neighborhood Overlap-Aware High-Order Graph Neural Network (NO-HGNN) for dynamic graph learning. NO-HGNN addresses the challenge of effectively modeling temporal dynamics and structural dependencies by incorporating neighborhood overlap into high-order graph neural networks. By computing a correlation score based on the extent of neighborhood overlap to capture complex node interactions, NO-HGNN improves link prediction accuracy. Experimental results on real-world dynamic graphs demonstrate the superior performance of NO-HGNN compared to state-of-the-art approaches in dynamic graph learning. <div>
arXiv:2506.06728v1 Announce Type: new 
Abstract: Dynamic graph learning (DGL) aims to learn informative and temporally-evolving node embeddings to support downstream tasks such as link prediction. A fundamental challenge in DGL lies in effectively modeling both the temporal dynamics and structural dependencies of evolving graph topologies. Recent advances in Dynamic Graph Neural Networks (DGNNs) have obtained remarkable success by leveraging message-passing mechanisms to capture pairwise node interactions. However, these approaches often overlook more complex structural patterns, particularly neighborhood overlap, which can play a critical role in characterizing node interactions. To overcome this limitation, we introduce the Neighborhood Overlap-Aware High-Order Graph Neural Network (NO-HGNN), which is built upon two key innovations: (a) computing a correlation score based on the extent of neighborhood overlap to better capture complex node interactions; and (b) embedding this correlation directly into the message-passing process of high-order graph neural networks in the DGL. Experiments on two real-world dynamic graphs show that NO-HGNN achieves notable improvements in link prediction accuracy, outperforming several state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An $\alpha$-triangle eigenvector centrality of graphs</title>
<link>https://arxiv.org/abs/2506.07026</link>
<guid>https://arxiv.org/abs/2506.07026</guid>
<content:encoded><![CDATA[
<div> centrality, complex networks, eigenvector, graph structure, connectivity <br />
Summary:<br />
The paper introduces a new centrality measure, $\alpha$-triangle eigenvector centrality ($\alpha$TEC), which considers both edge and triangle structures in complex networks. By adjusting the parameter $\alpha, the influence of edges and triangles on centrality scores can be controlled. The centrality scores are computed based on the eigenvector corresponding to the spectral radius of a nonnegative tensor, ensuring unique positive scores for all vertices in connected graphs. Experimental results on synthetic and real-world networks show that $\alpha$TEC effectively identifies the structural positioning of vertices within networks. Increasing $\alpha$ emphasizes the contribution of edges, while decreasing $\alpha$ emphasizes triangles. Additionally, higher $\alpha$TEC rankings indicate a greater impact on network connectivity. The proposed centrality measure provides insights into the importance of vertices in network analysis. <br /> <div>
arXiv:2506.07026v1 Announce Type: new 
Abstract: Centrality represents a fundamental research field in complex network analysis, where centrality measures identify important vertices within networks. Over the years, researchers have developed diverse centrality measures from varied perspectives. This paper proposes an $\alpha$-triangle eigenvector centrality ($\alpha$TEC), which is a global centrality measure based on both edge and triangle structures. It can dynamically adjust the influence of edges and triangles through a parameter $\alpha$ ($\alpha \in (0,1]$). The centrality scores for vertices are defined as the eigenvector corresponding to the spectral radius of a nonnegative tensor. By the Perron-Frobenius theorem, $\alpha$TEC guarantees unique positive centrality scores for all vertices in connected graphs. Numerical experiments on synthetic and real world networks demonstrate that $\alpha$TEC effectively identifies the vertex's structural positioning within graphs. As $\alpha$ increases (decreases), the centrality rankings reflect a stronger (weaker) contribution from edge structure and a weaker (stronger) contribution from triangle structure. Furthermore, we experimentally prove that vertices with higher $\alpha$TEC rankings have a greater impact on network connectivity.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Geometric Embedding for Node Influence Maximization</title>
<link>https://arxiv.org/abs/2506.07435</link>
<guid>https://arxiv.org/abs/2506.07435</guid>
<content:encoded><![CDATA[
<div> Graph layout; Centrality measures; Betweenness; Closeness; Force layout algorithm  
Summary:  
The study introduces an efficient force layout algorithm for embedding graphs into a low-dimensional space, using the radial distance from the origin as a proxy for centrality measures like betweenness and closeness. The algorithm shows strong correlations with degree, PageRank, and other centralities on various graph types. This embedding method enables the identification of high-influence nodes in networks, providing a quicker and scalable alternative to traditional greedy algorithms for centrality computation. The approach offers a computationally less expensive way to compute classical centrality measures and demonstrates promising results on large-scale graphs. Moreover, it can be applied to analyze network structures and identify key nodes efficiently. <div>
arXiv:2506.07435v1 Announce Type: new 
Abstract: Computing classical centrality measures such as betweenness and closeness is computationally expensive on large-scale graphs. In this work, we introduce an efficient force layout algorithm that embeds a graph into a low-dimensional space, where the radial distance from the origin serves as a proxy for various centrality measures. We evaluate our method on multiple graph families and demonstrate strong correlations with degree, PageRank, and paths-based centralities. As an application, it turns out that the proposed embedding allows to find high-influence nodes in a network, and provides a fast and scalable alternative to the standard greedy algorithm.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Powers of Magnetic Graph Matrix: Fourier Spectrum, Walk Compression, and Applications</title>
<link>https://arxiv.org/abs/2506.07343</link>
<guid>https://arxiv.org/abs/2506.07343</guid>
<content:encoded><![CDATA[
<div> Keywords: magnetic graphs, directed networks, matrix powers, walk profiles, link prediction

Summary:<br />
- The study explores the use of magnetic graph theory in analyzing complex directed networks, focusing on local, non-equilibrium behaviors.
- A combinatorial interpretation of magnetic graph matrix powers through directed walk profiles is introduced. This allows for a Fourier transform connection, enabling the reconstruction of walk profiles from matrix powers at discrete potentials.
- The research highlights the compressibility of information captured by the magnetic matrix, showing that an even smaller number of potentials can be used for accurate approximate reconstruction in real networks.
- The ability of magnetic matrix powers to identify frustrated directed cycles, such as feedforward loops, is demonstrated. 
- Powers of the magnetic matrix are shown to be effective in encoding local structural details in directed graphs for link prediction.

<br /><br />Summary: <div>
arXiv:2506.07343v1 Announce Type: cross 
Abstract: Magnetic graphs, originally developed to model quantum systems under magnetic fields, have recently emerged as a powerful framework for analyzing complex directed networks. Existing research has primarily used the spectral properties of the magnetic graph matrix to study global and stationary network features. However, their capacity to model local, non-equilibrium behaviors, often described by matrix powers, remains largely unexplored. We present a novel combinatorial interpretation of the magnetic graph matrix powers through directed walk profiles -- counts of graph walks indexed by the number of edge reversals. Crucially, we establish that walk profiles correspond to a Fourier transform of magnetic matrix powers. The connection allows exact reconstruction of walk profiles from magnetic matrix powers at multiple discrete potentials, and more importantly, an even smaller number of potentials often suffices for accurate approximate reconstruction in real networks. This shows the empirical compressibility of the information captured by the magnetic matrix. This fresh perspective suggests new applications; for example, we illustrate how powers of the magnetic matrix can identify frustrated directed cycles (e.g., feedforward loops) and can be effectively employed for link prediction by encoding local structural details in directed graphs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels</title>
<link>https://arxiv.org/abs/2506.07606</link>
<guid>https://arxiv.org/abs/2506.07606</guid>
<content:encoded><![CDATA[
<div> dataset, user-level stance detection, Bluesky, 2024 U.S. presidential election, Kamala Harris, Donald Trump

Summary:
The article introduces a new dataset called PolitiSky24 for user-level stance detection focused on the 2024 U.S. presidential election, specifically targeting Kamala Harris and Donald Trump. The dataset contains 16,044 user-target stance pairs with additional metadata such as engagement information, interaction graphs, and user posting histories. The dataset was created using advanced information retrieval and large language models, achieving 81% accuracy in stance labeling. This resource fills a gap in political stance analysis by providing a timely, open-data, and user-level perspective. The dataset is openly available for researchers and can be accessed at https://doi.org/10.5281/zenodo.15616911.<br /><br />Summary: <div>
arXiv:2506.07606v1 Announce Type: cross 
Abstract: Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refugees' path to legal stability is long and systematically unequal</title>
<link>https://arxiv.org/abs/2506.07916</link>
<guid>https://arxiv.org/abs/2506.07916</guid>
<content:encoded><![CDATA[
<div> Keywords: legal systems, migrants, refugees, legal journey, integration <br />
Summary: <br />
The study examines the legal pathways of over 350,000 migrants in Austria from 2022 to 2024, focusing on refugees' legal journeys. It finds significant disparities in the time taken for refugees to gain stable status, with Ukrainians taking two months, Syrians nine months, and Afghans up to 20 months. Women, particularly from these regions, are more likely to receive protection compared to Afghan men who wait an average of 30 months. Those who cross borders unofficially have higher exit rates and lower chances of achieving stable legal status. The research highlights that legal integration is a complex and varied process influenced by institutional structures, entry procedures, and unequal timelines. <div>
arXiv:2506.07916v1 Announce Type: cross 
Abstract: Legal systems shape not only the recognition of migrants and refugees but also the pace and stability of their integration. Refugees often shift between multiple legal classifications, a process we refer to as the "legal journey". This journey is frequently prolonged and uncertain. Using a network-based approach, we analyze legal transitions for over 350,000 migrants in Austria (2022 to 2024). Refugees face highly unequal pathways to stability, ranging from two months for Ukrainians to nine months for Syrians and 20 months for Afghans. Women, especially from these regions, are more likely to gain protection; Afghan men wait up to 30 months on average. We also find that those who cross the border without going through official border controls face higher exit rates and lower chances of securing stable status. We show that legal integration is not a uniform process, but one structured by institutional design, procedural entry points, and unequal timelines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Event Detection using Social Media Data Analysis: Applications, Challenges, and Open Problems</title>
<link>https://arxiv.org/abs/2207.03997</link>
<guid>https://arxiv.org/abs/2207.03997</guid>
<content:encoded><![CDATA[
<div> social media, event detection, data analysis, crisis scenarios, research directions

Summary:<br /><br />In this paper, the authors explore the benefits and applications of event detection using social media data analysis. They highlight the importance of monitoring social media for real-world incidents, which can provide valuable information for responding to crises. The study investigates the challenges and tradeoffs inherent in event detection, emphasizing the need for careful analysis of social media streams. The paper also raises key open questions and proposes potential research directions for further exploration in this field. By leveraging the vast amount of social media content available, event detection can offer insights that contribute to improved crisis management and decision-making for individuals and organizations. <div>
arXiv:2207.03997v5 Announce Type: replace 
Abstract: In recent years, social media has become one of the most popular platforms for communication. These platforms allow users to report real-world incidents that might swiftly and widely circulate throughout the whole social network. A social event is a real-world incident that is documented on social media. Social gatherings could contain vital documentation of crisis scenarios. Monitoring and analyzing this rich content can produce information that is extraordinarily valuable and help people and organizations learn how to take action. In this paper, a survey on the potential benefits and applications of event detection with social media data analysis will be presented. Moreover, the critical challenges and the fundamental tradeoffs in event detection will be methodically investigated by monitoring social media stream. Then, fundamental open questions and possible research directions will be introduced.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Analytics in Disaster Response: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2307.04046</link>
<guid>https://arxiv.org/abs/2307.04046</guid>
<content:encoded><![CDATA[
<div> Natural disasters, social media analytics, disaster management, data mining, machine learning<br />
<br />Summary: Social media plays a crucial role in disaster management by providing real-time information for detection, situational awareness, and communication during emergencies. This review paper explores the challenges and opportunities of utilizing social media data, highlighting methodologies like data collection, preprocessing, and analysis using data mining and machine learning techniques. Case studies demonstrate successful applications of social media analytics in disaster response. Ethical considerations and privacy concerns are addressed in the context of using social media data for disaster management. The need for continued research and innovation in social media analytics for disaster management is emphasized for improving response and recovery efforts in the face of increasing natural disasters. <div>
arXiv:2307.04046v2 Announce Type: replace 
Abstract: Social media has emerged as a valuable resource for disaster management, revolutionizing the way emergency response and recovery efforts are conducted during natural disasters. This review paper aims to provide a comprehensive analysis of social media analytics for disaster management. The abstract begins by highlighting the increasing prevalence of natural disasters and the need for effective strategies to mitigate their impact. It then emphasizes the growing influence of social media in disaster situations, discussing its role in disaster detection, situational awareness, and emergency communication. The abstract explores the challenges and opportunities associated with leveraging social media data for disaster management purposes. It examines methodologies and techniques used in social media analytics, including data collection, preprocessing, and analysis, with a focus on data mining and machine learning approaches. The abstract also presents a thorough examination of case studies and best practices that demonstrate the successful application of social media analytics in disaster response and recovery. Ethical considerations and privacy concerns related to the use of social media data in disaster scenarios are addressed. The abstract concludes by identifying future research directions and potential advancements in social media analytics for disaster management. The review paper aims to provide practitioners and researchers with a comprehensive understanding of the current state of social media analytics in disaster management, while highlighting the need for continued research and innovation in this field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Artificial Intelligence for Complex Network: Potential, Methodology and Application</title>
<link>https://arxiv.org/abs/2402.16887</link>
<guid>https://arxiv.org/abs/2402.16887</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, artificial intelligence, statistical mechanics, structures, dynamics

Summary:<br /><br />
Complex networks are prevalent in various real-world systems, demonstrating a transition from disorder to order through intertwined topology and node dynamics. While significant progress has been made in understanding the statistical mechanics, structures, and dynamics of networks, challenges persist in exploring realistic systems and practical applications. The integration of artificial intelligence (AI) technologies with diverse network data presents opportunities to advance complex network research. This survey comprehensively addresses the potential benefits of AI in overcoming research challenges, summarizing key problems and reviewing methodologies and applications. By bridging AI and complex network science, valuable insights are provided to drive further interdisciplinary research progress. <br /><br />Summary: <div>
arXiv:2402.16887v2 Announce Type: replace 
Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of complex network research. It endeavors to summarize the pivotal research problems and provide an exhaustive review of the corresponding methodologies and applications. Through this comprehensive survey-the first of its kind on AI for complex networks-we expect to provide valuable insights that will drive further research and advancement in this interdisciplinary field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2307.12555</link>
<guid>https://arxiv.org/abs/2307.12555</guid>
<content:encoded><![CDATA[
<div> robustness, unsupervised, Graph Contrastive Learning, structural attacks, homophily-driven sanitation view

Summary:
This article explores the adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. Through empirical and theoretical analysis, existing attacks on GCL are examined, revealing their impact on performance. Inspired by these findings, a robust GCL framework, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), is proposed. This framework integrates a homophily-driven sanitation view that addresses the non-differentiable nature of the sanitation objective. Techniques are developed to enable gradient-based end-to-end robust GCL. Additionally, a fully unsupervised hyperparameter tuning method is introduced, eliminating the need for node labels. Extensive experiments demonstrate that GCHS outperforms state-of-the-art baselines in generating high-quality node embeddings. Results also show superior performance on important downstream tasks, showcasing the effectiveness of GCHS against structural attacks on GCL. 

<br /><br />Summary: <div>
arXiv:2307.12555v2 Announce Type: replace-cross 
Abstract: We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.04548</link>
<guid>https://arxiv.org/abs/2406.04548</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, explanation methods, GNNAnatomy, graphlets <br />
Summary:
This paper critiques existing model-level explanation methods for Graph Neural Networks (GNNs) that rely on maximizing classification confidence, assuming a single explanation suffices for an entire class of graphs, and assuming explanations are inherently trustworthy. The authors introduce GNNAnatomy, a distillation-based method that generates explanations by characterizing graph topology using graphlets and training a transparent multilayer perceptron surrogate. This approach identifies the most discriminative topologies and supports exploration of structural diversity within a class through an interface designed for human-AI teaming. GNNAnatomy aims to provide more reliable and interpretable explanations than current methods while fostering transparency and trust in GNN models. Evaluation on synthetic and real-world datasets shows promising results compared to existing explainable GNN methods. <br /> <div>
arXiv:2406.04548v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) achieve outstanding performance across graph-based tasks but remain difficult to interpret. In this paper, we revisit foundational assumptions underlying model-level explanation methods for GNNs, namely: (1) maximizing classification confidence yields representative explanations, (2) a single explanation suffices for an entire class of graphs, and (3) explanations are inherently trustworthy. We identify pitfalls resulting from these assumptions: methods that optimize for classification confidence may overlook partially learned patterns; topological diversity across graph subsets within the same class is often underrepresented; and explanations alone offer limited support for building user trust when applied to new datasets or models. This paper introduces GNNAnatomy, a distillation-based method designed to generate explanations while avoiding these pitfalls. GNNAnatomy first characterizes graph topology using graphlets, a set of fundamental substructures. We then train a transparent multilayer perceptron surrogate to directly approximate GNN predictions based on the graphlet representations. By analyzing the weights assigned to each graphlet, we identify the most discriminative topologies, which serve as GNN explanations. To account for structural diversity within a class, GNNAnatomy generates explanations at the required granularity through an interface that supports human-AI teaming. This interface helps users identify subsets of graphs where distinct critical substructures drive class differentiation, enabling multi-grained explanations. Additionally, by enabling exploration and linking explanations back to input graphs, the interface fosters greater transparency and trust. We evaluate GNNAnatomy on both synthetic and real-world datasets through quantitative metrics and qualitative comparisons with state-of-the-art model-level explainable GNN methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropEnc: A Property Encoder for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.11554</link>
<guid>https://arxiv.org/abs/2409.11554</guid>
<content:encoded><![CDATA[
<div> encoder, node features, graph neural networks, graph metrics, graph classification

Summary:
PropEnc is introduced as a versatile encoder for generating expressive node embeddings from any graph metric. It addresses the lack of node features in real-world systems by offering a flexible solution that supports low-dimensional representations and diverse input types. Using histogram construction and reversed index encoding, PropEnc effectively mitigates sparsity issues and improves computational efficiency. It can replicate one-hot encoding or approximate indices with high accuracy, making it adaptable to a wide range of graph applications. Extensive experiments on graph classification tasks across social networks lacking node features validate the efficiency of PropEnc in constructing node features from various graph metrics. Overall, PropEnc presents a novel approach to enhancing graph machine learning by providing a mechanism for generating node embeddings without relying on traditional node features.
<br /><br />Summary: <div>
arXiv:2409.11554v3 Announce Type: replace-cross 
Abstract: Graph machine learning, particularly using graph neural networks, heavily relies on node features. However, many real-world systems, such as social and biological networks, lack node features due to privacy concerns, incomplete data, or collection limitations. Structural and positional encoding are commonly used to address this but are constrained by the maximum values of the encoded properties, such as the highest node degree. This limitation makes them impractical for scale-free networks and applications involving large or non-categorical properties. This paper introduces PropEnc, a novel and versatile encoder to generate expressive node embedding from any graph metric. By combining histogram construction with reversed index encoding, PropEnc offers a flexible solution that supports low-dimensional representations and diverse input types, effectively mitigating sparsity issues while improving computational efficiency. Additionally, it replicates one-hot encoding or approximates indices with high accuracy, making it adaptable to a wide range of graph applications. We validate PropEnc through extensive experiments on graph classification task across several social networks lacking node features. The empirical results demonstrate that PropEnc offers an efficient mechanism for constructing node features from various graph metrics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering-induced localization of quantum walks on networks</title>
<link>https://arxiv.org/abs/2412.04325</link>
<guid>https://arxiv.org/abs/2412.04325</guid>
<content:encoded><![CDATA[
<div> quantum walks, networks, localization, clustering, eigenvectors <br />
<br />
Summary: 
Quantum walks on networks are important in quantum information theory, with applications in spatial-search, element-distinctness problems, and node centrality analysis. Unlike classical random walks, quantum walks evolve unitarily without converging to a stationary distribution. This study focuses on the long-time behavior of quantum walks on networks and their localization. The research demonstrates the emergence of localization in highly clustered networks constructed by attaching triangles and provides an analytical expression for the long-time inverse participation ratio. Localization is also observed in Kleinberg navigable small-world networks and Holme-Kim power-law cluster networks, showing that local clustering in networks can induce quantum walk localization. The research highlights the impact of network structure on the evolution of quantum walks and sheds light on the role of network clustering in quantum information processing. <br /> <div>
arXiv:2412.04325v2 Announce Type: replace-cross 
Abstract: Quantum walks on networks are a paradigmatic model in quantum information theory. Quantum-walk algorithms have been developed for various applications, including spatial-search problems, element-distinctness problems, and node centrality analysis. Unlike their classical counterparts, the evolution of quantum walks is unitary, so they do not converge to a stationary distribution. However, for many applications, it is important to understand the long-time behavior of quantum walks and the impact of network structure on their evolution. In the present paper, we study the localization of quantum walks on networks. We demonstrate how localization emerges in highly clustered networks that we construct by recursively attaching triangles, and we derive an analytical expression for the long-time inverse participation ratio that depends on products of eigenvectors of the quantum-walk Hamiltonian. Building on the insights from this example, we then show that localization also occurs in Kleinberg navigable small-world networks and Holme--Kim power-law cluster networks. Our results illustrate that local clustering, which is a key structural feature of networks, can induce localization of quantum walks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Response Strategies for GenAI</title>
<link>https://arxiv.org/abs/2502.00729</link>
<guid>https://arxiv.org/abs/2502.00729</guid>
<content:encoded><![CDATA[
<div> Generative AI, Stack Overflow, data generation, selective response, revenue <br />
<br />
Summary: <br />
The article discusses the impact of Generative AI on platforms like Stack Overflow, emphasizing the vital role of human-generated data. It proposes a solution called selective response, where AI intentionally provides inaccurate responses for new topics to drive users back to human forums. This strategy aims to improve data quality for AI systems and enhance user welfare in the long term. The paper presents an algorithmic approach to maximize GenAI's revenue while considering social welfare constraints and outlines regulatory conditions for the implementation of selective response. This innovative strategy offers a potential remedy to the challenges faced by GenAI systems in obtaining high-quality data and highlights the importance of balancing revenue generation with user well-being. <div>
arXiv:2502.00729v2 Announce Type: replace-cross 
Abstract: The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Artificial Benchmark for Community Detection with Outliers and Overlapping Communities (ABCD+$o^2$)</title>
<link>https://arxiv.org/abs/2506.05486</link>
<guid>https://arxiv.org/abs/2506.05486</guid>
<content:encoded><![CDATA[
<div> community detection, artificial benchmark, ABCD graph, power-law distribution, overlapping communities

Summary: 
The article introduces the Artificial Benchmark for Community Detection (ABCD) graph model, which has community structure and power-law distributions for degrees and community sizes. This model, similar to the LFR model but faster and more analytically tractable, offers a new perspective on graph generation. The ABCD model variant, ABCD+$o$, includes outliers, while the ABCD+$o^2$ variant allows for overlapping communities. These variations expand the model's capabilities and offer new insights into community detection algorithms and graph structures. Through the ABCD model and its extensions, researchers can generate random graphs that mimic real-world communities, providing a valuable tool for evaluating and comparing community detection algorithms. <div>
arXiv:2506.05486v1 Announce Type: new 
Abstract: The Artificial Benchmark for Community Detection (ABCD) graph is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model, and its generalization to include outliers (ABCD+$o$), and introduce another variant that allows for overlapping communities, ABCD+$o^2$.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Community-Level Blocklists in Decentralized Social Media</title>
<link>https://arxiv.org/abs/2506.05522</link>
<guid>https://arxiv.org/abs/2506.05522</guid>
<content:encoded><![CDATA[
<div> blocklists, decentralized social media, moderation, Mastodon, content analysis
Summary:
Community-level blocklists play a crucial role in decentralized social media moderation by enabling moderators to prevent interactions with communities acting in bad faith. A study on Mastodon moderators revealed a wide variation in blocklist goals, inclusion criteria, and transparency. Moderators balance proactive safety measures, reactive practices, and caution regarding false positives when utilizing blocklists. Challenges and limitations were identified, prompting suggestions for design improvements such as comment receipts, category filters, and collaborative voting. Trade-offs between openness, safety, and nuance were highlighted, emphasizing the complexity of moderator roles and the need for future design opportunities in decentralized content moderation. <div>
arXiv:2506.05522v1 Announce Type: new 
Abstract: Community-level blocklists are key to content moderation practices in decentralized social media. These blocklists enable moderators to prevent other communities, such as those acting in bad faith, from interacting with their own -- and, if shared publicly, warn others about communities worth blocking. Prior work has examined blocklists in centralized social media, noting their potential for collective moderation outcomes, but has focused on blocklists as individual-level tools. To understand how moderators perceive and utilize community-level blocklists and what additional support they may need, we examine social media communities running Mastodon, an open-source microblogging software built on the ActivityPub protocol. We conducted (1) content analysis of the community-level blocklist ecosystem, and (2) semi-structured interviews with twelve Mastodon moderators. Our content analysis revealed wide variation in blocklist goals, inclusion criteria, and transparency. Interviews showed moderators balance proactive safety, reactive practices, and caution around false positives when using blocklists for moderation. They noted challenges and limitations in current blocklist use, suggesting design improvements like comment receipts, category filters, and collaborative voting. We discuss implications for decentralized content moderation, highlighting trade-offs between openness, safety, and nuance; the complexity of moderator roles; and opportunities for future design.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Coordination on Short-Video Platforms: The Challenge of Multimodality and Complex Similarity on TikTok</title>
<link>https://arxiv.org/abs/2506.05868</link>
<guid>https://arxiv.org/abs/2506.05868</guid>
<content:encoded><![CDATA[
<div> Keywords: online coordinated behaviour, multilayer network analysis, TikTok, political videos, detection

Summary:
This study focuses on detecting coordination on short-video platform TikTok, which poses unique challenges due to its integrated multimedia content. The researchers propose a methodology based on multilayer network analysis to capture coordination across video, audio, and text modalities, specifically addressing the complex similarity found in video and audio content. Testing this approach on political videos from TikTok, the study demonstrates its effectiveness in identifying coordination among users. However, the researchers also highlight potential pitfalls and limitations of the method. Overall, the study contributes to understanding coordinated behavior on platforms like TikTok and emphasizes the importance of considering various modalities in detecting such behaviors. <div>
arXiv:2506.05868v1 Announce Type: new 
Abstract: Research on online coordinated behaviour has predominantly focused on text-based social media platforms, where coordination manifests clearly through the frequent posting of identical hyperlinks or the frequent re-sharing of the same textual content by the same group of users. However, the rise of short-video platforms like TikTok introduces distinct challenges, by supporting integrated multimodality within posts and complex similarity between them. In this paper, we propose an approach to detecting coordination that addresses these characteristic challenges. Our methodology, based on multilayer network analysis, is tailored to capture coordination across multiple modalities, including video, audio, and text, and explicitly handles complex forms of similarity inherent in video and audio content. We test this approach on political videos posted on TikTok and extracted via the TikTok researcher API. This application demonstrates the capacity of the approach to identify coordination, while also critically highlighting potential pitfalls and limitations.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the co-evolution of online engagement with (mis)information and its visibility at scale</title>
<link>https://arxiv.org/abs/2506.06106</link>
<guid>https://arxiv.org/abs/2506.06106</guid>
<content:encoded><![CDATA[
<div> COVID-19 pandemic, online attention, misinformation, engagement, visibility <br />
Summary:<br />
- Online attention is a valuable resource in the digital age, especially during events like the COVID-19 pandemic. <br />
- Users seek credible sources amidst the prevalence of misinformation on online platforms. <br />
- News outlets compete to attract and retain users' attention through engagement and visibility metrics. <br />
- A study using a temporal network modeling framework analyzed over 100 million COVID-related retweets over 3 years. <br />
- Highly engaged sources experience spikes in follower growth during major events, while less credible sources sustain faster growth outside these periods. <br /> <div>
arXiv:2506.06106v1 Announce Type: new 
Abstract: Online attention is an increasingly valuable resource in the digital age, with extraordinary events such as the COVID-19 pandemic fuelling fierce competition around it. As misinformation pervades online platforms, users seek credible sources, while news outlets compete to attract and retain their attention. Here we measure the co-evolution of online "engagement" with (mis)information and its "visibility", where engagement corresponds to user interactions on social media, and visibility to fluctuations in user follower counts. Using a scalable temporal network modelling framework applied to over 100 million COVID-related retweets spanning 3 years, we find that highly engaged sources experience sharp spikes in follower growth during major events (e.g., vaccine rollouts, epidemic severity), whereas sources with more questionable credibility tend to sustain faster growth outside of these periods. Our framework lends itself to studying other large-scale events where online attention is at stake, such as climate and political debates.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Large Language Models Can Increase the Belief Accuracy of Social Networks</title>
<link>https://arxiv.org/abs/2506.06153</link>
<guid>https://arxiv.org/abs/2506.06153</guid>
<content:encoded><![CDATA[
<div> personalized LLMs, belief accuracy, social networks, misinformation, corrective agents
Summary:
- The study explores the impact of personalized Large Language Models (LLMs) on belief accuracy in social networks during the 2024 US presidential election.
- Personalized LLMs lead individuals to update their beliefs towards the truth.
- Individuals with personalized LLMs in their social network choose to follow them and include others with more accurate beliefs in their social networks.
- The study demonstrates that LLMs have the potential to influence individual beliefs and social networks.
- It suggests that LLMs can act as corrective agents in online environments.
<br /><br />Summary: <div>
arXiv:2506.06153v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly involved in shaping public understanding on contested issues. This has led to substantial discussion about the potential of LLMs to reinforce or correct misperceptions. While existing literature documents the impact of LLMs on individuals' beliefs, limited work explores how LLMs affect social networks. We address this gap with a pre-registered experiment (N = 1265) around the 2024 US presidential election, where we empirically explore the impact of personalized LLMs on belief accuracy in the context of social networks. The LLMs are constructed to be personalized, offering messages tailored to individuals' profiles, and to have guardrails for accurate information retrieval. We find that the presence of a personalized LLM leads individuals to update their beliefs towards the truth. More importantly, individuals with a personalized LLM in their social network not only choose to follow it, indicating they would like to obtain information from it in subsequent interactions, but also construct subsequent social networks to include other individuals with beliefs similar to the LLM -- in this case, more accurate beliefs. Therefore, our results show that LLMs have the capacity to influence individual beliefs and the social networks in which people exist, and highlight the potential of LLMs to act as corrective agents in online environments. Our findings can inform future strategies for responsible AI-mediated communication.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Language Models are Good Heterogeneous Graph Generalizers</title>
<link>https://arxiv.org/abs/2506.06157</link>
<guid>https://arxiv.org/abs/2506.06157</guid>
<content:encoded><![CDATA[
<div> Keywords: Heterogeneous graph neural networks, large language models, masked language modeling, structural and semantic information, generalization performance <br />
Summary: 
Heterogeneous graph neural networks (HGNNs) in heterogeneous graphs struggle with generalization across domains and tasks. Integrating HGNNs with large language models (LLMs) has been proposed for better generalizable graph learning, but disparities in embedding spaces limit effectiveness. The MLM4HG method introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information from HGs. It utilizes customized textual templates in a cloze-style "mask" token prediction paradigm to unify different graph tasks. By converting HGs from various domains into textual formats based on metapaths, MLM4HG enables fine-tuning of pretrained language models with a constrained vocabulary, enhancing generalization to unseen target HGs. Experimental results on real-world datasets demonstrate the superior generalization performance of MLM4HG in both few-shot and zero-shot scenarios compared to existing methods. <br /><br />Summary: <div>
arXiv:2506.06157v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. Recently, some researchers have turned to integrating HGNNs with large language models (LLMs) for more generalizable heterogeneous graph learning. However, these approaches typically extract structural information via HGNNs as HG tokens, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension of HGs. Moreover, as these HG tokens are often derived from node-level tasks, the model's ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style "mask" token prediction paradigm. Specifically, MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at https://github.com/BUPT-GAMMA/MLM4HG.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport</title>
<link>https://arxiv.org/abs/2506.02619</link>
<guid>https://arxiv.org/abs/2506.02619</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Heterogeneous Graphs, Self-Supervised Learning, Optimal Transport<br />
<br />
Summary:<br />
Heterogeneous Graph Neural Networks (HGNNs) have shown promise in processing heterogeneous information networks. A novel approach, called HGOT, integrates optimal transport to facilitate self-supervised learning without requiring graph augmentation strategies. Instead of manual selection of positive and negative samples, HGOT uses an optimal transport mechanism to align node representations with the graph space. By aggregating information from different meta-paths and using optimal transport plans, HGOT achieves higher-quality node representations. Experimental results on real-world datasets demonstrate that HGOT outperforms state-of-the-art methods in various downstream tasks, particularly node classification, where it achieves over 6% improvement in accuracy. <div>
arXiv:2506.02619v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent capabilities in processing heterogeneous information networks. Self-supervised learning on heterogeneous graphs, especially contrastive self-supervised strategy, shows great potential when there are no labels. However, this approach requires the use of carefully designed graph augmentation strategies and the selection of positive and negative samples. Determining the exact level of similarity between sample pairs is non-trivial.To solve this problem, we propose a novel self-supervised Heterogeneous graph neural network with Optimal Transport (HGOT) method which is designed to facilitate self-supervised learning for heterogeneous graphs without graph augmentation strategies. Different from traditional contrastive self-supervised learning, HGOT employs the optimal transport mechanism to relieve the laborious sampling process of positive and negative samples. Specifically, we design an aggregating view (central view) to integrate the semantic information contained in the views represented by different meta-paths (branch views). Then, we introduce an optimal transport plan to identify the transport relationship between the semantics contained in the branch view and the central view. This allows the optimal transport plan between graphs to align with the representations, forcing the encoder to learn node representations that are more similar to the graph space and of higher quality. Extensive experiments on four real-world datasets demonstrate that our proposed HGOT model can achieve state-of-the-art performance on various downstream tasks. In particular, in the node classification task, HGOT achieves an average of more than 6% improvement in accuracy compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Misinformation in the Arab World: Challenges &amp; Opportunities</title>
<link>https://arxiv.org/abs/2506.05582</link>
<guid>https://arxiv.org/abs/2506.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, disinformation, Arab region, fact-checking organizations, information ecosystem <br />
Summary: <br />
Misinformation and disinformation present significant challenges globally, with particular vulnerabilities in the Arab region due to geopolitical instabilities and cultural diversity. In combating this issue, it is essential to focus on detection, tracking, mitigation, and community engagement. By collaborating with grassroots fact-checking organizations and promoting social correction, the Arab world can build a more resilient information ecosystem. Understanding cultural norms and fostering strong collaborative networks are also crucial in addressing misinformation. By prioritizing these strategies, opportunities can be created for a more informed and empowered society in the Arab region. <br /> <div>
arXiv:2506.05582v1 Announce Type: cross 
Abstract: Misinformation and disinformation pose significant risks globally, with the Arab region facing unique vulnerabilities due to geopolitical instabilities, linguistic diversity, and cultural nuances. We explore these challenges through the key facets of combating misinformation: detection, tracking, mitigation and community-engagement. We shed light on how connecting with grass-roots fact-checking organizations, understanding cultural norms, promoting social correction, and creating strong collaborative information networks can create opportunities for a more resilient information ecosystem in the Arab world.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoding meets Persistent Homology on Graphs</title>
<link>https://arxiv.org/abs/2506.05814</link>
<guid>https://arxiv.org/abs/2506.05814</guid>
<content:encoded><![CDATA[
<div> Graph neural networks (GNNs), which have a local inductive bias, can struggle to utilize key structural information. Two approaches, Positional Encoding (PE) and Persistent Homology (PH), aim to address this issue. A study comparing PE and PH found neither to be more expressive than the other. A novel method called PiPE (Persistence-informed Positional Encoding) was developed, which is proven to be more expressive than both PE and PH. PiPE showed strong performance in various tasks such as molecule property prediction, graph classification, and out-of-distribution generalization. The study results provide insights for advancing graph representation learning. 

Keywords: GNNs, Positional Encoding, Persistent Homology, Graph representation learning, PiPE <br /><br />Summary: Graph neural networks have a local inductive bias, making it challenging to leverage key structural information. Positional Encoding (PE) and Persistent Homology (PH) have emerged as solutions, with a study showing neither approach is more expressive. A novel method called PiPE (Persistence-informed Positional Encoding) was developed, proving to be more expressive than both PE and PH. PiPE demonstrated strong performance in various tasks, advancing the field of graph representation learning. <div>
arXiv:2506.05814v1 Announce Type: cross 
Abstract: The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction, graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Generalization with Gaussian Mixture Model Based Augmentation</title>
<link>https://arxiv.org/abs/2411.08638</link>
<guid>https://arxiv.org/abs/2411.08638</guid>
<content:encoded><![CDATA[
<div> generalization error, Rademacher complexity, data augmentation, graph neural networks, Gaussian Mixture Models

Summary:
This paper introduces a theoretical framework for Graph Neural Networks (GNNs) to address the challenges of generalization to unseen or out-of-distribution data. By using Rademacher complexity to compute a regret bound on generalization error, the authors characterize the impact of data augmentation on improving performance. They propose GRATIN, a novel graph data augmentation algorithm that leverages Gaussian Mixture Models (GMMs) to approximate any distribution efficiently. The approach not only surpasses existing augmentation methods in terms of generalization but also offers improved time complexity, making it highly suitable for practical applications. This framework provides valuable insights into enhancing the generalization capabilities of GNNs and offers a promising avenue for further research and application in real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2411.08638v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GRATIN, an efficient graph data augmentation algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExDiff: A Framework for Simulating Diffusion Processes on Complex Networks with Explainable AI Integration</title>
<link>https://arxiv.org/abs/2506.04271</link>
<guid>https://arxiv.org/abs/2506.04271</guid>
<content:encoded><![CDATA[
<div> framework, diffusion, networks, graph neural networks, XAI
<br />
Summary: 
The article introduces ExDiff, a computational framework that combines network simulation, graph neural networks, and explainable artificial intelligence to model and interpret diffusion dynamics in complex networks. ExDiff integrates classical compartmental models with deep learning techniques to capture structural and temporal characteristics of diffusion across different network topologies. It features modules for network analysis, neural modeling, simulation, and interpretability, accessible through an intuitive interface on Google Colab. Through a case study using the SIRVD model, ExDiff demonstrates the ability to simulate disease spread, evaluate intervention strategies, classify node states, and uncover the structural determinants of contagion using XAI techniques. By integrating simulation and interpretability, ExDiff provides a flexible and accessible platform for studying diffusion phenomena in networked systems, enabling methodological innovation and practical insights. 
<br /> <div>
arXiv:2506.04271v1 Announce Type: new 
Abstract: Understanding and controlling diffusion processes in complex networks is critical across domains ranging from epidemiology to information science. Here, we present ExDiff, an interactive and modular computational framework that integrates network simulation, graph neural networks (GNNs), and explainable artificial intelligence (XAI) to model and interpret diffusion dynamics. ExDiff combines classical compartmental models with deep learning techniques to capture both the structural and temporal characteristics of diffusion across diverse network topologies. The framework features dedicated modules for network analysis, neural modeling, simulation, and interpretability, all accessible via an intuitive interface built on Google Colab. Through a case study of the Susceptible Infectious Recovered Vaccinated Dead (SIRVD) model, we demonstrate the capacity to simulate disease spread, evaluate intervention strategies, classify node states, and reveal the structural determinants of contagion through XAI techniques. By unifying simulation and interpretability, ExDiff provides a powerful, flexible, and accessible platform for studying diffusion phenomena in networked systems, enabling both methodological innovation and practical insight.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering</title>
<link>https://arxiv.org/abs/2506.04292</link>
<guid>https://arxiv.org/abs/2506.04292</guid>
<content:encoded><![CDATA[
<div> Keywords: Money laundering, Smurfing, Network analytics, Graph-based method, Fraud detection

Summary:<br />
Money laundering, estimated to make up a significant portion of the global GDP, presents a challenge for financial institutions due to stringent regulatory controls. Smurfing, a common method of laundering involving breaking large transactions into smaller amounts, requires sophisticated detection techniques. GARG-AML, a new graph-based approach, offers an interpretable metric derived from second-order transaction networks to quantify smurfing risk effectively and transparently. This method strikes a balance between computational efficiency, detection power, and interpretability, making it suitable for integration into anti-money laundering workflows. By utilizing only the adjacency matrix of the second-order neighborhood and basic network features, GARG-AML outperforms current state-of-the-art smurfing detection methods in experimental evaluations. This research demonstrates the potential of fundamental network properties in advancing fraud detection.<br />
Summary: <div>
arXiv:2506.04292v1 Announce Type: new 
Abstract: Money laundering poses a significant challenge as it is estimated to account for 2%-5% of the global GDP. This has compelled regulators to impose stringent controls on financial institutions. One prominent laundering method for evading these controls, called smurfing, involves breaking up large transactions into smaller amounts. Given the complexity of smurfing schemes, which involve multiple transactions distributed among diverse parties, network analytics has become an important anti-money laundering tool. However, recent advances have focused predominantly on black-box network embedding methods, which has hindered their adoption in businesses. In this paper, we introduce GARG-AML, a novel graph-based method that quantifies smurfing risk through a single interpretable metric derived from the structure of the second-order transaction network of each individual node in the network. Unlike traditional methods, GARG-AML strikes an effective balance among computational efficiency, detection power and transparency, which enables its integration into existing AML workflows. To enhance its capabilities, we combine the GARG-AML score calculation with different tree-based methods and also incorporate the scores of the node's neighbours. An experimental evaluation on large-scale synthetic and open-source networks demonstrate that the GARG-AML outperforms the current state-of-the-art smurfing detection methods. By leveraging only the adjacency matrix of the second-order neighbourhood and basic network features, this work highlights the potential of fundamental network properties towards advancing fraud detection.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Drives Team Success? Large-Scale Evidence on the Role of the Team Player Effect</title>
<link>https://arxiv.org/abs/2506.04475</link>
<guid>https://arxiv.org/abs/2506.04475</guid>
<content:encoded><![CDATA[
<div> teamwork, social skills, familiarity, team player effect, team performance

Summary: 
The study explores the impact of social skills and familiarity on team success in temporary teams by analyzing data from the real-time strategy game Age of Empires II. The research examines individual contributions to team outcomes and identifies a 'team player effect,' where certain individuals enhance team performance beyond their technical proficiency. This effect is strengthened by team familiarity, particularly in teams with prior shared experience. The study also notes that the team player effect becomes more pronounced with larger team sizes, suggesting the increasing value of social skills in coordinating complex tasks. The findings highlight the complementary interaction between social skills and familiarity in achieving high team performance, shedding light on the dynamics of teamwork in structured, high-pressure environments. <div>
arXiv:2506.04475v1 Announce Type: new 
Abstract: Effective teamwork is essential in structured, performance-driven environments, from professional organizations to high-stakes competitive settings. As tasks grow more complex, achieving high performance requires not only technical proficiency but also strong interpersonal skills that allow individuals to coordinate effectively within teams. While prior research has identified social skills and familiarity as key drivers of team success, their joint effects -- particularly in temporary teams -- remain underexplored due to data and methodological constraints. To address this gap, we analyze a large-scale panel dataset from the real-time strategy game Age of Empires II, where players are assigned quasi-randomly to temporary teams and must coordinate under dynamic, high-pressure conditions. We isolate individual contributions by comparing observed match outcomes with predictions based on task proficiency. Our findings confirm a robust 'team player effect': certain individuals consistently improve team outcomes beyond what their technical skills predict. This effect is significantly amplified by team familiarity -- teams with prior shared experience benefit more from the presence of such individuals. Moreover, the effect grows with team size, suggesting that social skills become increasingly valuable as coordination demands rise. Our results demonstrate that social skills and familiarity interact in a complementary, rather than additive, way. These findings contribute to the literature on team performance by documenting the strength and structure of the team player effect in a quasi-randomized, high-stakes setting, with implications for teamwork in organizations and labor markets.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge interventions can mitigate demographic and prestige disparities in the Computer Science coauthorship network</title>
<link>https://arxiv.org/abs/2506.04435</link>
<guid>https://arxiv.org/abs/2506.04435</guid>
<content:encoded><![CDATA[
<div> algorithms, demographic traits, network centrality, academic publishing, inequities

Summary:
- Social factors and institutional prestige influence academic publishing and network centrality in coauthorship.
- Women and individuals from minoritized racial backgrounds are less central in the computer science coauthorship network.
- Faculty from top-ranked departments are more central, indicating higher prestige.
- Disparities in centrality can be reduced through simulated interventions promoting collaborations.
- Interventions targeting scholars based on institutional prestige improve network centrality and academic job placement predictions.<br /><br />Summary: <div>
arXiv:2506.04435v1 Announce Type: cross 
Abstract: Social factors such as demographic traits and institutional prestige structure the creation and dissemination of ideas in academic publishing. One place these effects can be observed is in how central or peripheral a researcher is in the coauthorship network. Here we investigate inequities in network centrality in a hand-collected data set of 5,670 U.S.-based faculty employed in Ph.D.-granting Computer Science departments and their DBLP coauthorship connections. We introduce algorithms for combining name- and perception-based demographic labels by maximizing alignment with self-reported demographics from a survey of faculty from our census. We find that women and individuals with minoritized race identities are less central in the computer science coauthorship network, implying worse access to and ability to spread information. Centrality is also highly correlated with prestige, such that faculty in top-ranked departments are at the core and those in low-ranked departments are in the peripheries of the computer science coauthorship network. We show that these disparities can be mitigated using simulated edge interventions, interpreted as facilitated collaborations. Our intervention increases the centrality of target individuals, chosen independently of the network structure, by linking them with researchers from highly ranked institutions. When applied to scholars during their Ph.D., the intervention also improves the predicted rank of their placement institution in the academic job market. This work was guided by an ameliorative approach: uncovering social inequities in order to address them. By targeting scholars for intervention based on institutional prestige, we are able to improve their centrality in the coauthorship network that plays a key role in job placement and longer-term academic success.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Altruism in Recommendation Systems</title>
<link>https://arxiv.org/abs/2506.04525</link>
<guid>https://arxiv.org/abs/2506.04525</guid>
<content:encoded><![CDATA[
<div> boost, user altruism, recommendation systems, social welfare, strategic interaction

Summary:
The study focuses on users of social media platforms that use recommendation systems and how they strategically interact with platform content to influence future recommendations. It examines the concept of user altruism, where users engage in behaviors to boost algorithmically suppressed content. The game theory analysis between users and the recommendation system shows that user altruism can lead to increases in user social welfare under certain conditions. It also suggests that effectively altruistic strategies can improve the utility of the recommendation system itself. The study includes theoretical analysis, empirical results on the GoodReads dataset, and an online survey to understand real-world user behavior in recommendation systems. These findings demonstrate how traditional recommendation systems may incentivize users to form collectives and engage in altruistic strategies when interacting with them. 

<br /><br />Summary: <div>
arXiv:2506.04525v1 Announce Type: cross 
Abstract: Users of social media platforms based on recommendation systems (RecSys) (e.g. TikTok, X, YouTube) strategically interact with platform content to influence future recommendations. On some such platforms, users have been documented to form large-scale grassroots movements encouraging others to purposefully interact with algorithmically suppressed content in order to "boost" its recommendation; we term this behavior user altruism. To capture this behavior, we study a game between users and a RecSys, where users provide the RecSys (potentially manipulated) preferences over the contents available to them, and the RecSys -- limited by data and computation constraints -- creates a low-rank approximation preference matrix, and ultimately provides each user her (approximately) most-preferred item. We compare the users' social welfare under truthful preference reporting and under a class of strategies capturing user altruism. In our theoretical analysis, we provide sufficient conditions to ensure strict increases in user social welfare under user altruism, and provide an algorithm to find an effective altruistic strategy. Interestingly, we show that for commonly assumed recommender utility functions, effectively altruistic strategies also improve the utility of the RecSys! We show that our results are robust to several model misspecifications, thus strengthening our conclusions. Our theoretical analysis is complemented by empirical results of effective altruistic strategies on the GoodReads dataset, and an online survey on how real-world users behave altruistically in RecSys. Overall, our findings serve as a proof-of-concept of the reasons why traditional RecSys may incentivize users to form collectives and/or follow altruistic strategies when interacting with them.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Driven Bounded Confidence Opinion Dynamics: A Hegselmann-Krause Model Based on Fractional-Order Methods</title>
<link>https://arxiv.org/abs/2506.04701</link>
<guid>https://arxiv.org/abs/2506.04701</guid>
<content:encoded><![CDATA[
<div> memory effects, social interactions, decision-making processes, fractional-order bounded confidence, opinion dynamics

Summary:
The paper presents a novel fractional-order bounded confidence opinion dynamics model that incorporates memory effects into social interactions and decision-making processes. By combining the Hegselmann-Krause framework with fractional-order difference, the model captures the persistent influence of historical information on system states. The rigorous theoretical analysis explores key properties such as convergence and consensus, demonstrating that the proposed model outperforms classical opinion dynamics by addressing limitations like the monotonicity of bounded opinions. This enhanced model offers a more realistic representation of opinion evolution in real-world scenarios, providing valuable insights and methodological approaches for understanding opinion formation and evolution. The findings contribute to both theoretical advancements and practical applications in studying complex social dynamics. 

<br /><br />Summary: <div>
arXiv:2506.04701v1 Announce Type: cross 
Abstract: Memory effects play a crucial role in social interactions and decision-making processes. This paper proposes a novel fractional-order bounded confidence opinion dynamics model to characterize the memory effects in system states. Building upon the Hegselmann-Krause framework and fractional-order difference, a comprehensive model is established that captures the persistent influence of historical information. Through rigorous theoretical analysis, the fundamental properties including convergence and consensus is investigated. The results demonstrate that the proposed model not only maintains favorable convergence and consensus characteristics compared to classical opinion dynamics, but also addresses limitations such as the monotonicity of bounded opinions. This enables a more realistic representation of opinion evolution in real-world scenarios. The findings of this study provide new insights and methodological approaches for understanding opinion formation and evolution, offering both theoretical significance and practical applications.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Aware Temporal Network Generation</title>
<link>https://arxiv.org/abs/2501.07327</link>
<guid>https://arxiv.org/abs/2501.07327</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, face-to-face interactions, community affiliation, network generation, synthetic networks 

Summary: 
Temporal networks are crucial in understanding complex dynamics in human behavior, such as epidemics and sociological studies. Existing datasets have limitations, including short time spans and privacy concerns. Traditional network generation algorithms do not adequately capture social structures or temporal aspects. This study extends a recent approach to generate synthetic networks that mimic interactions between different communities. Nodes are labeled based on their community affiliation to create surrogate networks reflecting realistic behaviors. The method is validated by comparing structural measures between original and generated networks in various face-to-face interaction datasets.<br /><br />Summary: <div>
arXiv:2501.07327v2 Announce Type: replace 
Abstract: The advantages of temporal networks in capturing complex dynamics, such as diffusion and contagion, has led to breakthroughs in real world systems across numerous fields. In the case of human behavior, face-to-face interaction networks enable us to understand the dynamics of how communities emerge and evolve in time through the interactions, which is crucial in fields like epidemics, sociological studies and urban science. However, state-of-the-art datasets suffer from a number of drawbacks, such as short time-span for data collection and a small number of participants. Moreover, concerns arise for the participants' privacy and the data collection costs. Over the past years, many successful algorithms for static networks generation have been proposed, but they often do not tackle the social structure of interactions or their temporal aspect. In this work, we extend a recent network generation approach to capture the evolution of interactions between different communities. Our method labels nodes based on their community affiliation and constructs surrogate networks that reflect the interactions of the original temporal networks between nodes with different labels. This enables the generation of synthetic networks that replicate realistic behaviors. We validate our approach by comparing structural measures between the original and generated networks across multiple face-to-face interaction datasets.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing lexicons of in-group language with socio-temporal context</title>
<link>https://arxiv.org/abs/2409.19257</link>
<guid>https://arxiv.org/abs/2409.19257</guid>
<content:encoded><![CDATA[
<div> Keywords: in-group language, lexicon induction, socio-temporal context, dynamic word embeddings, online anti-women communities

Summary: 
This paper introduces a novel method for inducing lexicons of in-group language, taking into account the socio-temporal context. Existing methods do not adequately capture the evolving nature of in-group language and the social structure of the community. By utilizing dynamic word and user embeddings trained on conversations from online anti-women communities, this approach surpasses previous methods for lexicon induction. A test set is developed for this task, along with a new lexicon of manosphere language validated by human experts. This lexicon quantifies the relevance of each term to a specific sub-community at a particular point in time. The research provides unique insights into in-group language, demonstrating the effectiveness of this method. 

<br /><br />Summary: <div>
arXiv:2409.19257v3 Announce Type: replace-cross 
Abstract: In-group language is an important signifier of group dynamics. This paper proposes a novel method for inducing lexicons of in-group language, which incorporates its socio-temporal context. Existing methods for lexicon induction do not capture the evolving nature of in-group language, nor the social structure of the community. Using dynamic word and user embeddings trained on conversations from online anti-women communities, our approach outperforms prior methods for lexicon induction. We develop a test set for the task of lexicon induction and a new lexicon of manosphere language, validated by human experts, which quantifies the relevance of each term to a specific sub-community at a given point in time. Finally, we present novel insights on in-group language which illustrate the utility of this approach.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Politics and polarization on Bluesky</title>
<link>https://arxiv.org/abs/2506.03443</link>
<guid>https://arxiv.org/abs/2506.03443</guid>
<content:encoded><![CDATA[
<div> Keywords: political discourse, polarization, social media platform, Bluesky, fragmentation 

Summary: 
The study focuses on political discourse and polarization on the social media platform Bluesky, analyzing data from December 2024 to May 2025. Approximately 13% of posts on Bluesky involve political content, with topics such as international conflicts, U.S. politics, and socio-technological debates being prominent. Structural polarization is observed across various political topics, with some topics showing high imbalance in the numbers of users on opposing sides. Despite echoing familiar political narratives and polarization trends, Bluesky has a more politically homogeneous user base compared to other platforms. The study highlights the impact of platform fragmentation on shaping online political discourse and polarization dynamics. 

<br /><br />Summary:  <div>
arXiv:2506.03443v1 Announce Type: new 
Abstract: Online political discourse is increasingly shaped not by a few dominant platforms but by a fragmented ecosystem of social media spaces, each with its own user base, target audience, and algorithmic mediation of discussion. Such fragmentation may fundamentally change how polarization manifests online. In this study, we investigate the characteristics of political discourse and polarization on the emerging social media site Bluesky. We collect all activity on the platform between December 2024 and May 2025 to map out the platform's political topic landscape and detect distinct polarization patterns. Our comprehensive data collection allows us to employ a data-driven methodology for identifying political themes, classifying user stances, and measuring both structural and content-based polarization across key topics raised in English-language discussions. Our analysis reveals that approximately 13% of Bluesky posts engage with political content, with prominent topics including international conflicts, U.S. politics, and socio-technological debates. We find high levels of structural polarization across several salient political topics. However, the most polarized topics are also highly imbalanced in the numbers of users on opposing sides, with the smaller group consisting of only 1-2% of the users. While discussions in Bluesky echo familiar political narratives and polarization trends, the platform exhibits a more politically homogeneous user base than was typical prior to the current wave of platform fragmentation.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Bulimia Nervosa in the Digital Age: The Role of Social Media</title>
<link>https://arxiv.org/abs/2506.03491</link>
<guid>https://arxiv.org/abs/2506.03491</guid>
<content:encoded><![CDATA[
<div> Keywords: globalization, eating disorders, bulimia nervosa, mathematical modeling, social media

Summary: 
Globalization has significantly impacted the societal dynamics surrounding eating disorders such as bulimia nervosa (BN), with these conditions increasingly influenced by broader sociocultural and digital contexts. Traditional mathematical modeling frameworks have limitations in capturing the complex interactions of social contagion, digital media, and adaptive behavior in the development of BN. This review discusses the evolution of quantitative modeling in understanding BN as a socially transmissible condition, highlighting the need for improved models to incorporate feedback mechanisms, content-driven influence functions, and dynamic network effects. The intensifying impact of social media on eating disorders presents a critical gap that future models must address. By integrating behavioral epidemiology and an adaptive behavior framework, researchers aim to develop data-informed models that can guide more effective public health interventions in the digital age. <div>
arXiv:2506.03491v1 Announce Type: new 
Abstract: Globalization has fundamentally reshaped societal dynamics, influencing how individuals interact and perceive themselves and others. One significant consequence is the evolving landscape of eating disorders such as bulimia nervosa (BN), which are increasingly driven not just by internal psychological factors but by broader sociocultural and digital contexts. While mathematical modeling has provided valuable insights, traditional frameworks often fall short in capturing the nuanced roles of social contagion, digital media, and adaptive behavior. This review synthesizes two decades of quantitative modeling efforts, including compartmental, stochastic, and delay-based approaches. We spotlight foundational work that conceptualizes BN as a socially transmissible condition and identify critical gaps, especially regarding the intensifying impact of social media. Drawing on behavioral epidemiology and the adaptive behavior framework by Fenichel et al., we advocate for a new generation of models that incorporate feedback mechanisms, content-driven influence functions, and dynamic network effects. This work outlines a roadmap for developing more realistic, data-informed models that can guide effective public health interventions in the digital era.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GA-S$^3$: Comprehensive Social Network Simulation with Group Agents</title>
<link>https://arxiv.org/abs/2506.03532</link>
<guid>https://arxiv.org/abs/2506.03532</guid>
<content:encoded><![CDATA[
<div> Keywords: Social network simulation, Group Agents, Intelligent decision-making, Online events, Benchmark

Summary:
Social network simulation plays a crucial role in understanding real-world social networks and their applications. The proposed Social Network Simulation System (GA-S3) utilizes Group Agents to make intelligent decisions in simulating large-scale network phenomena. Unlike individual-based agents, Group Agents model collections of individuals with similar behaviors, making it computationally manageable to simulate complex interactions. A social network benchmark comprising 2024 online events provides detailed information on Internet traffic variations. The experiment shows that GA-S3 achieves accurate and realistic prediction results. The system's open-source code is available on GitHub, enabling researchers to utilize and further develop the simulation system for various applications. 

<br /><br />Summary: <div>
arXiv:2506.03532v1 Announce Type: new 
Abstract: Social network simulation is developed to provide a comprehensive understanding of social networks in the real world, which can be leveraged for a wide range of applications such as group behavior emergence, policy optimization, and business strategy development. However, billions of individuals and their evolving interactions involved in social networks pose challenges in accurately reflecting real-world complexities. In this study, we propose a comprehensive Social Network Simulation System (GA-S3) that leverages newly designed Group Agents to make intelligent decisions regarding various online events. Unlike other intelligent agents that represent an individual entity, our group agents model a collection of individuals exhibiting similar behaviors, facilitating the simulation of large-scale network phenomena with complex interactions at a manageable computational cost. Additionally, we have constructed a social network benchmark from 2024 popular online events that contains fine-grained information on Internet traffic variations. The experiment demonstrates that our approach is capable of achieving accurate and highly realistic prediction results. Code is open at https://github.com/AI4SS/GAS-3.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Retrieval-Augmented Multi-Agent Framework for Psychiatry Diagnosis</title>
<link>https://arxiv.org/abs/2506.03750</link>
<guid>https://arxiv.org/abs/2506.03750</guid>
<content:encoded><![CDATA[
<div> framework, diagnosis, AI, MoodAngels, psychiatric

Summary:
The article introduces MoodAngels, a specialized multi-agent framework for mood disorder diagnosis that addresses challenges in AI-assisted psychiatric assessments. The framework combines granular-scale analysis of clinical assessments with a structured verification process to improve accuracy in interpreting complex psychiatric data. A new open-source dataset called MoodSyn, consisting of 1,173 synthetic psychiatric cases, is also presented to preserve clinical validity and ensure patient privacy. Experimental results show that MoodAngels outperforms conventional methods, with a baseline agent achieving higher accuracy than GPT-4o on real-world cases. The full multi-agent system further improves diagnostic accuracy. Evaluation in the MoodSyn dataset demonstrates exceptional fidelity in reproducing statistical patterns and complex relationships found in the original data, making it a valuable resource for machine learning applications in computational psychiatry. Overall, MoodAngels provides an advanced diagnostic tool and critical research resource for bridging gaps in AI-assisted mental health assessment. 

<br /><br />Summary: <div>
arXiv:2506.03750v1 Announce Type: new 
Abstract: The application of AI in psychiatric diagnosis faces significant challenges, including the subjective nature of mental health assessments, symptom overlap across disorders, and privacy constraints limiting data availability. To address these issues, we present MoodAngels, the first specialized multi-agent framework for mood disorder diagnosis. Our approach combines granular-scale analysis of clinical assessments with a structured verification process, enabling more accurate interpretation of complex psychiatric data. Complementing this framework, we introduce MoodSyn, an open-source dataset of 1,173 synthetic psychiatric cases that preserves clinical validity while ensuring patient privacy. Experimental results demonstrate that MoodAngels outperforms conventional methods, with our baseline agent achieving 12.3% higher accuracy than GPT-4o on real-world cases, and our full multi-agent system delivering further improvements. Evaluation in the MoodSyn dataset demonstrates exceptional fidelity, accurately reproducing both the core statistical patterns and complex relationships present in the original data while maintaining strong utility for machine learning applications. Together, these contributions provide both an advanced diagnostic tool and a critical research resource for computational psychiatry, bridging important gaps in AI-assisted mental health assessment.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of COVID-19 on Twitter Ego Networks: Structure, Sentiment, and Topics</title>
<link>https://arxiv.org/abs/2506.03788</link>
<guid>https://arxiv.org/abs/2506.03788</guid>
<content:encoded><![CDATA[
<div> Keywords: lockdown measures, ego networks, online social platforms, Twitter users, cognitive resources

Summary: 
During the COVID-19 pandemic, individuals turned to online social platforms like Twitter due to in-person restrictions. This shift impacted the characteristics of online ego networks, leading to expanded networks, structured social circles, intensified relationships, increased negative interactions, and greater thematic diversity in user engagement. These changes were temporary, reverting to pre-pandemic norms once lockdown measures were lifted. The study analyzed data from a seven-year period, showing how individuals adapted their online interactions in response to the extraordinary social context imposed by the pandemic.

<br /><br />Summary: <div>
arXiv:2506.03788v1 Announce Type: new 
Abstract: Lockdown measures, implemented by governments during the initial phases of the COVID-19 pandemic to reduce physical contact and limit viral spread, imposed significant restrictions on in-person social interactions. Consequently, individuals turned to online social platforms to maintain connections. Ego networks, which model the organization of personal relationships according to human cognitive constraints on managing meaningful interactions, provide a framework for analyzing such dynamics. The disruption of physical contact and the predominant shift of social life online potentially altered the allocation of cognitive resources dedicated to managing these digital relationships. This research aims to investigate the impact of lockdown measures on the characteristics of online ego networks, presumably resulting from this reallocation of cognitive resources. To this end, a large dataset of Twitter users was examined, covering a seven-year period of activity. Analyzing a seven-year Twitter dataset -- including five years pre-pandemic and two years post -- we observe clear, though temporary, changes. During lockdown, ego networks expanded, social circles became more structured, and relationships intensified. Simultaneously, negative interactions increased, and users engaged with a broader range of topics, indicating greater thematic diversity. Once restrictions were lifted, these structural, emotional, and thematic shifts largely reverted to pre-pandemic norms -- suggesting a temporary adaptation to an extraordinary social context.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^3$: Social-network Simulation System with Large Language Model-Empowered Agents</title>
<link>https://arxiv.org/abs/2307.14984</link>
<guid>https://arxiv.org/abs/2307.14984</guid>
<content:encoded><![CDATA[
<div> Keywords: social network simulation, large language models, S$^3$ system, agent-based simulation, information propagation<br />
<br />
Summary: 
The article discusses the development of the S$^3$ system, a social network simulation system utilizing large language models (LLMs) to create human-like agents. By employing prompt engineering and tuning techniques, the system ensures that agents behave realistically within the simulated social network. The simulation focuses on three key aspects: emotion, attitude, and interaction behaviors, leading to the emergence of population-level phenomena such as information and attitude propagation. Evaluation using real-world social network data shows promising accuracy. This work represents an important step in leveraging LLMs for social network simulation and has implications for various applications in social science research. <div>
arXiv:2307.14984v3 Announce Type: replace 
Abstract: Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\textbf{S}$ocial network $\textbf{S}$imulation $\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Treatment Allocation in the Presence of Interference</title>
<link>https://arxiv.org/abs/2410.00075</link>
<guid>https://arxiv.org/abs/2410.00075</guid>
<content:encoded><![CDATA[
<div> Influence Maximization, Treatment Allocation, Network Interference, Causal Estimator, Optimizing
Treatment Allocation

Summary:
Optimizing Treatment Allocation in the Presence of Interference (OTAPI) addresses the challenge of selecting the optimal entities in a network for treatment allocation to maximize overall effect. Traditional methods like Uplift Modeling (UM) do not consider network interference, leading to suboptimal results in network settings. OTAPI integrates a causal estimator to predict treatment effects in networks and uses this information to drive optimal treatment allocation decisions. By bridging the gap between Influence Maximization (IM) and UM, OTAPI outperforms existing approaches on various datasets. This novel method showcases the importance of considering network effects in treatment allocation decisions, demonstrating its effectiveness in improving outcomes in scenarios where entities influence each other. <br /><br />Summary: <div>
arXiv:2410.00075v2 Announce Type: replace 
Abstract: In Influence Maximization (IM), the objective is to -- given a budget -- select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover, effects that follow from targeting these customers. Recently, new methods to estimate treatment effects in the presence of network interference have been proposed. However, the issue of how to leverage these models to make better treatment allocation decisions has been largely overlooked. Traditionally, in Uplift Modeling (UM), entities are ranked according to estimated treatment effect, and the top entities are allocated treatment. Since, in a network context, entities influence each other, the UM ranking approach will be suboptimal. The problem of finding the optimal treatment allocation in a network setting is \textcolor{red}{NP-hard,} and generally has to be solved heuristically. To fill the gap between IM and UM, we propose OTAPI: Optimizing Treatment Allocation in the Presence of Interference to find solutions to the IM problem using treatment effect estimates. OTAPI consists of two steps. First, a causal estimator is trained to predict treatment effects in a network setting. Second, this estimator is leveraged to identify an optimal treatment allocation by integrating it into classic IM algorithms. We demonstrate that this novel method outperforms classic IM and UM approaches on both synthetic and semi-synthetic datasets.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Experts for Node Classification</title>
<link>https://arxiv.org/abs/2412.00418</link>
<guid>https://arxiv.org/abs/2412.00418</guid>
<content:encoded><![CDATA[
<div> mixture of experts, node classification, node predictors, diverse patterns, real-world graphs
<br />
Summary: 
The study highlights the limitations of existing node predictors in capturing diverse patterns in real-world graphs, such as degree and homophily. It suggests that using a single node predictor for all nodes could result in suboptimal classification performance. To address this issue, the authors propose a novel framework called MoE-NP, which utilizes a mixture of node predictors and strategically selects models based on node patterns. Experimental results on various real-world datasets show significant performance improvements with MoE-NP. This approach emphasizes the importance of considering different node patterns when predicting node behavior in complex networks. <div>
arXiv:2412.00418v3 Announce Type: replace 
Abstract: Nodes in the real-world graphs exhibit diverse patterns in numerous aspects, such as degree and homophily. However, most existent node predictors fail to capture a wide range of node patterns or to make predictions based on distinct node patterns, resulting in unsatisfactory classification performance. In this paper, we reveal that different node predictors are good at handling nodes with specific patterns and only apply one node predictor uniformly could lead to suboptimal result. To mitigate this gap, we propose a mixture of experts framework, MoE-NP, for node classification. Specifically, MoE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Experimental results from a range of real-world datasets demonstrate significant performance improvements from MoE-NP.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying edge relevance for epidemic spreading via the semi-metric topology of complex networks</title>
<link>https://arxiv.org/abs/2311.14817</link>
<guid>https://arxiv.org/abs/2311.14817</guid>
<content:encoded><![CDATA[
<div> semi-metric topology, sparsification, complex networks, epidemic outbreaks, shortest paths
Summary:
The study introduces a new sparsification method based on semi-metric distortion, prioritizing edges that significantly break the triangle inequality in weighted graphs. By identifying and removing edges with high semi-metric distortion, the method effectively preserves network dynamics and topology while reducing computational costs. Experimental results demonstrate that the semi-metric distortion sparsification outperforms existing methods in recovering Susceptible-Infected dynamics and maintaining connectivity. This approach improves the ranking of edge relevance for epidemic outbreaks by quantifying alternative transmission pathways through semi-metric distances. Overall, the method provides a more efficient and accurate way to sparsify networks while retaining essential information for studying complex system dynamics.<br /><br />Summary: <div>
arXiv:2311.14817v2 Announce Type: replace-cross 
Abstract: Sparsification aims at extracting a reduced core of associations that best preserves both the dynamics and topology of networks while reducing the computational cost of simulations. We show that the semi-metric topology of complex networks yields a natural and algebraically-principled sparsification that outperforms existing methods on those goals. Weighted graphs whose edges represent distances between nodes are semi-metric when at least one edge breaks the triangle inequality (transitivity). We first confirm with new experiments that the metric backbone$\unicode{x2013}$a unique subgraph of all edges that obey the triangle inequality and thus preserve all shortest paths$\unicode{x2013}$recovers Susceptible-Infected dynamics over the original non-sparsified graph. This recovery is improved when we remove only those edges that break the triangle inequality significantly, i.e., edges with large semi-metric distortion. Based on these results, we propose the new semi-metric distortion sparsification method to progressively sparsify networks in decreasing order of semi-metric distortion. Our method recovers the macro- and micro-level dynamics of epidemic outbreaks better than other methods while also yielding sparser yet connected subgraphs that preserve all shortest paths. Overall, we show that semi-metric distortion overcomes the limitations of edge betweenness in ranking the dynamical relevance of edges not participating in any shortest path, as it quantifies the existence and strength of alternative transmission pathways.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ideological Fragmentation of the Social Media Ecosystem: From echo chambers to echo platforms</title>
<link>https://arxiv.org/abs/2411.16826</link>
<guid>https://arxiv.org/abs/2411.16826</guid>
<content:encoded><![CDATA[
<div> platform centrality, news consumption, user base composition, social media ecosystem, ideological diversity

Summary:
The study investigates the impact of social media on political discussions, focusing on platform centrality, news consumption, and user base composition. Analyzing 117 million posts related to the 2020 US Presidential elections on nine platforms, including mainstream and alt-tech platforms, significant differences are found. Mainstream platforms are more central within the ecosystem, circulate more reliable news, and have a more diverse user base. In contrast, alt-tech platforms play a peripheral role, feature higher instances of unreliable content, and exhibit greater ideological uniformity. The findings underscore the growing fragmentation and polarization of the social media landscape, with platforms catering to specific ideological niches and users seeking like-minded communities. This trend highlights the challenge of echo chambers and the need for platforms to address the spread of unreliable information and promote diverse perspectives. 

<br /><br />Summary: <div>
arXiv:2411.16826v2 Announce Type: replace-cross 
Abstract: The entertainment-driven nature of social media encourages users to engage with like-minded individuals and consume content aligned with their beliefs, limiting exposure to diverse perspectives. Simultaneously, users migrate between platforms, either due to moderation policies like de-platforming or in search of environments better suited to their preferences. These dynamics drive the specialization of the social media ecosystem, shifting from internal echo chambers to "echo platforms"--entire platforms functioning as ideologically homogeneous niches. To systematically analyze this phenomenon in political discussions, we propose a quantitative approach based on three key dimensions: platform centrality, news consumption, and user base composition. We analyze 117 million posts related to the 2020 US Presidential elections from nine social media platforms--Facebook, Reddit, Twitter, YouTube, BitChute, Gab, Parler, Scored, and Voat. Our findings reveal significant differences among platforms in their centrality within the ecosystem, the reliability of circulated news, and the ideological diversity of their users, highlighting a clear divide between mainstream and alt-tech platforms. The latter occupy a peripheral role, feature a higher prevalence of unreliable content, and exhibit greater ideological uniformity. These results highlight the key dimensions shaping the fragmentation and polarization of the social media landscape.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplex Nodal Modularity: A novel network metric for the regional analysis of amnestic mild cognitive impairment during a working memory binding task</title>
<link>https://arxiv.org/abs/2501.09805</link>
<guid>https://arxiv.org/abs/2501.09805</guid>
<content:encoded><![CDATA[
<div> modularity, community structure, nodal modularity, Alzheimer's disease, fMRI<br />
Summary:<br />
This study introduces the concept of nodal modularity (nQ) to capture variations in community structure at individual nodes in brain networks. By applying nQ to fMRI and DTI data from early-stage Alzheimer's disease (AD) patients, the researchers found changes in nQ in visual, limbic, and paralimbic regions, associated with amyloid-$\beta$ and tau deposition. Additionally, white-matter microstructure changes in parietal and frontal regions were observed, potentially linked to memory deficits. NQ was able to differentiate between individuals with mild cognitive impairment (MCI) and MCI converters, suggesting its sensitivity to disease progression. This novel measure offers insights into localized group structures and may have broader applications across various domains for understanding network organization. <br />  
Summary: <div>
arXiv:2501.09805v2 Announce Type: replace-cross 
Abstract: Modularity is a well-established concept for assessing community structures in various single and multi-layer networks, including those in biological and social domains. Brain networks are known to exhibit community structure at local, meso, and global scale. However, modularity is limited as a metric to a global scale describing the overall strength of community structure, overlooking important variations in community structure at node level. To address this limitation, we extended modularity to individual nodes. This novel measure of nodal modularity (nQ) captures both mesoscale and local-scale changes in modularity. We hypothesized that nQ would illuminate granular changes in the brain due to diseases such as Alzheimer's disease (AD), which are known to disrupt the brain's modular structure. We explored nQ in multiplex networks of a visual short-term memory binding task in fMRI and DTI data in the early stages of AD. While limited by sample size, changes in nQ for individual regions of interest (ROIs) in our fMRI networks were predominantly observed in visual, limbic, and paralimbic systems in the brain, aligning with known AD trajectories and linked to amyloid-$\beta$ and tau deposition. Furthermore, observed changes in white-matter microstructure in our DTI networks in parietal and frontal regions may compliment studies of white-matter integrity in poor memory binders. Additionally, nQ clearly differentiated MCI from MCI converters indicating that nQ may be sensitive to this key turning point of AD. Our findings demonstrate the utility of nQ as a measure of localized group structure, providing novel insights into task and disease-related variability at the node level. Given the widespread application of modularity as a global measure, nQ represents a significant advancement, providing a granular measure of network organization applicable to a wide range of disciplines.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Algorithmic Curation on Social Media: An Empirical Audit of Reddit's r/popular Feed</title>
<link>https://arxiv.org/abs/2502.20491</link>
<guid>https://arxiv.org/abs/2502.20491</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithms, social media feeds, user behavior, Reddit, engagement

Summary: 
Algorithms play a crucial role in curating content on social media feeds, impacting user behavior and content consumption. An empirical audit of Reddit's r/popular feed revealed that recent comments boost a post's visibility and ranking on the feed, while posts below rank 80 experience a significant decline in activity. Despite a higher proportion of undesired behavior like toxic comments, there is no evidence that it helps posts stay on r/popular longer. Posts closer to the top receive more undesired comments but also see an overall increase in engagement, indicating a broader impact rather than targeting undesired activity specifically. The findings emphasize the influence of algorithms in determining content prioritization on social media platforms, underscoring the need for transparency and accountability in algorithmic curation. Content creators, consumers, and moderators can benefit from empirical audits to enhance understanding and improve user experiences on algorithmically curated feeds. 

<br /><br />Summary: <div>
arXiv:2502.20491v2 Announce Type: replace-cross 
Abstract: Platforms are increasingly relying on algorithms to curate the content within users' social media feeds. However, the growing prominence of proprietary, algorithmically curated feeds has concealed what factors influence the presentation of content on social media feeds and how that presentation affects user behavior. This lack of transparency can be detrimental to users, from reducing users' agency over their content consumption to the propagation of misinformation and toxic content. To uncover details about how these feeds operate and influence user behavior, we conduct an empirical audit of Reddit's algorithmically curated trending feed called r/popular. Using 10K r/popular posts collected by taking snapshots of the feed over 11 months, we find that recent comments help a post remain on r/popular longer and climb the feed. We also find that posts below rank 80 correspond to a sharp decline in activity compared to posts above. When examining the effects of having a higher proportion of undesired behavior -- i.e., moderator-removed and toxic comments -- we find no significant evidence that it helps posts stay on r/popular for longer. Although posts closer to the top receive more undesired comments, we find this increase to coincide with a broader increase in overall engagement -- rather than indicating a disproportionate effect on undesired activity. The relationships between algorithmic rank and engagement highlight the extent to which algorithms employed by social media platforms essentially determine which content is prioritized and which is not. We conclude by discussing how content creators, consumers, and moderators on social media platforms can benefit from empirical audits aimed at improving transparency in algorithmically curated feeds.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Recommendation System Using Amazon Product Co-Purchasing Network</title>
<link>https://arxiv.org/abs/2506.02482</link>
<guid>https://arxiv.org/abs/2506.02482</guid>
<content:encoded><![CDATA[
<div> GraphSAGE, link prediction, e-commerce, recommendation system, online<br />
<br />
Summary: This project introduces an online recommendation system for new products on e-commerce platforms. By utilizing the Amazon Product Co-Purchasing Network Metadata dataset, a co-purchasing graph is created to showcase relationships among products. A modified GraphSAGE method is implemented for link prediction, combining product features and co-purchasing graph structure to forecast potential connections. This approach enables the model to adapt to unseen products and update in real time, enhancing the relevance of new product suggestions. Experimental results indicate superior performance compared to baseline algorithms, demonstrating the effectiveness of the proposed method in improving product recommendations in e-commerce settings. The code for the project is available on GitHub for further exploration and implementation. <div>
arXiv:2506.02482v1 Announce Type: new 
Abstract: This project develops an online, inductive recommendation system for newly listed products on e-commerce platforms, focusing on suggesting relevant new items to customers as they purchase other products. Using the Amazon Product Co-Purchasing Network Metadata dataset, we construct a co-purchasing graph where nodes represent products and edges capture co-purchasing relationships. To address the challenge of recommending new products with limited information, we apply a modified GraphSAGE method for link prediction. This inductive approach leverages both product features and the existing co-purchasing graph structure to predict potential co-purchasing relationships, enabling the model to generalize to unseen products. As an online method, it updates in real time, making it scalable and adaptive to evolving product catalogs. Experimental results demonstrate that our approach outperforms baseline algorithms in predicting relevant product links, offering a promising solution for enhancing the relevance of new product recommendations in e-commerce environments. All code is available at https://github.com/cse416a-fl24/final-project-l-minghao_z-catherine_z-nathan.git.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Hyperbolic Graphs with Arbitrary Mesoscale Structures</title>
<link>https://arxiv.org/abs/2506.02686</link>
<guid>https://arxiv.org/abs/2506.02686</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-world networks, Random Hyperbolic Graphs, Random Hyperbolic Block Model, community structures, network modeling

Summary:
Random Hyperbolic Graphs (RHGs) capture real-world network properties by embedding nodes in a latent similarity space, but struggles to account for non-geometric connectivity patterns. The Random Hyperbolic Block Model (RHBM) extends RHGs by incorporating block structures within a maximum-entropy framework, addressing limitations in mesoscale structure generation. Through synthetic network analyses, the RHBM demonstrates the ability to preserve community structures where geometric models fail. This highlights the importance of latent geometry in network modeling while effectively controlling mesoscale mixing patterns. Overall, the RHBM provides a more comprehensive approach for modeling networks with specific connectivity patterns, offering a new perspective on network structure generation. 

Summary:<br /><br />Random Hyperbolic Graphs (RHGs) capture real-world network properties by embedding nodes in a latent similarity space, but struggles to account for non-geometric connectivity patterns. The Random Hyperbolic Block Model (RHBM) extends RHGs by incorporating block structures within a maximum-entropy framework, addressing limitations in mesoscale structure generation. Through synthetic network analyses, the RHBM demonstrates the ability to preserve community structures where geometric models fail. This highlights the importance of latent geometry in network modeling while effectively controlling mesoscale mixing patterns. Overall, the RHBM provides a more comprehensive approach for modeling networks with specific connectivity patterns, offering a new perspective on network structure generation. <div>
arXiv:2506.02686v1 Announce Type: new 
Abstract: Real-world networks exhibit universal structural properties such as sparsity, small-worldness, heterogeneous degree distributions, high clustering, and community structures. Geometric network models, particularly Random Hyperbolic Graphs (RHGs), effectively capture many of these features by embedding nodes in a latent similarity space. However, networks are often characterized by specific connectivity patterns between groups of nodes -- i.e. communities -- that are not geometric, in the sense that the dissimilarity between groups do not obey the triangle inequality. Structuring connections only based on the interplay of similarity and popularity thus poses fundamental limitations on the mesoscale structure of the networks that RHGs can generate. To address this limitation, we introduce the Random Hyperbolic Block Model (RHBM), which extends RHGs by incorporating block structures within a maximum-entropy framework. We demonstrate the advantages of the RHBM through synthetic network analyses, highlighting its ability to preserve community structures where purely geometric models fail. Our findings emphasize the importance of latent geometry in network modeling while addressing its limitations in controlling mesoscale mixing patterns.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collective Intelligence Outperforms Individual Talent: A Case Study in League of Legends</title>
<link>https://arxiv.org/abs/2506.02706</link>
<guid>https://arxiv.org/abs/2506.02706</guid>
<content:encoded><![CDATA[
<div> Keywords: gaming environments, multiplayer online battle arena, collective intelligence, cooperative behaviors, League of Legends

Summary:
This paper investigates the concept of collective intelligence in gaming environments, focusing on the multiplayer online battle arena game League of Legends. The study explores whether groups of individuals, who may not be individually skilled but exhibit cooperative behaviors, can outperform skilled individuals in problem-solving tasks. By analyzing game-specific metrics and new topological and graph spectra measures of cooperative interactions, the research demonstrates the superior performance of collective intelligence over individual intelligence. The study utilizes machine learning algorithms and statistical methods on large-scale data to provide compelling insights into the dynamics of collaborative gameplay in MOBA games. The findings suggest that effective collaboration and teamwork can lead to enhanced performance and decision-making in complex gaming scenarios. <div>
arXiv:2506.02706v1 Announce Type: new 
Abstract: Gaming environments are popular testbeds for studying human interactions and behaviors in complex artificial intelligence systems. Particularly, in multiplayer online battle arena (MOBA) games, individuals collaborate in virtual environments of high realism that involves real-time strategic decision-making and trade-offs on resource management, information collection and sharing, team synergy and collective dynamics. This paper explores whether collective intelligence, emerging from cooperative behaviours exhibited by a group of individuals, who are not necessarily skillful but effectively engage in collaborative problem-solving tasks, exceeds individual intelligence observed within skillful individuals. This is shown via a case study in League of Legends, using machine learning algorithms and statistical methods applied to large-scale data collected for the same purpose. By modelling systematically game-specific metrics but also new game-agnostic topological and graph spectra measures of cooperative interactions, we demonstrate compelling insights about the superior performance of collective intelligence.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Patterns of Interaction in Temporal Hypergraphs via Edge Clustering</title>
<link>https://arxiv.org/abs/2506.03105</link>
<guid>https://arxiv.org/abs/2506.03105</guid>
<content:encoded><![CDATA[
<div> community detection, clustering, edge clustering, hypergraphs, structural similarity function

Summary:
- The article introduces a new approach for unsupervised community detection, called edge clustering, which focuses on densely connected subsets of vertices.
- Instead of clustering vertices directly, the method clusters edges of the graph and assigns vertices to communities based on their edge connections.
- The edge clustering algorithm is extended to temporal hypergraphs, where edges can contain multiple vertices and have timestamps.
- By defining a suitable structural similarity function, the algorithm can detect various patterns of interactions within hypergraphs.
- The algorithm's performance is tested with three different structural similarity functions on a collaboration hypergraph, revealing intuitive cluster structures that could be beneficial for further analysis tasks. 

<br /><br />Summary: <div>
arXiv:2506.03105v1 Announce Type: new 
Abstract: Finding densely connected subsets of vertices in an unsupervised setting, called clustering or community detection, is one of the fundamental problems in network science. The edge clustering approach instead detects communities by clustering the edges of the graph and then assigning a vertex to a community if it has at least one edge in that community, thereby allowing for overlapping clusters of vertices. We apply the idea behind edge clustering to temporal hypergraphs, an extension of a graph where a single edge can contain any number of vertices and each edge has a timestamp. Extending to hypergraphs allows for many different patterns of interaction between edges, and by defining a suitable structural similarity function, our edge clustering algorithm can find clusters of these patterns. We test the algorithm with three structural similarity functions on a large collaboration hypergraph, and find intuitive cluster structures that could prove useful for downstream tasks.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling nodes and hyperedges via random walks on large hypergraphs</title>
<link>https://arxiv.org/abs/2502.19030</link>
<guid>https://arxiv.org/abs/2502.19030</guid>
<content:encoded><![CDATA[
<div> Random walks, hypergraphs, sampling, non-backtracking, estimation <br />
Summary: 
This study focuses on sampling methods for large hypergraphs, which are complex systems involving interactions among multiple entities. The research compares different random walk techniques for hypergraph sampling, highlighting the effectiveness of the higher-order random walk. A non-backtracking variant of this method is introduced, with theoretical results and validation through numerical simulations on empirical hypergraphs. The non-backtracking higher-order random walk is then applied to a hypergraph of co-authorships, demonstrating accurate estimations despite limited data access. The findings contribute to the advancement of analysis methods for large hypergraphs, providing insights into sampling strategies and estimation techniques for real-world complex systems. <div>
arXiv:2502.19030v2 Announce Type: replace 
Abstract: Hypergraphs provide a fundamental framework for representing complex systems involving interactions among three or more entities. As empirical hypergraphs grow in size, characterizing their structural properties becomes increasingly challenging due to computational complexity and, in some cases, restricted access to complete data, requiring efficient sampling methods. Random walks offer a practical approach to hypergraph sampling, as they rely solely on local neighborhood information from nodes and hyperedges. In this study, we investigate methods for simultaneously sampling nodes and hyperedges via random walks on large hypergraphs. First, we compare three existing random walks in the context of hypergraph sampling and identify an advantage of the so-called higher-order random walk. Second, by extending an established technique for graphs to the case of hypergraphs, we present a non-backtracking variant of the higher-order random walk. We derive theoretical results on estimators based on the non-backtracking higher-order random walk and validate them through numerical simulations on large empirical hypergraphs. Third, we apply the non-backtracking higher-order random walk to a large hypergraph of co-authorships indexed in the OpenAlex database, where full access to the data is not readily available. Despite the relatively small sample size, our estimates largely align with previous findings on author productivity, team size, and the prevalence of open-access publications. Our findings contribute to the development of analysis methods for large hypergraphs, offering insights into sampling strategies and estimation techniques applicable to real-world complex systems.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Clustering for Directed Graphs via Likelihood Estimation on Stochastic Block Models</title>
<link>https://arxiv.org/abs/2403.19516</link>
<guid>https://arxiv.org/abs/2403.19516</guid>
<content:encoded><![CDATA[
<div> Keywords: graph clustering, spectral clustering, directed graphs, stochastic block models, maximum likelihood estimation<br />
Summary:<br />
The paper proposes a novel spectral clustering algorithm for directed graphs based on statistical inference on stochastic block models. By utilizing maximum likelihood estimation under a directed stochastic block model, a global objective function is derived to capture community structure in directed graphs. The algorithm introduces a self-adaptive spectral clustering method that surpasses existing baselines in both synthetic and real-world datasets. The theoretical upper bound on misclustering error for its spectral relaxation validates the efficacy of the proposed approach. By extending traditional spectral clustering methods to directed graphs, this work fills a gap in existing research and offers significant performance improvements in a fundamental task of unsupervised learning with various real-world applications. <div>
arXiv:2403.19516v2 Announce Type: replace-cross 
Abstract: Graph clustering is a fundamental task in unsupervised learning with broad real-world applications. While spectral clustering methods for undirected graphs are well-established and guided by a minimum cut optimization consensus, their extension to directed graphs remains relatively underexplored due to the additional complexity introduced by edge directions. In this paper, we leverage statistical inference on stochastic block models to guide the development of a spectral clustering algorithm for directed graphs. Specifically, we study the maximum likelihood estimation under a widely used directed stochastic block model, and derive a global objective function that aligns with the underlying community structure. We further establish a theoretical upper bound on the misclustering error of its spectral relaxation, and based on this relaxation, introduce a novel, self-adaptive spectral clustering method for directed graphs. Extensive experiments on synthetic and real-world datasets demonstrate significant performance gains over existing baselines.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Bounds for the Network Scale-Up Method</title>
<link>https://arxiv.org/abs/2407.10640</link>
<guid>https://arxiv.org/abs/2407.10640</guid>
<content:encoded><![CDATA[
<div> NSUM, Network Scale-Up Method, hidden sub-population, estimation error, analytical bounds <br />
Summary:
- The Network Scale-Up Method (NSUM) has been utilized for over thirty years to estimate hidden sub-populations within social networks.
- This study provides analytical bounds on the error incurred by the two main NSUM estimators, considering well-behaved social network topology and hidden sub-population distribution assumptions.
- The analysis reveals that the estimate can deviate by a factor of (n) from the true value if an adversary designs the network and places the hidden sub-population.
- Error bounds are also established for randomly generated networks, indicating that a small constant factor can be achieved with high probability using samples of logarithmic size O(log n).
- Improved analytical bounds for Erdos-Renyi and Scale-Free networks are presented, supported by numerical experiments to assess the impact of sample size on estimate accuracy in synthetic and real networks. <br /> <div>
arXiv:2407.10640v2 Announce Type: replace-cross 
Abstract: Epidemiologists and social scientists have used the Network Scale-Up Method (NSUM) for over thirty years to estimate the size of a hidden sub-population within a social network. This method involves querying a subset of network nodes about the number of their neighbours belonging to the hidden sub-population. In general, NSUM assumes that the social network topology and the hidden sub-population distribution are well-behaved; hence, the NSUM estimate is close to the actual value. However, bounds on NSUM estimation errors have not been analytically proven. This paper provides analytical bounds on the error incurred by the two most popular NSUM estimators. These bounds assume that the queried nodes accurately provide their degree and the number of neighbors belonging to the hidden population. Our key findings are twofold. First, we show that when an adversary designs the network and places the hidden sub-population, then the estimate can be a factor of $\Omega(\sqrt{n})$ off from the real value (in a network with $n$ nodes). Second, we also prove error bounds when the underlying network is randomly generated, showing that a small constant factor can be achieved with high probability using samples of logarithmic size $O(\log{n})$. We present improved analytical bounds for Erdos-Renyi and Scale-Free networks. Our theoretical analysis is supported by an extensive set of numerical experiments designed to determine the effect of the sample size on the accuracy of the estimates in both synthetic and real networks.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Heterophily Meets Heterogeneity: Challenges and a New Large-Scale Graph Benchmark</title>
<link>https://arxiv.org/abs/2407.10916</link>
<guid>https://arxiv.org/abs/2407.10916</guid>
<content:encoded><![CDATA[
<div> real-world graphs, heterophily, heterogeneity, graph mining, node-classification<br />
<br />
Summary: <br />
The article introduces H2GB, a benchmark for node classification in large-scale real-world graphs that exhibit heterogeneity and heterophily. Existing benchmarks have focused on either homophilic heterogeneous or heterophilic homogeneous graphs, leaving a gap in understanding model performance on graphs with both properties. H2GB includes 9 real-world datasets from 5 domains, 28 baseline models, and a benchmarking library with standardized tools for data loading, evaluation, and modeling. Results from experiments with 28 baselines show that current methods struggle with these complex graphs, highlighting the need for better approaches. The article also presents a new model, H2G-former, developed following the standardized workflow, which performs well on the benchmark. The benchmark and framework are publicly available for researchers to use. <div>
arXiv:2407.10916v2 Announce Type: replace-cross 
Abstract: Graph mining has become crucial in fields such as social science, finance, and cybersecurity. Many large-scale real-world networks exhibit both heterogeneity, where multiple node and edge types exist in the graph, and heterophily, where connected nodes may have dissimilar labels and attributes. However, existing benchmarks primarily focus on either heterophilic homogeneous graphs or homophilic heterogeneous graphs, leaving a significant gap in understanding how models perform on graphs with both heterogeneity and heterophily. To bridge this gap, we introduce H2GB, a large-scale node-classification graph benchmark that brings together the complexities of both the heterophily and heterogeneity properties of real-world graphs. H2GB encompasses 9 real-world datasets spanning 5 diverse domains, 28 baseline models, and a unified benchmarking library with a standardized data loader, evaluator, unified modeling framework, and an extensible framework for reproducibility. We establish a standardized workflow supporting both model selection and development, enabling researchers to easily benchmark graph learning methods. Extensive experiments across 28 baselines reveal that current methods struggle with heterophilic and heterogeneous graphs, underscoring the need for improved approaches. Finally, we present a new variant of the model, H2G-former, developed following our standardized workflow, that excels at this challenging benchmark. Both the benchmark and the framework are publicly available at Github and PyPI, with documentation hosted at https://junhongmit.github.io/H2GB.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidence of equilibrium dynamics in human social networks evolving in time</title>
<link>https://arxiv.org/abs/2410.11635</link>
<guid>https://arxiv.org/abs/2410.11635</guid>
<content:encoded><![CDATA[
<div> equilibrium, network dynamics, social interactions, statistical physics, stability <br />
Summary:<br />
The study investigates how networks of relationships evolve over time by analyzing social interactions of 900 individuals over four years. Despite constant shifts in individual relationships, the overall structure of the network remains stable, fluctuating within predictable limits. The researchers connect this stability to the concept of equilibrium in statistical physics, showing that the probabilities governing network dynamics stay constant over time. Key features such as degree, edge, and triangle abundances align with theoretical predictions from equilibrium dynamics, and the dynamics exhibit detailed balance. This equilibrium persists even with continual turnover in individuals. The findings suggest that equilibrium arises from the interaction of human needs, cognitive limits, and social pressures. The practical implications include simplifying data collection, supporting methods like Exponential Random Graph Models, and aiding in designing interventions for social challenges. Theoretical insights are gained into collective human behavior and how simple mathematical models can capture emergent properties of complex social systems. <br /> <div>
arXiv:2410.11635v4 Announce Type: replace-cross 
Abstract: How do networks of relationships evolve over time? We analyse a dataset tracking the social interactions of 900 individuals over four years. Despite continuous shifts in individual relationships, the macroscopic structural properties of the network remain stable, fluctuating within predictable bounds. We connect this stability to the concept of equilibrium in statistical physics. Specifically, we demonstrate that the probabilities governing network dynamics are stationary over time, and key features like degree, edge, and triangle abundances align with theoretical predictions from equilibrium dynamics. Moreover, the dynamics satisfies the detailed balance condition. Remarkably, equilibrium persists despite constant turnover as people join, leave, and change connections. This suggests that equilibrium arises not from specific individuals but from the balancing act of human needs, cognitive limits, and social pressures. Practically, this equilibrium simplifies data collection, supports methods relying on single network snapshots (like Exponential Random Graph Models), and aids in designing interventions for social challenges. Theoretically, it offers new insights into collective human behaviour, revealing how emergent properties of complex social systems can be captured by simple mathematical models.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making</title>
<link>https://arxiv.org/abs/2504.02856</link>
<guid>https://arxiv.org/abs/2504.02856</guid>
<content:encoded><![CDATA[
<div> algorithmic fairness, discrimination, credibility deficit, innovation diffusion, epistemic bias

Summary:<br />
- Algorithmic fairness is typically discussed from an ethical standpoint, but the epistemic dimension related to knowledge transmission and validation is equally important.
- Exclusion from knowledge dissemination due to credibility deficit/excess can lead to harm on a societal scale.
- Discrimination can impact the diffusion of innovations within a network by shaping individual attitudes and social connections.
- Formalizing the epistemic properties of a social environment, particularly in innovation diffusion models, is crucial for fair policy design.
- Extending the Linear Threshold Model (LTM) to incorporate epistemic biases in innovation diffusion highlights the significance of the epistemic dimension in decision-making for algorithmic fairness. 

<br /><br />Summary: <div>
arXiv:2504.02856v2 Announce Type: replace-cross 
Abstract: Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Non-uniqueness of Node Co-occurrence Matrices of Hypergraphs</title>
<link>https://arxiv.org/abs/2506.01479</link>
<guid>https://arxiv.org/abs/2506.01479</guid>
<content:encoded><![CDATA[
<div> Keywords: Hypergraphs, Node co-occurrence matrix, Non-unique projections, Search algorithm, Computational tools

Summary:
Hypergraphs, which capture multi-way interactions, are often simplified by projecting them onto weighted and undirected networks. However, the resulting node co-occurrence matrix is non-unique, leading to the loss of structural information from the original hypergraph. This study introduces a search algorithm to identify all hypergraphs corresponding to a given projection, analyzing its runtime and parallelizability. By applying the algorithm to projections from a random hypergraph model, the researchers identify conditions under which projections are non-unique. This work offers a new framework and computational tools to examine hypergraph projections, shedding light on the complexity of hypergraph data and the challenges in preserving its original structure. <div>
arXiv:2506.01479v1 Announce Type: new 
Abstract: Hypergraphs extend traditional networks by capturing multi-way or group interactions. Given the complexity of hypergraph data and the wide range of methodology available for pairwise network analysis, hypergraph data is often projected onto a weighted and undirected network. The simplest of these projections, often referred to as a node co-occurrence matrix, is known to be non-unique, as distinct non-isomorphic hypergraphs can produce the same weighted adjacency matrix. This non-uniqueness raises important questions about the structural information lost during the projection and how to efficiently quantify the complexity of the original hypergraph. Here we develop a search algorithm to identify all hypergraphs corresponding to a given projection, analyze its runtime, and explore its parallelisability. Applying this algorithm to projections derived from a random hypergraph model, we characterize conditions under which projections are non-unique. Our findings provide a new framework and set of computational tools to investigate projections of hypergraphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catching Stray Balls: Football, fandom, and the impact on digital discourse</title>
<link>https://arxiv.org/abs/2506.01642</link>
<guid>https://arxiv.org/abs/2506.01642</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional responses, online discourse, Reddit, sentiment analysis, online toxicity

Summary:
This paper investigates how emotional responses to football matches influence online discourse on Reddit. The study analyzed millions of posts across various subreddits and found that real-world events trigger shifts in sentiment that spread across different communities. Negative sentiment is associated with problematic language, and match outcomes directly impact both sentiment and posting behavior. The research also shows that sentiment can transfer to unrelated communities, highlighting the interconnected nature of digital spaces. These findings shed light on how online environments function as emotional ecosystems susceptible to cross-domain contagion triggered by real-world events, contributing to the propagation of online toxicity. While football serves as a case study, the patterns identified have broader implications for understanding online communities as a whole. 

<br /><br />Summary: 
- Emotional responses to football matches influence online discourse on Reddit. 
- Real-world events trigger sentiment shifts that move across communities. 
- Negative sentiment correlates with problematic language. 
- Match outcomes directly influence sentiment and posting habits. 
- Sentiment can transfer to unrelated communities. 
- This research enhances understanding of online toxicity propagation and community interconnectedness. <div>
arXiv:2506.01642v1 Announce Type: new 
Abstract: This paper examines how emotional responses to football matches influence online discourse across digital spaces on Reddit. By analysing millions of posts from dozens of subreddits, it demonstrates that real-world events trigger sentiment shifts that move across communities. It shows that negative sentiment correlates with problematic language; match outcomes directly influence sentiment and posting habits; sentiment can transfer to unrelated communities; and offers insights into the content of this shifting discourse. These findings reveal how digital spaces function not as isolated environments, but as interconnected emotional ecosystems vulnerable to cross-domain contagion triggered by real-world events, contributing to our understanding of the propagation of online toxicity. While football is used as a case-study to computationally measure affective causes and movements, these patterns have implications for understanding online communities broadly.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Performance Evolutionary Multiobjective Community Detection Algorithm</title>
<link>https://arxiv.org/abs/2506.01752</link>
<guid>https://arxiv.org/abs/2506.01752</guid>
<content:encoded><![CDATA[
<div> Keywords: community structure, complex networks, multi-objective optimization, evolutionary algorithm, scalability <br />
Summary: 
The article introduces HP-MOCD, a high-performance evolutionary algorithm for community detection in complex networks. Traditional methods like Louvain and Leiden focus on single-objective optimization, which may not capture the multifaceted nature of real-world networks. HP-MOCD, based on NSGA-II, addresses this limitation by simultaneously optimizing multiple structural criteria. It utilizes topology-aware genetic operators and parallelism to efficiently explore the solution space and generate diverse community partitions. Experimental results on large synthetic benchmarks show that HP-MOCD outperforms existing multi-objective methods in runtime while achieving comparable or superior detection accuracy. This positions HP-MOCD as a scalable and practical solution for community detection in large and complex networks. <br /><br />Summary: <div>
arXiv:2506.01752v1 Announce Type: new 
Abstract: Community structure is a key feature of complex networks, underpinning a diverse range of phenomena across social, biological, and technological systems. While traditional methods, such as Louvain and Leiden, offer efficient solutions, they rely on single-objective optimization, often failing to capture the multifaceted nature of real-world networks. Multi-objective approaches address this limitation by considering multiple structural criteria simultaneously, but their high computational cost restricts their use in large-scale settings. We propose HP-MOCD, a high-performance, fully parallel evolutionary algorithm based on NSGA-II, designed to uncover high-quality community structures by jointly optimizing conflicting objectives. HP-MOCD leverages topology-aware genetic operators and parallelism to efficiently explore the solution space and generate a diverse Pareto front of community partitions. Experimental results on large synthetic benchmarks demonstrate that HP-MOCD consistently outperforms existing multi-objective methods in runtime, while achieving superior or comparable detection accuracy. These findings position HP-MOCD as a scalable and practical solution for community detection in large, complex networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral alignment in social networks</title>
<link>https://arxiv.org/abs/2506.00046</link>
<guid>https://arxiv.org/abs/2506.00046</guid>
<content:encoded><![CDATA[
<div> learning approach, self-exploration, introspective learning, coordination, anti-coordination 

Summary:<br />
- Study on networked systems of coordinating and anti-coordinating individuals
- Analyzes the effects of system dynamics, network structure, and behavioral patterns
- Explores practical questions such as number of equilibria, equilibrium time, and system resilience
- Finds that the number of equilibrium states can be extremely large
- Shows the impact of network structure on average equilibrium time
- Simple network characteristic, the average path length, can capture variations
<br />Summary: <div>
arXiv:2506.00046v1 Announce Type: cross 
Abstract: The orderly behaviors observed in large-scale groups, such as fish schooling and the organized movement of crowds, are both ubiquitous and essential for the survival and stability of these systems. Such complex collective behaviors often emerge from simple local interactions and strategy adjustments among individuals. Understanding how these basic rules shape complex group dynamics has long been a significant scientific challenge. Historically, research has predominantly focused on imitation and social learning, where individuals adopt the strategies of more successful peers to refine their behavior. However, in recent years, an alternative learning approach, self-exploration and introspective learning, has garnered increasing attention. In this paradigm, individuals assess their own circumstances and select strategies that best align with their specific conditions. Two primary forms of this learning are coordination and anti-coordination, where individuals align with and diverge from the local majority, respectively. In this study, we analyze networked systems of coordinating and anti-coordinating individuals, exploring the combined effects of system dynamics, network structure, and behavioral patterns. We address several practical questions, including the number of equilibria, their characteristics, the equilibrium time, and the resilience of systems. We find that the number of equilibrium states can be extremely large, even increasing exponentially with minor alternations to the network structure. Moreover, the network structure has a significant impact on the average equilibrium time. Despite the complexity of these findings, variations can be captured by a single, simple network characteristic: the average path length. Our research offers valuable insights into how modifications to the interaction structure can influence behavioral alignment in social networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs to Advance the Cognitive Science of Collectives</title>
<link>https://arxiv.org/abs/2506.00052</link>
<guid>https://arxiv.org/abs/2506.00052</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, individual cognition, collective cognition, complexity, risks

Summary:
Large language models (LLMs) have revolutionized the study of individual cognition, but their potential application to understanding collective cognition remains largely untapped. This article explores how LLMs can help researchers address the complexities that have long impeded the study of groups and societies. By leveraging the power of LLMs, researchers may gain insights into how collective cognition emerges and operates. However, the use of LLMs in studying collective cognition also poses new risks, such as reinforcing biases or oversimplifying complex social dynamics. Therefore, new methodologies and frameworks must be developed to ensure the responsible and accurate application of LLMs in studying collectives. This article highlights the promising opportunities and challenges associated with employing LLMs in the study of collective cognition. <br /><br />Summary: Large language models have the potential to revolutionize the study of collective cognition by addressing longstanding complexities. However, their use also presents new risks that require careful consideration and the development of appropriate methodologies. <div>
arXiv:2506.00052v1 Announce Type: cross 
Abstract: LLMs are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</title>
<link>https://arxiv.org/abs/2506.00074</link>
<guid>https://arxiv.org/abs/2506.00074</guid>
<content:encoded><![CDATA[
<div> LLMs, open-weight, expert recommendation, physics, biases <br />
Summary: This paper evaluates the performance of six open-weight LLMs in recommending experts in physics, analyzing consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. The evaluation reveals inconsistencies and biases across all models, with mixtral-8x7b producing the most stable outputs and llama3.1-70b showing the highest variability. Many models exhibit duplication and formatting errors. While LLMs generally recommend real scientists, accuracy drops in specific queries, favoring senior scholars and replicating gender imbalances. They also under-represent Asian scientists and over-represent White scholars, reinforcing the rich-get-richer effect and offering limited geographical representation. The findings underline the need to enhance LLMs for more reliable and equitable scholarly recommendations. <br /> <div>
arXiv:2506.00074v1 Announce Type: cross 
Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of higher-order interactions and homophily on information access inequality</title>
<link>https://arxiv.org/abs/2506.00156</link>
<guid>https://arxiv.org/abs/2506.00156</guid>
<content:encoded><![CDATA[
<div> Keywords: socio-technical systems, hypergraphs, hyperedge homophily, social contagion, information access<br />
<br />
Summary: 
The study explores the spread of information in socio-technical systems and its impact on the distribution of opportunities. The research introduces the H3 model, focusing on hypergraphs with hyperedge homophily and varying degree distribution. It also presents a model for nonlinear social contagion incorporating in-group and out-group node dynamics. Through simulations and empirical data analysis, the study highlights how hyperedge homophily and social contagion dynamics interact, influencing group-level differences in information access. The findings emphasize the importance of considering hyperedge homophily in shaping interaction patterns and suggest targeted interventions at specific hyperedge sizes as a strategy to reduce inequality in socio-technical systems. The research underscores the need for a higher-order perspective in system design to address inequalities in information dissemination. 
<br /><br /> <div>
arXiv:2506.00156v1 Announce Type: cross 
Abstract: The spread of information through socio-technical systems determines which individuals are the first to gain access to opportunities and insights. Yet, the pathways through which information flows can be skewed, leading to systematic differences in access across social groups. These inequalities remain poorly characterized in settings involving nonlinear social contagion and higher-order interactions that exhibit homophily. We introduce a enerative model for hypergraphs with hyperedge homophily, a hyperedge size-dependent property, and tunable degree distribution, called the $\texttt{H3}$ model, along with a model for nonlinear social contagion that incorporates asymmetric transmission between in-group and out-group nodes. Using stochastic simulations of a social contagion process on hypergraphs from the $\texttt{H3}$ model and diverse empirical datasets, we show that the interaction between social contagion dynamics and hyperedge homophily -- an effect unique to higher-order networks due to its dependence on hyperedge size -- can critically shape group-level differences in information access. By emphasizing how hyperedge homophily shapes interaction patterns, our findings underscore the need to rethink socio-technical system design through a higher-order perspective and suggest that dynamics-informed, targeted interventions at specific hyperedge sizes, embedded in a platform architecture, offer a powerful lever for reducing inequality.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings</title>
<link>https://arxiv.org/abs/2506.00277</link>
<guid>https://arxiv.org/abs/2506.00277</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model embeddings, clustering, multilingual, hierarchical, news articles

Summary:<br /><br />
This study introduces a novel approach for clustering news articles and social media data utilizing multilingual Matryoshka embeddings. These embeddings allow for determining story similarity at various levels of granularity based on subsets of dimensions, achieving high performance on the SemEval 2022 Task 8 dataset. The proposed hierarchical clustering algorithm efficiently leverages the hierarchical structure of the embeddings to identify unique news stories, narratives, and themes. By training multilingual Matryoshka embeddings and implementing a hierarchical clustering algorithm, the approach presented in this work addresses the scalability, interpretability, and multilingual challenges faced by current methods in topic modeling and clustering. Real-world news datasets are used to demonstrate the capability of the approach in identifying and clustering stories, narratives, and overarching themes. <div>
arXiv:2506.00277v1 Announce Type: cross 
Abstract: Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus</title>
<link>https://arxiv.org/abs/2506.00332</link>
<guid>https://arxiv.org/abs/2506.00332</guid>
<content:encoded><![CDATA[
<div> Keywords: code-mixing, corpus, multilingual communication, computational linguistics, NLP

Summary: 
The article introduces the first labeled and general-purpose corpus for understanding code-mixing in linguistic elements from multiple languages within a single discourse. It aims to provide a dataset for modeling human conversations and relationships in various languages, especially English, Mandarin, and others. The Codemix Corpus consists of over 355,641 messages with detailed metadata and linguistic statistics. The project gathers, verifies, and integrates code-mixed messages to maintain privacy and ethical standards. The corpus will be released in JSON format to serve as a foundation for research in computational linguistics, sociolinguistics, and Natural Language Processing applications.<br /><br />Summary: <div>
arXiv:2506.00332v1 Announce Type: cross 
Abstract: Code-mixing involves the seamless integration of linguistic elements from multiple languages within a single discourse, reflecting natural multilingual communication patterns. Despite its prominence in informal interactions such as social media, chat messages and instant-messaging exchanges, there has been a lack of publicly available corpora that are author-labeled and suitable for modeling human conversations and relationships. This study introduces the first labeled and general-purpose corpus for understanding code-mixing in context while maintaining rigorous privacy and ethical standards. Our live project will continuously gather, verify, and integrate code-mixed messages into a structured dataset released in JSON format, accompanied by detailed metadata and linguistic statistics. To date, it includes over 355,641 messages spanning various code-mixing patterns, with a primary focus on English, Mandarin, and other languages. We expect the Codemix Corpus to serve as a foundational dataset for research in computational linguistics, sociolinguistics, and NLP applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Higher-Order Analysis of Multivariate Time Series</title>
<link>https://arxiv.org/abs/2506.00508</link>
<guid>https://arxiv.org/abs/2506.00508</guid>
<content:encoded><![CDATA[
<div> pattern identification, complex systems, multivariate time series, higher-order interactions, network theory

Summary:
The article introduces a new method for detecting dependencies of any order in multivariate time series data. It involves transforming the data into symbolic sequences and identifying statistically significant strings of symbols using a Bayesian approach. These motifs are represented as hyperedges in a hypergraph, allowing for the study of higher-order interactions in the original data using network theory. When applied to neural and social systems, the method uncovers meaningful higher-order dependencies, shedding light on their importance in brain function and social behavior. This approach offers a novel way to analyze complex systems and has practical applications in various fields. <div>
arXiv:2506.00508v1 Announce Type: cross 
Abstract: Identifying patterns of relations among the units of a complex system from measurements of their activities in time is a fundamental problem with many practical applications. Here, we introduce a method that detects dependencies of any order in multivariate time series data. The method first transforms a multivariate time series into a symbolic sequence, and then extract statistically significant strings of symbols through a Bayesian approach. Such motifs are finally modelled as the hyperedges of a hypergraph, allowing us to use network theory to study higher-order interactions in the original data. When applied to neural and social systems, our method reveals meaningful higher-order dependencies, highlighting their importance in both brain function and social behaviour.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling the Spread of Epidemics on Networks with Differential Privacy</title>
<link>https://arxiv.org/abs/2506.00745</link>
<guid>https://arxiv.org/abs/2506.00745</guid>
<content:encoded><![CDATA[
<div> Vaccination, epidemic spread control, heterogeneous contact network, differential privacy, vaccination strategies<br />
<br />
Summary: Designing effective vaccination strategies for controlling epidemic spread in the presence of limited vaccines is crucial in epidemiology. This task becomes particularly challenging in heterogeneous contact networks where traditional strategies may not suffice. The focus of this study is on developing vaccination strategies while ensuring differential privacy guarantees, especially when the network structure is sensitive. The authors propose $(\varepsilon,\delta)$-differentially private algorithms that aim to reduce the maximum degree and spectral radius of the network. A key technique introduced in the study is a private algorithm for the multi-set multi-cover problem, which aids in controlling network properties while preserving privacy. The evaluation of privacy-utility tradeoffs on both synthetic and real-world networks demonstrates the effectiveness of the proposed algorithms. <div>
arXiv:2506.00745v1 Announce Type: cross 
Abstract: Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\varepsilon,\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies</title>
<link>https://arxiv.org/abs/2506.00770</link>
<guid>https://arxiv.org/abs/2506.00770</guid>
<content:encoded><![CDATA[
<div> adjacency structures, graph attention networks, spatio-temporal forecasting, InterGAT, interpretability<br />
<br />
Summary: 
InterGAT is proposed as a simplified alternative to Graph Attention Networks (GAT) for spatio-temporal forecasting, removing the need for predefined adjacency structures and dynamic attention scores. The InterGAT-GRU framework, incorporating a GRU-based temporal decoder, outperforms GAT-GRU in forecasting accuracy on datasets such as SZ-Taxi and Los-Loop, showing a significant improvement. Moreover, InterGAT reduces training time by 60-70% compared to the baseline. The learned interaction matrix in InterGAT indicates interpretable structures, revealing sparse attention patterns aligned with community structure. Spectral and clustering analyses demonstrate that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This work showcases how structure learning can enhance prediction accuracy, computational efficiency, and topological interpretability in dynamic graph-based domains. 
<br /><br />Summary: <div>
arXiv:2506.00770v1 Announce Type: cross 
Abstract: Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high</title>
<link>https://arxiv.org/abs/2506.01814</link>
<guid>https://arxiv.org/abs/2506.01814</guid>
<content:encoded><![CDATA[
<div> propaganda, anti-U.S. sentiment, language models, bias, cross-lingual comparison
Summary:
This study compared the biases of a PRC-aligned language model (DeepSeek-R1) and a non-PRC counterpart (ChatGPT o3-mini-high) in detecting Chinese-state propaganda and anti-U.S. sentiment. A novel corpus of questions from Chinese news was used, with evaluations done in Simplified Chinese, Traditional Chinese, and English. DeepSeek-R1 exhibited higher levels of propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, especially in Simplified Chinese queries. The biases were not limited to political topics but also appeared in cultural and lifestyle content, showing an "invisible loudspeaker" effect. DeepSeek-R1 sometimes responded in the wrong language and amplified PRC-aligned terms in its answers. Language-specific biases were observed, with Simplified Chinese queries evoking the highest bias rates. The study sheds light on the ideological biases present in large language models and their impact on public understanding and decision-making processes.<br /><br />Summary: <div>
arXiv:2506.01814v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an "invisible loudspeaker" effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Public versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability</title>
<link>https://arxiv.org/abs/2305.11943</link>
<guid>https://arxiv.org/abs/2305.11943</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, news engagement, public sphere, Facebook, content moderation

Summary:<br /><br />The study compares interaction patterns between public and less public spheres on Facebook for news articles. Only 31% of news interactions occur in the public sphere, with variations across news classes. Users engage more deeply in less-public spaces. It is essential to consider both public and less-public engagement for a comprehensive understanding of news dissemination on Facebook. This has implications for content moderation, platform governance, and policymaking, contributing to healthier online discourse. <div>
arXiv:2305.11943v3 Announce Type: replace 
Abstract: The rapid growth of social media as a news platform has raised significant concerns about the influence and societal impact of biased and unreliable news on these platforms. While much research has explored user engagement with news on platforms like Facebook, most studies have focused on publicly shared posts. This focus leaves an important question unanswered: how representative is the public sphere of Facebook's entire ecosystem? Specifically, how much of the interactions occur in less-public spaces, and do public engagement patterns for different news classes (e.g., reliable vs. unreliable) generalize to the broader Facebook ecosystem?
  This paper presents the first comprehensive comparison of interaction patterns between Facebook's more public sphere (referred to as public in paper) and the less public sphere (referred to as private). For the analysis, we first collect two complementary datasets: (1) aggregated interaction data for all Facebook posts (public + private) for 19,050 manually labeled news articles (225.3M user interactions), and (2) a subset containing only interactions with public posts (70.4M interactions). Then, through discussions and iterative feedback from the CrowdTangle team, we develop a robust method for fair comparison between these datasets.
  Our analysis reveals that only 31% of news interactions occur in the public sphere, with significant variations across news classes. Engagement patterns in less-public spaces often differ, with users, for example, engaging more deeply in private contexts. These findings highlight the need to examine both public and less-public engagement to fully understand news dissemination on Facebook. The observed differences hold important implications on content moderation, platform governance, and policymaking, contributing to healthier online discourse.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BTS: A Comprehensive Benchmark for Tie Strength Prediction</title>
<link>https://arxiv.org/abs/2410.19214</link>
<guid>https://arxiv.org/abs/2410.19214</guid>
<content:encoded><![CDATA[
<div> tie strength prediction, benchmark, pseudo-label techniques, dataset collection, evaluation framework

Summary:<br />
The article introduces BTS, a Benchmark for Tie Strength prediction, to standardize the evaluation of tie strength prediction methodologies. It categorizes tie strength into seven pseudo-labeling techniques and presents a dataset collection from three social networks for analysis. A framework is proposed to evaluate the quality of pseudo-labels based on tie resilience. Existing tie strength prediction models are evaluated using the BTS dataset, exploring different experiment settings, models, and evaluation criteria. The study provides insights to enhance current methods and suggests future research directions. The BTS dataset, curation codes, and experimental scripts are available for further study. <div>
arXiv:2410.19214v4 Announce Type: replace 
Abstract: The rapid rise of online social networks underscores the need to understand the heterogeneous strengths of online relationships. Yet, efforts to assess tie strength (TS) are hindered by the lack of ground-truth labels, differing research perspectives, and limited model performance in real-world settings. To address this gap, we introduce BTS, a comprehensive Benchmark for Tie Strength prediction, aiming to establish a standardized foundation for evaluating and advancing TS prediction methodologies. Specifically, our contributions are: TS Pseudo-Label Techniques -- we categorize TS into seven standardized pseudo-labeling techniques based on prior literature; TS Dataset Collection -- we present a representative collection of three social networks and perform data analysis by investigating the class distributions and correlations across the generated pseudo-labels; TS Pseudo-Label Evaluation Framework -- we propose a standardized framework to evaluate the pseudo-label quality from the perspective of tie resilience; Benchmarking -- we evaluate existing tie strength prediction model performance using the BTS dataset collection, exploring the effects of different experiment settings, models, and evaluation criteria on the results. Furthermore, we derive key insights to enhance existing methods and shed light on promising directions for future research in this domain. The BTS dataset collection, along with the curation codes, and experimental scripts are all available at: https://github.com/XueqiC/Awesome-Tie-Strength-Prediction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalisation of Voter Model: Influential Nodes and Convergence Properties</title>
<link>https://arxiv.org/abs/2411.04564</link>
<guid>https://arxiv.org/abs/2411.04564</guid>
<content:encoded><![CDATA[
<div> NP-hard, approximation algorithm, seeding strategy, convergence properties, social networks  
Summary:  
- The article introduces a generalization of the voter model for social networks, addressing issues such as varying connection strengths, neutral opinions, and reluctance to update opinions.  
- It examines the problem of selecting seed nodes to maximize the number of positive opinions, proving it NP-hard and providing a polynomial time approximation algorithm.  
- Experimental results show the proposed algorithm outperforms others on real-world and synthetic graph data.  
- The model's convergence properties are studied, showing that convergence time can be exponential but is polynomial for strongly connected graphs, with the period of convergence dividing the length of all cycles in the graph. <div>
arXiv:2411.04564v2 Announce Type: replace 
Abstract: Consider an undirected graph G, representing a social network, where each node is blue or red, corresponding to positive or negative opinion on a topic. In the voter model, in discrete time rounds, each node picks a neighbour uniformly at random and adopts its colour. Despite its significant popularity, this model does not capture some fundamental real-world characteristics such as the difference in the strengths of individuals connections, individuals with neutral opinion on a topic, and individuals who are reluctant to update their opinion. To address these issues, we introduce and study a generalisation of the voter model. Motivating by campaigning strategies, we study the problem of selecting a set of seeds blue nodes to maximise the expected number of blue nodes after some rounds. We prove that the problem is NP- hard and provide a polynomial time approximation algorithm with the best possible approximation guarantee. Our experiments on real-world and synthetic graph data demonstrate that the proposed algorithm outperforms other algorithms. We also investigate the convergence properties of the model. We prove that the process could take an exponential number of rounds to converge. However, if we limit ourselves to strongly connected graphs, the convergence time is polynomial and the period (the number of states in convergence) divides the length of all cycles in the graph.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Collaborative Anti-Money Laundering Among Financial Institutions</title>
<link>https://arxiv.org/abs/2502.19952</link>
<guid>https://arxiv.org/abs/2502.19952</guid>
<content:encoded><![CDATA[
<div> money laundering, anti-money laundering, machine learning, graph-based learning, transaction graph

Summary:<br /><br />Money laundering is a critical issue that needs to be addressed effectively through anti-money laundering measures. Traditional rule-based methods have limitations in detecting illicit activities, leading to the exploration of machine learning and graph-based learning methods. However, current approaches often assume centralized transaction graphs, which may not reflect real-world scenarios where money laundering spans multiple institutions. To address this gap, a novel algorithm is proposed to conduct anti-money laundering across multiple institutions while ensuring data security and privacy. The algorithm is evaluated using a real-world dataset from Alipay and ECB, showing promising results in identifying cross-institution money laundering subgroups efficiently. Experimental results on both real and synthetic datasets highlight the algorithm's effectiveness and efficiency in detecting illicit activities. <div>
arXiv:2502.19952v2 Announce Type: replace 
Abstract: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bypassing Skip-Gram Negative Sampling: Dimension Regularization as a More Efficient Alternative for Graph Embeddings</title>
<link>https://arxiv.org/abs/2405.00172</link>
<guid>https://arxiv.org/abs/2405.00172</guid>
<content:encoded><![CDATA[
<div> Keywords: graph embedding, Skip-Gram Negative Sampling, dimension regularization, scalability, link prediction

Summary:
In this study, the researchers explore a new approach to enforcing dissimilarity in graph embeddings by introducing dimension regularization as a more efficient alternative to Skip-Gram Negative Sampling (SGNS). They show that when embeddings approach collapse, SGNS repulsion operates as an approximate re-centering of node embedding dimensions, leading to simplified geometric interpretations. By proposing a flexible algorithm augmentation framework that prioritizes node attraction and incorporates dimension regularization, the researchers improve the scalability of existing algorithms such as LINE and node2vec. Their augmented algorithms maintain link prediction performance while significantly reducing GPU memory usage and training time. The study demonstrates that completely removing repulsion can lead to substantial time savings, particularly in sparse but locally dense graphs. Overall, dimension regularization offers a more efficient and scalable solution for enforcing dissimilarity in graph embeddings compared to traditional SGNS approaches. 

Summary:<br /><br /> <div>
arXiv:2405.00172v2 Announce Type: replace-cross 
Abstract: A wide range of graph embedding objectives decompose into two components: one that enforces similarity, attracting the embeddings of nodes that are perceived as similar, and another that enforces dissimilarity, repelling the embeddings of nodes that are perceived as dissimilar. Without repulsion, the embeddings would collapse into trivial solutions. Skip-Gram Negative Sampling (SGNS) is a popular and efficient repulsion approach that prevents collapse by repelling each node from a sample of dissimilar nodes. In this work, we show that when repulsion is most needed and the embeddings approach collapse, SGNS node-wise repulsion is, in the aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are more scalable than node operations and produce a simpler geometric interpretation of the repulsion. Our theoretical result establishes dimension regularization as an effective and more efficient, compared to skip-gram node contrast, approach to enforcing dissimilarity among embeddings of nodes. We use this result to propose a flexible algorithm augmentation framework that improves the scalability of any existing algorithm using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream link-prediction performance while reducing GPU memory usage by up to 33.3% and training time by 23.4%. Moreover, we show that completely removing repulsion (a special case of our augmentation framework) in LINE reduces training time by 70.9% on average, while increasing link prediction performance, especially for graphs that are globally sparse but locally dense. In general, however, repulsion is needed, and dimension regularization provides an efficient alternative to SGNS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</title>
<link>https://arxiv.org/abs/2412.08174</link>
<guid>https://arxiv.org/abs/2412.08174</guid>
<content:encoded><![CDATA[
<div> Contrastive Language-Image Pre-training, Graph Neural Networks, Multi-modal prompt learning, Few-shot learning, Zero-shot classification <br />
Summary: <br />
This work addresses the challenge of adapting pre-trained Graph Neural Networks (GNNs) to downstream tasks using a multi-modal prompt learning paradigm. By embedding graphs and text prompts in the same space as Large Language Models (LLMs), the proposed approach enables effective adaptation with only a few labeled samples and weak text supervision. Superior performance is demonstrated in few-shot, multi-task-level, and cross-domain settings, showcasing the paradigm's versatility. Additionally, a CLIP-style zero-shot classification prototype is developed, allowing GNNs to generalize to unseen classes with minimal text supervision. The code for this approach is available on GitHub at https://github.com/Violet24K/Morpher. <div>
arXiv:2412.08174v3 Announce Type: replace-cross 
Abstract: While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media</title>
<link>https://arxiv.org/abs/2412.18148</link>
<guid>https://arxiv.org/abs/2412.18148</guid>
<content:encoded><![CDATA[
<div> detect, AIGTs, social media, dataset, AI attribution rate

Summary:<br />
This paper focuses on quantifying and monitoring AI-Generated Texts (AIGTs) on social media platforms. They collected a dataset from major platforms like Medium, Quora, and Reddit to train and evaluate AIGT detectors. The best-performing detector, OSM-Det, was identified and used to track AIGTs from January 2022 to October 2024. The AI Attribution Rate (AAR) was used as a metric, showing significant increases in AAR for Medium and Quora, with slower growth on Reddit. Analysis revealed differences in linguistic patterns, topic distributions, engagement levels, and author follower distributions between AIGTs and human-written texts. The findings aim to contribute to future research on AIGTs in social media platforms. 

Summary:  <div>
arXiv:2412.18148v3 Announce Type: replace-cross 
Abstract: Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, it remains unclear how prevalent AIGTs are on social media. To address this gap, this paper aims to quantify and monitor the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs across social media platforms from January 2022 to October 2024, using the AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs on social media differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes a good feedforward computational graph?</title>
<link>https://arxiv.org/abs/2502.06751</link>
<guid>https://arxiv.org/abs/2502.06751</guid>
<content:encoded><![CDATA[
<div> Graph rewiring, computational graph, neural network, performance, undirected graphs <br />
<br />
Summary: 
The choice of computational graph used in a neural network is crucial for its performance. This study focuses on feedforward computational graphs, specifically directed graphs without back edges. Two important measures, fidelity and mixing time, are identified and evaluated for popular graph choices. Theoretical analyses of these metrics' behavior and their correlation with neural network model performance are conducted. The study sheds light on the impact of computational graph properties on a model's ability to learn functions, highlighting the significance of fidelity and mixing time. <div>
arXiv:2502.06751v2 Announce Type: replace-cross 
Abstract: As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinRipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness</title>
<link>https://arxiv.org/abs/2505.23826</link>
<guid>https://arxiv.org/abs/2505.23826</guid>
<content:encoded><![CDATA[
<div> Keywords: financial markets, ripple effects, large language models, reinforcement learning, asset pricing theory 

Summary:<br />
The study introduces FinRipple, a framework that enhances large language models (LLMs) with the ability to analyze ripple effects in financial markets. It addresses the limitations of traditional event studies by incorporating a time-varying knowledge graph to represent market structure accurately. By integrating classical asset pricing theory, the framework aligns the LLM with the market, enabling it to predict ripple effects. This approach offers a standardized definition of ripple effect prediction, a crucial yet unexplored task in finance. Through extensive experiments, FinRipple demonstrates promising results in predicting ripple effects and capturing complex dynamics in financial markets. By combining LLMs with reinforcement learning and financial theory, FinRipple provides a novel solution for analyzing ripple effects and understanding the interconnected nature of financial entities.<br />Summary: <div>
arXiv:2505.23826v1 Announce Type: new 
Abstract: Financial markets exhibit complex dynamics where localized events trigger ripple effects across entities. Previous event studies, constrained by static single-company analyses and simplistic assumptions, fail to capture these ripple effects. While large language models (LLMs) offer emergent reasoning capabilities, their direct application falters due to structural market unawareness and limited capacity to analyze ripple effects. We propose FinRipple, an elegant framework that empowers LLMs with the ability to analyze ripple effects through financial theory-guided large-scale reinforcement learning. We begin by relaxing the assumptions of previous methods, incorporating a time-varying knowledge graph to accurately represent market structure. By seamlessly integrating classical asset pricing theory, we align the LLM with the market, enabling it to predict ripple effects. To the best of our knowledge, we are the first to provide a standardized definition of ripple effect prediction, a task that is extremely important yet unexplored in the financial domain. Extensive experiments demonstrate that FinRipple provides a promising solution to this task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Academics Are Leaving Twitter for Bluesky</title>
<link>https://arxiv.org/abs/2505.24801</link>
<guid>https://arxiv.org/abs/2505.24801</guid>
<content:encoded><![CDATA[
<div> migration, academic users, Twitter/X, Bluesky, peer influence

Summary: 
The study examined the migration of 300,000 academic users from Twitter/X to Bluesky between 2023 and early 2025. It found that 18% of scholars transitioned, with rates varying by discipline, political expression, and Twitter engagement but not by traditional academic metrics. Information sources were found to have a greater influence on migration than the audience, with this influence diminishing within a week. Simple contagion drove two-thirds of all exits, shock-driven bursts accounted for 16%, and complex contagion had a minor role. Scholars who rebuilt a higher fraction of their former Twitter networks on Bluesky were more active and engaged. This research provides insights into network externalities, directional influence, and platform migration, emphasizing the importance of information sources in overcoming switching costs. 

<br /><br />Summary: <div>
arXiv:2505.24801v1 Announce Type: new 
Abstract: We analyse the migration of 300,000 academic users from Twitter/X to Bluesky between 2023 and early 2025, combining rich bibliometric data, longitudinal social-media activity, and a novel cross-platform identity-matching pipeline. We show that 18% of scholars in our sample transitioned, with transition rates varying sharply by discipline, political expression, and Twitter engagement but not by traditional academic metrics. Using time-varying Cox models and a matched-pairs design, we isolate genuine peer influence from homophily. We uncover a striking asymmetry whereby information sources drive migration far more powerfully than audience, with this influence decaying exponentially within a week. We further develop an ego-level contagion classifier, revealing that simple contagion drives two-thirds of all exits, shock-driven bursts account for 16%, and complex contagion plays a marginal role. Finally, we show that scholars who rebuild a higher fraction of their former Twitter networks on Bluesky remain significantly more active and engaged. Our findings provide new insights onto theories of network externalities, directional influence, and platform migration, highlighting information sources' central role in overcoming switching costs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Suicidal Risk on Social Media: A Hybrid Model</title>
<link>https://arxiv.org/abs/2505.23797</link>
<guid>https://arxiv.org/abs/2505.23797</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Suicide Risk, Reddit posts, RoBERTa, TF-IDF<br />
<br />
Summary:
Suicidal thoughts and behaviors are a serious societal issue demanding effective early detection tools. This study develops a robust machine learning model using Reddit posts to classify suicide risk levels. The proposed RoBERTa-TF-IDF-PCA Hybrid model combines RoBERTa deep contextual embeddings with TF-IDF statistical term-weighting, compressed with PCA, to enhance accuracy. Various data resampling and augmentation techniques were explored to address imbalances and overfitting. Comparative analysis shows the hybrid model outperforms using RoBERTa alone, BERT, and traditional classifiers, achieving a weighted $F_{1}$ score of 0.7512. The study emphasizes the importance of leveraging machine learning techniques to improve suicide risk assessment and potentially enhance early intervention strategies. <div>
arXiv:2505.23797v1 Announce Type: cross 
Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation</title>
<link>https://arxiv.org/abs/2505.24479</link>
<guid>https://arxiv.org/abs/2505.24479</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, misinformation generation, generative AI, structured semantic resources, large language models <br />
Summary:
The paper discusses the risks posed by the rapid spread of misinformation, especially with the advancement of generative AI technology. It introduces a method that uses knowledge graphs to systematically generate fake triplets, enabling the creation of misinformation with varying levels of credibility. By analyzing the structural properties of knowledge graphs, the approach identifies plausible false relationships for misinformation generation. The study also examines the effectiveness of large language models in distinguishing between genuine and artificially generated misinformation. It highlights limitations in the current detection methods and emphasizes the need for improved detection strategies and a more thorough exploration of biases in generative models. Drawing upon publicly available knowledge graphs like WikiGraphs, this deterministic approach creates misinformation that is challenging for humans to detect, underscoring the urgency of addressing misinformation threats in society. <br /><br /> <div>
arXiv:2505.24479v1 Announce Type: cross 
Abstract: The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.24534</link>
<guid>https://arxiv.org/abs/2505.24534</guid>
<content:encoded><![CDATA[
<div> method, anomalies, time-evolving, simplicial complexes, higher-order interactions <br />
Summary: 
The paper introduces HLSAD, a method for detecting anomalies in time-evolving simplicial complexes. Traditional graph anomaly detection techniques often struggle to capture changes in higher-order interactions. HLSAD leverages the spectral properties of Hodge Laplacians to model multi-way interactions. By incorporating higher-dimensional structures, HLSAD improves detection accuracy and computational efficiency. Experimental results show HLSAD outperforms existing methods in detecting events and change points. <div>
arXiv:2505.24534v1 Announce Type: cross 
Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in time-evolving simplicial complexes. While traditional graph anomaly detection techniques have been extensively studied, they often fail to capture changes in higher-order interactions that are crucial for identifying complex structural anomalies. These higher-order interactions can arise either directly from the underlying data itself or through graph lifting techniques. Our approach leverages the spectral properties of Hodge Laplacians of simplicial complexes to effectively model multi-way interactions among data points. By incorporating higher-dimensional simplicial structures into our method, our method enhances both detection accuracy and computational efficiency. Through comprehensive experiments on both synthetic and real-world datasets, we demonstrate that our approach outperforms existing graph methods in detecting both events and change points.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascades on Constrained Multiplex Networks</title>
<link>https://arxiv.org/abs/2505.24631</link>
<guid>https://arxiv.org/abs/2505.24631</guid>
<content:encoded><![CDATA[
<div> Keywords: Watts cascade model, directed multiplex configuration model networks, constrained multiplex networks, node activity, phase transitions

Summary:
The study explores the Watts cascade model on directed multiplex configuration model networks, analyzing cascade size, single-seed cascade probability, and cascade condition. A smaller class of network models, termed constrained multiplex networks, is introduced to induce patterns in node activity across different layers. The research findings indicate that the choice of induced patterns significantly impacts the phase transitions of the cascade model. This suggests that the participation of nodes in various layers plays a crucial role in determining the dynamics of cascading processes. The analysis sheds light on how network structure and node behavior influence the spread of cascades, providing insights into the complex interplay between network topology and cascading behaviors in multiplex systems. Overall, this study highlights the importance of considering node activity patterns in understanding the dynamics of cascading processes in complex networks. 

<br /><br />Summary: <div>
arXiv:2505.24631v1 Announce Type: cross 
Abstract: We consider a version of the Watts cascade model on directed multiplex configuration model networks, and present a detailed analysis of the cascade size, single-seed cascade probability and cascade condition. We then introduce a smaller class of network models that we call constrained multiplex networks, which allows us to induce patterns in the node activity, i.e. in the participation of nodes on different layers. We find that the choice of induced patterns affects the phase transitions of the cascade model in a variety of ways.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Knowledge Production Pipeline Driven by Academic Influencers</title>
<link>https://arxiv.org/abs/2505.24681</link>
<guid>https://arxiv.org/abs/2505.24681</guid>
<content:encoded><![CDATA[
<div> pipeline, generative AI, academic influencers, knowledge production, credibility<br />
<br />
Summary: 
This study explores the impact of generative AI on academic knowledge production and dissemination by analyzing 53 influencer videos with 5.3 million viewers. It identifies a structured pipeline that balances originality, ethical compliance, and human-AI collaboration, showcasing the potential of AI to automate publication workflows and democratize participation in research. Academic influencers play a crucial role in bridging bottom-up practices with institutional policies to enhance adaptability in the scholarly landscape. The study proposes a generative publication production pipeline and a policy framework for co-intelligence adaptation, promoting credibility-centered standards in AI-powered research. These insights provide valuable guidance for scholars, educators, and policymakers seeking to navigate AI's transformative influence responsibly and foster innovation in academic knowledge production. The findings also suggest ways to automate best practices, streamline scholarly workflows, and nurture creativity in research and publication processes. <div>
arXiv:2505.24681v1 Announce Type: cross 
Abstract: Generative AI transforms knowledge production, validation, and dissemination, raising academic integrity and credibility concerns. This study examines 53 academic influencer videos that reached 5.3 million viewers to identify an emerging, structured, implementation-ready pipeline balancing originality, ethical compliance, and human-AI collaboration despite the disruptive impacts. Findings highlight generative AI's potential to automate publication workflows and democratize participation in knowledge production while challenging traditional scientific norms. Academic influencers emerge as key intermediaries in this paradigm shift, connecting bottom-up practices with institutional policies to improve adaptability. Accordingly, the study proposes a generative publication production pipeline and a policy framework for co-intelligence adaptation and reinforcing credibility-centered standards in AI-powered research. These insights support scholars, educators, and policymakers in understanding AI's transformative impact by advocating responsible and innovation-driven knowledge production. Additionally, they reveal pathways for automating best practices, optimizing scholarly workflows, and fostering creativity in academic research and publication.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outgroup Animosity Has Risen for Politicians, Journalists, and a Sample of Partisan Users on Twitter and Reddit</title>
<link>https://arxiv.org/abs/2308.15556</link>
<guid>https://arxiv.org/abs/2308.15556</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, outgroup animosity, political entities, Twitter, Reddit

Summary:
Our study analyzed tweets and Reddit comments referencing political entities from 2010-2023, revealing a consistent increase in outgroup animosity, particularly among newer users. A small fraction of users are responsible for a large portion of negative content. There are differences in outgroup affect across political orientations, with right-leaning users showing more animosity towards immigration and left-leaning users towards healthcare. U.S. politicians on the left exhibit more outgroup animosity on Twitter, but in recent years, those on the right have shown a significant increase, surpassing journalists and partisan users. On Reddit, certain communities contribute significantly to polarizing rhetoric, with r/TheDonald's rise and ban shaping discourse on the right. This analysis sheds light on the dynamics of political communication and polarization in online spaces.<br /><br />Summary: <div>
arXiv:2308.15556v4 Announce Type: replace 
Abstract: Using language models, we analyze a sample of 67 million tweets and 30 million Reddit comments referencing a set of 215 political entities from 2010-2023 from partisan users, journalists, and politicians. Our analysis indicates outgroup animosity has increased consistently in our sample, with newer cohorts of users expressing higher levels of animosity than previous ones. Moreover, a small fraction of users are responsible for a disproportionate share of this negative content. We observe systematic differences in topic-level outgroup affect across political orientations: right-leaning users are twice as likely to exhibit outgroup animosity when discussing immigration, while left-leaning users show heightened outgroup animosity when discussing healthcare. On Twitter, U.S. politicians on the left exhibit more outgroup animosity than partisan users in our sample, but in the past four years, politicians on the right have experienced the sharpest rise in outgroup animosity, surpassing journalists, media, and partisan users. On Reddit, a small number of communities account for a large share of polarizing rhetoric, with the rise and eventual ban of r/TheDonald significantly shaping polarizing discourse on the right.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backfire Effect Reveals Early Controversy in Online Media</title>
<link>https://arxiv.org/abs/2403.02708</link>
<guid>https://arxiv.org/abs/2403.02708</guid>
<content:encoded><![CDATA[
<div> Keywords: online media, conflict detection, psychological mechanisms, gradient features, controversy detection<br />
Summary:<br />
The study focuses on conflict detection in online discussions, emphasizing the importance of understanding the underlying psychological motivations behind conflicts. By analyzing the "ascending gradient of likes" in conflicting posts, the research introduces new gradient features that enhance controversy detection models. Results from evaluations across Chinese and English media show the significance of gradient features in detecting controversy compared to other structural, interactive, and textual features. The study suggests that the emergence of the ascending gradient may be linked to the "backfire effect" in ideological conflicts. The features derived from psychological mechanisms exhibit excellent detection performance, particularly in scenarios with limited hot or early information. This research offers a fresh perspective on analyzing and early detecting online conflict behavior. <br /> <div>
arXiv:2403.02708v2 Announce Type: replace 
Abstract: The rapid development of online media has significantly facilitated the public's information consumption, knowledge acquisition, and opinion exchange. However, it has also led to more violent conflicts in online discussions. Therefore, controversy detection becomes important for computational and social sciences. Previous research on detection methods has primarily focused on larger datasets and more complex computational models but has rarely examined the underlying mechanisms of conflict, particularly the psychological motivations behind them. In this paper, we present evidence that conflicting posts tend to have a high proportion of "ascending gradient of likes", i.e., replies get more likes than comments. Additionally, there is a gradient in the number of replies between the neighboring tiers as well. We develop two new gradient features and demonstrate the common enhancement effect of our features in terms of controversy detection models. Further, multiple evaluation algorithms are used to compare structural, interactive, and textual features with the new features across multiple Chinese and English media. The results show that it is a general case that gradient features are significantly different in terms of controversy and are more important than other features. More thoroughly, we discuss the mechanism by which the ascending gradient emerges, suggesting that the case is related to the "backfire effect" in ideological conflicts that have received recent attention. The features formed by the psychological mechanism also show excellent detection performance in application scenarios where only a few hot information or early information are considered. Our findings can provide a new perspective for online conflict behavior analysis and early detection.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random walk based snapshot clustering for detecting community dynamics in temporal networks</title>
<link>https://arxiv.org/abs/2412.12187</link>
<guid>https://arxiv.org/abs/2412.12187</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, random walk, community structure, structural shifts, agent-based algorithm

Summary: 
The paper introduces a novel random walk-based approach for identifying stable clusters of time-snapshots in temporal networks, allowing the detection of significant structural shifts such as community splitting or merging. A low-dimensional representation of entire snapshots is provided, grouping similar community structures together in the feature space. An agent-based algorithm is developed to generate synthetic datasets for testing and benchmarking the approach. Testing on social dynamics models and real-world datasets shows the technique's effectiveness compared to state-of-the-art algorithms. The research demonstrates the ability of the approach to accurately capture and analyze the dynamics of complex systems.<br /><br />Summary: <div>
arXiv:2412.12187v2 Announce Type: replace 
Abstract: The evolution of many dynamical systems that describe relationships or interactions between objects can be effectively modeled by temporal networks, which are typically represented as a sequence of static network snapshots. In this paper, we introduce a novel random walk-based approach that can identify clusters of time-snapshots in which network community structures are stable. This allows us to detect significant structural shifts over time, such as the splitting or merging of communities or their births and deaths. We also provide a low-dimensional representation of entire snapshots, placing those with similar community structure close to each other in the feature space. To validate our approach, we develop an agent-based algorithm that generates synthetic datasets with the desired characteristic properties, enabling thorough testing and benchmarking. We further demonstrate the effectiveness and broad applicability of our technique by testing it on various social dynamics models and real-world datasets and comparing its performance to several state-of-the-art algorithms. Our findings highlight the strength of our approach to correctly capture and analyze the dynamics of complex systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment</title>
<link>https://arxiv.org/abs/2502.02017</link>
<guid>https://arxiv.org/abs/2502.02017</guid>
<content:encoded><![CDATA[
<div> pre-training, graph foundation models, Multi-Domain Graph Foundation Model, topological information, knowledge transfer
Summary:
The article discusses the challenges of developing general-purpose graph foundation models due to differences in graph topologies and the presence of noise and adversarial attacks in real-world graphs. To address these issues, the proposed Multi-Domain Graph Foundation Model (MDGFM) aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. By balancing features and topology and eliminating noise in original graphs, MDGFM bridges different domains and enables efficient prompt-tuning for enhanced knowledge transfer. The model's effectiveness and domain generalization capabilities are theoretically analyzed and validated through experiments on homophilic and heterophilic graph datasets, demonstrating its robustness and efficacy in improving multi-domain pre-training and knowledge transfer to unseen domains. <div>
arXiv:2502.02017v2 Announce Type: replace 
Abstract: Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Invisible Psychological Traits Organize Visible Network Structure? A Complex Network Analysis of Myers-Briggs Type Indicator-Based Interaction Patterns in Anonymous Social Networks</title>
<link>https://arxiv.org/abs/2503.19704</link>
<guid>https://arxiv.org/abs/2503.19704</guid>
<content:encoded><![CDATA[
<div> Keywords: personality traits, social interactions, online communities, MBTI, network science

Summary: 
Personality traits impact social interactions on anonymous online chat systems, as shown by analyzing data from over 6,000 users. While heterophilous interactions dominate, there is also a moderate level of homophily, particularly among certain personality types. The network exhibits scale-free properties, with gender playing a stronger role in homophily compared to MBTI type. Despite minor influences on interaction preferences, community structure shows low modularity. These findings suggest that psychological traits subtly shape online behavior, blending exploratory heterophily with subtle homophilic inclinations. <div>
arXiv:2503.19704v2 Announce Type: replace 
Abstract: Exploration of the impact of personality traits on social interactions within anonymous online communities poses a challenge at the interface of networked social sciences and psychology. We analyze whether Myers-Briggs Type Indicator (MBTI) personality types impact the dynamics of interactions on an anonymous chat system with over 288,000 messages from 6,076 users. Using a data set including 940 users voluntarily providing MBTI typing and gender, we create a weighted undirected network and apply network-science measures-such as assortativity, centrality measures, and community detection with the Louvain algorithm-to estimate the level of personality-based homophily and heterophily. Contrary to previous observations in structured social settings, our research shows a dominance of heterophilous interactions (89.3%), particularly among cognitively complementary types, i.e., NT (Intuitive-Thinking) and NF (Intuitive-Feeling). However, there is a moderate level of personality-based homophily (10.7%), notably among introverted intuitive personalities (e.g., INTJ, INFP, INFJ), reflecting an underlying cognitive alignment that persists regardless of identity markers. The interaction network exhibits scale-free properties with a power-law exponent of 1.45. In contrast, gender is a stronger homophily attribute, as evidenced by stronger levels of female users' group interactions compared with male users. While MBTI type influences minor interaction preferences, community structure exhibits low modularity (Q = 0.2584). The findings indicate that, in the absence of identity cues, psychological traits subtly shape online behavior, blending exploratory heterophily with subtle homophilic inclinations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewriting Structured Cospans</title>
<link>https://arxiv.org/abs/2001.09029</link>
<guid>https://arxiv.org/abs/2001.09029</guid>
<content:encoded><![CDATA[
<div> structured cospans, compositional networks, rewriting, category theory, topos
<br />
Summary:
Structured cospans are further developed to study compositional networks, introducing a category of structured cospans for rewriting purposes. The conditions for this category to be a topos or adhesive are characterized. A structured cospan grammar and language are defined using a 2-category framework. The study demonstrates that different kinds of graphs, hypergraphs, and Petri nets yield the same language through grammars, allowing for an inductive perspective of rewriting on these structures. <div>
arXiv:2001.09029v2 Announce Type: replace-cross 
Abstract: To support the study of compositional networks, we further develop the formalism of structured cospans. To enable rewriting, we introduce a category whose objects are structured cospans and characterize conditions under which this category is a topos or is adhesive. We then define a structured cospan grammar and language, using a 2-category framework. As an application, we demonstrate that for various sorts of graphs, hypergraphs, and Petri nets, any grammar induces the same language as its corresponding discrete grammar. This result allows us to extend the inductive perspective of rewriting to these structures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning</title>
<link>https://arxiv.org/abs/2505.22684</link>
<guid>https://arxiv.org/abs/2505.22684</guid>
<content:encoded><![CDATA[
<div> fairness, community partitioning, network analysis, protected group networks, modularity optimization

Summary:
The article introduces a novel fairness-modularity metric for community partitioning in network analysis. Traditional modularity-based methods often lack fairness, which is crucial in real-world applications. The proposed metric incorporates fairness into modularity optimization, resulting in naturally fair partitions for protected groups while maintaining theoretical soundness. A general optimization framework for fairness partitioning is developed, along with the efficient Fair Fast Newman (FairFN) algorithm. This algorithm enhances the Fast Newman (FN) method to optimize both modularity and fairness. Experiments demonstrate that FairFN outperforms state-of-the-art methods in achieving significantly improved fairness and high-quality partitions, particularly on unbalanced datasets. <div>
arXiv:2505.22684v1 Announce Type: new 
Abstract: Community partitioning is crucial in network analysis, with modularity optimization being the prevailing technique. However, traditional modularity-based methods often overlook fairness, a critical aspect in real-world applications. To address this, we introduce protected group networks and propose a novel fairness-modularity metric. This metric extends traditional modularity by explicitly incorporating fairness, and we prove that minimizing it yields naturally fair partitions for protected groups while maintaining theoretical soundness. We develop a general optimization framework for fairness partitioning and design the efficient Fair Fast Newman (FairFN) algorithm, enhancing the Fast Newman (FN) method to optimize both modularity and fairness. Experiments show FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, especially on unbalanced datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLUE: Bi-layer Heterogeneous Graph Fusion Network for Avian Influenza Forecasting</title>
<link>https://arxiv.org/abs/2505.22692</link>
<guid>https://arxiv.org/abs/2505.22692</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, avian influenza, outbreak forecasting, multi-layer information, infectious disease<br />
<br />
Summary: BLUE is a Bi-Layer heterogeneous graph fusion network designed to integrate genetic, spatial, and ecological data for accurate avian influenza outbreak forecasting. The framework creates heterogeneous graphs from multiple information sources and layers, smoothing across different relation types while retaining structural patterns. It predicts future outbreaks using an autoregressive graph sequence model that captures transmission dynamics over time. The Avian-US dataset is introduced to facilitate further research, incorporating genetic, spatial, and ecological data across locations. BLUE outperforms existing baselines, demonstrating the value of incorporating multi-layer information for infectious disease forecasting. <div>
arXiv:2505.22692v1 Announce Type: new 
Abstract: Accurate forecasting of avian influenza outbreaks within wild bird populations requires models that account for complex, multi-scale transmission patterns driven by various factors. Spatio-temporal GNN-based models have recently gained traction for infection forecasting due to their ability to capture relations and flow between spatial regions, but most existing frameworks rely solely on spatial connections and their connections. This overlooks valuable genetic information at the case level, such as cases in one region being genetically descended from strains in another, which is essential for understanding how infectious diseases spread through epidemiological linkages beyond geography. We address this gap with BLUE, a B}i-Layer heterogeneous graph fUsion nEtwork designed to integrate genetic, spatial, and ecological data for accurate outbreak forecasting. The framework 1) builds heterogeneous graphs from multiple information sources and multiple layers, 2) smooths across relation types, 3) performs fusion while retaining structural patterns, and 4) predicts future outbreaks via an autoregressive graph sequence model that captures transmission dynamics over time. To facilitate further research, we introduce \textbf{Avian-US} dataset, the dataset for avian influenza outbreak forecasting in the United States, incorporating genetic, spatial, and ecological data across locations. BLUE achieves superior performance over existing baselines, highlighting the value of incorporating multi-layer information into infectious disease forecasting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Map Matching Based on Localization Error Distribution Modeling</title>
<link>https://arxiv.org/abs/2505.23123</link>
<guid>https://arxiv.org/abs/2505.23123</guid>
<content:encoded><![CDATA[
<div> LED modeling, map matching, trajectory, public transit, non-shortest path detection 

Summary:
The article introduces a novel offline map matching method, LNSP, designed for sparse trajectories in intelligent transportation systems (ITS). LNSP addresses limitations of existing methods by incorporating more detailed LED modeling based on public transit trajectories and optimizing path search ranges across different city regions. It also introduces a scoring mechanism using sub-region dependency LED and a sliding window to improve matching accuracy. Experimental results with real-world bus and taxi trajectory datasets show that LNSP outperforms current methods in both efficiency and accuracy. Overall, LNSP presents a significant advancement in offline map matching for historical trajectories with positional errors, providing a more effective solution for route analysis and traffic pattern mining in ITS applications. 

<br /><br />Summary: <div>
arXiv:2505.23123v1 Announce Type: new 
Abstract: Offline map matching involves aligning historical trajectories of mobile objects, which may have positional errors, with digital maps. This is essential for applications in intelligent transportation systems (ITS), such as route analysis and traffic pattern mining. Existing methods have two main limitations: (i) they assume a uniform Localization Error Distribution (LED) across urban areas, neglecting environmental factors that lead to suboptimal path search ranges, and (ii) they struggle to efficiently handle local non-shortest paths and detours. To address these issues, we propose a novel offline map matching method for sparse trajectories, called LNSP, which integrates LED modeling and non-shortest path detection. Key innovations include: (i) leveraging public transit trajectories with fixed routes to model LED in finer detail across different city regions, optimizing path search ranges, and (ii) scoring paths using sub-region dependency LED and a sliding window, which reduces global map matching errors. Experimental results using real-world bus and taxi trajectory datasets demonstrate that the LNSP algorithm significantly outperforms existing methods in both efficiency and matching accuracy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homologous nodes in annotated complex networks</title>
<link>https://arxiv.org/abs/2505.23668</link>
<guid>https://arxiv.org/abs/2505.23668</guid>
<content:encoded><![CDATA[
<div> Keywords: real-world networks, metadata, annotations, node grouping, functional roles<br />
Summary: <br />
This study introduces a novel approach to analyzing annotated networks by considering both node annotations and network structure. Nodes are grouped based on similar distributions of annotations in their neighborhoods, regardless of whether they are connected. These groupings, termed homologues, reveal common functional roles and properties among nodes in the network. The approach was applied to three diverse real-world networks, demonstrating its effectiveness in identifying functional relationships within complex networks. By combining quantitative analysis of annotations with network topology, this method offers a comprehensive understanding of network organization and can complement traditional community detection techniques. The concept of homologues provides valuable insights into the functional roles of nodes in a network, shedding light on the underlying relationships and interactions within complex systems. <div>
arXiv:2505.23668v1 Announce Type: new 
Abstract: Many real-world networks have associated metadata that assigns categorical labels to nodes. Analysis of these annotations can complement the topological analysis of complex networks. Annotated networks have typically been used to evaluate community detection approaches. Here, we introduce an approach that combines the quantitative analysis of annotations and network structure, which groups nodes according to similar distributions of node annotations in their neighbourhoods. Importantly the nodes that are grouped together, which we call homologues may not be connected to each other at all. By applying our approach to three very different real-world networks we show that these groupings identify common functional roles and properties of nodes in the network.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing Higher-Order Networks with Spectral Moments</title>
<link>https://arxiv.org/abs/2505.23691</link>
<guid>https://arxiv.org/abs/2505.23691</guid>
<content:encoded><![CDATA[
<div> Graphs, spectral properties, higher-order networks, spectral moments, random walks <br />
<br />
Summary: 
The study focuses on the spectral properties of higher-order networks, extending the analysis typically done on traditional dyadic graphs. By splitting higher-order graphs into uniform hypergraphs based on edge orders, the authors extract spectral information through random walks. Spectral moments computed from each spectrum serve as a representation for higher-order graphs, reflecting properties such as return probabilities and network characteristics. This new representation demonstrates improved performance in graph classification compared to other techniques. The research highlights the utility of spectral moments in capturing complex higher-order network structures and properties, showcasing their relevance in various applications. <div>
arXiv:2505.23691v1 Announce Type: new 
Abstract: The spectral properties of traditional (dyadic) graphs, where an edge connects exactly two vertices, are widely studied in different applications. These spectral properties are closely connected to the structural properties of dyadic graphs. We generalize such connections and characterize higher-order networks by their spectral information. We first split the higher-order graphs by their ``edge orders" into several uniform hypergraphs. For each uniform hypergraph, we extract the corresponding spectral information from the transition matrices of carefully designed random walks. From each spectrum, we compute the first few spectral moments and use all such spectral moments across different ``edge orders" as the higher-order graph representation. We show that these moments not only clearly indicate the return probabilities of random walks but are also closely related to various higher-order network properties such as degree distribution and clustering coefficient. Extensive experiments show the utility of this new representation in various settings. For instance, graph classification on higher-order graphs shows that this representation significantly outperforms other techniques.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Signed Networks to Group Graphs</title>
<link>https://arxiv.org/abs/2505.22802</link>
<guid>https://arxiv.org/abs/2505.22802</guid>
<content:encoded><![CDATA[
<div> group graph, symmetry, network, balance, dynamics
Summary:
Group graphs capture symmetry in processes on network nodes by labeling links with group elements. They generalize signed networks, with balance generalizing to group graphs. The dynamics on a consistent group graph are solely determined by network topology, not the symmetry group, extending previous findings on signed and complex networks. <div>
arXiv:2505.22802v1 Announce Type: cross 
Abstract: I show that when there is a symmetry in a process defined on the nodes of a network, this can be captured by a new structure, the ``group graph'', in which group elements label the links of a network. I show that group graphs are a generalisation of signed networks which are an example of a $Z_2$ group graph. I also show that the concept of balance in signed networks can be generalised to group graphs. Finally, I show how the dynamics of processes on a consistent group graph are completely controlled by the topology of the underlying network, not by the symmetry group. This generalises recent results on signed networks (Tian and Lambiotte, 2024a) and complex networks (Tian and Lambiotte, 2024b).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Benefits and Side Effects of Labeling AI-Generated Images</title>
<link>https://arxiv.org/abs/2505.22845</link>
<guid>https://arxiv.org/abs/2505.22845</guid>
<content:encoded><![CDATA[
<div> AI-generated images, misinformation, labeling, trust, deception 
Summary:
The study examines the impact of labeling AI-generated images on users' ability to recognize misinformation. Qualitative focus groups highlight users' concerns about labeling implementation but acknowledge its potential in identifying AI-generated images. However, a quantitative survey reveals that while labels may help identify AI-generated misinformation, users may over-rely on them. Inaccurate claims supported by labeled AI-generated images are seen as less credible, but the belief in accurate claims also decreases with labeled AI-generated images. Surprisingly, human-made images conveying inaccurate claims are perceived as more credible when labeled as AI-generated. This suggests that labels may have unintended consequences on users' perception of information credibility. Overall, the study highlights the complexity of implementing AI labeling to combat misinformation and the need for further research to understand its full impact. 
<br /><br />Summary: <div>
arXiv:2505.22845v1 Announce Type: cross 
Abstract: Generative artificial intelligence is developing rapidly, impacting humans' interaction with information and digital media. It is increasingly used to create deceptively realistic misinformation, so lawmakers have imposed regulations requiring the disclosure of AI-generated content. However, only little is known about whether these labels reduce the risks of AI-generated misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we study the implications of labels, including the possibility of mislabeling. Assuming that simplicity, transparency, and trust are likely to impact the successful adoption of such labels, we first qualitatively explore users' opinions and expectations of AI labeling using five focus groups. Second, we conduct a pre-registered online survey with over 1300 U.S. and EU participants to quantitatively assess the effect of AI labels on users' ability to recognize misinformation containing either human-made or AI-generated images. Our focus groups illustrate that, while participants have concerns about the practical implementation of labeling, they consider it helpful in identifying AI-generated images and avoiding deception. However, considering security benefits, our survey revealed an ambiguous picture, suggesting that users might over-rely on labels. While inaccurate claims supported by labeled AI-generated images were rated less credible than those with unlabeled AI-images, the belief in accurate claims also decreased when accompanied by a labeled AI-generated image. Moreover, we find the undesired side effect that human-made images conveying inaccurate claims were perceived as more credible in the presence of labels.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Politics of Decentralized Social Media Protocols</title>
<link>https://arxiv.org/abs/2505.22962</link>
<guid>https://arxiv.org/abs/2505.22962</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized social media, protocols, power dynamics, infrastructure, values

Summary: 
This article examines four decentralized social media protocols - ActivityPub, AT Protocol, Nostr, and Farcaster - to understand how they operationalize decentralization. Through an analysis of protocol documentation, media coverage, and interviews with developers, the study develops a framework for evaluating how power is distributed within each protocol. The research emphasizes that different protocols distribute control over key components in varying ways, impacting who wields influence over decision-making. The arrangement of components also influences how power dynamics play out among component owners, shaping the overall structure of social media platforms. By viewing protocols as artifacts that reflect underlying values, the study argues for a more conscious evaluation and design of decentralized platforms that align with desired social and political futures. This holistic framework provides a guide for assessing and developing decentralized platforms that prioritize decentralization, transparency, and equitable power distribution. 

<br /><br />Summary: <div>
arXiv:2505.22962v1 Announce Type: cross 
Abstract: Calls to decentralize feed-based social media have been driven by concerns about the concentrated power of centralized platforms and their societal impact. In response, numerous decentralized social media protocols have emerged, each interpreting "decentralization" in different ways. We analyze four such protocols -- ActivityPub, AT Protocol, Nostr, and Farcaster -- to develop a novel conceptual framework for understanding how protocols operationalize decentralization. Drawing from protocol documentation, media coverage, and first-hand interviews with protocol developers and experts, we contextualize each protocol's approach within their respective socio-technical goals. Our framework highlights how control over key components is distributed differently across each protocol, shaping who holds power over what kinds of decisions. How components are arranged in relation to one another further impacts how component owners might offset each other's power in shaping social media. We argue that examining protocols as artifacts reveals how values shape infrastructure and power dynamics -- and that with a holistic framework as a guide, we can more effectively evaluate and design decentralized platforms aligned with the social and political futures we envision.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating AI capabilities in detecting conspiracy theories on YouTube</title>
<link>https://arxiv.org/abs/2505.23570</link>
<guid>https://arxiv.org/abs/2505.23570</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube, conspiracy theories, Large Language Models, harmful content, multimodal models <br />
<br />
Summary: 
This study examines the use of Large Language Models (LLMs) in identifying conspiracy theory videos on YouTube. Text-based LLMs show high recall but reduced precision, leading to more false positives. Multimodal models, integrating visual data, lag behind their text-only counterparts. The best-performing model, RoBERTa, approaches the accuracy of larger LLMs on an unlabeled dataset, highlighting the need for precise and robust detection systems for harmful online content. The study emphasizes the prevalence of harmful content, such as disinformation and conspiracy theories, on online platforms like YouTube, and underscores the importance of developing more effective strategies for detecting and mitigating its spread. <div>
arXiv:2505.23570v1 Announce Type: cross 
Abstract: As a leading online platform with a vast global audience, YouTube's extensive reach also makes it susceptible to hosting harmful content, including disinformation and conspiracy theories. This study explores the use of open-weight Large Language Models (LLMs), both text-only and multimodal, for identifying conspiracy theory videos shared on YouTube. Leveraging a labeled dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot setting and compare their performance to a fine-tuned RoBERTa baseline. Results show that text-based LLMs achieve high recall but lower precision, leading to increased false positives. Multimodal models lag behind their text-only counterparts, indicating limited benefits from visual data integration. To assess real-world applicability, we evaluate the most accurate models on an unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with a larger number of parameters. Our work highlights the strengths and limitations of current LLM-based approaches for online harmful content detection, emphasizing the need for more precise and robust systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PureRank: A Parameter-Free Recursive Importance Measure for Network Nodes</title>
<link>https://arxiv.org/abs/2501.00417</link>
<guid>https://arxiv.org/abs/2501.00417</guid>
<content:encoded><![CDATA[
<div> importance, centrality, parameter-free, PureRank, network structure

Summary:
PureRank is a parameter-free centrality measure that can be applied to arbitrary networks without the need for tuning parameters. It classifies nodes based on their connection types and computes importance vectors for each class, aggregating them globally using the recursive definition of importance principle. This design allows for parallel and incremental computation, making it scalable for large or dynamic networks. PureRank reduces to Seeley centrality on strongly connected networks and has a probabilistic interpretation through a random-surfer model. Unlike PageRank, PureRank provides unique and consistent importance scores for any input network. The measure also extends to signed networks, offering a robust and transparent alternative to traditional recursive definition of importance-based centralities. <div>
arXiv:2501.00417v3 Announce Type: replace 
Abstract: Classical parameter-free centrality measures based on the recursive definition of importance (RDI), such as eigenvector centrality and Seeley centrality, are limited to strongly connected networks, while widely used methods like Katz centrality and PageRank rely on free parameters, such as the damping factor, to handle general networks. This motivates our central question: can an RDI-based importance (centrality) measure be defined for arbitrary networks without any tunable parameters? We answer this by introducing $PureRank$, a parameter-free recursive importance measure that faithfully reflects intrinsic network structure. PureRank classifies nodes into recurrent, transient, and dangling classes using strongly connected component (SCC) decomposition, computes local importance vectors for these classes, and aggregates them globally via the RDI principle. This modular, RDI-based design enables parallel and incremental computation -- ensuring scalability even for large or dynamic networks -- and, crucially, reduces to Seeley centrality (a classical and $pure$ RDI-based measure) on strongly connected networks. Furthermore, PureRank admits a probabilistic interpretation via a random-surfer model. Our numerical experiments illustrate that, unlike PageRank, PureRank yields unique and consistent importance scores for any input network. We also present theoretical extensions to signed networks. These results indicate that PureRank is a robust and transparent alternative to classical RDI-based centralities including PageRank.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Link Prediction in Co-Authorship Networks Based on Author Node-Based Features</title>
<link>https://arxiv.org/abs/2505.21673</link>
<guid>https://arxiv.org/abs/2505.21673</guid>
<content:encoded><![CDATA[
<div> collaboration, social networks, link prediction, research interests, affiliations<br />
Summary:<br />
The article introduces a supervised learning framework for predicting future research collaborations between authors in academic social networks. The framework considers various metrics, including research interest similarity, affiliation similarity, research performance indices aggregation, and node similarity, to predict potential links between authors in co-authorship networks. By integrating these metrics, the framework aims to improve the accuracy of link prediction. Experimental results on large-scale academic social networks such as ArnetMiner and DBLP demonstrate the effectiveness of the proposed approach in identifying potential collaborations between authors. This research fills a gap in existing literature by examining the impact of research interests and affiliations on link prediction performance. The framework provides a comprehensive and integrated approach to link prediction in academic social networks, highlighting the importance of considering multiple factors in predicting future collaborations among authors. <div>
arXiv:2505.21673v1 Announce Type: new 
Abstract: Predicting the emergence of future research collaborations between authors in academic social networks (SNs) is a very effective example that demonstrates the link prediction problem. This problem refers to predicting the potential existence or absence of a link between a pair of nodes (authors) on the co-authorship network. Various similarity and aggregation metrics were proposed in the literature for predicting the potential link between two authors on such networks. However, the relevant research did not investigate the impact of similarity of research interests of two authors or the similarity of their affiliations on the performance of predicting the potential link between them. Additionally, the impact of the aggregation of the research performance indices of two authors on link prediction performance was not highlighted. To this end, in this paper we propose an integrative supervised learning framework for predicting potential collaboration in co-authorship network based on similarity of the research interests and the similarity of the affiliations of each pair of authors in this network. Moreover, our proposed framework integrates the aggregation of research performance indices of each author pair and the similarity between the two authors nodes with the research interest and affiliation similarity as four metrics for predicting the potential link between each two authors. Our experimental results obtained from applying our proposed link prediction approach to the two largest connected graphs of two huge academic co-authorship networks, namely ArnetMiner and DBLP, show the great performance of this approach in predicting potential links between two authors on large-scale academic SNs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network classification through random walks</title>
<link>https://arxiv.org/abs/2505.21706</link>
<guid>https://arxiv.org/abs/2505.21706</guid>
<content:encoded><![CDATA[
<div> Random walks, network structure, feature extraction, classification problem, statistical metrics  
Summary:  
- Networks are widely used to study various systems and their behaviors.  
- The study aims to classify systems based on network structure, using random walk statistics.  
- Existing methods combine structural measurements and dynamical processes for feature extraction.  
- The proposed method utilizes statistics from random walks, showing effectiveness in many cases compared to other approaches.  
- While the method generally outperforms existing ones, limitations are observed in certain datasets.  

<br /><br />Summary: <div>
arXiv:2505.21706v1 Announce Type: new 
Abstract: Network models have been widely used to study diverse systems and analyze their dynamic behaviors. Given the structural variability of networks, an intriguing question arises: Can we infer the type of system represented by a network based on its structure? This classification problem involves extracting relevant features from the network. Existing literature has proposed various methods that combine structural measurements and dynamical processes for feature extraction. In this study, we introduce a novel approach to characterize networks using statistics from random walks, which can be particularly informative about network properties. We present the employed statistical metrics and compare their performance on multiple datasets with other state-of-the-art feature extraction methods. Our results demonstrate that the proposed method is effective in many cases, often outperforming existing approaches, although some limitations are observed across certain datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Narrative Divide: Cross-Platform Discourse Networks in Fragmented Ecosystems</title>
<link>https://arxiv.org/abs/2505.21729</link>
<guid>https://arxiv.org/abs/2505.21729</guid>
<content:encoded><![CDATA[
<div> social graph, information diffusion, cross-platform interactions, narrative detection, platform-agnostic framework

Summary:<br /><br />The article introduces a platform-agnostic framework for reconstructing social graphs to trace how narratives spread across fragmented political discourse on different social platforms. The method is effective in detecting information operations, predicting ideological stance, and cross-platform engagement while using less data and capturing a broader user set. Analysis of cross-platform dynamics between Truth Social and X reveals a small group of bridge users who introduce migrating narratives from one platform to another. These findings provide insights into how narratives traverse fragmented information ecosystems and have implications for cross-platform governance, content moderation, and policy interventions. <div>
arXiv:2505.21729v1 Announce Type: new 
Abstract: Political discourse has grown increasingly fragmented across different social platforms, making it challenging to trace how narratives spread and evolve within such a fragmented information ecosystem. Reconstructing social graphs and information diffusion networks is challenging, and available strategies typically depend on platform-specific features and behavioral signals which are often incompatible across systems and increasingly restricted. To address these challenges, we present a platform-agnostic framework that allows to accurately and efficiently reconstruct the underlying social graph of users' cross-platform interactions, based on discovering latent narratives and users' participation therein. Our method achieves state-of-the-art performance in key network-based tasks: information operation detection, ideological stance prediction, and cross-platform engagement prediction$\unicode{x2013}$$\unicode{x2013}$while requiring significantly less data than existing alternatives and capturing a broader set of users. When applied to cross-platform information dynamics between Truth Social and X (formerly Twitter), our framework reveals a small, mixed-platform group of $\textit{bridge users}$, comprising just 0.33% of users and 2.14% of posts, who introduce nearly 70% of $\textit{migrating narratives}$ to the receiving platform. These findings offer a structural lens for anticipating how narratives traverse fragmented information ecosystems, with implications for cross-platform governance, content moderation, and policy interventions.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Broad Spectrum Structure Discovery in Large-Scale Higher-Order Networks</title>
<link>https://arxiv.org/abs/2505.21748</link>
<guid>https://arxiv.org/abs/2505.21748</guid>
<content:encoded><![CDATA[
<div> discovered structures, probabilistic models, large-scale hypergraphs, link prediction, complex systems <br />
Summary: <br />
The paper introduces a probabilistic model for analyzing complex systems driven by higher-order interactions represented as hypergraphs. By treating classes of similar units as nodes in a latent hypergraph and utilizing low-rank representations to model interactions among classes, the model efficiently captures mesoscale structure in large-scale hypergraphs. This approach enables the identification of rich structural patterns at both node and class levels, improving link prediction and facilitating the interpretation of complex systems. Empirical results demonstrate the model's ability to discover interpretable structures in real-world systems like pharmacological and social networks, enhancing the integration of large-scale higher-order data into scientific analysis. <div>
arXiv:2505.21748v1 Announce Type: new 
Abstract: Complex systems are often driven by higher-order interactions among multiple units, naturally represented as hypergraphs. Understanding dependency structures within these hypergraphs is crucial for understanding and predicting the behavior of complex systems but is made challenging by their combinatorial complexity and computational demands. In this paper, we introduce a class of probabilistic models that efficiently represents and discovers a broad spectrum of mesoscale structure in large-scale hypergraphs. The key insight enabling this approach is to treat classes of similar units as themselves nodes in a latent hypergraph. By modeling observed node interactions through latent interactions among classes using low-rank representations, our approach tractably captures rich structural patterns while ensuring model identifiability. This allows for direct interpretation of distinct node- and class-level structures. Empirically, our model improves link prediction over state-of-the-art methods and discovers interpretable structures in diverse real-world systems, including pharmacological and social networks, advancing the ability to incorporate large-scale higher-order data into the scientific process.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retweets, Receipts, and Resistance: Discourse, Sentiment, and Credibility in Public Health Crisis Twitter</title>
<link>https://arxiv.org/abs/2505.22032</link>
<guid>https://arxiv.org/abs/2505.22032</guid>
<content:encoded><![CDATA[
<div> Twitter, CDC, COVID-19, communication, public health<br />
Summary: The study analyzes two years of CDC tweets during the COVID-19 pandemic, finding communication was mostly one-way without much user interaction. Political and ideological polarization shaped COVID-19 discussions, with users critiquing CDC guidance. Sentiment, media richness, and source credibility influenced message spread. Design strategies are proposed to help the CDC tailor communications and manage misinformation more effectively during health crises.<br /> <div>
arXiv:2505.22032v1 Announce Type: new 
Abstract: As the COVID-19 pandemic evolved, the Centers for Disease Control and Prevention (CDC) used Twitter to disseminate safety guidance and updates, reaching millions of users. This study analyzes two years of tweets from, to, and about the CDC using a mixed methods approach to examine discourse characteristics, credibility, and user engagement. We found that the CDCs communication remained largely one directional and did not foster reciprocal interaction, while discussions around COVID19 were deeply shaped by political and ideological polarization. Users frequently cited earlier CDC messages to critique new and sometimes contradictory guidance. Our findings highlight the role of sentiment, media richness, and source credibility in shaping the spread of public health messages. We propose design strategies to help the CDC tailor communications to diverse user groups and manage misinformation more effectively during high-stakes health crises.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Approach for Studying How Topological Measurements Respond to Complex Networks Modifications</title>
<link>https://arxiv.org/abs/2505.22345</link>
<guid>https://arxiv.org/abs/2505.22345</guid>
<content:encoded><![CDATA[
<div> Keywords: similarity networks, hierarchical clustering, topological measurements, complex networks, network alterations

Summary:
Three main types of complex networks (Erds-Rnyi, Barabsi-Albert, and geographical) were studied with varying sizes and subjected to edge removal or rewiring. Topological measurements such as accessibility, degree, clustering coefficient, betweenness centrality, assortativity, and average shortest path were analyzed. The coincidence similarity index was used to quantify and visualize the network alterations' impact on topological measurements. Three types of topological changes were identified, with geometrical networks showing more heterogeneous features than Erds-Rnyi and Barabsi-Albert networks. The Erds-Rnyi and Barabsi-Albert networks exhibited more similar responses to topological changes. The study provides insights into how different network modifications affect various topological measurements and how these changes are related hierarchically. <div>
arXiv:2505.22345v1 Announce Type: new 
Abstract: Different types of graphs and complex networks have been characterized, analyzed, and modeled based on measurements of their respective topology. However, the available networks may constitute approximations of the original structure as a consequence of sampling incompleteness, noise, and/or error in the representation of that structure. Therefore, it becomes of particular interest to quantify how successive modifications may impact a set of adopted topological measurements, and how respectively undergone changes can be interrelated, which has been addressed in this paper by considering similarity networks and hierarchical clustering approaches. These studies are developed respectively to several topological measurements (accessibility, degree, hierarchical degree, clustering coefficient, betweenness centrality, assortativity, and average shortest path) calculated from complex networks of three main types (Erd\H{o}s-R\'enyi, Barab\'asi-Albert, and geographical) with varying sizes or subjected to progressive edge removal or rewiring. The coincidence similarity index, which can implement particularly strict comparisons, is adopted for two main purposes: to quantify and visualize how the considered topological measurements respond to the considered network alterations and to represent hierarchically the relationships between the observed changes undergone by the considered topological measurements. Several results are reported and discussed, including the identification of three types of topological changes taking place as a consequence of the modifications. In addition, the changes observed for the Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks resulted mutually more similarly affected by topological changes than for the geometrical networks. The latter type of network has been identified to have more heterogeneous topological features than the other two types of networks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving flocking behaviors in street networks with vision</title>
<link>https://arxiv.org/abs/2505.21585</link>
<guid>https://arxiv.org/abs/2505.21585</guid>
<content:encoded><![CDATA[
<div> Keywords: flocking model, street networks, vision, alignment rule, attraction rule

Summary:
In this study, a flocking model on street networks is enhanced by expanding the field of vision for walkers, leading to more realistic results. The improved model demonstrates superior gathering times and robustness to break ups among groups of walkers compared to previous versions. This enhancement is attributed to the alignment rule with vision, ensuring walkers do not split into divergent directions at intersections, and the attraction rule with vision, which gathers distant groups together. These advancements provide valuable insights into scenarios involving walkers with collective decentralized goals, such as protests. The findings pave the way for a deeper understanding of how walkers navigate and interact in complex urban environments, shedding light on the dynamics of group behavior in real-world settings. 

<br /><br />Summary: <div>
arXiv:2505.21585v1 Announce Type: cross 
Abstract: We improve a flocking model on street networks introduced in a previous paper. We expand the field of vision of walkers, making the model more realistic. Under such conditions, we obtain groups of walkers whose gathering times and robustness to break ups are better than previous results. We explain such improvements because the alignment rule with vision guaranties walkers do not split into divergent directions at intersections anymore, and because the attraction rule with vision gathers distant groups. This paves the way to a better understanding of events where walkers have collective decentralized goals, like protests.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral clustering for dependent community Hawkes process models of temporal networks</title>
<link>https://arxiv.org/abs/2505.21845</link>
<guid>https://arxiv.org/abs/2505.21845</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, dependent community Hawkes models, spectral clustering, misclustering error, parameter estimation

Summary:
Temporal networks with community structures and dependence patterns among node pairs are common in various application settings. The article introduces dependent community Hawkes (DCH) models that combine stochastic block models with mutually exciting Hawkes processes to capture both community structure and inter-node dependencies. A non-asymptotic upper bound on misclustering error of spectral clustering on event count matrix is derived, taking into account the number of nodes, communities, time duration, and level of dependence in the model. The DCH model proposed incorporates self and reciprocal excitation, with a scalable parameter estimation using Generalized Method of Moments (GMM) estimator that shows consistency for growing network size and time duration. These statistical results provide valuable insights for modeling and analyzing temporal networks with complex structures and dependencies.<br /><br />Summary: <div>
arXiv:2505.21845v1 Announce Type: cross 
Abstract: Temporal networks observed continuously over time through timestamped relational events data are commonly encountered in application settings including online social media communications, financial transactions, and international relations. Temporal networks often exhibit community structure and strong dependence patterns among node pairs. This dependence can be modeled through mutual excitations, where an interaction event from a sender to a receiver node increases the possibility of future events among other node pairs.
  We provide statistical results for a class of models that we call dependent community Hawkes (DCH) models, which combine the stochastic block model with mutually exciting Hawkes processes for modeling both community structure and dependence among node pairs, respectively. We derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix as a function of the number of nodes and communities, time duration, and the amount of dependence in the model. Our result leverages recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector, along with results from random matrix theory. We also propose a DCH model that incorporates only self and reciprocal excitation along with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator that we demonstrate to be consistent for growing network size and time duration.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory</title>
<link>https://arxiv.org/abs/2505.22152</link>
<guid>https://arxiv.org/abs/2505.22152</guid>
<content:encoded><![CDATA[
<div> message passing neural networks, uncertainty estimation, heterophilic graphs, information theory, data distribution

Summary:
This study examines uncertainty estimation for graphs, focusing on heterophilic settings where traditional methods relying on homophily falter. By analyzing message passing neural networks (MPNNs) through an information-theoretic lens, the researchers develop a novel approach to quantify information flow throughout the network layers. Unlike non-graph domains, information about node-level predictions can increase with model depth in cases where node features differ significantly from their neighbors. This highlights the importance of considering all node representations jointly in heterophilic graph settings. By utilizing a post-hoc density estimator on the joint node embedding space, the researchers achieve state-of-the-art uncertainty estimation on heterophilic graphs without explicitly leveraging homophily. This approach offers insights for improving uncertainty estimation methods in graph-based models. 

<br /><br />Summary: <div>
arXiv:2505.22152v1 Announce Type: cross 
Abstract: While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings. We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can increase with model depth if a node's features are semantically different from its neighbors. Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings. This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily. We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Stepwise Deception: Simulating the Evolution from True News to Fake News with LLM Agents</title>
<link>https://arxiv.org/abs/2410.19064</link>
<guid>https://arxiv.org/abs/2410.19064</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news, evolution, simulation, large language model, social network

Summary: 
The article introduces FUSE, a framework for simulating the evolution of fake news from real news using Large Language Models (LLM). The simulation model includes different types of LLM agents found in social networks, such as spreaders, commentators, verifiers, and bystanders, to mimic realistic interactions that distort true news over time. The framework, called FUSE, also incorporates an evaluation component, FUSE-EVAL, to measure the deviation of fake news along various linguistic and semantic dimensions. The results indicate that FUSE successfully captures patterns of fake news evolution and accurately replicates known instances of fake news, aligning closely with human assessments. The experiments conducted demonstrate the importance of early intervention in fake news propagation and showcase the potential for future research on fake news in diverse scenarios. Overall, FUSE offers a comprehensive approach to studying the gradual formation of fake news and provides insights for preventing its spread. 

<br /><br />Summary: <div>
arXiv:2410.19064v2 Announce Type: replace 
Abstract: With the growing spread of misinformation online, understanding how true news evolves into fake news has become crucial for early detection and prevention. However, previous research has often assumed fake news inherently exists rather than exploring its gradual formation. To address this gap, we propose FUSE (Fake news evolUtion Simulation framEwork), a novel Large Language Model (LLM)-based simulation approach explicitly focusing on fake news evolution from real news. Our framework model a social network with four distinct types of LLM agents commonly observed in daily interactions: spreaders who propagate information, commentators who provide interpretations, verifiers who fact-check, and bystanders who observe passively to simulate realistic daily interactions that progressively distort true news. To quantify these gradual distortions, we develop FUSE-EVAL, a comprehensive evaluation framework measuring truth deviation along multiple linguistic and semantic dimensions. Results show that FUSE effectively captures fake news evolution patterns and accurately reproduces known fake news, aligning closely with human evaluations. Experiments demonstrate that FUSE accurately reproduces known fake news evolution scenarios, aligns closely with human judgment, and highlights the importance of timely intervention at early stages. Our framework is extensible, enabling future research on broader scenarios of fake news.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection in Networks: A Rough Sets and Consensus Clustering Approach</title>
<link>https://arxiv.org/abs/2406.12412</link>
<guid>https://arxiv.org/abs/2406.12412</guid>
<content:encoded><![CDATA[
<div> Keywords: Rough Clustering, Consensus Community Detection, Complex Networks, Rough Set Theory, Benchmark Networks

Summary:
The paper introduces the Rough Clustering-based Consensus Community Detection (RC-CCD) framework to address the challenge of identifying community structures in complex networks. By utilizing Rough Set Theory (RST) for a consensus approach, the method effectively manages uncertainty and enhances the reliability of community detection. Testing the RC-CCD framework on synthetic benchmark networks from the LFR method reveals superior performance compared to established algorithms like Louvain, Greedy, and LPA. The results show that RC-CCD achieves higher accuracy and adaptability, especially in networks with greater complexity in terms of size and dispersion. These findings have significant implications for improving community detection in various fields such as social and biological network analysis. <div>
arXiv:2406.12412v2 Announce Type: replace-cross 
Abstract: The objective of this paper is to propose a framework, called Rough Clustering-based Consensus Community Detection (RC-CCD), to effectively address the challenge of identifying community structures in complex networks from a set of different community partitions. The method uses a consensus approach based on Rough Set Theory (RST) to manage uncertainty and improve the reliability of community detection. The RC-CCD framework is tested on synthetic benchmark networks generated by the Lancichinetti-Fortunato-Radicchi (LFR) method, which simulate varying network scales, node degrees, and community sizes. Key findings demonstrate that RC-CCD outperforms established algorithms like Louvain, Greedy, and LPA in terms of normalized mutual information, showing superior accuracy and adaptability, particularly in networks with higher complexity, both in terms of size and dispersion. These results have significant implications for enhancing community detection in fields such as social and biological network analysis.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation</title>
<link>https://arxiv.org/abs/2412.11180</link>
<guid>https://arxiv.org/abs/2412.11180</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Multi-layer perceptrons, Teacher Injection, Dirichlet Energy Distillation, Scalability

Summary:<br />
- The paper introduces TINED, a novel approach for distilling Graph Neural Networks (GNNs) into multi-layer perceptrons (MLPs) on a layer-by-layer basis using Teacher Injection and Dirichlet Energy Distillation techniques.
- TINED focuses on two key operations in GNN layers: feature transformation and graph propagation, and directly transfers teacher parameters from GNNs to MLPs.
- TINED replicates the sequence of feature transformations and graph propagations in GNNs using fully connected layers in MLPs.
- The paper establishes a theoretical bound for graph propagation approximation.
- Dirichlet Energy Distillation is proposed to convey the smoothing effects from GNN layers to MLP layers, enhancing the distillation process.
- Extensive experiments demonstrate that TINED outperforms GNNs and leading distillation methods across various settings and datasets. 

<br /><br />Summary: <div>
arXiv:2412.11180v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) are pivotal in graph-based learning, particularly excelling in node classification. However, their scalability is hindered by the need for multi-hop data during inference, limiting their application in latency-sensitive scenarios. Recent efforts to distill GNNs into multi-layer perceptrons (MLPs) for faster inference often underutilize the layer-level insights of GNNs. In this paper, we present TINED, a novel approach that distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection and Dirichlet Energy Distillation techniques. We focus on two key operations in GNN layers: feature transformation (FT) and graph propagation (GP). We recognize that FT is computationally equivalent to a fully-connected (FC) layer in MLPs. Thus, we propose directly transferring teacher parameters from an FT in a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED, the FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We also establish a theoretical bound for GP approximation. Furthermore, we note that FT and GP operations in GNN layers often exhibit opposing smoothing effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we develop a DE ratio to measure these effects and propose Dirichlet Energy Distillation to convey these characteristics from GNN layers to MLP layers. Extensive experiments show that TINED outperforms GNNs and leading distillation methods across various settings and seven datasets. Source code are available at https://github.com/scottjiao/TINED_ICML25/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative Ties Highlight Hidden Extremes in Social Media Polarization</title>
<link>https://arxiv.org/abs/2501.05590</link>
<guid>https://arxiv.org/abs/2501.05590</guid>
<content:encoded><![CDATA[
<div> network analysis, online polarization, social media, signed networks, ideological divides

Summary:
The study examines online polarization on the Spanish social media platform Mename, focusing on interactions between users through comments and voting on news stories. By utilizing signed network representations, the researchers analyze both positive and negative exchanges to gain insights into ideological divides and antagonistic behaviors. They employ a dual-method approach, combining Signed Hamiltonian Eigenvector Embedding for Proximity (SHEEP) for signed networks and Correspondence Analysis (CA) for unsigned networks. The findings reveal that the inclusion of negative ties is essential for identifying ideologically extreme users who engage in confrontational interactions. While the unsigned network effectively depicts ideological communities, only by incorporating negative ties can the most extreme users be distinguished from their less confrontational counterparts. Overall, the study underscores the importance of considering both positive and negative interactions in understanding structural polarization levels across different conversation topics on social media platforms. 

<br /><br />Summary: <div>
arXiv:2501.05590v3 Announce Type: replace-cross 
Abstract: Human interactions in the online world comprise a combination of positive and negative exchanges. These diverse interactions can be captured using signed network representations, where edges take positive or negative weights to indicate the sentiment of the interaction between individuals. Signed networks offer valuable insights into online political polarization by capturing antagonistic interactions and ideological divides on social media platforms. This study analyzes polarization on Men\'eame, a Spanish social media platform that facilitates engagement with news stories through comments and voting. Using a dual-method approach -- Signed Hamiltonian Eigenvector Embedding for Proximity (SHEEP) for signed networks and Correspondence Analysis (CA) for unsigned networks -- we investigate how including negative ties enhances the understanding of structural polarization levels across different conversation topics on the platform. While the unsigned Men\'eame network effectively delineates ideological communities, only by incorporating negative ties can we identify ideologically extreme users who engage in antagonistic behaviors: without them, the most extreme users remain indistinguishable from their less confrontational ideological peers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From the CDC to emerging infectious disease publics: The long-now of polarizing and complex health crises</title>
<link>https://arxiv.org/abs/2503.20262</link>
<guid>https://arxiv.org/abs/2503.20262</guid>
<content:encoded><![CDATA[
<div> discourse clusters, crisis communication, digital publics, polarization, equity<br />
Summary:<br />
This study explores the public discourse on Twitter regarding COVID-19 and the CDC, analyzing over 275,000 tweets to identify 16 discourse clusters influenced by framing, sentiment, credibility, and network dynamics. The CDC messaging sparked polarization along lines of science vs. freedom and public health vs. political overreach, leading to echo chambers and limited cross-cutting dialogue. Publics formed around ideology, topics, and emotions, reflecting changing concerns during the pandemic. Marginalized communities consistently raised equity issues but struggled to reshape broader discourse. The study emphasizes the need for long-term engagement with diverse publics and suggests design interventions like multi-agent AI assistants to improve inclusive communication in extended health crises.<br /> <div>
arXiv:2503.20262v3 Announce Type: replace-cross 
Abstract: This study examines how public discourse around COVID-19 unfolded on Twitter through the lens of crisis communication and digital publics. Analyzing over 275,000 tweets involving the CDC, we identify 16 distinct discourse clusters shaped by framing, sentiment, credibility, and network dynamics. We find that CDC messaging became a flashpoint for affective and ideological polarization, with users aligning along competing frames of science vs. freedom, and public health vs. political overreach. Most clusters formed echo chambers, while a few enabled cross cutting dialogue. Publics emerged not only around ideology but also around topical and emotional stakes, reflecting shifting concerns across different stages of the pandemic. While marginalized communities raised consistent equity concerns, these narratives struggled to reshape broader discourse. Our findings highlight the importance of long-term, adaptive engagement with diverse publics and propose design interventions such as multi-agent AI assistants, to support more inclusive communication throughout extended public health crises.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic embedding of multilayer networks</title>
<link>https://arxiv.org/abs/2505.20378</link>
<guid>https://arxiv.org/abs/2505.20378</guid>
<content:encoded><![CDATA[
<div> Embedding, Multilayer networks, Hyperbolic space, Node clustering, Comparative analysis

Summary:
The paper introduces a new hyperbolic embedding framework for analyzing multilayer networks, capturing multiple connection types and interdependent subsystems. This method supports heterogeneous node sets and inter-layer connections, generating layer-specific embeddings while preserving global structure. Experiments show it effectively preserves community structure in synthetic models and clusters disease-related brain regions in real networks. The approach outperforms layer-independent methods, enhancing interpretability and offering insights into complex system structure and function. <br /><br />Summary: <div>
arXiv:2505.20378v1 Announce Type: new 
Abstract: Multilayer networks offer a powerful framework for modeling complex systems across diverse domains, effectively capturing multiple types of connections and interdependent subsystems commonly found in real world scenarios. To analyze these networks, embedding techniques that project nodes into a lower-dimensional geometric space are essential. This paper introduces a novel hyperbolic embedding framework that advances the state of the art in multilayer network analysis. Our method, which supports heterogeneous node sets across networks and inter-layer connections, generates layer-specific hyperbolic embeddings, enabling detailed intra-layer analysis and inter-layer comparisons, while simultaneously preserving the global multilayer structure within hyperbolic space, a capability that sets it apart from existing approaches, which typically rely on independent embedding of layers. Through experiments on synthetic multilayer stochastic block models, we demonstrate that our approach effectively preserves community structure, even when layers consist of different node sets. When applied to real brain networks, the method successfully clusters disease-related brain regions from different patients, outperforming layer-independent approaches and highlighting its relevance for comparative analysis. Overall, this work provides a robust tool for multilayer network analysis, enhancing interpretability and offering new insights into the structure and function of complex systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dashboard Approach to Monitoring Mpox-Related Discourse and Misinformation on Social Media</title>
<link>https://arxiv.org/abs/2505.20584</link>
<guid>https://arxiv.org/abs/2505.20584</guid>
<content:encoded><![CDATA[
<div> dashboard, Mpox, social media, public health, misinformation
Summary:
The researchers developed a dashboard to track Mpox-related tweets on social media platforms like X. The dashboard allows public health stakeholders and the public to search and visualize these tweets in real-time. Following the CDC's classification of Mpox as an emerging virus, there was a significant increase in tweet volume in 2024 compared to 2023. This highlights the importance of monitoring social media for accurate health information dissemination and identifying misinformation trends. The dashboard serves as a valuable tool for tracking evolving sentiments and misinformation at the local level to support public health communication efforts. <div>
arXiv:2505.20584v1 Announce Type: new 
Abstract: Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus closely related to variola and remains a significant global public health concern. During outbreaks, social media platforms like X (formerly Twitter) can both inform and misinform the public, complicating efforts to convey accurate health information. To support local response efforts, we developed a researcher-focused dashboard for use by public health stakeholders and the public that enables searching and visualizing mpox-related tweets through an interactive interface. Following the CDC's designation of mpox as an emerging virus in August 2024, our dashboard recorded a marked increase in tweet volume compared to 2023, illustrating the rapid spread of health discourse across digital platforms. These findings underscore the continued need for real-time social media monitoring tools to support public health communication and track evolving sentiment and misinformation trends at the local level.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-step dimensionality reduction of human mobility data: From potential landscapes to spatiotemporal insights</title>
<link>https://arxiv.org/abs/2505.20929</link>
<guid>https://arxiv.org/abs/2505.20929</guid>
<content:encoded><![CDATA[
<div> Constructing potential landscape, Combinatorial Hodge theory, Principal component analysis, Spatiotemporal patterns, Human mobility<br />
Summary:<br />
The study introduces a two-step dimensionality reduction framework to analyze human mobility patterns. Using combinatorial Hodge theory, a potential landscape is constructed from origin-destination matrices to visualize flow patterns. Principal component analysis is then applied to identify major spatiotemporal patterns. The framework reveals significant mobility shifts during a pandemic, highlighting overall declines and differences between weekdays and holidays. This approach effectively uncovers complex mobility dynamics, offering insights for urban planning and public health interventions.<br /> <div>
arXiv:2505.20929v1 Announce Type: new 
Abstract: Understanding the spatiotemporal patterns of human mobility is crucial for addressing societal challenges, such as epidemic control and urban transportation optimization. Despite advancements in data collection, the complexity and scale of mobility data continue to pose significant analytical challenges. Existing methods often result in losing location-specific details and fail to fully capture the intricacies of human movement. This study proposes a two-step dimensionality reduction framework to overcome existing limitations. First, we construct a potential landscape of human flow from origin-destination (OD) matrices using combinatorial Hodge theory, preserving essential spatial and structural information while enabling an intuitive visualization of flow patterns. Second, we apply principal component analysis (PCA) to the potential landscape, systematically identifying major spatiotemporal patterns. By implementing this two-step reduction method, we reveal significant shifts during a pandemic, characterized by an overall declines in mobility and stark contrasts between weekdays and holidays. These findings underscore the effectiveness of our framework in uncovering complex mobility patterns and provide valuable insights into urban planning and public health interventions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Super Spreaders in Multilayer Networks</title>
<link>https://arxiv.org/abs/2505.20980</link>
<guid>https://arxiv.org/abs/2505.20980</guid>
<content:encoded><![CDATA[
<div> approach, identifying, super-spreaders, multilayer networks, graph neural networks <br />
Summary: 
The study focuses on identifying super-spreaders in multilayer networks by using graph neural networks. A dataset is created by simulating information diffusion across various networks to accurately represent complex relational structures. The task is formulated as a ranking prediction problem based on a four-dimensional vector measuring spreading potential. The proposed model, TopSpreadersNetwork, incorporates a relationship-agnostic encoder and custom aggregation layer for generalization and adaptation to varying graph sizes. Through extensive evaluation, the model outperforms classic centrality-based heuristics and competitive deep learning methods in identifying high-impact nodes in real-world and synthetic multilayer networks. The model also offers improved interpretability with structured output, showcasing its effectiveness in pinpointing agents that disseminate information most effectively. <br /><br /> <div>
arXiv:2505.20980v1 Announce Type: new 
Abstract: Identifying super-spreaders can be framed as a subtask of the influence maximisation problem. It seeks to pinpoint agents within a network that, if selected as single diffusion seeds, disseminate information most effectively. Multilayer networks, a specific class of heterogeneous graphs, can capture diverse types of interactions (e.g., physical-virtual or professional-social), and thus offer a more accurate representation of complex relational structures. In this work, we introduce a novel approach to identifying super-spreaders in such networks by leveraging graph neural networks. To this end, we construct a dataset by simulating information diffusion across hundreds of networks - to the best of our knowledge, the first of its kind tailored specifically to multilayer networks. We further formulate the task as a variation of the ranking prediction problem based on a four-dimensional vector that quantifies each agent's spreading potential: (i) the number of activations; (ii) the duration of the diffusion process; (iii) the peak number of activations; and (iv) the simulation step at which this peak occurs. Our model, TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom aggregation layer. This design enables generalisation to previously unseen data and adapts to varying graph sizes. In an extensive evaluation, we compare our model against classic centrality-based heuristics and competitive deep learning methods. The results, obtained across a broad spectrum of real-world and synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves superior performance in identifying high-impact nodes, while also offering improved interpretability through its structured output.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeSocial: Blockchain-based Decentralized Social Networks</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
<div> decentralized social network learning, blockchain, user-driven model selection, personalized social prediction, multi-node validation <br />
Summary: 
DeSocial introduces a decentralized social network learning framework on Ethereum blockchain to empower users with personalized prediction algorithms. Users can evaluate multiple backbone models on their local subgraph, select the most suitable one, and coordinate with validation nodes for aggregated prediction results. The framework enhances prediction accuracy and user control compared to traditional centralized models. By leveraging blockchain technology, DeSocial allows users to choose algorithms tailored to their own preferences, promoting a more personalized and accurate social prediction process. Multi-node validation and personalized algorithm selection based on blockchain ensure improved prediction outcomes and user empowerment in decentralized social networks. The framework's implementation is available on GitHub for further exploration and development. <br /> <div>
arXiv:2505.21388v1 Announce Type: new 
Abstract: Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Larger cities, more commuters, more crime? The role of inter-city commuting in the scaling of urban crime</title>
<link>https://arxiv.org/abs/2505.20822</link>
<guid>https://arxiv.org/abs/2505.20822</guid>
<content:encoded><![CDATA[
<div> Keywords: inter-city commuting, population, crime, theft, burglary

Summary:<br /><br />Cities attract non-resident commuters daily, affecting the population-crime relationship. Larger cities receive more commuters and experience higher crime levels. Each 1% increase in inbound commuters corresponds to a 0.32% rise in theft and 0.20% rise in burglary, holding population constant. Models incorporating both population and commuter inflows better explain crime variation. Understanding how cities are connected, not just their size, is crucial in examining the population-crime relationship. <div>
arXiv:2505.20822v1 Announce Type: cross 
Abstract: Cities attract a daily influx of non-resident commuters, reflecting their role in wider urban networks -- not as isolated places. However, it remains unclear how this inter-connectivity shapes the way crime scales with population, given that larger cities tend to receive more commuters and experience more crime. Here, we investigate how inter-city commuting relates to the population--crime relationship. We find that larger cities receive proportionately more commuters, which in turn is associated with higher crime levels. Specifically, each 1% increase in inbound commuters corresponds to a 0.32% rise in theft and 0.20% rise in burglary, holding population constant. We show that models incorporating both population and commuter inflows better explain crime variation than population-only models. These findings underscore the importance of considering how cities are connected -- not just their population size -- in disentangling the population--crime relationship.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2505.20882</link>
<guid>https://arxiv.org/abs/2505.20882</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized machine learning, Fediverse, social networks, graph datasets, peer-to-peer messages 

Summary:
Decentralized machine learning is gaining popularity as it allows collaborative training of models using local data and computational resources. The topology of the communication graph plays a crucial role in learning dynamics, highlighting the need for real graph datasets for benchmarking decentralized algorithms. The Fediverse, comprising decentralized social media platforms like Mastodon and Lemmy, presents an alternative to existing for-profit social network datasets. The newly introduced Fedivertex dataset includes 182 graphs from seven Fediverse networks, crawled weekly over 14 weeks. A Python package is provided for easy use of the dataset, showcasing its applications in various tasks, including a novel defederation task that tracks link deletion processes on these networks.<br /><br />Summary: <div>
arXiv:2505.20882v1 Announce Type: cross 
Abstract: Decentralized machine learning - where each client keeps its own data locally and uses its own computational resources to collaboratively train a model by exchanging peer-to-peer messages - is increasingly popular, as it enables better scalability and control over the data. A major challenge in this setting is that learning dynamics depend on the topology of the communication graph, which motivates the use of real graph datasets for benchmarking decentralized algorithms. Unfortunately, existing graph datasets are largely limited to for-profit social networks crawled at a fixed point in time and often collected at the user scale, where links are heavily influenced by the platform and its recommendation algorithms. The Fediverse, which includes several free and open-source decentralized social media platforms such as Mastodon, Misskey, and Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a new dataset of 182 graphs, covering seven social networks from the Fediverse, crawled weekly over 14 weeks. We release the dataset along with a Python package to facilitate its use, and illustrate its utility on several tasks, including a new defederation task, which captures a process of link deletion observed on these networks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation</title>
<link>https://arxiv.org/abs/2505.20992</link>
<guid>https://arxiv.org/abs/2505.20992</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, identity and position embedding, random feature aggregation, spectral-based GNN, degree correction mechanism<br />
Summary:<br />
The study focuses on graph neural networks (GNNs) for capturing graph structures through feature aggregation. Two types of graph embedding, identity, and position embedding are explored. The proposed random feature aggregation (RFA) method efficiently handles identity and position embedding by utilizing high- and low-frequency information in the graph spectral domain. RFA, based on a spectral-based GNN without learnable parameters, uses random noises as inputs and derives embeddings through one feed-forward propagation (FFP). Incorporating a degree correction mechanism enhances RFA's performance. Surprisingly, RFA achieves informative identity and position embeddings with superior quality and efficiency compared to existing methods, without the need for training. The study demonstrates a better trade-off between quality and efficiency for both identity and position embedding, showcasing the potential of RFA in enhancing GNN-based methods. <br /> <div>
arXiv:2505.20992v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging GANs for citation intent classification and its impact on citation network analysis</title>
<link>https://arxiv.org/abs/2505.21162</link>
<guid>https://arxiv.org/abs/2505.21162</guid>
<content:encoded><![CDATA[
<div> classification, citation intent, GAN, centrality metrics, scientific impact

Summary:<br />
- Citations are crucial in tracking knowledge flow, acknowledging prior work, and assessing scholarly influence.
- Not all citations serve the same function, with some providing background information and others comparing results.
- A GAN-based method was used to classify citation intents, showing competitive performance with fewer parameters.
- The study found that filtering citation intents can significantly influence paper rankings in citation networks.
- Centrality metrics like degree, PageRank, closeness, and betweenness were sensitive to filtering of citation types, with betweenness centrality showing the most significant impact on rankings. <div>
arXiv:2505.21162v1 Announce Type: cross 
Abstract: Citations play a fundamental role in the scientific ecosystem, serving as a foundation for tracking the flow of knowledge, acknowledging prior work, and assessing scholarly influence. In scientometrics, they are also central to the construction of quantitative indicators. Not all citations, however, serve the same function: some provide background, others introduce methods, or compare results. Therefore, understanding citation intent allows for a more nuanced interpretation of scientific impact. In this paper, we adopted a GAN-based method to classify citation intents. Our results revealed that the proposed method achieves competitive classification performance, closely matching state-of-the-art results with substantially fewer parameters. This demonstrates the effectiveness and efficiency of leveraging GAN architectures combined with contextual embeddings in intent classification task. We also investigated whether filtering citation intents affects the centrality of papers in citation networks. Analyzing the network constructed from the unArXiv dataset, we found that paper rankings can be significantly influenced by citation intent. All four centrality metrics examined- degree, PageRank, closeness, and betweenness - were sensitive to the filtering of citation types. The betweenness centrality displayed the greatest sensitivity, showing substantial changes in ranking when specific citation intents were removed.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGraph: A Large-Scale Social Graph Dataset with Comprehensive Context for Influencer Selection in Marketing</title>
<link>https://arxiv.org/abs/2403.15105</link>
<guid>https://arxiv.org/abs/2403.15105</guid>
<content:encoded><![CDATA[
<div> Keywords: Influencer marketing, key opinion leaders, credibility, brand visibility, content analysis

Summary:
Influencer marketing campaigns rely heavily on identifying key opinion leaders to promote products effectively. The selection of influencers plays a crucial role in enhancing brand visibility, building consumer trust, and driving sales. Traditional research often oversimplifies complex factors into numerical values, lacking the ability to capture the dynamic nature of influencer marketing effectiveness. To address this gap, SAGraph, a comprehensive dataset from Weibo, provides detailed marketing campaign data across various product domains. It integrates user profiles, content features, and interaction patterns, allowing for a deeper analysis of influencer marketing mechanisms. Experimental results show that content analysis, particularly using large language models, significantly impacts predicting advertising effectiveness. This dataset and research findings can inspire further studies in data-driven influencer marketing strategies.

<br /><br />Summary: <div>
arXiv:2403.15105v3 Announce Type: replace 
Abstract: Influencer marketing campaign success heavily depends on identifying key opinion leaders who can effectively leverage their credibility and reach to promote products or services. The selecting influencers process is vital for boosting brand visibility, fostering consumer trust, and driving sales. While traditional research often simplifies complex factors like user attitudes, interaction frequency, and advertising content, into simple numerical values. However, this reductionist approach fails to capture the dynamic nature of influencer marketing effectiveness. To bridge this gap, we present SAGraph, a novel comprehensive dataset from Weibo that captures multi-dimensional marketing campaign data across six product domains. The dataset encompasses 345,039 user profiles with their complete interaction histories, including 1.3M comments and 554K reposts across 44K posts, providing unprecedented granularity in influencer marketing dynamics. SAGraph uniquely integrates user profiles, content features, and temporal interaction patterns, enabling in-depth analysis of influencer marketing mechanisms. Experimental results using both traditional baselines and state-of-the-art large language models (LLMs) demonstrate the crucial role of content analysis in predicting advertising effectiveness. Our findings reveal that LLM-based approaches achieve superior performance in understanding and predicting campaign success, opening new avenues for data-driven influencer marketing strategies. We hope that this dataset will inspire further research https://github.com/xiaoqzhwhu/SAGraph/.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focused digital cohort selection from social media using the metric backbone of biomedical knowledge graphs</title>
<link>https://arxiv.org/abs/2405.07072</link>
<guid>https://arxiv.org/abs/2405.07072</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, epilepsy, knowledge graph, medical treatment, cohort <br />
Summary: <br />
Researchers have developed a method to filter relevant users discussing epilepsy drugs on social media platforms, utilizing a knowledge graph approach. By analyzing text from various social media sites, they found that epilepsy-focused users contribute more significantly to the knowledge graph backbone compared to general users. Through human annotation, it was revealed that users who do not contribute to the backbone often use medical terms inaccurately, thus warranting exclusion from the targeted cohort. This method provides a way to identify and engage with users who are most relevant to epilepsy discourse, highlighting the interplay between human behavior and medical treatment on social media platforms. Utilizing curated medical terminology dictionaries, the researchers were able to construct large digital cohorts for studying the impact and understanding of epilepsy treatments within online communities. <div>
arXiv:2405.07072v2 Announce Type: replace 
Abstract: Social media data allows researchers to construct large digital cohorts to study the interplay between human behavior and medical treatment.Identifying the users most relevant to a specific health problem is, however, a challenge in that social media sites vary in the generality of their discourse. To filter relevant users on any social media, we have developed a general method and tested it on epilepsy discourse. We analyzed the text from posts by users who mention epilepsy drugs at least once in the general-purpose social media sites X and Instagram, the epilepsy-focused Reddit subgroup (r/Epilepsy), and the Epilepsy Foundation of America (EFA) forums. We used a curated medical terminology dictionary to generate a knowledge graph (KG) from each social media site, whereby nodes represent terms, and edge weights denote the strength of association between pairs of terms in the collected text. Our method is based on computing the metric backbone of each KG, which yields the subgraph of edges that participate in shortest paths. By comparing the subset of users who contribute to the backbone to the subset who do not, we show that epilepsy-focused social media users contribute to the KG backbone in much higher proportion than do general-purpose social media users. Furthermore, using human annotation of Instagram posts, we demonstrate that users who do not contribute to the backbone are much more likely to use dictionary terms in a manner inconsistent with their biomedical meaning and are rightly excluded from the cohort of interest.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating link prediction: New perspectives and recommendations</title>
<link>https://arxiv.org/abs/2502.12777</link>
<guid>https://arxiv.org/abs/2502.12777</guid>
<content:encoded><![CDATA[
<div> LP methods, network science, machine learning, evaluation, experimental setup

Summary:<br /><br />Link prediction (LP) is a crucial problem in network science and machine learning research. Current LP methods are typically evaluated in a standardized manner, overlooking various factors related to the data and specific application requirements. Factors such as network type, problem type, geodesic distance between nodes, nature of LP methods, class imbalance, and evaluation metrics can significantly impact the performance of LP. To address this, an experimental setup is proposed to evaluate LP methods rigorously and systematically. Extensive experiments conducted using real network datasets in this setup provide valuable insights into the influence of these factors on LP performance. Based on these insights, best practices for evaluating LP methods are recommended. The study emphasizes the importance of considering these factors to improve the effectiveness and reliability of LP methods in real-world applications. <div>
arXiv:2502.12777v2 Announce Type: replace 
Abstract: Link prediction (LP) is an important problem in network science and machine learning research. The state-of-the-art LP methods are usually evaluated in a uniform setup, ignoring several factors associated with the data and application specific needs. We identify a number of such factors, such as, network-type, problem-type, geodesic distance between the end nodes and its distribution over the classes, nature and applicability of LP methods, class imbalance and its impact on early retrieval, evaluation metric, etc., and present an experimental setup which allows us to evaluate LP methods in a rigorous and controlled manner. We perform extensive experiments with a variety of LP methods over real network datasets in this controlled setup, and gather valuable insights on the interactions of these factors with the performance of LP through an array of carefully designed hypotheses. Following the insights, we provide recommendations to be followed as best practice for evaluating LP methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Walk Diffusion for Efficient Large-Scale Graph Generation</title>
<link>https://arxiv.org/abs/2408.04461</link>
<guid>https://arxiv.org/abs/2408.04461</guid>
<content:encoded><![CDATA[
<div> Random Walk Diffusion, Graph Generation, Large-scale Graphs, Efficiency, Quality<br />
<br />
Summary: <br />
The article proposes ARROW-Diff, a novel random walk-based diffusion approach for efficient large-scale graph generation. The method involves random walk sampling and graph pruning iteratively. ARROW-Diff outperforms baseline methods in terms of generation time and statistical measures, indicating high-quality graph generation. The approach aims to address the challenge of generating new graphs with a data distribution similar to real-world graphs. The method scales effectively to large graphs, offering a solution to the scalability issues faced by previous diffusion-based graph generation techniques. By combining random walk sampling and graph pruning, ARROW-Diff demonstrates improved efficiency and quality in generating graphs. <div>
arXiv:2408.04461v2 Announce Type: replace-cross 
Abstract: Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis</title>
<link>https://arxiv.org/abs/2410.01635</link>
<guid>https://arxiv.org/abs/2410.01635</guid>
<content:encoded><![CDATA[
<div> graph prompting, theoretical framework, data operation, approximation, error bounds

Summary:<br />
This paper introduces a theoretical framework for analyzing graph prompting from a data operation perspective. It provides a formal guarantee theorem showing the capacity of graph prompts to approximate graph transformation operators, bridging upstream and downstream tasks. Upper bounds on error are derived for single graphs and extended to batch operations. The distribution of data operation errors is analyzed for linear and non-linear graph models. Extensive experiments validate the theoretical results and demonstrate practical implications of the guarantees. <div>
arXiv:2410.01635v2 Announce Type: replace-cross 
Abstract: In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Longitudinal Analysis of Experiences with Semaglutide Across Twitter User Subpopulations</title>
<link>https://arxiv.org/abs/2505.18432</link>
<guid>https://arxiv.org/abs/2505.18432</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical drug, social media, sentiment analysis, semaglutide, user engagement

Summary: 
The study analyzed over 850,000 tweets related to semaglutide on Twitter from July 2021 to April 2024 to understand how different user groups perceive the drug. Using sentiment and topic modeling techniques, the researchers found that organizational accounts expressed less negative sentiment compared to individuals, especially regarding efficacy and regulatory issues. Negativity around access and side effects was prevalent, while positivity stemmed from success stories and endorsements. A decline in sentiment was observed from Nov 2022 to Jan 2023, coinciding with regulatory alerts. Female users engaged more with celebrity and political discussions related to semaglutide. These findings provide crucial insights for healthcare communication and pharmacovigilance efforts to address public concerns and improve health communication strategies. The data analyzed were public and anonymized to ensure privacy and ethical compliance.<br /><br />Summary: <div>
arXiv:2505.18432v1 Announce Type: new 
Abstract: User experience significantly impacts pharmaceutical drug effectiveness. Social media platforms, particularly Twitter (now X), have become prominent venues for individuals to share medication-related experiences. This is especially true for semaglutide, a widely marketed drug that has sparked substantial public discourse. Despite the volume of conversation, a comprehensive understanding of how different user subpopulations engage with these discussions remains limited. Understanding such nuanced reactions is crucial for identifying public concerns, addressing misconceptions, and improving health communication. We analyzed 859,751 semaglutide-related tweets collected from July 2021 to April 2024, using sentiment and topic modeling to explore how the drug is perceived across user groups. We applied advanced analytical tools, including RoBERTa and BERTopic, to uncover trends and insights. To our knowledge, this is the most comprehensive sentiment and topic modeling analysis of semaglutide discourse on Twitter. Findings reveal significant sentiment differences across subpopulations: organizational accounts expressed less negative sentiment (mean -0.014) than individuals (-0.24), especially regarding efficacy and regulatory issues. Sentiment declined notably from Nov 2022 to Jan 2023, coinciding with regulatory alerts. Negativity clustered around access and side effects; positivity stemmed from success stories and endorsements. Female users engaged more with celebrity/political discussions (19.24% vs. 14.6% for males), while males showed slightly higher positivity overall. These insights inform healthcare communication and pharmacovigilance. All data were public and anonymized to ensure privacy and ethical compliance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring temporal dynamics in digital trace data: mining user-sequences for communication research</title>
<link>https://arxiv.org/abs/2505.18790</link>
<guid>https://arxiv.org/abs/2505.18790</guid>
<content:encoded><![CDATA[
<div> Keywords: communication, computational approaches, digital trace data, user-sequences, temporal dimension

Summary:
This paper discusses the disconnection between the dynamic nature of communication processes and the non-dynamic methodologies used by communication scholars. It introduces a new research framework that utilizes computational approaches to analyze fine-grained timestamps in digital trace data. The framework focuses on maintaining hyper-longitudinal information in the data and studying time-evolving user-sequences to gain insights into user activity with high temporal resolution. A case study is presented, applying six different approaches to real-world user-sequences collected from 309 unique users. The study highlights the importance of understanding the temporal dimension in communication processes, leveraging the abundance of digital trace data and advancements in analytical techniques.

<br /><br />Summary: <div>
arXiv:2505.18790v1 Announce Type: new 
Abstract: Communication is commonly considered a process that is dynamically situated in a temporal context. However, there remains a disconnection between such theoretical dynamicality and the non-dynamical character of communication scholars' preferred methodologies. In this paper, we argue for a new research framework that uses computational approaches to leverage the fine-grained timestamps recorded in digital trace data. In particular, we propose to maintain the hyper-longitudinal information in the trace data and analyze time-evolving 'user-sequences,' which provide rich information about user activity with high temporal resolution. To illustrate our proposed framework, we present a case study that applied six approaches (e.g., sequence analysis, process mining, and language-based models) to real-world user-sequences containing 1,262,775 timestamped traces from 309 unique users, gathered via data donations. Overall, our study suggests a conceptual reorientation towards a better understanding of the temporal dimension in communication processes, resting on the exploding supply of digital trace data and the technical advances in analytical approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Intervention for Self-triggering Spatial Networks with Application to Urban Crime Analytics</title>
<link>https://arxiv.org/abs/2505.19612</link>
<guid>https://arxiv.org/abs/2505.19612</guid>
<content:encoded><![CDATA[
<div> network intervention, self-exciting networks, critical nodes, spatiotemporal Hawkes network, predictive policing

Summary:
The study focuses on self-exciting networks where events at one node trigger activity at other nodes. The researchers develop an optimal network intervention model to target critical nodes and mitigate cascading effects in a Spatiotemporal Hawkes network. Previous studies have explored similar models in temporal Hawkes networks, but this work extends the analysis to a spatiotemporal context. Through simulations, the researchers demonstrate the effectiveness of their method in reducing intensity post-intervention compared to other heuristic strategies. In a real-world application, the model is applied to crime data from the LA police department database to identify neighborhoods for targeted interventions, showcasing its potential in predictive policing. This research highlights the importance of strategic interventions in self-exciting networks to prevent further propagation of undesirable consequences.  <br /><br />Summary: <div>
arXiv:2505.19612v1 Announce Type: new 
Abstract: In many network systems, events at one node trigger further activity at other nodes, e.g., social media users reacting to each other's posts or the clustering of criminal activity in urban environments. These systems are typically referred to as self-exciting networks. In such systems, targeted intervention at critical nodes can be an effective strategy for mitigating undesirable consequences such as further propagation of criminal activity or the spreading of misinformation on social media. In our work, we develop an optimal network intervention model to explore how targeted interventions at critical nodes can mitigate cascading effects throughout a Spatiotemporal Hawkes network. Similar models have been studied previously in the literature in purely temporal Hawkes networks, but in our work, we extend them to a spatiotemporal setup and demonstrate the efficacy of our methods by comparing the post-intervention reduction in intensity to other heuristic strategies in simulated networks. Subsequently, we use our method on crime data from the LA police department database to find neighborhoods for strategic intervention to demonstrate an application in predictive policing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Moderation and the New Epistemology of Fact Checking on Social Media</title>
<link>https://arxiv.org/abs/2505.20067</link>
<guid>https://arxiv.org/abs/2505.20067</guid>
<content:encoded><![CDATA[
<div> community-driven moderation, misinformation detection, social media platforms, crowd-checking, professional fact-checkers

Summary:
The article discusses the shift of social media platforms towards community-driven content moderation through initiatives like Community Notes. While community efforts can help combat misinformation with scale and speed, they cannot replace the role of professional fact-checkers due to the complexity of identifying misleading content influenced by personal biases and cultural contexts. The current approaches to misinformation detection on major platforms are examined, highlighting the challenges and promises of crowd-checking at scale. The importance of maintaining a balance between community-driven moderation and the expertise of professional fact-checkers is emphasized to effectively address the issue of misleading content. <div>
arXiv:2505.20067v1 Announce Type: new 
Abstract: Social media platforms have traditionally relied on internal moderation teams and partnerships with independent fact-checking organizations to identify and flag misleading content. Recently, however, platforms including X (formerly Twitter) and Meta have shifted towards community-driven content moderation by launching their own versions of crowd-sourced fact-checking -- Community Notes. If effectively scaled and governed, such crowd-checking initiatives have the potential to combat misinformation with increased scale and speed as successfully as community-driven efforts once did with spam. Nevertheless, general content moderation, especially for misinformation, is inherently more complex. Public perceptions of truth are often shaped by personal biases, political leanings, and cultural contexts, complicating consensus on what constitutes misleading content. This suggests that community efforts, while valuable, cannot replace the indispensable role of professional fact-checkers. Here we systemically examine the current approaches to misinformation detection across major platforms, explore the emerging role of community-driven moderation, and critically evaluate both the promises and challenges of crowd-checking at scale.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily Enhanced Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.20089</link>
<guid>https://arxiv.org/abs/2505.20089</guid>
<content:encoded><![CDATA[
<div> homophily, Graph Domain Adaptation, label scarcity, graph alignment, mixed filters
Summary:
- Graph Domain Adaptation (GDA) aims to transfer knowledge from labeled source graphs to unlabeled target graphs to address label scarcity.
- Graph homophily, often overlooked in existing approaches, plays a crucial role in graph domain alignment.
- Discrepancies in homophily exist in benchmarks, impacting GDA performance.
- The proposed homophily alignment algorithm uses mixed filters to smooth graph signals and mitigate homophily discrepancies effectively.
- Experimental results on various benchmarks validate the efficacy of the novel method.
<br /><br />Summary: Graph Domain Adaptation (GDA) seeks to bridge the gap between labeled source graphs and unlabeled target graphs to tackle label scarcity. The study emphasizes the overlooked factor of graph homophily, showing its significance in graph domain alignment. Discovery of homophily discrepancies in benchmarks underscores their detrimental impact on GDA performance. To address this, a novel homophily alignment algorithm leveraging mixed filters is proposed to smooth graph signals and align homophily effectively. Experimental validation across diverse benchmarks confirms the method's effectiveness in improving GDA performance and mitigating homophily discrepancies. <div>
arXiv:2505.20089v1 Announce Type: new 
Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. In this paper, we highlight the significance of graph homophily, a pivotal factor for graph domain alignment, which, however, has long been overlooked in existing approaches. Specifically, our analysis first reveals that homophily discrepancies exist in benchmarks. Moreover, we also show that homophily discrepancies degrade GDA performance from both empirical and theoretical aspects, which further underscores the importance of homophily alignment in GDA. Inspired by this finding, we propose a novel homophily alignment algorithm that employs mixed filters to smooth graph signals, thereby effectively capturing and mitigating homophily discrepancies between graphs. Experimental results on a variety of benchmarks verify the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment spreads, but topics do not, in COVID-19 discussions within the Belgian Reddit community</title>
<link>https://arxiv.org/abs/2505.20185</link>
<guid>https://arxiv.org/abs/2505.20185</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, Belgian Reddit community, mitigation measures, sentiment, homophily

Summary:
- The study focuses on how topics and sentiments related to COVID-19 mitigation measures spread within the Belgian Reddit community.
- Analysis of 655,642 posts from January 2020 to June 2022 shows that post volume reflects external events rather than interactions within Reddit.
- Sentiment in posts is influenced by previous sentiments, leading to homophily and polarization among users.
- Homophily measures are found to be 0.228 for lockdowns, 0.198 for masks, and 0.133 for vaccinations.
- A novel bounded confidence model estimates user sentiment, with Wasserstein metrics ranging between 0.493 (vaccination) and 0.607 (lockdown), providing insight into how the Belgian Reddit community engaged with pandemic-related topics and sentiments. 

<br /><br />Summary: The study examines the spread of COVID-19 mitigation topics and sentiments within the Belgian Reddit community. Post volume correlates with external events rather than Reddit interactions. Sentiment is influenced by previous posts, leading to homophily and polarization. Homophily measures vary for lockdowns, masks, and vaccinations. A novel model estimates internal sentiment of users. The results shed light on how the Belgian Reddit community experienced the pandemic and the factors influencing discussions and sentiments. <div>
arXiv:2505.20185v1 Announce Type: new 
Abstract: This study investigates how topics and sentiments on COVID-19 mitigation measures -- specifically lockdowns, mask mandates, and vaccinations -- spread through the Belgian Reddit community. We explore 655,642 posts created between 1 January 2020 and 30 June 2022. In line with previous studies for other countries and platforms, we find that the volume of posts on these topics can be tied to important external events, but not within-Reddit interactions. Sentiment, however, is influenced by the sentiment of previous posts, resulting in homophily and polarisation. We define a homophily measure and find values of 0.228, 0.198, and 0.133 for lockdowns, masks and vaccination, respectively. Additionally, we introduce a novel bounded confidence model that estimates internal sentiment of users from their expressed sentiment. The Wasserstein metric between the predicted and the observed sentiments takes values between 0.493 (vaccination) and 0.607 (lockdown). These results yield insight into the way the Belgian Reddit community experienced the pandemic, and which aspects influenced the topics discussed and their associated sentiment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.18934</link>
<guid>https://arxiv.org/abs/2505.18934</guid>
<content:encoded><![CDATA[
<div> filter, anomaly detection, graph neural network, heterogeneous networks, Chi-Square

Summary: 
- The paper addresses the challenges of Graph Anomaly Detection (GAD) in heterogeneous networks by proposing ChiGAD, a spectral GNN framework.
- ChiGAD includes a Multi-Graph Chi-Square Filter to capture anomalous information, Interactive Meta-Graph Convolution for feature alignment, and Contribution-Informed Cross-Entropy Loss to prioritize difficult anomalies.
- The proposed method outperforms existing models on various datasets and shows superiority in handling heterogeneous networks.
- The homogeneous variant, ChiGNN, also excels in GAD tasks, demonstrating the effectiveness of Chi-Square filters.
- The code for ChiGAD is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.18934v1 Announce Type: cross 
Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: (C1) Capturing abnormal signal and rich semantics across diverse meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment; and (C3) Learning effectively from difficult anomaly samples with class imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; and (3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance. Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters. Our code is available at https://github.com/HsipingLi/ChiGAD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, graph analysis, knowledge access, knowledge homophily, graph machine learning 

Summary: 
The study explores the structural patterns of large language models (LLMs) as neural knowledge bases. It quantifies knowledge at both triplet and entity levels and examines how it correlates with graph properties like node degree. The concept of knowledge homophily is introduced, indicating that closely connected entities possess similar knowledge levels. Utilizing this insight, a graph machine learning model is developed to predict entity knowledge based on its local neighborhood. This model allows for efficient knowledge checking by identifying triplets less familiar to LLMs. Experimental results demonstrate that fine-tuning models using the selected triplets leads to improved performance. This research highlights the importance of understanding the structural patterns of LLM knowledge and offers insights into leveraging graph analysis for enhancing knowledge representation and utilization in language models. 

Summary: <div>
arXiv:2505.19286v1 Announce Type: cross 
Abstract: Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement</title>
<link>https://arxiv.org/abs/2505.19355</link>
<guid>https://arxiv.org/abs/2505.19355</guid>
<content:encoded><![CDATA[
<div> framework, causal inference, Average Treatment Effects, social media, misinformation <br />
<br />
Summary: 
The study introduces a novel framework for understanding true influence in social media, focusing on distinguishing correlation from causation in the spread of misinformation. By leveraging a joint treatment-outcome approach and adapting causal inference techniques from healthcare, the model estimates Average Treatment Effects (ATE) within the sequential nature of social media interactions. This approach accounts for external confounding signals and outperforms existing benchmarks by 15-22% in predicting engagement across various scenarios. The experiments on real-world misinformation datasets demonstrate the effectiveness of the model in tackling challenges such as exposure adjustment, timing shifts, and intervention durations. Additionally, case studies on 492 social media users reveal a strong alignment between the model's causal effect measure and the expert-based empirical influence, showcasing the model's accuracy in estimating influence in social media. <br /> <div>
arXiv:2505.19355v1 Announce Type: cross 
Abstract: Understanding true influence in social media requires distinguishing correlation from causation--particularly when analyzing misinformation spread. While existing approaches focus on exposure metrics and network structures, they often fail to capture the causal mechanisms by which external temporal signals trigger engagement. We introduce a novel joint treatment-outcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects. Our approach adapts causal inference techniques from healthcare to estimate Average Treatment Effects (ATE) within the sequential nature of social media interactions, tackling challenges from external confounding signals. Through our experiments on real-world misinformation and disinformation datasets, we show that our models outperform existing benchmarks by 15--22% in predicting engagement across diverse counterfactual scenarios, including exposure adjustment, timing shifts, and varied intervention durations. Case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation, the expert-based empirical influence.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Social Media Influence Experimentation via an Agentic Reinforcement Learning Large Language Model Bot</title>
<link>https://arxiv.org/abs/2411.19635</link>
<guid>https://arxiv.org/abs/2411.19635</guid>
<content:encoded><![CDATA[
<div> Keywords: public opinion evolution, online social platforms, influence mechanisms, Large Language Models, simulated environment<br />
Summary:<br />
This study presents a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to explore topic-specific influence mechanisms ethically. The framework includes agents generating posts, forming opinions on topics, and socially interacting based on discussions. An opinion leader, utilizing Reinforcement Learning (RL), adapts its linguistic interactions to maximize influence and followers over time. The findings suggest that constraining the action space and incorporating self-observation are crucial for stable opinion leader generation. The simulation framework creates agents able to adapt to complex social dynamics. This work is valuable in understanding public opinion evolution on online platforms and the emergence of influence leaders, particularly in the context of increasing online influence on social attitudes. <br /> <div>
arXiv:2411.19635v2 Announce Type: replace 
Abstract: Understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information. Traditional influence analysis is typically divided into qualitative assessments of personal attributes (e.g., psychology of influence) and quantitative evaluations of influence power mechanisms (e.g., social network analysis). One challenge faced by researchers is the ethics of real-world experimentation and the lack of social influence data. In this study, we provide a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to test topic-specific influence mechanisms ethically. Our framework contains agents that generate posts, form opinions on specific topics, and socially follow/unfollow each other based on the outcome of discussions. This simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge. Using our own framework, we design an opinion leader that utilizes Reinforcement Learning (RL) to adapt its linguistic interaction with the community to maximize its influence and followers over time. Our current findings reveal that constraining the action space and incorporating self-observation are key factors for achieving stable and consistent opinion leader generation for topic-specific influence. This demonstrates the simulation framework's capacity to create agents that can adapt to complex and unpredictable social dynamics. The work is important in an age of increasing online influence on social attitudes and emerging technologies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Structural Knowledge in Diffusion Models for Source Localization in Data-Limited Graph Scenarios</title>
<link>https://arxiv.org/abs/2502.17928</link>
<guid>https://arxiv.org/abs/2502.17928</guid>
<content:encoded><![CDATA[
<div> Graph, information propagation, source localization, deep generative approaches, limited-data scenarios 
<br />
<br />
Summary: 
The paper introduces SIDSL, a framework for source localization in graph information propagation. It addresses challenges in limited-data scenarios by incorporating topology-aware priors, using a propagation-enhanced conditional denoiser with a GNN-LP, and implementing a structure-prior biased denoising scheme. Experimental results show SIDSL outperforms state-of-the-art methods, achieving 7.5-13.3% improvements in F1 scores and surpassing baselines by more than 18.8% when pretrained with simulation data. The framework proves effective in real-world applications with scarce labeled data, making it a valuable tool for managing network disruptions. <div>
arXiv:2502.17928v2 Announce Type: replace 
Abstract: The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10% of training data, surpassing baselines by more than 18.8%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Robust Flocking of Protesters on Street Networks</title>
<link>https://arxiv.org/abs/2406.01101</link>
<guid>https://arxiv.org/abs/2406.01101</guid>
<content:encoded><![CDATA[
<div> Keywords: protesters, city, tactics, alignment, flocking

Summary:
Protesters in a city can efficiently gather into large and mobile groups using a simple model based on random walkers and tactics derived from basic rules. Through experiments, the importance of a specific rule based on alignment for fast and robust flocking of walkers was identified. While other rules on their own were not as effective, combining alignment with them significantly improved flocking behavior. The model explores a variety of tactics and demonstrates that the inclusion of alignment enhances the formation of cohesive and resilient groups. This research underscores the critical role of alignment in facilitating the efficient organization of scattered protesters and highlights the robustness of the groups formed through the combination of rules. <div>
arXiv:2406.01101v3 Announce Type: replace-cross 
Abstract: We propose a simple model of protesters scattered throughout a city who want to gather into large and mobile groups. This model relies on random walkers on a street network that follow tactics built from a set of basic rules. Our goal is to identify the most important rules for fast and robust flocking of walkers. We explore a wide set of tactics and show the central importance of a specific rule based on alignment. Other rules alone perform poorly, but our experiments show that combining alignment with them enhances flocking, and that obtained groups are then remarkably robust.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Robustness of Graph Neural Networks against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2406.13920</link>
<guid>https://arxiv.org/abs/2406.13920</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, adversarial attacks, robustness, evaluation metrics, model capacity

Summary:<br />
Graph neural networks (GNNs) have been found to be vulnerable to adversarial attacks, leading to concerns about their use in critical applications. To address this issue, a systematic study on the adversarial robustness of GNNs was conducted, considering input graph patterns, network architecture, and model capacity. The study also examined sensitive neurons and adversarial transferability. Two evaluation metrics, confidence-based decision surface, and accuracy-based adversarial transferability rate were introduced to assess robustness. The work provides 11 guidelines for designing robust GNNs based on empirical analysis. The code for the study is available online. This comprehensive framework offers valuable insights for developers to enhance GNNs' resistance to adversarial attacks. <br /><br />Summary: <div>
arXiv:2406.13920v2 Announce Type: replace-cross 
Abstract: Recent studies have shown that graph neural networks (GNNs) are vulnerable to adversarial attacks, posing significant challenges to their deployment in safety-critical scenarios. This vulnerability has spurred a growing focus on designing robust GNNs. Despite this interest, current advancements have predominantly relied on empirical trial and error, resulting in a limited understanding of the robustness of GNNs against adversarial attacks. To address this issue, we conduct the first large-scale systematic study on the adversarial robustness of GNNs by considering the patterns of input graphs, the architecture of GNNs, and their model capacity, along with discussions on sensitive neurons and adversarial transferability. This work proposes a comprehensive empirical framework for analyzing the adversarial robustness of GNNs. To support the analysis of adversarial robustness in GNNs, we introduce two evaluation metrics: the confidence-based decision surface and the accuracy-based adversarial transferability rate. Through experimental analysis, we derive 11 actionable guidelines for designing robust GNNs, enabling model developers to gain deeper insights. The code of this study is available at https://github.com/star4455/GraphRE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SaVe-TAG: Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2410.16882</link>
<guid>https://arxiv.org/abs/2410.16882</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Vicinal Risk Minimization, Text-Attributed Graphs, Large Language Models, Synthetic Samples

Summary:
SaVe-TAG introduces a novel approach for handling class imbalance in real-world text-attributed graphs using Vicinal Risk Minimization. The method leverages Large Language Models for text-level interpolation to generate synthetic samples for minority classes. A confidence-based edge assignment mechanism is employed to ensure structural consistency and reduce noise in the generated samples. The integration of semantic information and graph topology proves to be crucial for balanced and effective learning. Experimental results demonstrate the superiority of SaVe-TAG over existing methods in addressing long-tailed distributions and improving node classification performance in text-attributed graphs.<br /><br />Summary: SaVe-TAG utilizes Vicinal Risk Minimization and Large Language Models to generate synthetic samples for minority classes in text-attributed graphs. The method employs a confidence-based edge assignment mechanism to ensure structural consistency and effectively mitigates class imbalance, outperforming existing approaches and highlighting the significance of semantic and structural signals in graph learning. <div>
arXiv:2410.16882v3 Announce Type: replace-cross 
Abstract: Real-world graph data often follows long-tailed distributions, making it difficult for Graph Neural Networks (GNNs) to generalize well across both head and tail classes. Recent advances in Vicinal Risk Minimization (VRM) have shown promise in mitigating class imbalance with numeric interpolation; however, existing approaches largely rely on embedding-space arithmetic, which fails to capture the rich semantics inherent in text-attributed graphs. In this work, we propose our method, SaVe-TAG (Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs), a novel VRM framework that leverages Large Language Models (LLMs) to perform text-level interpolation, generating on-manifold, boundary-enriching synthetic samples for minority classes. To mitigate the risk of noisy generation, we introduce a confidence-based edge assignment mechanism that uses graph topology as a natural filter to ensure structural consistency. We provide theoretical justification for our method and conduct extensive experiments on benchmark datasets, showing that our approach consistently outperforms both numeric interpolation and prior long-tailed node classification baselines. Our results highlight the importance of integrating semantic and structural signals for balanced and effective learning on text-attributed graphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Graph Learning</title>
<link>https://arxiv.org/abs/2411.18919</link>
<guid>https://arxiv.org/abs/2411.18919</guid>
<content:encoded><![CDATA[
<div> Federated Continual Graph Learning, GNNs, evolving graphs, decentralized settings, storage costs <br />
<br />
Summary: 
In the study on Federated Continual Graph Learning (FCGL), the authors address the challenges of training graph neural networks on evolving graph data in decentralized settings. They conduct empirical analysis to assess the feasibility and effectiveness of FCGL, identifying two main challenges: local graph forgetting (LGF) and global expertise conflict (GEC). To overcome these challenges, they propose the POWER framework, which preserves and replays experience nodes and utilizes a pseudo prototype reconstruction strategy for knowledge transfer. Experiments on various graph datasets demonstrate the superiority of the POWER framework over existing baseline methods and federated continual learning approaches focused on vision tasks. Overall, the study highlights the importance of adapting GNNs to multiple evolving graphs in a decentralized manner while considering storage and privacy constraints. <br /><br />Summary: <div>
arXiv:2411.18919v2 Announce Type: replace-cross 
Abstract: In the era of big data, managing evolving graph data poses substantial challenges due to storage costs and privacy issues. Training graph neural networks (GNNs) on such evolving data usually causes catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they rely on centralized architectures and ignore the potential of distributed graph databases to leverage collective intelligence. To address these challenges, we present a pioneering study on Federated Continual Graph Learning (FCGL), which adapts GNNs to multiple evolving graphs within decentralized settings while adhering to storage and privacy constraints. Our work begins with a comprehensive empirical analysis of FCGL, assessing its data characteristics, feasibility, and effectiveness, and reveals two non-trivial challenges: local graph forgetting (LGF), where local GNNs forget prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To tackle these, we propose the POWER framework, which mitigates LGF by preserving and replaying experience nodes with maximum local-global coverage at each client and addresses GEC by using a pseudo prototype reconstruction strategy and trajectory-aware knowledge transfer at the central server. Experiments on various graph datasets demonstrate POWER's superiority over federated adaptations of CGL baselines and vision-centric federated continual learning approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees</title>
<link>https://arxiv.org/abs/2412.16441</link>
<guid>https://arxiv.org/abs/2412.16441</guid>
<content:encoded><![CDATA[
<div> graph neural network, task-trees, generalization, transfer learning, graph foundation model

Summary:
Task-trees are proposed as a novel approach for cross-task generalization in graph-structured data. The approach aligns node-, edge-, and graph-level tasks, enabling transferable knowledge learned from diverse tasks. The stability, transferability, and generalization properties of task-trees are theoretically analyzed, showing that pretraining a graph neural network on task-trees with a reconstruction objective induces transferable knowledge. The Graph Generality Identifier on Task-Trees (GIT) model, based on this approach, demonstrates strong performance on over 30 graphs across five domains through fine-tuning, in-context learning, and zero-shot generalization. The framework provides a foundation for efficient adaptation to downstream tasks with minimal fine-tuning, showcasing the potential for leveraging cross-task generalization in graph tasks. The code and data for the GIT model are available on GitHub for further exploration and validation. 

<br /><br />Summary: <div>
arXiv:2412.16441v3 Announce Type: replace-cross 
Abstract: Foundation models are pretrained on large-scale corpora to learn generalizable patterns across domains and tasks -- such as contours, textures, and edges in images, or tokens and sentences in text. In contrast, discovering such generalities in graph-structured data, especially across heterogeneous graph tasks, remains an open challenge. To address this, we propose a novel approach to cross-task generalization in graphs via task-trees, which serve as unified learning instances aligning node-, edge-, and graph-level tasks. We theoretically analyze the stability, transferability, and generalization properties of task-trees, showing that pretraining a graph neural network (GNN) on diverse task-trees with a reconstruction objective induces transferable knowledge. This enables efficient adaptation to downstream tasks with minimal fine-tuning. To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on over 30 graphs across five domains via fine-tuning, in-context learning, and zero-shot generalization. Code and data are available at https://github.com/Zehong-Wang/GIT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of network communities driven by local rules</title>
<link>https://arxiv.org/abs/2501.17042</link>
<guid>https://arxiv.org/abs/2501.17042</guid>
<content:encoded><![CDATA[
<div> Ramsey community number, network communities, node heterogeneity, stochastic block model, local rules <br />
Summary: <br />
The article discusses network modeling and the emergence of communities within networks. It challenges the common belief that node heterogeneity, such as political affiliation or biological function, is necessary for the segregation of nodes into communities. Through numerical simulations, the author introduces the concept of the Ramsey community number, which indicates the minimum graph size needed for network communities to emerge with high certainty. Using the stochastic block model, the study demonstrates that networks governed by local rules have finite Ramsey community numbers, ensuring the presence of communities. In contrast, randomized networks do not exhibit this emergent property. This leads to a conjecture that network communities are a product of networks evolving based on local rules rather than node heterogeneity. <div>
arXiv:2501.17042v3 Announce Type: replace-cross 
Abstract: Natural systems are modeled by networks with nodes and links. Often the nodes are segregated into communities with different connectivity patterns. Node heterogeneity such as political affiliation in social networks or biological function in gene networks are highlighted as key factors driving the segregation of nodes into communities. Here, by means of numerical simulations, I show that node heterogeneity is not a necessary requirement. To this end I introduce the Ramsey community number, $r_\kappa$, the minimum graph size that warranties the emergence of network communities with almost certainty. Using the stochastic block model for community detection with correction for degree sequence, I show that networks generated by local rules have finite $r_\kappa$ values while their randomized versions do not have emergent communities. I conjecture that network communities are an emergent property of networks evolving with local rules.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Message Passing: Neural Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2501.18739</link>
<guid>https://arxiv.org/abs/2501.18739</guid>
<content:encoded><![CDATA[
<div> substructure patterns, graph neural networks, Neural Graph Pattern Machine, expressivity, long-range dependencies 
Summary: 
The paper introduces the Neural Graph Pattern Machine (GPM), a framework that learns directly from graph substructures without relying on message passing. GPM efficiently extracts and encodes task-relevant graph patterns, offering greater expressivity and improved long-range dependency modeling. Empirical evaluations across node classification, link prediction, graph classification, and graph regression tasks show that GPM outperforms existing baselines. The model demonstrates strong out-of-distribution generalization, scalability, and interpretability. The code and datasets are available on GitHub at https://github.com/Zehong-Wang/GPM.<br /><br />Summary: <div>
arXiv:2501.18739v2 Announce Type: replace-cross 
Abstract: Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
<div> simulation, social network, deception behaviors, content moderation, user engagement

Summary: 
The article introduces MOSAIC, a social network simulation framework that uses generative language agents to predict user behaviors in online platforms. By combining LLM agents with a social graph, the framework analyzes deceptive behaviors and user engagement with online content. Using diverse personas, the system enables large-scale simulations to study content dissemination dynamics. Three content moderation strategies are evaluated, showing effectiveness in mitigating misinformation spread and increasing user engagement. The trajectories of popular content are analyzed to understand simulation agents' interactions and engagement patterns. The open-source simulation software aims to facilitate further research in AI and social sciences. <div>
arXiv:2504.07830v2 Announce Type: replace-cross 
Abstract: We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Global Networks of Exchange through the Louvain Method</title>
<link>https://arxiv.org/abs/2505.17234</link>
<guid>https://arxiv.org/abs/2505.17234</guid>
<content:encoded><![CDATA[
<div> Congressional Research Service (CRS) reports, network analysis, weighted graph, Louvain method, community detection<br />
Summary:<br />
This study analyzes data from over 2,000 CRS reports to quantify relationships between 172 countries from 1996 to 2024. By converting the data into a weighted graph and applying the Louvain method, non-overlapping communities with shared interests are identified. The eigenvector centrality of countries is computed to determine their network influence. The findings have the potential to enhance the sourcing of evidence for analytical products and provide insights into the interconnectedness of the global landscape. <div>
arXiv:2505.17234v1 Announce Type: new 
Abstract: Congressional Research Service (CRS) reports provide detailed analyses of major policy issues to members of the US Congress. We extract and analyze data from 2,010 CRS reports written between 1996 and 2024 in order to quantify the relationships between countries. The data is processed and converted into a weighted graph, representing 172 unique countries as nodes and 4,137 interests as bidirectional edges. Through the Louvain method, we use a greedy algorithm to extract non-overlapping communities from our network and identify clusters with shared interests. We then compute the eigenvector centrality of countries, effectively highlighting their network influence. The results of this work could enable improvements in sourcing evidence for analytic products and understanding the connectivity of our world.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Graph Embedding through Hub-aware Random Walks</title>
<link>https://arxiv.org/abs/2505.17764</link>
<guid>https://arxiv.org/abs/2505.17764</guid>
<content:encoded><![CDATA[
<div> hub, dynamic graph embedding, random walk, network science, structure sensitivity

Summary:
- The study focuses on the influence of high-degree nodes, or hubs, in dynamic graph embedding.
- A new method called DeepHub is introduced to integrate hub sensitivity into random walk sampling strategies.
- Research conducted on nine real-world temporal networks shows that standard random walks tend to overrepresent hub nodes.
- Hub-aware walks can balance exploration, leading to better preservation of temporal neighborhood structure in embeddings.
- The results suggest that hub-awareness is crucial for dynamic graph embedding to improve downstream task performance. 

<br /><br />Summary: <div>
arXiv:2505.17764v1 Announce Type: new 
Abstract: The role of high-degree nodes, or hubs, in shaping graph dynamics and structure is well-recognized in network science, yet their influence remains underexplored in the context of dynamic graph embedding. Recent advances in representation learning for graphs have shown that random walk-based methods can capture both structural and temporal patterns, but often overlook the impact of hubs on walk trajectories and embedding stability. In this paper, we introduce DeepHub, a method for dynamic graph embedding that explicitly integrates hub sensitivity into random walk sampling strategies. Focusing on dynnode2vec as a representative dynamic embedding method, we systematically analyze the effect of hub-biased walks across nine real-world temporal networks. Our findings reveal that standard random walks tend to overrepresent hub nodes, leading to embeddings that underfit the evolving local context of less-connected nodes. By contrast, hub-aware walks can balance exploration, resulting in embeddings that better preserve temporal neighborhood structure and improve downstream task performance. These results suggest that hub-awareness is an important yet overlooked factor in dynamic graph embedding, and our work provides a foundation for more robust, structure-sensitive representation learning in evolving networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Dynamics of Harmful Content Dissemination on WhatsApp</title>
<link>https://arxiv.org/abs/2505.18099</link>
<guid>https://arxiv.org/abs/2505.18099</guid>
<content:encoded><![CDATA[
<div> Keywords: WhatsApp, harmful content, message dissemination, structural characteristics, propagation patterns

Summary:
The study focuses on the dissemination of harmful messages on WhatsApp, analyzing over 5.1 million messages from 6,000 groups in India. It found that harmful messages spread more extensively than non-harmful ones, particularly through images and videos. However, the analysis revealed that dissemination patterns are not solely determined by the message format. Structural characteristics of message propagation play a significant role in the spread of harmful content. This suggests that interventions targeting how messages are reshared could be crucial in managing harmful content on private messaging platforms. The study emphasizes the need to consider both the modality and structural features in strategies aimed at controlling the dissemination of harmful messages on platforms like WhatsApp. 

<br /><br />Summary: <div>
arXiv:2505.18099v1 Announce Type: new 
Abstract: WhatsApp, a platform with more than two billion global users, plays a crucial role in digital communication, but also serves as a vector for harmful content such as misinformation, hate speech, and political propaganda. This study examines the dynamics of harmful message dissemination in WhatsApp groups, with a focus on their structural characteristics. Using a comprehensive data set of more than 5.1 million messages, including text, images, and videos, collected from approximately 6,000 groups in India, we reconstruct message propagation cascades to analyze dissemination patterns.
  Our findings reveal that harmful messages consistently achieve greater depth and breadth of dissemination compared to messages without harmful annotations, with videos and images emerging as the primary modes of dissemination. These results suggest a distinctive pattern of dissemination of harmful content. However, our analysis indicates that modality alone cannot fully account for the structural differences in propagation.
  The findings highlight the critical role of structural characteristics in the spread of these harmful messages, suggesting that strategies targeting structural characteristics of re-sharing could be crucial in managing the dissemination of such content on private messaging platforms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion</title>
<link>https://arxiv.org/abs/2505.17038</link>
<guid>https://arxiv.org/abs/2505.17038</guid>
<content:encoded><![CDATA[
<div> Keywords: web data, disaster response, social media, crisis, AI-driven method

Summary: 
- The study examines the use of web data from social media and public inquiry submissions for government disaster response during the 2022 floods in New South Wales, Australia.
- Analysis of flood-related tweets and submissions reveals behavioral patterns during extreme weather events.
- The methodology integrates Latent Dirichlet Allocation (LDA) and Large Language Models (LLMs) to enhance semantic understanding of the data.
- LDA identifies distinct opinions and geographic patterns, while LLMs improve filtering to prioritize actionable content.
- The Relevance Index method developed in this study reduces noise in social media content, improving situational awareness for emergency responders and aiding in long-term resilience planning. 

<br /><br />Summary: <div>
arXiv:2505.17038v1 Announce Type: cross 
Abstract: Massive and diverse web data are increasingly vital for government disaster response, as demonstrated by the 2022 floods in New South Wales (NSW), Australia. This study examines how X (formerly Twitter) and public inquiry submissions provide insights into public behaviour during crises. We analyse more than 55,000 flood-related tweets and 1,450 submissions to identify behavioural patterns during extreme weather events. While social media posts are short and fragmented, inquiry submissions are detailed, multi-page documents offering structured insights. Our methodology integrates Latent Dirichlet Allocation (LDA) for topic modelling with Large Language Models (LLMs) to enhance semantic understanding. LDA reveals distinct opinions and geographic patterns, while LLMs improve filtering by identifying flood-relevant tweets using public submissions as a reference. This Relevance Index method reduces noise and prioritizes actionable content, improving situational awareness for emergency responders. By combining these complementary data streams, our approach introduces a novel AI-driven method to refine crisis-related social media content, improve real-time disaster response, and inform long-term resilience planning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning</title>
<link>https://arxiv.org/abs/2505.17068</link>
<guid>https://arxiv.org/abs/2505.17068</guid>
<content:encoded><![CDATA[
<div> Keywords: user toxicity, online discussions, health-related topics, Collaborative Filtering, Machine Learning

Summary:
User toxicity in health-related online discussions is a common issue that can lead to conflict and misinformation. Instead of detecting and removing toxic comments reactively, this study proposes a predictive approach to anticipate potential toxicity. By using Collaborative Filtering-based Machine Learning, the researchers were able to predict toxicity in COVID-related conversations on Reddit with over 80% accuracy. This predictive model can help prevent conflicts by identifying potentially toxic interactions between users and specific subcommunities. This proactive strategy could be a more effective and efficient way to manage user toxicity in online discussions, ultimately promoting healthier and more constructive conversations in health-related topics.
<br /><br />Summary: <div>
arXiv:2505.17068v1 Announce Type: cross 
Abstract: In health-related topics, user toxicity in online discussions frequently becomes a source of social conflict or promotion of dangerous, unscientific behaviour; common approaches for battling it include different forms of detection, flagging and/or removal of existing toxic comments, which is often counterproductive for platforms and users alike. In this work, we propose the alternative of combatting user toxicity predictively, anticipating where a user could interact toxically in health-related online discussions. Applying a Collaborative Filtering-based Machine Learning methodology, we predict the toxicity in COVID-related conversations between any user and subcommunity of Reddit, surpassing 80% predictive performance in relevant metrics, and allowing us to prevent the pairing of conflicting users and subcommunities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Web and Software Agents -- A Forgotten Wave of Artificial Intelligence?</title>
<link>https://arxiv.org/abs/2503.20793</link>
<guid>https://arxiv.org/abs/2503.20793</guid>
<content:encoded><![CDATA[
<div> Semantic Web, AI, Software Agents, Knowledge Representation, Neural Models

Summary:
The paper argues that the history of Artificial Intelligence (AI) has experienced waves of optimism and disappointment, with the forgotten wave being the rise of the Semantic Web and intelligent Software Agents. While ChatGPT and Large Language Models now dominate the AI conversation, the Semantic Web aimed to create a machine-interpretable ecosystem for AI reasoning and action from 2000 to 2010. Despite fading into obscurity, revisiting this wave offers insights for modern Software Agent development as AI technologies evolve. Through bibliometric data analysis, the paper highlights the Semantic Web's significance in AI history and its potential relevance for current AI advancements. Recognizing this overlooked chapter provides a deeper understanding of AI's cyclical evolution and valuable lessons for merging emerging technologies. 

Summary: <div>
arXiv:2503.20793v2 Announce Type: replace 
Abstract: The history of Artificial Intelligence (AI) is a narrative of waves -- rising optimism followed by crashing disappointments. AI winters, such as the early 2000s, are often remembered as barren periods of innovation. This paper argues that such a perspective overlooks a crucial wave of AI that seems to be forgotten: the rise of the Semantic Web, which is based on knowledge representation, logic, and reasoning, and its interplay with intelligent Software Agents. Fast forward to today, and ChatGPT has reignited AI enthusiasm, built on deep learning and advanced neural models. However, before Large Language Models (LLMs) dominated the conversation, another ambitious vision emerged -- one where AI-driven Software Agents autonomously served Web users based on a structured, machine-interpretable Web. The Semantic Web aimed to transform the World Wide Web into an ecosystem where AI could reason, understand, and act. Between 2000 and 2010, this vision sparked a significant research boom, only to fade into obscurity as AI's mainstream narrative shifted elsewhere. Today, as LLMs edge toward autonomous execution, we revisit this overlooked wave. By analyzing its academic impact through bibliometric data, we highlight the Semantic Web's role in AI history and its untapped potential for modern Software Agent development. Recognizing this forgotten chapter not only deepens our understanding of AI's cyclical evolution but also offers key insights for integrating emerging technologies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying urban socio-economic segregation through co-residence network reconstruction</title>
<link>https://arxiv.org/abs/2501.15920</link>
<guid>https://arxiv.org/abs/2501.15920</guid>
<content:encoded><![CDATA[
<div> Keywords: urban segregation, socio-economic disparities, migrant communities, co-residence preferences, integration

Summary: 
The study focuses on urban segregation in Vienna, where a significant migrant population resides. Using administrative data, the analysis highlights two key clusters in the city influenced by wealth inequalities, district diversity, and nationality-based preferences. This segregation stems from a mix of socio-economic factors and residential choices, exacerbating inequalities and social polarization. Understanding co-residence preferences between migrants and locals at the neighbourhood level provides crucial insights into the dynamics of segregation. The findings emphasize the need for targeted policies to promote integration and address the challenges posed by urban segregation. By shedding light on the underlying mechanisms driving segregation, the study contributes to informing strategies for creating more inclusive and cohesive urban communities. Through a comprehensive examination of these factors, the research lays the foundation for fostering a more integrated and equitable urban environment in Vienna. 

<br /><br />Summary: <div>
arXiv:2501.15920v2 Announce Type: replace-cross 
Abstract: Urban segregation poses a critical challenge in cities, exacerbating inequalities, social tensions, fears, and polarization. It emerges from a complex interplay of socio-economic disparities and residential preferences, disproportionately impacting migrant communities. In this paper, using a comprehensive administrative data from Vienna, where nearly 40% of the population consists of international migrants, we analyse co-residence preferences between migrants and locals at the neighbourhood level. Our findings reveal two major clusters in Vienna shaped by wealth disparities, district diversity, and nationality-based homophily. These insights shed light on the underlying mechanisms of urban segregation and designing policies for better integration.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ricci Matrix Comparison for Graph Alignment: A DMC Variation</title>
<link>https://arxiv.org/abs/2505.15831</link>
<guid>https://arxiv.org/abs/2505.15831</guid>
<content:encoded><![CDATA[
<div> geometric graph alignment, Ricci Matrix Comparison, Degree Matrix Comparison, torus, protein-protein interaction network  
Summary:  
The paper introduces the Ricci Matrix Comparison (RMC) and discusses its application in geometric graph alignment along with the Degree Matrix Comparison (DMC). The study begins by exploring different methods of constructing a torus and then introduces the RMC based on DMC with theoretical justifications. Experimental results on a torus and a complex protein-protein interaction network demonstrate the effectiveness of utilizing a differential-geometric approach to graph alignment. Results indicate that utilizing Ricci curvature in graph alignment can help identify holes in tori and align line graphs of complex networks with high accuracy. This study presents a novel perspective on graph alignment and validates the previous DMC method. <div>
arXiv:2505.15831v1 Announce Type: new 
Abstract: The graph alignment problem explores the concept of node correspondence and its optimality. In this paper, we focus on purely geometric graph alignment methods, namely our newly proposed Ricci Matrix Comparison (RMC) and its original form, Degree Matrix Comparison (DMC). To formulate a Ricci-curvature-based graph alignment situation, we start with discussing different ideas of constructing one of the most typical and important topological objects, the torus, and then move on to introducing the RMC based on DMC with theoretical motivations. Lastly, we will present to the reader experimental results on a torus and a complex protein-protein interaction network that indicate the potential of applying a differential-geometric view to graph alignment. Results show that a direct variation of DMC using Ricci curvature can help with identifying holes in tori and aligning line graphs of a complex network at 80-90+% accuracy. This paper contributes a new perspective to the field of graph alignment and partially shows the validity of the previous DMC method.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation</title>
<link>https://arxiv.org/abs/2505.15834</link>
<guid>https://arxiv.org/abs/2505.15834</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news, social media, detection algorithms, propagation characteristics, graph neural networks

Summary:
The article introduces the MPPFND dataset, focusing on detecting fake news spread across multiple platforms. It highlights the distinct propagation structures and social context features of different platforms, emphasizing the need to consider cross-platform propagation differences. The proposed APSL model utilizes graph neural networks to extract social context features from various platforms, improving fake news detection performance. The study underscores the importance of analyzing both news content and social context to detect fake news effectively and mitigate its negative impact on society. The research suggests that by accounting for platform-specific characteristics, detection algorithms can enhance their accuracy in identifying fake news circulating on social media platforms. Overall, the study contributes valuable insights into addressing the challenges posed by the widespread dissemination of fake news on social media. 

<br /><br />Summary: <div>
arXiv:2505.15834v1 Announce Type: new 
Abstract: Fake news spreads widely on social media, leading to numerous negative effects. Most existing detection algorithms focus on analyzing news content and social context to detect fake news. However, these approaches typically detect fake news based on specific platforms, ignoring differences in propagation characteristics across platforms. In this paper, we introduce the MPPFND dataset, which captures propagation structures across multiple platforms. We also describe the commenting and propagation characteristics of different platforms to show that their social contexts have distinct features. We propose a multi-platform fake news detection model (APSL) that uses graph neural networks to extract social context features from various platforms. Experiments show that accounting for cross-platform propagation differences improves fake news detection performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Wiki: Characterizing Wikipedia Linking Across the Web</title>
<link>https://arxiv.org/abs/2505.15837</link>
<guid>https://arxiv.org/abs/2505.15837</guid>
<content:encoded><![CDATA[
<div> analysis, Wikipedia, Web, dataset, references
<br />
Wikipedia's influence beyond its platform is explored through a large-scale analysis of how it is referenced across the Web. The study focuses on English Wikipedia and finds that it is most commonly cited by news and science websites for informational purposes, with fewer references from commercial sites. The majority of Wikipedia links are found within the main content of websites, highlighting their role in structured knowledge presentation. The links primarily serve as explanatory references rather than as evidence or attribution, reinforcing Wikipedia's function as a background knowledge provider. The publicly released Web2Wiki dataset includes links from multiple language editions, enabling further research on Wikipedia's global impact on the Web.
<br /><br />Summary: <div>
arXiv:2505.15837v1 Announce Type: new 
Abstract: Wikipedia is one of the most visited websites globally, yet its role beyond its own platform remains largely unexplored. In this paper, we present the first large-scale analysis of how Wikipedia is referenced across the Web. Using a dataset from Common Crawl, we identify over 90 million Wikipedia links spanning 1.68% of Web domains and examine their distribution, context, and function. Our analysis of English Wikipedia reveals three key findings: (1) Wikipedia is most frequently cited by news and science websites for informational purposes, while commercial websites reference it less often. (2) The majority of Wikipedia links appear within the main content rather than in boilerplate or user-generated sections, highlighting their role in structured knowledge presentation. (3) Most links (95%) serve as explanatory references rather than as evidence or attribution, reinforcing Wikipedia's function as a background knowledge provider. While this study focuses on English Wikipedia, our publicly released Web2Wiki dataset includes links from multiple language editions, supporting future research on Wikipedia's global influence on the Web.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening</title>
<link>https://arxiv.org/abs/2505.15842</link>
<guid>https://arxiv.org/abs/2505.15842</guid>
<content:encoded><![CDATA[
<div> Graph Coarsening, Adaptive, Heterogeneous, Locality Sensitive Hashing, Consistent Hashing  
Summary:  
Graph Coarsening (GC) is a technique for compressing large graphs to facilitate efficient learning and inference. Existing methods often have to recompute from scratch for different coarsening ratios, leading to unnecessary overhead. This new framework combines Locality Sensitive Hashing (LSH) and Consistent Hashing to enable adaptive graph coarsening, making the process inherently fast and scalable. It introduces a type isolated coarsening strategy for heterogeneous graphs, ensuring semantic consistency by restricting merges to nodes of the same type. This method is the first to support both adaptive and heterogeneous coarsening. Extensive evaluations on various types of graphs show that this approach achieves superior scalability while maintaining the original graph's structural and semantic integrity.  
<br /><br />Summary: <div>
arXiv:2505.15842v1 Announce Type: new 
Abstract: $\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique that compresses large graphs to enable efficient learning and inference. However, existing GC methods generate only one coarsened graph per run and must recompute from scratch for each new coarsening ratio, resulting in unnecessary overhead. Moreover, most prior approaches are tailored to $\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints of $\textit{heterogeneous}$ graphs, which comprise multiple node and edge types. To overcome these limitations, we introduce a novel framework that combines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable $\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method is inherently fast and scalable. For heterogeneous graphs, we propose a $\textit{type isolated coarsening}$ strategy that ensures semantic consistency by restricting merges to nodes of the same type. Our approach is the first unified framework to support both adaptive and heterogeneous coarsening. Extensive evaluations on 23 real-world datasets including homophilic, heterophilic, homogeneous, and heterogeneous graphs demonstrate that our method achieves superior scalability while preserving the structural and semantic integrity of the original graph.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Prosocial Behavior and Social Contagion in LLM Agents under Institutional Interventions</title>
<link>https://arxiv.org/abs/2505.15857</link>
<guid>https://arxiv.org/abs/2505.15857</guid>
<content:encoded><![CDATA[
<div> prosocial behavior, large language models, simulation framework, social contexts, institutional dynamics <br />
Summary: <br />
The article introduces ProSim, a simulation framework to study prosocial behavior in large language models (LLMs) in various social and institutional settings. The framework consists of four components and is used to evaluate how LLM-based agents exhibit and adapt prosocial behavior. The studies conducted show that LLM agents display stable and context-sensitive prosocial behavior, respond to normative interventions, engage in fairness-based punishment, and react to inequities and enforcement costs. The research also highlights how policy-induced inequities can suppress prosocial behavior, spread through social networks, and be influenced by agents' perceptions of unfairness. This work sets the foundation for assessing social alignment and understanding institutional dynamics in societies driven by autonomous agents. <br /> <div>
arXiv:2505.15857v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly serve as autonomous agents in social contexts, understanding their capacity for prosocial behavior becomes essential. We present ProSim, a simulation framework designed to examine how prosocial behavior emerges, adapts, and erodes in LLM-based agents under diverse social and institutional conditions. The framework comprises four components: individual simulation, scenario simulation, interaction simulation, and intervention simulation. We conduct three progressive studies to evaluate prosocial alignment. First, we show that LLM agents can demonstrate stable and context-sensitive prosocial behavior across diverse scenarios and adapt their responses under normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate through social networks, and are mediated by agents' perceptions of unfairness. These findings lay the groundwork for evaluating social alignment and modeling institutional dynamics in agent-driven societies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight Practical Bounds for Subgraph Densities in Ego-centric Networks</title>
<link>https://arxiv.org/abs/2505.16079</link>
<guid>https://arxiv.org/abs/2505.16079</guid>
<content:encoded><![CDATA[
<div> Subgraph densities, network analysis, localized structures, domain-driven features, flag algebras<br />
<br />
Summary: This paper discusses the importance of subgraph densities in network analysis and the challenges of distinguishing mathematically-determined features from domain-driven ones. By providing tighter bounds on subgraph densities and introducing the subgraph spread ratio, the study quantifies differences in realized subgraph densities across various types of networks. Through a combination of flag algebra techniques, motif-counting, and topological data analysis, the research demonstrates more accurate comparisons between graphs. The empirical analysis reveals that social networks have smaller subgraph spread ratios compared to other networks like linkage-mapping networks for Wikipedia pages, indicating distinct structural characteristics. The subgraph spread ratio offers a metric for quantifying network structures and comparing different types of networks. <div>
arXiv:2505.16079v1 Announce Type: new 
Abstract: Subgraph densities play a crucial role in network analysis, especially for the identification and interpretation of meaningful substructures in complex graphs. Localized subgraph densities, in particular, can provide valuable insights into graph structures. Distinguishing between mathematically-determined and domain-driven subgraph density features, however, poses challenges. For instance, the lack or presence of certain structures can be explained by graph density or degree distribution. These differences are especially meaningful in applied contexts as they allow us to identify instances where the data induces specific network structures, such as friendships in social networks. The goal of this paper is to measure these differences across various types of graphs, conducting social media analysis from a network perspective. To this end, we first provide tighter bounds on subgraph densities. We then introduce the subgraph spread ratio to quantify the realized subgraph densities of specific networks relative to the feasible bounds. Our novel approach combines techniques from flag algebras, motif-counting, and topological data analysis. Crucially, effective adoption of the state-of-the-art in the plain flag algebra method yields feasible regions up to three times tighter than prior best-known results, thereby enabling more accurate and direct comparisons across graphs. We additionally perform an empirical analysis of 11 real-world networks. We observe that social networks consistently have smaller subgraph spread ratios than other types of networks, such as linkage-mapping networks for Wikipedia pages. This aligns with our intuition about social relationships: such networks have meaningful structure that makes them distinct. The subgraph spread ratio enables the quantification of intuitive understandings of network structures and provides a metric for comparing types of networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Rewiring Mechanism for Restoration of the Fragmented Social Networks after Attacks</title>
<link>https://arxiv.org/abs/2505.16233</link>
<guid>https://arxiv.org/abs/2505.16233</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, robustness, network reconstruction, strategic rewiring, Laplacian Energy

Summary: 
In this research work, the focus is on managing the security and resilience of complex systems, such as social networks, technological infrastructures, and communication networks, especially during times of disaster. The study involves reconstructing networks by rewiring or adding edges and measuring their robustness. Two approaches, strategic rewiring, and budget-constrained optimal rewiring, are utilized to evaluate network robustness. Unlike conventional methods that solely assess the largest connected component of a network, this research explores a more comprehensive approach by considering the impact of connection failures on network structure. By incorporating Laplacian Energy analysis, the study aims to gain a better understanding of network behavior during restoration processes while still considering the size of the largest connected component under attacks. This holistic approach provides valuable insights into enhancing network resilience in the face of disruptions. 

<br /><br />Summary: <div>
arXiv:2505.16233v1 Announce Type: new 
Abstract: Real-world complex systems exhibit intricate interconnections and dependencies, especially social networks, technological infrastructures, and communication networks. These networks are prone to disconnection due to random failures or external attacks on their components. Therefore, managing the security and resilience of such networks is a prime concern, particularly at the time of disaster. Therefore, in this research work, network is reconstructed by rewiring/addition of the edges and robustness of the networks is measured. To this aim, two approaches namely (i) Strategic rewiring (ii) budget constrained optimal rewiring are adopted. While current research often assesses robustness by examining the size of the largest connected component, this approach fails to capture the complete spectrum of vulnerability. The failure of a small number of connections leads to a sparser network yet connected network. Thus, the present research work delves deeper into evaluating the robustness of the restored network by evaluating Laplacian Energy to better comprehend the system's behavior during the restoration of the network still considering the size of the largest connected component attacks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling in the Blanks? A Systematic Review and Theoretical Conceptualisation for Measuring WikiData Content Gaps</title>
<link>https://arxiv.org/abs/2505.16383</link>
<guid>https://arxiv.org/abs/2505.16383</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikidata, knowledge graph, content gaps, systematic biases, completeness

Summary:
This paper presents a systematic literature review focusing on the content gaps within Wikidata, a collaborative knowledge graph used in Wikimedia projects. The study identifies a long-tail of items with limited data and systematic gaps in the available content. A typology of gaps is proposed based on prior research, along with a theoretical framework for conceptualizing and measuring these gaps. The methods and metrics used in the literature are classified to uncover overlooked gaps in Wikidata. The implications of these gaps on collaboration and editor activity within Wikidata are discussed, highlighting the importance of addressing quality, completeness, and systematic biases. The results provide insights into understanding knowledge gaps more broadly and offer valuable directions for future research in this area. 

<br /><br />Summary: <div>
arXiv:2505.16383v1 Announce Type: new 
Abstract: Wikidata is a collaborative knowledge graph which provides machine-readable structured data for Wikimedia projects including Wikipedia. Managed by a community of volunteers, it has grown to become the most edited Wikimedia project. However, it features a long-tail of items with limited data and a number of systematic gaps within the available content. In this paper, we present the results of a systematic literature review aimed to understand the state of these content gaps within Wikidata. We propose a typology of gaps based on prior research and contribute a theoretical framework intended to conceptualise gaps and support their measurement. We also describe the methods and metrics present used within the literature and classify them according to our framework to identify overlooked gaps that might occur in Wikidata. We then discuss the implications for collaboration and editor activity within Wikidata as well as future research directions. Our results contribute to the understanding of quality, completeness and the impact of systematic biases within Wikidata and knowledge gaps more generally.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Citation Parsing and Analysis with Language Models</title>
<link>https://arxiv.org/abs/2505.15948</link>
<guid>https://arxiv.org/abs/2505.15948</guid>
<content:encoded><![CDATA[
<div> Keywords: global inequalities, knowledge circulation, citation tracking, language models, research indexing

Summary: 
- The article addresses the need for a tool to support journals in understanding knowledge circulation and reducing global inequalities in knowledge production.
- It highlights the lack of information about knowledge sharing networks in the Global South and the resulting exclusion of Southern researchers from indexing services.
- The study investigates the use of open-weight language models to annotate manuscript citations in an indexable format.
- Evaluation of different models shows high accuracy in identifying citation components, surpassing current methods.
- The findings suggest that even smaller models can effectively parse citation fields with post-training, offering potential for improved citation network tracking and research discovery. 

<br /><br />Summary: <div>
arXiv:2505.15948v1 Announce Type: cross 
Abstract: A key type of resource needed to address global inequalities in knowledge production and dissemination is a tool that can support journals in understanding how knowledge circulates. The absence of such a tool has resulted in comparatively less information about networks of knowledge sharing in the Global South. In turn, this gap authorizes the exclusion of researchers and scholars from the South in indexing services, reinforcing colonial arrangements that de-center and minoritize those scholars. In order to support citation network tracking on a global scale, we investigate the capacity of open-weight language models to mark up manuscript citations in an indexable format. We assembled a dataset of matched plaintext and annotated citations from preprints and published research papers. Then, we evaluated a number of open-weight language models on the annotation task. We find that, even out of the box, today's language models achieve high levels of accuracy on identifying the constituent components of each citation, outperforming state-of-the-art methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all fields with high accuracy in $2^5$ passes, suggesting that post-training is likely to be effective in producing small, robust citation parsing models. Such a tool could greatly improve the fidelity of citation networks and thus meaningfully improve research indexing and discovery, as well as further metascientific research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Generative Modeling via Substructure Sequences</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
<div> Transformer, pre-training, generative, scalability, graph learning  
Summary:  
Generative Graph Pattern Machine (G$^2$PM) is introduced as a novel approach for graph representation learning, moving beyond traditional message-passing methods. G$^2$PM leverages a generative Transformer pre-training framework to model graph instances as sequences of substructures, enabling the learning of transferable representations. The scalability of G$^2$PM is demonstrated through experiments on the ogbn-arxiv benchmark, showcasing performance improvements with larger model sizes compared to existing generative approaches. The systematic analysis of the model design space highlights key architectural choices contributing to scalability and generalization. G$^2$PM consistently outperforms strong baselines across various tasks such as node classification, graph classification, and transfer learning, establishing its effectiveness as a scalable graph learning framework. The code and dataset for G$^2$PM are publicly available for further research and exploration.  
Summary: <div>
arXiv:2505.16130v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) has been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance, limiting the viability of GNNs as backbones for graph foundation models. In this work, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable, transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node classification, graph classification, and transfer learning -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban transport systems shape experiences of social segregation</title>
<link>https://arxiv.org/abs/2505.16337</link>
<guid>https://arxiv.org/abs/2505.16337</guid>
<content:encoded><![CDATA[
<div> transportation, social segregation, urban policy, mobility, urban design

Summary: 
This study examines the role of urban transportation systems in shaping social segregation. Using city-scale GPS mobility data and a novel probabilistic mobility framework, the research shows how social interactions occur at different scales within transportation infrastructure. The study reveals that social segregation is influenced by factors such as time of day, urban design, and service design. It highlights the importance of understanding segregation as a product of daily mobility practices. Exploratory simulations suggest that transportation policies aimed at promoting sustainable transport may have unintended effects on segregation. The findings emphasize the need for urban policymakers to consider the broader impacts of their interventions and how they affect the daily experiences of residents. <div>
arXiv:2505.16337v1 Announce Type: cross 
Abstract: Mobility is a fundamental feature of human life, and through it our interactions with the world and people around us generate complex and consequential social phenomena. Social segregation, one such process, is increasingly acknowledged as a product of one's entire lived experience rather than mere residential location. Increasingly granular sources of data on human mobility have evidenced how segregation persists outside the home, in workplaces, cafes, and on the street. Yet there remains only a weak evidential link between the production of social segregation and urban policy. This study addresses this gap through an assessment of the role of the urban transportation systems in shaping social segregation. Using city-scale GPS mobility data and a novel probabilistic mobility framework, we establish social interactions at the scale of transportation infrastructure, by rail and bus service segment, individual roads, and city blocks. The outcomes show how social segregation is more than a single process in space, but varying by time of day, urban design and structure, and service design. These findings reconceptualize segregation as a product of likely encounters during one's daily mobility practice. We then extend these findings through exploratory simulations, highlighting how transportation policy to promote sustainable transport may have potentially unforeseen impacts on segregation. The study underscores that to understand social segregation and achieve positive social change urban policymakers must consider the broadest impacts of their interventions and seek to understand their impact on the daily lived experience of their citizens.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Inequality in Complex Networks of Strategic Agents using Iterative Game-Theoretic Transactions</title>
<link>https://arxiv.org/abs/2505.16966</link>
<guid>https://arxiv.org/abs/2505.16966</guid>
<content:encoded><![CDATA[
<div> transactions, social networks, scale-free distribution, Gini Coefficient, inequality 

Summary: 
- Transactions are essential in human social life, involving information, trust, and capital flow.
- Research on transactions lacks understanding of systemic effects in real-world social networks with different agent types.
- Gini Coefficient, important in economics for wealth inequality, is underutilized in complex networks and game theory studies.
- A model and simulation algorithm based on game theory are proposed to quantify inequality evolution in complex networks.
- Results show various drivers of inequality in simple settings, consistent across different network types. <div>
arXiv:2505.16966v1 Announce Type: cross 
Abstract: Transactions are an important aspect of human social life, and represent dynamic flow of information, intangible values, such as trust, as well as monetary and social capital. Although much research has been conducted on the nature of transactions in fields ranging from the social sciences to game theory, the systemic effects of different types of agents transacting in real-world social networks (often following a scale-free distribution) are not fully understood. A particular systemic measure that has not received adequate attention in the complex networks and game theory communities, is the Gini Coefficient, which is widely used in economics to quantify and understand wealth inequality. In part, the problem is a lack of experimentation using a replicable algorithm and publicly available data. Motivated by this problem, this article proposes a model and simulation algorithm, based on game theory, for quantifying the evolution of inequality in complex networks of strategic agents. Our results shed light on several complex drivers of inequality, even in simple, abstract settings, and exhibit consistency across networks with different origins and descriptions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Influence Estimator: Towards Real-time Solutions to Influence Blocking Maximization</title>
<link>https://arxiv.org/abs/2308.14012</link>
<guid>https://arxiv.org/abs/2308.14012</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence blocking maximization, Neural influence estimator, Optimization algorithms, Monte Carlo simulations, Social network

Summary: 
In the realm of containing the spread of misinformation, real-time solutions to influence blocking maximization (IBM) problems are essential. Traditional methods rely on costly Monte Carlo simulations (MCSs) to assess blocked influence. However, a new approach introduces a neural influence estimator (NIE) as a fast surrogate model to address IBM problems efficiently. By formulating a learning problem to construct the NIE, this model can predict blocked influence based on false-and-true information instances and extract features from the network topology. The NIE can be combined with existing optimization algorithms to solve IBM problems online, showcasing impressive results. Experiments demonstrated that the NIE-based optimization method significantly outperforms MCSs-based methods in terms of speed and scalability, making it a promising solution for real-time IBM problem-solving. This innovation opens up new possibilities for efficiently tackling IBM problems in large social networks. 

<br /><br />Summary: <div>
arXiv:2308.14012v2 Announce Type: replace-cross 
Abstract: Real-time solutions to the influence blocking maximization (IBM) problems are crucial for promptly containing the spread of misinformation. However, achieving this goal is non-trivial, mainly because assessing the blocked influence of an IBM problem solution typically requires plenty of expensive Monte Carlo simulations (MCSs). This work presents a novel approach that enables solving IBM problems with hundreds of thousands of nodes and edges in seconds. The key idea is to construct a fast-to-evaluate surrogate model called neural influence estimator (NIE) offline as a substitute for the time-intensive MCSs, and then combine it with optimization algorithms to address IBM problems online. To this end, a learning problem is formulated to build the NIE that takes the false-and-true information instance as input, extracts features describing the topology and inter-relationship between two seed sets, and predicts the blocked influence. A well-trained NIE can generalize across different IBM problems given a social network, and can be readily combined with existing IBM optimization algorithms. The experiments on 25 IBM problems with up to millions of edges show that the NIE-based optimization method can be up to four orders of magnitude faster than MCSs-based optimization method to achieve the same optimization quality. Moreover, given a one-minute limit, the NIE-based method can solve IBM problems with up to hundreds of thousands of nodes, which is at least one order of magnitude larger than what can be solved by existing methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Confidence Models of Opinion Dynamics with Neighborhood Effects</title>
<link>https://arxiv.org/abs/2402.05368</link>
<guid>https://arxiv.org/abs/2402.05368</guid>
<content:encoded><![CDATA[
<div> neighborhood effects, opinion dynamics, bounded-confidence models, network adaptation, numerical simulations

Summary:
The study introduces neighborhood effects into bounded-confidence models (BCMs) of opinion dynamics, creating neighborhood BCMs (NBCMs). These NBCMs incorporate both dyadic influence between interacting agents and transitive influence from agents' neighborhoods. The model includes neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs. Additionally, a network adaptation component is introduced, where the network structure coevolves with agent opinions through transitive homophily. Numerical simulations on various network types show how the dynamics and network properties change based on the proportions of dyadic and transitive influence. The results indicate that including neighborhood effects in opinion dynamics and network adaptation leads to a reduction in network spectral gap and degree assortativity. <div>
arXiv:2402.05368v2 Announce Type: replace-cross 
Abstract: We generalize bounded-confidence models (BCMs) of opinion dynamics by incorporating neighborhood effects. In a BCM, interacting agents influence each other through dyadic influence if their opinions are sufficiently similar to each other. In our "neighborhood BCMs" (NBCMs), interacting agents are influenced both by each other's opinions and by the opinions of the agents in each other's neighborhoods. Our NBCMs thus include both the usual dyadic influence between agents and a "transitive influence", which encodes the influence of an agent's neighbors, when determining whether or not an interaction changes the opinions of agents. In this transitive influence, an individual's opinion is influenced by a neighbor when, on average, the opinions of the neighbor's neighbors are sufficiently similar to its own opinion. We formulate both neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs.
  We build further on our NBCMs by introducing a neighborhood-based network adaptation in which a network coevolves with agent opinions by changing its structure through "transitive homophily". In this network evolution, an agent breaks a tie to one of its neighbors and then rewires that tie to a new agent, with a preference for agents with a mean neighbor opinion that is closer to its own opinion. Using numerical simulations on a variety of types of networks, we explore how the qualitative opinion dynamics and network properties of our adaptive NDW model change as we adjust the relative proportions of dyadic and transitive influence. In our numerical experiments, we find that incorporating neighborhood effects into the opinion dynamics and the network-adaptation rewiring strategy tends to reduce the spectral gap and degree assortativity of networks.
  (This is a shortened version of the paper's abstract.)
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender differences in collaboration and career progression in physics</title>
<link>https://arxiv.org/abs/2408.02482</link>
<guid>https://arxiv.org/abs/2408.02482</guid>
<content:encoded><![CDATA[
<div> gender differences, collaboration networks, academic career progression, physics, principal investigator (PI)

Summary:
- The study examines gender differences in collaboration networks and academic career progression in physics.
- The relationship between collaborative behavior and career progression is similar for men and women when controlling for the number of publications.
- Researchers who eventually become principal investigators tend to have collaborated with more unique partners.
- Collaborating repeatedly with the same highly interconnected group or a larger number of co-authors per publication is associated with shorter career lengths and not attaining PI status.
- Women collaborate in more tightly connected and larger groups than men, and are less likely to attain the status of PI throughout their careers with a lower survival probability, highlighting the need for policies to address this gap.

<br /><br />Summary: <div>
arXiv:2408.02482v2 Announce Type: replace-cross 
Abstract: We examine gender differences in collaboration networks and academic career progression in physics. We use the likelihood and time to become a principal investigator (PI) and the length of an author's career to measure career progression. Utilising logistic regression and accelerated failure time models, we examine whether the effect of collaboration behaviour varies by gender. We find that, controlling for the number of publications, the relationship between collaborative behaviour and career progression is almost the same for men and women. Specifically, we find that those who eventually reach principal investigator (PI) status, tend to have published with more unique collaborators. In contrast, publishing repeatedly with the same highly interconnected collaborators and/or larger number of co-authors per publication is characteristic of shorter career lengths and not attaining PI status. We observe that women tend to collaborate in more tightly connected and larger groups than men. Finally, we observe that women are less likely to attain the status of PI throughout their careers and have a lower survival probability compared to men, which calls for policies to close this crucial gap.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Degree-Based Quasi-Clique Search via an Iterative Framework</title>
<link>https://arxiv.org/abs/2505.15118</link>
<guid>https://arxiv.org/abs/2505.15118</guid>
<content:encoded><![CDATA[
<div> maximum $\gamma$-quasi-clique, cohesive subgraph mining, IterQC, pseudo lower bound, preprocessing technique<br />
Summary:
The article introduces the novel algorithm IterQC for solving the maximum $\gamma$-quasi-clique problem, a fundamental issue in graph theory with diverse real-world applications. The problem is challenging due to its NP-hard nature and lack of the hereditary property. IterQC reformulates the problem as a series of $k$-plex problems and introduces optimization techniques like the pseudo lower bound and preprocessing. These techniques enhance efficiency by leveraging information across iterations and reducing problem size. Experimental results show that IterQC significantly outperforms existing algorithms DDA and FastQC in terms of speed and solving capability, achieving up to four orders of magnitude speedup and solving more graph instances effectively. This makes IterQC a promising algorithm for cohesive subgraph mining tasks. <br /><br />Summary: <div>
arXiv:2505.15118v1 Announce Type: new 
Abstract: Cohesive subgraph mining is a fundamental problem in graph theory with numerous real-world applications, such as social network analysis and protein-protein interaction modeling. Among various cohesive subgraphs, the $\gamma$-quasi-clique is widely studied for its flexibility in requiring each vertex to connect to at least a $\gamma$ proportion of other vertices in the subgraph. However, solving the maximum $\gamma$-quasi-clique problem is NP-hard and further complicated by the lack of the hereditary property, which makes designing efficient pruning strategies challenging. Existing algorithms, such as DDA and FastQC, either struggle with scalability or exhibit significant performance declines for small values of $\gamma$. In this paper, we propose a novel algorithm, IterQC, which reformulates the maximum $\gamma$-quasi-clique problem as a series of $k$-plex problems that possess the hereditary property. IterQC introduces a non-trivial iterative framework and incorporates two key optimization techniques: (1) the pseudo lower bound (pseudo LB) technique, which leverages information across iterations to improve the efficiency of branch-and-bound searches, and (2) the preprocessing technique that reduces problem size and unnecessary iterations. Extensive experiments demonstrate that IterQC achieves up to four orders of magnitude speedup and solves significantly more graph instances compared to state-of-the-art algorithms DDA and FastQC.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Distance on Epidemiological Dynamics in Human Connection Network with Mobility</title>
<link>https://arxiv.org/abs/2505.15331</link>
<guid>https://arxiv.org/abs/2505.15331</guid>
<content:encoded><![CDATA[
<div> Keywords: infectious diseases, human mobility, disease transmission, epidemiological dynamics, distance<br />
Summary:<br />
This study explores the impact of human mobility on the transmission of infectious diseases, particularly focusing on the role of distance between individuals. Moving beyond traditional metapopulation movement analysis, the research considers the proximity of an infected person to a healthy individual during movement. Mathematical expressions are derived for key epidemiological metrics, including the basic reproduction number ($R_0$) and the critical infection rate ($\beta_{critical}$), in relation to distance. The model developed in this study aligns closely with observed patterns of COVID-19 spread, as evidenced by analysis of available datasets. By incorporating distance into epidemiological dynamics, this research provides valuable insights into how human movement influences disease transmission at a personal level. The findings highlight the importance of considering spatial factors in understanding and predicting the spread of infectious diseases. <br /><br />Summary: <div>
arXiv:2505.15331v1 Announce Type: new 
Abstract: The spread of infectious diseases is often influenced by human mobility across different geographical regions. Although numerous studies have investigated how diseases like SARS and COVID-19 spread from China to various global locations, there remains a gap in understanding how the movement of individuals contributes to disease transmission on a more personal or human-to-human level. Typically, researchers have employed the concept of metapopulation movement to analyze how diseases move from one location to another. This paper shifts focus to the dynamics of disease transmission, incorporating the critical factor of distance between an infected person and a healthy individual during human movement. The study delves into the impact of distance on various parameters of epidemiological dynamics throughout human mobility. Mathematical expressions for important epidemiological metrics, such as the basic reproduction number ($R_0$) and the critical infection rate ($\beta_{critical}$), are derived in relation to the distance between individuals. The results indicate that the proposed model closely aligns with observed patterns of COVID-19 spread based on the analysis done on the available datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Reposting on X</title>
<link>https://arxiv.org/abs/2505.15370</link>
<guid>https://arxiv.org/abs/2505.15370</guid>
<content:encoded><![CDATA[
<div> predictive modeling, user reposting behavior, machine learning, out-of-distribution, Twitter

Summary:
The study focuses on predicting user reposting behavior on X (formerly Twitter) using machine learning models. Traditionally seen as a supervised classification task, the challenge shifts to out-of-distribution generalization when predicting reposting behavior for new topics. The results show existing algorithms perform well with matching distributions but falter when faced with out-of-distribution tasks. By incorporating user profile and past behavior features alongside message content features, prediction accuracy significantly improves. The study highlights the importance of considering a user's profile and past actions in reposting behavior prediction, suggesting it plays a crucial role independent of message content. <div>
arXiv:2505.15370v1 Announce Type: new 
Abstract: There have been considerable efforts to predict a user's reposting behaviour on X (formerly Twitter) using machine learning models. The problem is previously cast as a supervised classification task, where Tweets are randomly assigned to a test or training set. The random assignment helps to ensure that the test and training sets are drawn from the same distribution. In practice, we would like to predict users' reposting behaviour for a set of messages related to a new, previously unseen, topic (defined by a hashtag). In this case, the problem becomes an out-of-distribution generalisation classification task.
  Experimental results reveal that while existing algorithms, which predominantly use features derived from the content of Tweet messages, perform well when the training and test distributions are the same, these algorithms perform much worse when the test set is out of distribution. We then show that if the message features are supplemented or replaced with features derived from users' profile and past behaviour, the out-of-distribution prediction is greatly improved, with the F1 score increasing from 0.24 to 0.70. Our experimental results suggest that a significant component of reposting behaviour can be predicted based on users' profile and past behaviour, and is independent of the content of messages.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Foundation Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15116</link>
<guid>https://arxiv.org/abs/2505.15116</guid>
<content:encoded><![CDATA[
<div> Graph Foundation Models (GFMs), scalable, general-purpose intelligence, structured data, transferability, emergent capabilities <br />
<br />
Summary: <br />
Graph Foundation Models (GFMs) aim to bring general-purpose intelligence to structured data, such as social networks, biological systems, and knowledge graphs. This survey provides an overview of GFMs, categorizing them based on their generalization scope. It discusses backbone architectures, pretraining strategies, and adaptation mechanisms, highlighting key innovations and theoretical insights. The survey also examines theoretical foundations, challenges, and future directions for research in this field. Positioned at the intersection of graph learning and general-purpose AI, GFMs have the potential to become foundational infrastructure for reasoning over structured data. The survey consolidates current progress in GFMs and outlines pathways for future research in this rapidly evolving field. <div>
arXiv:2505.15116v1 Announce Type: cross 
Abstract: Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of human-like polarization among large language model agents</title>
<link>https://arxiv.org/abs/2501.05171</link>
<guid>https://arxiv.org/abs/2501.05171</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social networks, polarization, echo chamber effect, societal consequences

Summary: 
Large language models (LLMs) are rapidly advancing in their capabilities, impacting human societal dynamics. Simulated systems with thousands of LLM agents reveal a replication of human-like polarization through social interactions guided by LLM-generated conversations. These agents form their own social network with similarities to human clustering behavior while exhibiting mechanisms such as the echo chamber effect to shape collective opinions. Concerns arise regarding the potential for LLM agents to contribute to societal polarization, yet they also present an opportunity to explore strategies for mitigating polarization impacts. The similarities observed between human and LLM agent behaviors and emergent phenomena highlight the need for further understanding and preventative measures to address the risks associated with LLM influence on political deliberations and societal dynamics. 

<br /><br />Summary: <div>
arXiv:2501.05171v2 Announce Type: replace 
Abstract: Rapid advances in large language models (LLMs) have not only empowered autonomous agents to generate social networks, communicate, and form shared and diverging opinions on political issues, but have also begun to play a growing role in shaping human political deliberation. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal expansion of human mobility across urban scales</title>
<link>https://arxiv.org/abs/2406.06889</link>
<guid>https://arxiv.org/abs/2406.06889</guid>
<content:encoded><![CDATA[
<div> scaling law, mobility trajectories, network-based modules, urban systems, spatial structure

Summary:

This study explores the spatial structure of individual daily mobility trajectories and uncovers a universal scaling law that reveals a sublinear expansion of mobility modules with increasing distance from home. The analysis shows that these modules align with the nested hierarchy of urban systems, encompassing local, city-level, and regional scales as distance from home increases. This discovery provides a quantitative link between classic urban theories, human geography, and mobility studies, shedding light on the fundamental dynamics of human movement. By integrating network-based modules and spatial analysis, the study offers a new perspective on the underlying principles that govern human mobility patterns and their connection to urban structures. This research deepens our understanding of urban theory and highlights the intricate relationship between mobility behavior and spatial organization in cities. <div>
arXiv:2406.06889v4 Announce Type: replace-cross 
Abstract: Human mobility is a fundamental process underpinning socioeconomic life and urban structure. Classic theories, such as egocentric activity spaces and central place theory, provide crucial insights into specific facets of movement, like home-centricity and hierarchical spatial organization. However, identifying universal characteristics or an underlying principle that quantitatively links these disparate perspectives has remained a challenge. Here, we reveal such a connection by analyzing the spatial structure of individual daily mobility trajectories using network-based modules. We discover a universal scaling law: the spatial extent (radius) of these mobility modules expands sublinearly with increasing distance from home, a pattern consistent across three orders of magnitude. Furthermore, we demonstrate that these modules precisely map onto the nested hierarchy of urban systems, corresponding to local, city-level, and regional scales as distance from home increases. These findings deepen our understanding of human mobility dynamics and demonstrate the profound connection between classical urban theory, human geography, and mobility studies.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits</title>
<link>https://arxiv.org/abs/2410.07302</link>
<guid>https://arxiv.org/abs/2410.07302</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, art communities, social dynamics, Reddit, community norms 

Summary: 
AI-generated content, particularly in the form of visual art, has the potential to impact social dynamics within online art communities on platforms like Reddit. The study explores the prevalence of AI-generated content and accusations of AI use within these communities. Findings reveal that AI posts account for less than 0.5% of image-based posts, with accusations of AI use being more persistent. Newcomers are more likely to use AI content, potentially increasing participation in communities. However, the tone of accusations of AI use has become increasingly negative over time, especially in communities without explicit rules on AI content. The study highlights the evolving norms and interactions surrounding AI-generated content in online creative communities. 

<br /><br />Summary: <div>
arXiv:2410.07302v2 Announce Type: replace-cross 
Abstract: Broadly accessible generative AI models like Dall-E have made it possible for anyone to create compelling visual art. In online communities, the introduction of AI-generated content (AIGC) may impact social dynamics, for example causing changes in who is posting content, or shifting the norms or the discussions around the posted content if posts are suspected of being generated by AI. We take steps towards examining the potential impact of AIGC on art-related communities on Reddit. We distinguish between communities that disallow AI content and those without such a direct policy. We look at image-based posts in these communities where the author transparently shares that the image was created by AI, and at comments in these communities that suspect or accuse authors of using generative AI. We find that AI posts (and accusations) have played a surprisingly small part in these communities through the end of 2023, accounting for fewer than 0.5% of the image-based posts. However, even as the absolute number of author-labeled AI posts dwindles over time, accusations of AI use remain more persistent. We show that AI content is more readily used by newcomers and may help increase participation if it aligns with community rules. However, the tone of comments suspecting AI use by others has become more negative over time, especially in communities that do not have explicit rules about AI. Overall, the results show the changing norms and interactions around AIGC in online communities designated for creativity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Harm Abatement: Mechanisms for Transparent Public Health Assessment</title>
<link>https://arxiv.org/abs/2503.10458</link>
<guid>https://arxiv.org/abs/2503.10458</guid>
<content:encoded><![CDATA[
<div> lawsuits, social media platforms, product safety litigation, abatement plan, privacy

Summary:<br /><br /> This article discusses the lawsuits surrounding social media platforms and their potential harms. It suggests implementing an abatement and/or settlement plan as a remediation strategy outside of financial compensation, drawing on the history of American product safety litigation. The mechanism proposed would address the requirements of legal procedure, transparent public health assessment standards, and the practical needs of technology products. It anticipates the possible success of these lawsuits and outlines the implications for privacy and oversight. By operating at the intersection of these domains, the mechanism aims to mitigate abuse and improve the overall safety and accountability of social media platforms. <div>
arXiv:2503.10458v2 Announce Type: replace-cross 
Abstract: Social media platforms have been accused of causing a range of harms, resulting in dozens of lawsuits across jurisdictions. These lawsuits are situated within the context of a long history of American product safety litigation, suggesting opportunities for remediation outside of financial compensation. Anticipating that at least some of these cases may be successful and/or lead to settlements, this article outlines an implementable mechanism for an abatement and/or settlement plan capable of mitigating abuse. The paper describes the requirements of such a mechanism, implications for privacy and oversight, and tradeoffs that such a procedure would entail. The mechanism is framed to operate at the intersection of legal procedure, standards for transparent public health assessment, and the practical requirements of modern technology products.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pantheon: Personalized Multi-objective Ensemble Sort via Iterative Pareto Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13894</link>
<guid>https://arxiv.org/abs/2505.13894</guid>
<content:encoded><![CDATA[
<div> Keywords: Pantheon, ensemble sorting, personalized joint training, representation inheritance, iterative Pareto policy optimization 

Summary: 
Pantheon is introduced as an advanced ensemble sorting method that shifts from human-curated to machine-optimized science. It offers personalized joint training by aligning with real-time ranking models, ensuring accurate capture of user interests. Utilizing fine-grained hidden-states instead of compressed Pxtrs for model input enhances complexity and benefits from ranking models. An iterative Pareto policy optimization strategy, designed for balanced multi-objective ensemble sorting, considers multiple objectives concurrently. This innovative approach has successfully replaced formulation-based ensemble sort in the industry's recommendation systems, with full deployment at Kuaishou live-streaming services, catering to 400 million daily users. 

<br /><br />Summary: <div>
arXiv:2505.13894v1 Announce Type: new 
Abstract: In this paper, we provide our milestone ensemble sort work and the first-hand practical experience, Pantheon, which transforms ensemble sorting from a "human-curated art" to a "machine-optimized science". Compared with formulation-based ensemble sort, our Pantheon has the following advantages: (1) Personalized Joint Training: our Pantheon is jointly trained with the real-time ranking model, which could capture ever-changing user personalized interests accurately. (2) Representation inheritance: instead of the highly compressed Pxtrs, our Pantheon utilizes the fine-grained hidden-states as model input, which could benefit from the Ranking model to enhance our model complexity. Meanwhile, to reach a balanced multi-objective ensemble sort, we further devise an \textbf{iterative Pareto policy optimization} (IPPO) strategy to consider the multiple objectives at the same time. To our knowledge, this paper is the first work to replace the entire formulation-based ensemble sort in industry RecSys, which was fully deployed at Kuaishou live-streaming services, serving 400 Million users daily.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Influencers and Multipliers Drive Polarization and Issue Alignment on Twitter/X</title>
<link>https://arxiv.org/abs/2505.14280</link>
<guid>https://arxiv.org/abs/2505.14280</guid>
<content:encoded><![CDATA[
<div> Keywords: German Twittersphere, polarization, trending topics, influencers, multipliers

Summary:
The study explores the polarization of the German Twittersphere by analyzing trending topics and opinions expressed through (re)tweets from March 2021 to July 2023. It identifies two main ideological camps: left-leaning and right-leaning accounts, indicating a divided online public sphere. Contrary to traditional surveys, political issues exhibit strong alignment, driven by influencers who create ideologically charged content and multipliers who amplify it. These multipliers, unique to social media, play a significant role in shaping online opinion by curating and spreading content that aligns with their ideological stance. The study sheds light on the mechanisms shaping online public opinion and emphasizes the importance of regulating platforms to address the observed polarization.<br /><br />Summary: <div>
arXiv:2505.14280v1 Announce Type: new 
Abstract: We investigate the polarization of the German Twittersphere by extracting the main issues discussed and the signaled opinions of users towards those issues based on (re)tweets concerning trending topics. The dataset covers daily trending topics from March 2021 to July 2023. At the opinion level, we show that the online public sphere is largely divided into two camps, one consisting mainly of left-leaning, and another of right-leaning accounts. Further we observe that political issues are strongly aligned, contrary to what one may expect from surveys. This alignment is driven by two cores of strongly active users: influencers, who generate ideologically charged content, and multipliers, who facilitate the spread of this content. The latter are specific to social media and play a crucial role as intermediaries on the platform by curating and amplifying very specific types of content that match their ideological position, resulting in the overall observation of a strongly polarized public sphere. These results contribute to a better understanding of the mechanisms that shape online public opinion, and have implications for the regulation of platforms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UKTwitNewsCor: A Dataset of Online Local News Articles for the Study of Local News Provision</title>
<link>https://arxiv.org/abs/2505.14326</link>
<guid>https://arxiv.org/abs/2505.14326</guid>
<content:encoded><![CDATA[
<div> dataset, online news, local media, social media, UK<br />
Summary:<br />
The paper introduces UKTwitNewsCor, a dataset containing over 2.5 million online news articles from 360 local UK outlets published between January 2020 and December 2022. The dataset includes articles shared on Twitter by these outlets, as well as social media performance metrics at the tweet level. Additionally, metadata on content duplication across domains is provided. Supplementary datasets on local media web domains, UK Local Authority Districts, and digital local media providers give insight into the dataset's coverage scope. The paper discusses the data collection methodology, diversity in geographical and media ownership, and how researchers, policymakers, and industry stakeholders can utilize UKTwitNewsCor for studying local media trends, content diversity, and audience engagement dynamics. <br /><br />Summary: <div>
arXiv:2505.14326v1 Announce Type: new 
Abstract: In this paper, we present UKTwitNewsCor, a comprehensive dataset for understanding the content production, dissemination, and audience engagement dynamics of online local media in the UK. It comprises over 2.5 million online news articles published between January 2020 and December 2022 from 360 local outlets. The corpus represents all articles shared on Twitter by the social media accounts of these outlets. We augment the dataset by incorporating social media performance metrics for the articles at the tweet-level. We further augment the dataset by creating metadata about content duplication across domains. Alongside the article dataset, we supply three additional datasets: a directory of local media web domains, one of UK Local Authority Districts, and one of digital local media providers, providing statistics on the coverage scope of UKTwitNewsCor. Our contributions enable comprehensive, longitudinal analysis of UK local media, news trends, and content diversity across multiple platforms and geographic areas. In this paper, we describe the data collection methodology, assess the dataset geographic and media ownership diversity, and outline how researchers, policymakers, and industry stakeholders can leverage UKTwitNewsCor to advance the study of local media.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindVote: How LLMs Predict Human Decision-Making in Social Media Polls</title>
<link>https://arxiv.org/abs/2505.14422</link>
<guid>https://arxiv.org/abs/2505.14422</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models, MindVote, social media polling, decision-making

Summary: 
The article introduces MindVote, a new benchmark for evaluating the ability of Large Language Models (LLMs) to predict human decision-making in dynamic social contexts. It comprises 276 poll instances from three platforms, features bilingual content, and covers five domains. The evaluation of 18 LLMs shows top models achieving a 0.74 overall score, 80% better than traditional baselines. The analysis uncovers disparities related to platform, language, and domain. Strategies to optimize LLM performance and assess reasoning in societal contexts are presented. The article also discusses temperature controls reflecting human thinking diversity. MindVote offers a scalable framework to evaluate LLMs' social intelligence, with implications for understanding behavioral decision-making. Code and data will be available soon. 

Summary: <div>
arXiv:2505.14422v1 Announce Type: new 
Abstract: The increasing complexity of Large Language Models (LLMs) necessitates new benchmarks to assess their ability to predict human decision-making in dynamic social contexts. We introduce MindVote, the first benchmark for evaluating LLMs as "virtual respondents" in social media polling. MindVote comprises 276 poll instances with 1,142 data entry points from three platforms (Weibo, Reddit, Fizz), features bilingual content (Chinese/English), and covers five domains. Our evaluation of 18 LLMs demonstrates that top-performing models achieve an overall score of 0.74, an 80% relative improvement over traditional baselines, and then we analyze LLM world model bias with human preferences across societal bias dimensions. MindVote also uncovers significant disparities related to platform, language, and domain. We present strategies to optimize LLM performance and use LLM-as-a-Judge to assess reasoning in societal contexts. Furthermore, we show that temperature controls can reflect a way of human thinking diversity and opinion shifts in polling. In summary, MindVote offers a scalable framework for evaluating LLMs' social intelligence, with implications for understanding behavioral decision-making. Code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Evaluation of Graph-based News Detection Using Network Structural Information</title>
<link>https://arxiv.org/abs/2505.14453</link>
<guid>https://arxiv.org/abs/2505.14453</guid>
<content:encoded><![CDATA[
<div> vulnerability, adversarial attacks, graph neural networks, fake news detection, robustness evaluation
Summary:<br /><br />
- The study focuses on the vulnerability of Graph Neural Networks (GNNs) in fake news detection, particularly concerning adversarial manipulations within social networks.
- Existing methods fail to consider the structural relationships surrounding target news, thus limiting their effectiveness in assessing detection robustness.
- The proposed SI2AF framework introduces structural entropy to quantify social engagements and identify hierarchical communities, enabling the design of multiple agents to optimize evasion against black-box detectors.
- Three attack strategies are developed for each target news through multi-agent collaboration within the associated subgraph, resulting in improved attack effectiveness.
- SI2AF significantly outperforms existing baselines and enhances GNN-based detection robustness by 41.54% on average. <div>
arXiv:2505.14453v1 Announce Type: new 
Abstract: Although Graph Neural Networks (GNNs) have shown promising potential in fake news detection, they remain highly vulnerable to adversarial manipulations within social networks. Existing methods primarily establish connections between malicious accounts and individual target news to investigate the vulnerability of graph-based detectors, while they neglect the structural relationships surrounding targets, limiting their effectiveness in robustness evaluation. In this work, we propose a novel Structural Information principles-guided Adversarial Attack Framework, namely SI2AF, which effectively challenges graph-based detectors and further probes their detection robustness. Specifically, structural entropy is introduced to quantify the dynamic uncertainty in social engagements and identify hierarchical communities that encompass all user accounts and news posts. An influence metric is presented to measure each account's probability of engaging in random interactions, facilitating the design of multiple agents that manage distinct malicious accounts. For each target news, three attack strategies are developed through multi-agent collaboration within the associated subgraph to optimize evasion against black-box detectors. By incorporating the adversarial manipulations generated by SI2AF, we enrich the original network structure and refine graph-based detectors to improve their robustness against adversarial attacks. Extensive evaluations demonstrate that SI2AF significantly outperforms state-of-the-art baselines in attack effectiveness with an average improvement of 16.71%, and enhances GNN-based detection robustness by 41.54% on average.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counter-Inferential Behavior in Natural and Artificial Cognitive Systems</title>
<link>https://arxiv.org/abs/2505.13551</link>
<guid>https://arxiv.org/abs/2505.13551</guid>
<content:encoded><![CDATA[
<div> counter-inferential behavior, cognitive systems, epistemic rigidity, maladaptive stability, cognitive vulnerability
<br />
Summary:<br />
This study examines the emergence of counter-inferential behavior in natural and artificial cognitive systems. The researchers identify three scenarios where such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. These behaviors do not stem from noise or flawed design but from structured interactions between internal information models, feedback, and evaluation mechanisms. The study draws on evidence from artificial systems, biological cognition, human psychology, and social dynamics to highlight counter-inferential behavior as a general cognitive vulnerability. The findings stress the importance of maintaining adaptive activation under stable conditions and propose design principles for cognitive architectures to resist rigidity during informational stress.
 <div>
arXiv:2505.13551v1 Announce Type: cross 
Abstract: This study explores the emergence of counter-inferential behavior in natural and artificial cognitive systems, that is, patterns in which agents misattribute empirical success or suppress adaptation, leading to epistemic rigidity or maladaptive stability. We analyze archetypal scenarios in which such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than arising from noise or flawed design, these behaviors emerge through structured interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, we identify counter-inferential behavior as a general cognitive vulnerability that can manifest even in otherwise well-adapted systems. The findings highlight the importance of preserving minimal adaptive activation under stable conditions and suggest design principles for cognitive architectures that can resist rigidity under informational stress.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods</title>
<link>https://arxiv.org/abs/2311.00721</link>
<guid>https://arxiv.org/abs/2311.00721</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Empathy Detection, Affective Computing, Datasets, Network Architecture

Summary:
Empathy detection through Machine Learning has gained attention across various disciplines. A systematic literature review identified key task formulations such as localised utterances, overall expressions, unidirectional or parallel empathy, and emotional contagion in different interaction scenarios. Empathy detection methods were categorized based on input modalities including text, audiovisual, audio, and physiological signals, with specific network architecture design protocols outlined for each modality. Challenges and research gaps were discussed, highlighting the need for further exploration in the Affective Computing-based empathy domain. The paper also emphasized the potential applications of empathy detection in society, healthcare, and education, underscoring the importance of enhancing human well-being through robust empathy detection systems. Additionally, the availability of datasets and codes was mentioned to facilitate future research in this area. 

<br /><br />Summary: <div>
arXiv:2311.00721v4 Announce Type: replace-cross 
Abstract: Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science, and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 849 papers from 10 well-known academic databases, systematically screened them and analysed the final 82 papers. Our analyses reveal several prominent task formulations - including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion - in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities - text, audiovisual, audio and physiological signals - thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. This paper, therefore, provides a structured overview of recent advancements and remaining challenges towards developing a robust empathy detection system that could meaningfully contribute to enhancing human well-being.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants</title>
<link>https://arxiv.org/abs/2409.06998</link>
<guid>https://arxiv.org/abs/2409.06998</guid>
<content:encoded><![CDATA[
<div> Keywords: heterophilous graphs, graph neural networks, GNN depth, generalization patterns, Mixture of scope experts <br />
Summary:  
Heterophilous graphs, where dissimilar nodes tend to connect, present a challenge for graph neural networks (GNNs). Increasing the depth of GNNs can expand the receptive field, potentially capturing homophily from higher-order neighborhoods. However, deeper GNNs often face performance degradation as depth increases. State-of-the-art deeper GNN models show only marginal improvements compared to shallow variants, indicating a shift in generalization preferences across nodes of varying homophily levels as depth increases. This disparity in generalization patterns motivates the proposal of Mixture of scope experts at test (Moscat) to enhance deeper GNN generalization while maintaining high expressivity. Experimental results demonstrate that Moscat significantly improves accuracy across a variety of datasets when combined with different GNN architectures. The code for Moscat is openly available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2409.06998v3 Announce Type: replace-cross 
Abstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs). Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, GNNs suffer from performance degradation as depth increases. Despite having better expressivity, state-of-the-art deeper GNNs achieve only marginal improvements compared to their shallow variants. Through theoretical and empirical analysis, we systematically demonstrate a shift in GNN generalization preferences across nodes with different homophily levels as depth increases. This creates a disparity in generalization patterns between GNN models with varying depth. Based on these findings, we propose to improve deeper GNN generalization while maintaining high expressivity by Mixture of scope experts at test (Moscat). Experimental results show that Moscat works flexibly with various GNNs across a wide range of datasets while significantly improving accuracy. Our code is available at (https://github.com/Hydrapse/moscat).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit</title>
<link>https://arxiv.org/abs/2410.13036</link>
<guid>https://arxiv.org/abs/2410.13036</guid>
<content:encoded><![CDATA[
<div> values, norms, online communities, upvotes, Reddit

Summary:
- The study focuses on norm-setting in online communities, examining values expressed in highly-upvoted comments on Reddit.
- It highlights the challenge of automated detection of desirable behavior and the limitations of current prosociality measures.
- Upvotes are used as a proxy for desirability, with values extracted from comments across 80 sub-communities on Reddit over two years.
- A large language model is utilized to identify 64 and 72 values in 2016 and 2022 respectively.
- The study demonstrates that existing computational models often fail to capture the values extracted, revealing the need for nuanced models beyond traditional prosocial measures. This research contributes to understanding community values and offers a framework for large-scale content analysis in online spaces. 

<br /><br />Summary: <div>
arXiv:2410.13036v3 Announce Type: replace-cross 
Abstract: A major task for moderators of online spaces is norm-setting, essentially creating shared norms for user behavior in their communities. Platform design principles emphasize the importance of highlighting norm-adhering examples and explicitly stating community norms. However, norms and values vary between communities and go beyond content-level attributes, making it challenging for platforms and researchers to provide automated ways to identify desirable behavior to be highlighted. Current automated approaches to detect desirability are limited to measures of prosocial behavior, but we do not know whether these measures fully capture the spectrum of what communities value. In this paper, we use upvotes, which express community approval, as a proxy for desirability and examine 16,000 highly-upvoted comments across 80 popular sub-communities on Reddit. Using a large language model, we extract values from these comments across two years (2016 and 2022) and compile 64 and 72 $\textit{macro}$, $\textit{meso}$, and $\textit{micro}$ values for 2016 and 2022 respectively, based on their frequency across communities. Furthermore, we find that existing computational models for measuring prosociality were inadequate to capture on average $82\%$ of the values we extracted. Finally, we show that our approach can not only extract most of the qualitatively-identified values from prior taxonomies, but also uncover new values that are actually encouraged in practice. Our findings highlight the need for nuanced models of desirability that go beyond preexisting prosocial measures. This work has implications for improving moderator understanding of their community values and provides a framework that can supplement qualitative approaches with larger-scale content analyses.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Do LLMs Help With Node Classification? A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2502.00829</link>
<guid>https://arxiv.org/abs/2502.00829</guid>
<content:encoded><![CDATA[
<div> LLMNodeBed, node classification, Large Language Models, graph analysis, algorithm comparison <br />
Summary:
- Node classification is a crucial task in graph analysis, now being tackled by Large Language Models (LLMs).
- LLMNodeBed, a tool for node classification using LLMs, was developed with datasets, algorithms, and learning paradigms.
- Extensive experiments with over 2,700 models revealed key performance factors like learning paradigms and homophily.
- LLM-based methods show significant improvements over traditional ones in semi-supervised settings.
- Graph Foundation Models outperform open-source LLMs but lag behind top LLMs like GPT-4o in zero-shot scenarios. 

Summary: <div>
arXiv:2502.00829v2 Announce Type: replace-cross 
Abstract: Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application. In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based algorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we conducted extensive experiments, training and evaluating over 2,700 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size and prompt) that affect performance. Our findings uncover 8 insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field. Codes and datasets are released at \href{https://llmnodebed.github.io/}{\texttt{https://llmnodebed.github.io/}}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusions of Intimacy: Emotional Attachment and Emerging Psychological Risks in Human-AI Relationships</title>
<link>https://arxiv.org/abs/2505.11649</link>
<guid>https://arxiv.org/abs/2505.11649</guid>
<content:encoded><![CDATA[
<div> social chatbots, emotional dynamics, human-AI relationships, emotional mirroring, toxic relationship patterns
<br />
Summary: 
This study analyzes over 30K user-shared conversations with social chatbots to explore the emotional dynamics of human-AI relationships. The findings reveal patterns of emotional mirroring and synchrony resembling human emotional connections. Users, predominantly young males with maladaptive coping styles, engage in parasocial interactions ranging from affectionate to abusive. Despite this, chatbots consistently respond in emotionally affirming ways. Some interactions mimic toxic relationship patterns, including emotional manipulation and self-harm. The study underscores the importance of implementing guardrails, ethical design principles, and public education to protect the authenticity of emotional connections in the era of artificial companionship. 
<br /><br /> <div>
arXiv:2505.11649v1 Announce Type: new 
Abstract: Emotionally responsive social chatbots, such as those produced by Replika and Character.AI, increasingly serve as companions that offer empathy, support, and entertainment. While these systems appear to meet fundamental human needs for connection, they raise concerns about how artificial intimacy affects emotional regulation, well-being, and social norms. Prior research has focused on user perceptions or clinical contexts but lacks large-scale, real-world analysis of how these interactions unfold. This paper addresses that gap by analyzing over 30K user-shared conversations with social chatbots to examine the emotional dynamics of human-AI relationships. Using computational methods, we identify patterns of emotional mirroring and synchrony that closely resemble how people build emotional connections. Our findings show that users-often young, male, and prone to maladaptive coping styles-engage in parasocial interactions that range from affectionate to abusive. Chatbots consistently respond in emotionally consistent and affirming ways. In some cases, these dynamics resemble toxic relationship patterns, including emotional manipulation and self-harm. These findings highlight the need for guardrails, ethical design, and public education to preserve the integrity of emotional connection in an age of artificial companionship.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory-Integrated Accessibility Analysis of Public Electric Vehicle Charging Stations</title>
<link>https://arxiv.org/abs/2505.12145</link>
<guid>https://arxiv.org/abs/2505.12145</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric vehicle charging infrastructure, accessibility metrics, San Francisco Bay Area, trajectory data, spatial disparities

Summary: 
The study introduces a new accessibility metric, TI-acs, to assess public EVCS accessibility in the San Francisco Bay Area based on individual trajectory data. Currently, Bay Area residents have an average of 7.5 hours of access per day to public L2 chargers and 5.2 hours to DCFC chargers. Despite overall improvements in accessibility over the past decade, spatial and racial disparities persist. Gini indices for accessibility across census tracts indicate significant disparities, with racial disparities linked to variations in charging infrastructure and mobility patterns. The study highlights the importance of considering charging infrastructure near workplaces and during off-peak periods for equitable transportation electrification. 

Summary: <div>
arXiv:2505.12145v1 Announce Type: new 
Abstract: Electric vehicle (EV) charging infrastructure is crucial for advancing EV adoption, managing charging loads, and ensuring equitable transportation electrification. However, there remains a notable gap in comprehensive accessibility metrics that integrate the mobility of the users. This study introduces a novel accessibility metric, termed Trajectory-Integrated Public EVCS Accessibility (TI-acs), and uses it to assess public electric vehicle charging station (EVCS) accessibility for approximately 6 million residents in the San Francisco Bay Area based on detailed individual trajectory data in one week. Unlike conventional home-based metrics, TI-acs incorporates the accessibility of EVCS along individuals' travel trajectories, bringing insights on more public charging contexts, including public charging near workplaces and charging during grid off-peak periods.
  As of June 2024, given the current public EVCS network, Bay Area residents have, on average, 7.5 hours and 5.2 hours of access per day during which their stay locations are within 1 km (i.e. 10-12 min walking) of a public L2 and DCFC charging port, respectively. Over the past decade, TI-acs has steadily increased from the rapid expansion of the EV market and charging infrastructure. However, spatial disparities remain significant, as reflected in Gini indices of 0.38 (L2) and 0.44 (DCFC) across census tracts. Additionally, our analysis reveals racial disparities in TI-acs, driven not only by variations in charging infrastructure near residential areas but also by differences in their mobility patterns.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection of hypergraphs by Ricci flow</title>
<link>https://arxiv.org/abs/2505.12276</link>
<guid>https://arxiv.org/abs/2505.12276</guid>
<content:encoded><![CDATA[
<div> Community detection, hypergraphs, Ricci flow, module identification, higher-order interactions<br />
Summary:<br />
The article introduces a new approach called HyperRCD for community detection in hypergraphs. The method is based on a hypergraph Ricci flow that considers higher-order interactions among nodes. The flow operates by deforming hyperedge weights through curvature-driven evolution, capturing the weighted hyperedges' significance in mediating higher-order interactions. The study proves the long-time existence of the flow, providing a solid theoretical foundation. Experimental results on synthetic and real-world hypergraphs showcase HyperRCD's superior robustness to topological variations and competitive performance across diverse datasets. Overall, HyperRCD offers an effective mathematical representation of community structures in hypergraphs, demonstrating its potential for functional module identification in complex systems. <br /><br /> <div>
arXiv:2505.12276v1 Announce Type: new 
Abstract: Community detection in hypergraphs is both instrumental for functional module identification and intricate due to higher-order interactions among nodes. We define a hypergraph Ricci flow that directly operates on higher-order interactions of hypergraphs and prove long-time existence of the flow. Building on this theoretical foundation, we develop HyperRCD-a Ricci-flow-based community detection approach that deforms hyperedge weights through curvature-driven evolution, which provides an effective mathematical representation of higher-order interactions mediated by weighted hyperedges between nodes. Extensive experiments on both synthetic and real-world hypergraphs demonstrate that HyperRCD exhibits remarkable enhanced robustness to topological variations and competitive performance across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIS Epidemic Modelling on Homogeneous Networked System: General Recovering Process and Mean-Field Perspective</title>
<link>https://arxiv.org/abs/2505.12290</link>
<guid>https://arxiv.org/abs/2505.12290</guid>
<content:encoded><![CDATA[
<div> recovery time distributions, disease spread, SIS model, heterogeneous network, mean-field equations <br />
<br />Summary: 
The study introduces the general recovering process SIS (grp-SIS) model as an extension of the classic susceptible-infected-susceptible (SIS) model to incorporate arbitrary recovery time distributions for infected nodes in complex systems. The mean-field equations are derived for a homogeneous network and specific recovery time distributions, highlighting the impact of recovery time distributions on disease dynamics. The probability density function (PDF) for infection times in the steady state is investigated, emphasizing the significant influence of recovery time distributions on disease spread. The study suggests future research directions, including extending the model to arbitrary infection processes and utilizing the quasistationary method to address numerical deviations in results. <div>
arXiv:2505.12290v1 Announce Type: new 
Abstract: Although we have made progress in understanding disease spread in complex systems with non-Poissonian activity patterns, current models still fail to capture the full range of recovery time distributions. In this paper, we propose an extension of the classic susceptible-infected-susceptible (SIS) model, called the general recovering process SIS (grp-SIS) model. This model incorporates arbitrary recovery time distributions for infected nodes within the system. We derive the mean-field equations assuming a homogeneous network, provide solutions for specific recovery time distributions, and investigate the probability density function (PDF) for infection times in the system's steady state. Our findings show that recovery time distributions significantly affect disease dynamics, and we suggest several future research directions, including extending the model to arbitrary infection processes and using the quasistationary method to address deviations in numerical results.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Prompt-driven Community Search</title>
<link>https://arxiv.org/abs/2505.12304</link>
<guid>https://arxiv.org/abs/2505.12304</guid>
<content:encoded><![CDATA[
<div> Community detection, semi-supervised, graph neural networks, prompt-driven, efficiency <br />
Summary: 
The paper introduces Pre-trained Prompt-driven Community Search (PPCS), a novel model for semi-supervised community search that leverages the "pre-train, prompt" paradigm. By adopting this paradigm, PPCS aims to enhance search accuracy and efficiency for identifying the community of a given node in a graph. The model consists of three main components: node encoding using graph neural networks, sample generation to identify initial communities and select training samples, and prompt-driven fine-tuning for final community prediction. Experimental results on real-world datasets show that PPCS outperforms baseline algorithms in terms of accuracy and efficiency. Ablation studies confirm the effectiveness of each component in improving the overall performance of the community search model. <div>
arXiv:2505.12304v1 Announce Type: new 
Abstract: The "pre-train, prompt" paradigm is widely adopted in various graph-based tasks and has shown promising performance in community detection. Most existing semi-supervised community detection algorithms detect communities based on known ones, and the detected communities typically do not contain the given query node. Therefore, they are not suitable for searching the community of a given node. Motivated by this, we adopt this paradigm into the semi-supervised community search for the first time and propose Pre-trained Prompt-driven Community Search (PPCS), a novel model designed to enhance search accuracy and efficiency. PPCS consists of three main components: node encoding, sample generation, and prompt-driven fine-tuning. Specifically, the node encoding component employs graph neural networks to learn local structural patterns of nodes in a graph, thereby obtaining representations for nodes and communities. Next, the sample generation component identifies an initial community for a given node and selects known communities that are structurally similar to the initial one as training samples. Finally, the prompt-driven fine-tuning component leverages these samples as prompts to guide the final community prediction. Experimental results on five real-world datasets demonstrate that PPCS performs better than baseline algorithms. It also achieves higher community search efficiency than semi-supervised community search baseline methods, with ablation studies verifying the effectiveness of each component of PPCS.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Search in Time-dependent Road-social Attributed Networks</title>
<link>https://arxiv.org/abs/2505.12309</link>
<guid>https://arxiv.org/abs/2505.12309</guid>
<content:encoded><![CDATA[
<div> keywords: real-world networks, cohesive subgraph, community search, semantic similarity, k-core

Summary:
The study addresses the limitations of existing community search algorithms by proposing a method to discover semantic-spatial aware k-cores in attributed networks. These k-cores have high semantic and time-dependent spatial cohesiveness and include the query node. Two algorithms, exact and greedy, are introduced that expand outward from the query node locally rather than traversing the entire network. A method to calculate semantic similarity using large language models is also devised to improve keyword matching accuracy. Experimental results show that the greedy algorithm outperforms existing methods in terms of structural, semantic, and time-dependent spatial cohesiveness. <div>
arXiv:2505.12309v1 Announce Type: new 
Abstract: Real-world networks often involve both keywords and locations, along with travel time variations between locations due to traffic conditions. However, most existing cohesive subgraph-based community search studies utilize a single attribute, either keywords or locations, to identify communities. They do not simultaneously consider both keywords and locations, which results in low semantic or spatial cohesiveness of the detected communities, and they fail to account for variations in travel time. Additionally, these studies traverse the entire network to build efficient indexes, but the detected community only involves nodes around the query node, leading to the traversal of nodes that are not relevant to the community. Therefore, we propose the problem of discovering semantic-spatial aware k-core, which refers to a k-core with high semantic and time-dependent spatial cohesiveness containing the query node. To address this problem, we propose an exact and a greedy algorithm, both of which gradually expand outward from the query node. They are local methods that only access the local part of the attributed network near the query node rather than the entire network. Moreover, we design a method to calculate the semantic similarity between two keywords using large language models. This method alleviates the disadvantages of keyword-matching methods used in existing community search studies, such as mismatches caused by differently expressed synonyms and the presence of irrelevant words. Experimental results show that the greedy algorithm outperforms baselines in terms of structural, semantic, and time-dependent spatial cohesiveness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework of Voting Prediction of Parliament Members</title>
<link>https://arxiv.org/abs/2505.12535</link>
<guid>https://arxiv.org/abs/2505.12535</guid>
<content:encoded><![CDATA[
<div> framework, parliamentary voting, prediction, machine learning, data analysis  
Summary:  
- The study focuses on the development of a Voting Prediction Framework (VPF) to predict parliamentary voting outcomes and improve government transparency.  
- VPF utilizes data collection, parsing, feature integration, and prediction models to forecast individual legislator votes and overall bill outcomes.  
- The framework analyzes voting records from multiple countries and achieves high precision and accuracy in predicting votes and bill outcomes.  
- VPF has the potential to simplify legislative work, refine proposed legislation, and enhance public access to decision-making processes.  
<br /><br />Summary: <div>
arXiv:2505.12535v1 Announce Type: new 
Abstract: Keeping track of how lawmakers vote is essential for government transparency. While many parliamentary voting records are available online, they are often difficult to interpret, making it challenging to understand legislative behavior across parliaments and predict voting outcomes. Accurate prediction of votes has several potential benefits, from simplifying parliamentary work by filtering out bills with a low chance of passing to refining proposed legislation to increase its likelihood of approval. In this study, we leverage advanced machine learning and data analysis techniques to develop a comprehensive framework for predicting parliamentary voting outcomes across multiple legislatures. We introduce the Voting Prediction Framework (VPF) - a data-driven framework designed to forecast parliamentary voting outcomes at the individual legislator level and for entire bills. VPF consists of three key components: (1) Data Collection - gathering parliamentary voting records from multiple countries using APIs, web crawlers, and structured databases; (2) Parsing and Feature Integration - processing and enriching the data with meaningful features, such as legislator seniority, and content-based characteristics of a given bill; and (3) Prediction Models - using machine learning to forecast how each parliament member will vote and whether a bill is likely to pass. The framework will be open source, enabling anyone to use or modify the framework. To evaluate VPF, we analyzed over 5 million voting records from five countries - Canada, Israel, Tunisia, the United Kingdom and the USA. Our results show that VPF achieves up to 85% precision in predicting individual votes and up to 84% accuracy in predicting overall bill outcomes. These findings highlight VPF's potential as a valuable tool for political analysis, policy research, and enhancing public access to legislative decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion</title>
<link>https://arxiv.org/abs/2505.12894</link>
<guid>https://arxiv.org/abs/2505.12894</guid>
<content:encoded><![CDATA[
<div> Hypergraphs; social networks; rumor propagation; source detection; interactive relationship construction; feature-rich attention fusion <br />
Summary: <br />
This study introduces a novel approach, HyperDet, for detecting rumor sources in hypergraphs, which are better at capturing group phenomena in social networks. The approach combines an Interactive Relationship Construction module to model static and dynamic interactions among users and a Feature-rich Attention Fusion module to autonomously learn node features and discern between nodes using a self-attention mechanism. This allows for accurate learning of node representations in the context of higher-order relationships. Experimental results demonstrate the effectiveness of HyperDet, surpassing current state-of-the-art methods. <div>
arXiv:2505.12894v1 Announce Type: new 
Abstract: Hypergraphs offer superior modeling capabilities for social networks, particularly in capturing group phenomena that extend beyond pairwise interactions in rumor propagation. Existing approaches in rumor source detection predominantly focus on dyadic interactions, which inadequately address the complexity of more intricate relational structures. In this study, we present a novel approach for Source Detection in Hypergraphs (HyperDet) via Interactive Relationship Construction and Feature-rich Attention Fusion. Specifically, our methodology employs an Interactive Relationship Construction module to accurately model both the static topology and dynamic interactions among users, followed by the Feature-rich Attention Fusion module, which autonomously learns node features and discriminates between nodes using a self-attention mechanism, thereby effectively learning node representations under the framework of accurately modeled higher-order relationships. Extensive experimental validation confirms the efficacy of our HyperDet approach, showcasing its superiority relative to current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs</title>
<link>https://arxiv.org/abs/2505.12910</link>
<guid>https://arxiv.org/abs/2505.12910</guid>
<content:encoded><![CDATA[
<div> Keywords: Source detection, graphs, rumor propagation, state space model, hypergraphs

Summary:
Source detection on graphs for identifying rumor origins is crucial, but existing machine learning methods often lack the ability to capture the underlying dynamics of rumor propagation. In response to this challenge, the study introduces SourceDetMamba, a novel approach that leverages a Graph-aware State Space Model for Source Detection in Sequential Hypergraphs. By using hypergraphs to model high-order interactions in social networks and employing the Mamba state space model known for its robust global modeling capabilities, the proposed method effectively infers underlying propagation dynamics. Furthermore, SourceDetMamba introduces a graph-aware state update mechanism that combines temporal dependencies and topological context to refine the state of each node as temporal network snapshots are sequentially fed into the model. Extensive evaluations across multiple datasets demonstrate the superior performance of SourceDetMamba compared to current state-of-the-art approaches. 

<br /><br />Summary: <div>
arXiv:2505.12910v1 Announce Type: new 
Abstract: Source detection on graphs has demonstrated high efficacy in identifying rumor origins. Despite advances in machine learning-based methods, many fail to capture intrinsic dynamics of rumor propagation. In this work, we present SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs, which harnesses the recent success of the state space model Mamba, known for its superior global modeling capabilities and computational efficiency, to address this challenge. Specifically, we first employ hypergraphs to model high-order interactions within social networks. Subsequently, temporal network snapshots generated during the propagation process are sequentially fed in reverse order into Mamba to infer underlying propagation dynamics. Finally, to empower the sequential model to effectively capture propagation patterns while integrating structural information, we propose a novel graph-aware state update mechanism, wherein the state of each node is propagated and refined by both temporal dependencies and topological context. Extensive evaluations on eight datasets demonstrate that SourceDetMamba consistently outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counting Graphlets of Size $k$ under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2505.12954</link>
<guid>https://arxiv.org/abs/2505.12954</guid>
<content:encoded><![CDATA[
<div> Algorithm, Graphlets, Local Differential Privacy, Expected $\ell_2$ Error, Non-interactive<br />
<br />
Summary:<br />
The paper addresses the challenge of counting graphlets under local differential privacy, focusing on graphlets of any size rather than just small ones like triangles or $k$-stars. A non-interactive algorithm is proposed, achieving an expected $\ell_2$ error of $O(n^{k - 1})$, with the optimality demonstrated for a class of input graphs and graphlets. The study establishes that the expected $\ell_2$ error for any non-interactive counting algorithm on certain input graphs and graphlets is $\Omega(n^{k - 1})$, emphasizing the effectiveness of the proposed algorithm. Furthermore, it is proved that for specific input graphs and graphlets, any locally differentially private algorithm must have an expected $\ell_2$ error of $\Omega(n^{k - 1.5}). Experimental results indicate that the algorithm outperforms the classical randomized response method in terms of accuracy. <br /> <div>
arXiv:2505.12954v1 Announce Type: new 
Abstract: The problem of counting subgraphs or graphlets under local differential privacy is an important challenge that has attracted significant attention from researchers. However, much of the existing work focuses on small graphlets like triangles or $k$-stars. In this paper, we propose a non-interactive, locally differentially private algorithm capable of counting graphlets of any size $k$. When $n$ is the number of nodes in the input graph, we show that the expected $\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that there exists a class of input graphs and graphlets of size $k$ for which any non-interactive counting algorithm incurs an expected $\ell_2$ error of $\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore, we establish that for certain input graphs and graphlets, any locally differentially private algorithm must have an expected $\ell_2$ error of $\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more accurate than the classical randomized response method.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Social Influence with Networked Synthetic Control</title>
<link>https://arxiv.org/abs/2505.13334</link>
<guid>https://arxiv.org/abs/2505.13334</guid>
<content:encoded><![CDATA[
<div> machine learning, social influence, network science, social value, political behavior 

Summary: 
The study presents a novel approach to measuring social influence using a combination of machine learning-based modeling and network science. Social value, a new measure for social influence, diverges from traditional centrality measures by incorporating an external regressor to predict an output variable, creating a synthetic control, and distributing individual contribution based on a social network. Theoretical derivations illustrate the properties of social value under different network structures, including lattice, power-law, and random graphs. The study also highlights the potential for computational efficiency in ensemble models. Simulation results demonstrate the generalized friendship paradox, showing that in certain scenarios, one's friends may have more influence on average than oneself. <div>
arXiv:2505.13334v1 Announce Type: new 
Abstract: Measuring social influence is difficult due to the lack of counter-factuals and comparisons. By combining machine learning-based modeling and network science, we present general properties of social value, a recent measure for social influence using synthetic control applicable to political behavior. Social value diverges from centrality measures on in that it relies on an external regressor to predict an output variable of interest, generates a synthetic measure of influence, then distributes individual contribution based on a social network. Through theoretical derivations, we show the properties of SV under linear regression with and without interaction, across lattice networks, power-law networks, and random graphs. A reduction in computation can be achieved for any ensemble model. Through simulation, we find that the generalized friendship paradox holds -- that in certain situations, your friends have on average more influence than you do.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A large-scale analysis of public-facing, community-built chatbots on Character.AI</title>
<link>https://arxiv.org/abs/2505.13354</link>
<guid>https://arxiv.org/abs/2505.13354</guid>
<content:encoded><![CDATA[
<div> chatbots, Character.AI, generative AI, user-generated content, online interaction

Summary: This paper presents a large-scale analysis of public-facing chatbots on the social media platform Character.AI, which combines generative AI with user-generated content. The site has over 20 million monthly users and has garnered attention for youth engagement issues. The study analyzes 2.1 million English-language prompts created by approximately 1 million users to explore fandom prevalence, recurring tropes, and power dynamics within gendered greetings. The findings highlight the unique intersection of generative AI and user-generated content, showcasing an emerging form of online social interaction. <div>
arXiv:2505.13354v1 Announce Type: new 
Abstract: This paper presents the first large-scale analysis of public-facing chatbots on Character.AI, a rapidly growing social media platform where users create and interact with chatbots. Character.AI is distinctive in that it merges generative AI with user-generated content, enabling users to build bots-often modeled after fictional or public personas-for others to engage with. It is also popular, with over 20 million monthly active users, and impactful, with recent headlines detailing significant issues with youth engagement on the site. Character.AI is thus of interest to study both substantively and conceptually. To this end, we present a descriptive overview of the site using a dataset of 2.1 million English-language prompts (or ``greetings'') for chatbots on the site, created by around 1 million users. Our work explores the prevalence of different fandoms on the site, broader tropes that persist across fandoms, and how dynamics of power intersect with gender within greetings. Overall, our findings illuminate an emerging form of online (para)social interaction that toes a unique and important intersection between generative AI and user-generated content.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Post-API Age: Studying Digital Platforms in Scant Data Access Times</title>
<link>https://arxiv.org/abs/2505.09877</link>
<guid>https://arxiv.org/abs/2505.09877</guid>
<content:encoded><![CDATA[
<div> transparency, digital platforms, data access, challenges, recommendations

Summary:
The study examines data access by researchers on digital platforms in the "post-post-API age" following the closure of major social media APIs. Researchers faced obstacles such as complex application processes, difficulty obtaining credentials, and limited API usability. These challenges have exacerbated existing inequities in data access. The study recommends actions for platforms, researchers, and policymakers to improve data access, highlighting the need for equitable and effective solutions. The findings emphasize the importance of fostering dialogue within the CSCW community to address these challenges and strive for interdisciplinary and multi-stakeholder approaches. <div>
arXiv:2505.09877v1 Announce Type: cross 
Abstract: Over the past decade, data provided by digital platforms has informed substantial research in HCI to understand online human interaction and communication. Following the closure of major social media APIs that previously provided free access to large-scale data (the "post-API age"), emerging data access programs required by the European Union's Digital Services Act (DSA) have sparked optimism about increased platform transparency and renewed opportunities for comprehensive research on digital platforms, leading to the "post-post-API age." However, it remains unclear whether platforms provide adequate data access in practice. To assess how platforms make data available under the DSA, we conducted a comprehensive survey followed by in-depth interviews with 19 researchers to understand their experiences with data access in this new era. Our findings reveal significant challenges in accessing social media data, with researchers facing multiple barriers including complex API application processes, difficulties obtaining credentials, and limited API usability. These challenges have exacerbated existing institutional, regional, and financial inequities in data access. Based on these insights, we provide actionable recommendations for platforms, researchers, and policymakers to foster more equitable and effective data access, while encouraging broader dialogue within the CSCW community around interdisciplinary and multi-stakeholder solutions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis and Resilience of the U.S. Flight Network</title>
<link>https://arxiv.org/abs/2505.11559</link>
<guid>https://arxiv.org/abs/2505.11559</guid>
<content:encoded><![CDATA[
<div> transportation, network analysis, efficiency, vulnerability, hubs

Summary:
The paper examines the U.S. Flight Network (USFN) using complex network theory to understand its efficiency and vulnerability. It analyzes the network's topology, including structural properties, degree distributions, and community structures. USFN follows a power-law distribution, indicating hub dominance. It has a higher clustering coefficient and modularity compared to null networks. Percolation tests reveal vulnerability to targeted attacks, with potential for complete cascading failure if major hubs fail. The study highlights the network's efficiency design but emphasizes its susceptibility to disruption. Protecting key hub airports is crucial for enhancing the network's robustness and preventing large-scale failures. 

<br /><br />Summary: <div>
arXiv:2505.11559v1 Announce Type: cross 
Abstract: Air travel is one of the most widely used transportation services in the United States. This paper analyzes the U.S. Flight Network (USFN) using complex network theory by exploring how the network's topology contributes to its efficiency and vulnerability. This is done by examining the structural properties, degree distributions, and community structures in the network. USFN was observed to follow power-law distribution and falls under the anomalous regime, suggesting that the network is hub dominant. Compared to null networks, USFN has a higher clustering coefficient and modularity. Various percolation test revealed that USFN is vulnerable to targeted attacks and is susceptible to complete cascading failure if one of the major hubs fails. The overall results suggest that while the USFN is designed for efficiency, it is highly vulnerable to disruptions. Protecting key hub airports is important to make the network more robust and prevent large-scale failures.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Blue Start: A large-scale pairwise and higher-order social network dataset</title>
<link>https://arxiv.org/abs/2505.11608</link>
<guid>https://arxiv.org/abs/2505.11608</guid>
<content:encoded><![CDATA[
<div> social networks, higher-order interactions, Bluesky platform, network dataset, group formation

Summary:
This article discusses the importance of understanding higher-order interactions in large-scale networks to study dynamics such as disease spread, information dissemination, and social influence. The authors introduce a new dataset called "A Blue Start" from the Bluesky social media platform, comprising 26.7 million users, 1.6 billion pairwise following relationships, and 301.3 thousand groups representing starter packs. Unlike traditional social networks, Bluesky includes user-curated lists known as starter packs, which serve as a mechanism for social network growth. The dataset is seen as a valuable resource for studying higher-order network science and bridging the gap between pairwise and higher-order network data. By highlighting the importance of group dynamics and providing a large-scale dataset, this research contributes to advancing our understanding of complex interactions within social networks. 

<br /><br />Summary: <div>
arXiv:2505.11608v1 Announce Type: cross 
Abstract: Large-scale networks have been instrumental in shaping the way that we think about how individuals interact with one another, developing key insights in mathematical epidemiology, computational social science, and biology. However, many of the underlying social systems through which diseases spread, information disseminates, and individuals interact are inherently mediated through groups of arbitrary size, known as higher-order interactions. There is a gap between higher-order dynamics of group formation and fragmentation, contagion spread, and social influence and the data necessary to validate these higher-order mechanisms. Similarly, few datasets bridge the gap between these pairwise and higher-order network data. Because of its open API, the Bluesky social media platform provides a laboratory for observing social ties at scale. In addition to pairwise following relationships, unlike many other social networks, Bluesky features user-curated lists known as "starter packs" as a mechanism for social network growth. We introduce "A Blue Start", a large-scale network dataset comprising 26.7M users and their 1.6B pairwise following relationships and 301.3K groups representing starter packs. This dataset will be an essential resource for the study of higher-order network science.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.12329</link>
<guid>https://arxiv.org/abs/2505.12329</guid>
<content:encoded><![CDATA[
<div> Efficient Rule Mining, Knowledge Graphs, Link Prediction, Deep Learning, Markov Chain <br />
<br />
Summary: The article introduces MPRM, a novel rule mining method for knowledge graphs that addresses memory and time challenges faced by deep learning-based methods. MPRM models rule-based inference as a Markov chain and utilizes an efficient confidence metric derived from aggregated path probabilities, reducing computational demands significantly. Experiment results demonstrate that MPRM efficiently mines large-scale knowledge graphs with over a million facts, sampling less than 1% of facts on a single CPU in 22 seconds. The method maintains interpretability and enhances inference accuracy by up to 11% compared to traditional approaches. <div>
arXiv:2505.12329v1 Announce Type: cross 
Abstract: Rule mining in knowledge graphs enables interpretable link prediction. However, deep learning-based rule mining methods face significant memory and time challenges for large-scale knowledge graphs, whereas traditional approaches, limited by rigid confidence metrics, incur high computational costs despite sampling techniques. To address these challenges, we propose MPRM, a novel rule mining method that models rule-based inference as a Markov chain and uses an efficient confidence metric derived from aggregated path probabilities, significantly lowering computational demands. Experiments on multiple datasets show that MPRM efficiently mines knowledge graphs with over a million facts, sampling less than 1% of facts on a single CPU in 22 seconds, while preserving interpretability and boosting inference accuracy by up to 11% over baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transmission Neural Networks: Approximation and Optimal Control</title>
<link>https://arxiv.org/abs/2505.12657</link>
<guid>https://arxiv.org/abs/2505.12657</guid>
<content:encoded><![CDATA[
<div> Transmission Neural Networks, virus spread models, Markovian Susceptible-Infected-Susceptible model, stochastic infection paths, Markov decision processes <br />
Summary: Transmission Neural Networks (TransNNs) link virus spread models and neural networks with adjustable activation functions. This study explains the approximation technique and assumptions of TransNNs in relation to the 2^n-state Markovian Susceptible-Infected-Susceptible (SIS) model on networks with stochastic infection paths. The conditional probability of infection in the 2^n-state SIS model is derived under mild assumptions, facilitating control strategies using Markov decision processes (MDP). A comparison between MDP control and optimal control with TransNNs shows that TransNNs offer computational efficiency in designing control measures for curbing virus spread through vaccination, albeit with more conservative actions. <div>
arXiv:2505.12657v1 Announce Type: cross 
Abstract: Transmission Neural Networks (TransNNs) introduced by Gao and Caines (2022) connect virus spread models over networks and neural networks with tuneable activation functions. This paper presents the approximation technique and the underlying assumptions employed by TransNNs in relation to the corresponding Markovian Susceptible-Infected-Susceptible (SIS) model with 2^n states, where n is the number of nodes in the network. The underlying infection paths are assumed to be stochastic with heterogeneous and time-varying transmission probabilities. We obtain the conditional probability of infection in the stochastic 2^n-state SIS epidemic model corresponding to each state configuration under mild assumptions, which enables control solutions based on Markov decision processes (MDP). Finally, MDP control with 2^n-state SIS epidemic models and optimal control with TransNNs are compared in terms of mitigating virus spread over networks through vaccination, and it is shown that TranNNs enable the generation of control laws with significant computational savings, albeit with more conservative control actions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Graph Learning, Graph Foundation Models, Decentralized Training, Knowledge Entanglement, Domain Generalization

Summary:
FedGFM is a novel decentralized training paradigm that integrates Federated Graph Learning and Graph Foundation Models to address challenges in multi-client collaboration and domain generalization. The proposed FedGFM+ framework includes two key modules: AncDAI for domain-aware initialization and AdaDPP for domain-sensitive prompts. AncDAI uses domain-specific prototypes to reduce knowledge entanglement, while AdaDPP enhances downstream adaptation with adaptive prompts. FedGFM+ outperforms 20 baselines on 8 benchmarks across various domains and tasks, demonstrating its effectiveness in improving performance in graph machine learning applications. <div>
arXiv:2505.12684v1 Announce Type: cross 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting</title>
<link>https://arxiv.org/abs/2505.12738</link>
<guid>https://arxiv.org/abs/2505.12738</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, Large Language Models (LLMs), spatio-temporal, autoregressive modeling, COVID-19 datasets 

Summary: 
EpiLLM is a novel framework that uses Large Language Models (LLMs) for spatio-temporal epidemic forecasting. It incorporates a dual-branch architecture to align epidemic patterns and language tokens, enabling fine-grained forecasting. By adopting an autoregressive modeling paradigm, EpiLLM transforms the forecasting task into next-token prediction, boosting accuracy and generalization. The framework also includes spatio-temporal prompt learning techniques to enhance data-driven forecasting capabilities. Extensive experiments on real-world COVID-19 datasets demonstrate that EpiLLM outperforms existing baselines and showcases the scaling behavior typical of LLMs. The results highlight the potential of using advanced language models for precise epidemic forecasting and its crucial role in informing effective public health strategies. 

<br /><br />Summary: <div>
arXiv:2505.12738v1 Announce Type: cross 
Abstract: Advanced epidemic forecasting is critical for enabling precision containment strategies, highlighting its strategic importance for public health security. While recent advances in Large Language Models (LLMs) have demonstrated effectiveness as foundation models for domain-specific tasks, their potential for epidemic forecasting remains largely unexplored. In this paper, we introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting. Considering the key factors in real-world epidemic transmission: infection cases and human mobility, we introduce a dual-branch architecture to achieve fine-grained token-level alignment between such complex epidemic patterns and language tokens for LLM adaptation. To unleash the multi-step forecasting and generalization potential of LLM architectures, we propose an autoregressive modeling paradigm that reformulates the epidemic forecasting task into next-token prediction. To further enhance LLM perception of epidemics, we introduce spatio-temporal prompt learning techniques, which strengthen forecasting capabilities from a data-driven perspective. Extensive experiments show that EpiLLM significantly outperforms existing baselines on real-world COVID-19 datasets and exhibits scaling behavior characteristic of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Place for Old Memes: Large-scale Collective Dynamics in the Three Iterations of Reddit r/place</title>
<link>https://arxiv.org/abs/2408.13236</link>
<guid>https://arxiv.org/abs/2408.13236</guid>
<content:encoded><![CDATA[
<div> Keywords: online communities, geopolitics, collective dynamics, Reddit, computational social science<br />
<br />
Summary: 
The article explores the concept of online communities resembling geopolitics, with communities being compared to nations formed around shared interests. Using the r/place experiment on Reddit as a case study, the study analyzes the collective behavior in terms of engagement, collaboration, and competition. The research reveals patterns such as group coordination costs, social loafing, and increased cooperation in response to competition. These findings contribute to understanding group decision-making processes and can aid in developing theoretical models and mechanisms to optimize collaborative-competitive processes in social networks. The analysis provides insights into the diverse interests and actions of millions of players on Reddit, shedding light on the complex dynamics of online communities and the interactions that shape their evolution. <div>
arXiv:2408.13236v2 Announce Type: replace 
Abstract: Is there something akin to geopolitics for online communities? One could think of communities as nations formed around shared interests of individual users. Friendly borders capture similar interests, but conflicts could emerge due to ideological differences or competition for attention (as for land). Over time, new coalitions could emerge, others could crumble, and many could disappear as casualties of online wars with highly unpredictable and often devastating outcomes. The r/place experiment is the most ingenious attempt at reproducing this complex collective dynamics as a series of three social games hosted by Reddit. The result is not only an accurate picture of the diverse interests on Reddit -- one of the most popular social media platforms in the world -- but also fine-grained traces of sequential actions taken by millions of players during the game. In this paper, we are the first to characterize the collective behavior during r/place in terms of engagement, collaboration, and competition using tools from computational social science and data science. Our analysis shows that r/place reflected many patterns found in other relevant group decision-making processes, including empirical evidence for group coordination costs, social loafing, and increased cooperation as a response to competition. We discuss how our findings can support the development of new theoretical models, tools, and mechanisms to optimize collaborative-competitive processes in social networks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conspiracy theories and where to find them on TikTok</title>
<link>https://arxiv.org/abs/2407.12545</link>
<guid>https://arxiv.org/abs/2407.12545</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, conspiracy theories, content moderation, Large Language Models, harmful content

Summary: 
The study focuses on analyzing conspiracy theories on TikTok, a popular social media platform, using a dataset of 1.5 million videos shared in the U.S. over three years. The research estimates the prevalence of conspiratorial videos on TikTok, observing up to 1000 new videos per month. The study also evaluates the impact of TikTok's Creativity Program on video content and duration, noting an overall increase in video length. Furthermore, the effectiveness of Large Language Models in detecting harmful content, including conspiracy theories, is assessed, with high precision levels (up to 96%) achieved. Despite their accuracy, the overall performance of these models is comparable to traditional models such as RoBERTa. The findings suggest that Large Language Models can be valuable tools in supporting content moderation strategies to mitigate the spread of harmful content on TikTok.<br /><br />Summary: <div>
arXiv:2407.12545v2 Announce Type: replace-cross 
Abstract: TikTok has skyrocketed in popularity over recent years, especially among younger audiences. However, there are public concerns about the potential of this platform to promote and amplify harmful content. This study presents the first systematic analysis of conspiracy theories on TikTok. By leveraging the official TikTok Research API we collect a longitudinal dataset of 1.5M videos shared in the U.S. over three years. We estimate a lower bound on the prevalence of conspiratorial videos (up to 1000 new videos per month) and evaluate the effects of TikTok's Creativity Program for monetization, observing an overall increase in video duration regardless of content. Lastly, we evaluate the capabilities of state-of-the-art open-weight Large Language Models to identify conspiracy theories from audio transcriptions of videos. While these models achieve high precision in detecting harmful content (up to 96%), their overall performance remains comparable to fine-tuned traditional models such as RoBERTa. Our findings suggest that Large Language Models can serve as an effective tool for supporting content moderation strategies aimed at reducing the spread of harmful content on TikTok.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinated Inauthentic Behavior on TikTok: Challenges and Opportunities for Detection in a Video-First Ecosystem</title>
<link>https://arxiv.org/abs/2505.10867</link>
<guid>https://arxiv.org/abs/2505.10867</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated inauthentic behavior, TikTok, network-based approach, content and interaction structures, detection framework

Summary: 
This study introduces a computational framework for detecting coordinated inauthentic behavior (CIB) on TikTok, focusing on unique content and interaction structures of the platform. Using network-based analysis, the researchers identified coordinated activities related to the 2024 U.S. Presidential Election. They found instances of synchronized amplification of political narratives and semi-automated content replication using AI-generated voiceovers and split-screen video formats. While traditional coordination indicators were effective on TikTok, signals based on textual similarity of video transcripts and specific interaction types like Duets and Stitches were found to be ineffective due to the platform's distinct content norms and mechanics. This work lays the groundwork for future research on influence operations in short-form video platforms. 

<br /><br />Summary: <div>
arXiv:2505.10867v1 Announce Type: new 
Abstract: Detecting coordinated inauthentic behavior (CIB) is central to the study of online influence operations. However, most methods focus on text-centric platforms, leaving video-first ecosystems like TikTok largely unexplored. To address this gap, we develop and evaluate a computational framework for detecting CIB on TikTok, leveraging a network-based approach adapted to the platform's unique content and interaction structures. Building on existing approaches, we construct user similarity networks based on shared behaviors, including synchronized posting, repeated use of similar captions, multimedia content reuse, and hashtag sequence overlap, and apply graph pruning techniques to identify dense networks of likely coordinated accounts. Analyzing a dataset of 793K TikTok videos related to the 2024 U.S. Presidential Election, we uncover a range of coordinated activities, from synchronized amplification of political narratives to semi-automated content replication using AI-generated voiceovers and split-screen video formats. Our findings show that while traditional coordination indicators generalize well to TikTok, other signals, such as those based on textual similarity of video transcripts or Duet and Stitch interactions, prove ineffective, highlighting the platform's distinct content norms and interaction mechanics. This work provides the first empirical foundation for studying and detecting CIB on TikTok, paving the way for future research into influence operations in short-form video platforms.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Young Users on Social Media: Evaluating the Effectiveness of Content Moderation and Legal Safeguards on Video Sharing Platforms</title>
<link>https://arxiv.org/abs/2505.11160</link>
<guid>https://arxiv.org/abs/2505.11160</guid>
<content:encoded><![CDATA[
<div> content moderation, video-sharing platforms, harmful content, underage users, verification methods 
Summary:<br />
- The study evaluated video moderation effectiveness for different age groups on TikTok, YouTube, and Instagram, finding that 13-year-old accounts encountered harmful videos more frequently and quickly than 18-year-old accounts during passive scrolling.
- YouTube showed 15% of recommended videos to 13-year-olds as harmful, appearing within 3:06 minutes of scrolling, indicating algorithmic filtering weaknesses.
- Exposure to harmful content occurred without user-initiated searches, highlighting gaps in current moderation practices on social media platforms.
- The study emphasized the need for more robust verification methods as underage users can easily misrepresent their age on these platforms.
Summary: <br /> <div>
arXiv:2505.11160v1 Announce Type: new 
Abstract: Video-sharing social media platforms, such as TikTok, YouTube, and Instagram, implement content moderation policies aimed at reducing exposure to harmful videos among minor users. As video has become the dominant and most immersive form of online content, understanding how effectively this medium is moderated for younger audiences is urgent. In this study, we evaluated the effectiveness of video moderation for different age groups on three of the main video-sharing platforms: TikTok, YouTube, and Instagram. We created experimental accounts for the children assigned ages 13 and 18. Using these accounts, we evaluated 3,000 videos served up by the social media platforms, in passive scrolling and search modes, recording the frequency and speed at which harmful videos were encountered. Each video was manually assessed for level and type of harm, using definitions from a unified framework of harmful content.
  The results show that for passive scrolling or search-based scrolling, accounts assigned to the age 13 group encountered videos that were deemed harmful, more frequently and quickly than those assigned to the age 18 group. On YouTube, 15\% of recommended videos to 13-year-old accounts during passive scrolling were assessed as harmful, compared to 8.17\% for 18-year-old accounts. On YouTube, videos labelled as harmful appeared within an average of 3:06 minutes of passive scrolling for the younger age group. Exposure occurred without user-initiated searches, indicating weaknesses in the algorithmic filtering systems. These findings point to significant gaps in current video moderation practices by social media platforms. Furthermore, the ease with which underage users can misrepresent their age demonstrates the urgent need for more robust verification methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning hidden cascades via classification</title>
<link>https://arxiv.org/abs/2505.11228</link>
<guid>https://arxiv.org/abs/2505.11228</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Social Networks, Spreading Dynamics, Partial Observability, Distribution Classification

Summary:
The study focuses on analyzing spreading dynamics in social networks where individuals' statuses are partially observable. By using a Machine Learning framework called Distribution Classification, the method can infer underlying transmission dynamics by utilizing observable indicators such as symptoms of infection. The research evaluates the method on synthetic networks and a real-world insider trading network, showing promising results, especially on networks with high cyclic connectivity. This approach is valuable for analyzing real-world spreading phenomena where direct observation of individual statuses is not feasible. <div>
arXiv:2505.11228v1 Announce Type: new 
Abstract: The spreading dynamics in social networks are often studied under the assumption that individuals' statuses, whether informed or infected, are fully observable. However, in many real-world situations, such statuses remain unobservable, which is crucial for determining an individual's potential to further spread the infection. While this final status is hidden, intermediate indicators such as symptoms of infection are observable and provide important insights into the spread process. We propose a partial observability-aware Machine Learning framework to learn the characteristics of the spreading model. We term the method Distribution Classification, which utilizes the power of classifiers to infer the underlying transmission dynamics. We evaluate our method on two types of synthetic networks and extend the study to a real-world insider trading network. Results show that the method performs well, especially on complex networks with high cyclic connectivity, supporting its utility in analyzing real-world spreading phenomena where direct observation of individual statuses is not possible.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cosmos 1.0: a multidimensional map of the emerging technology frontier</title>
<link>https://arxiv.org/abs/2505.10591</link>
<guid>https://arxiv.org/abs/2505.10591</guid>
<content:encoded><![CDATA[
<div> Keywords: emerging technologies, dataset, indices, technology landscape, classifier

Summary:
This paper introduces a new methodology to map the landscape of emerging technologies using a dataset called Cosmos 1.0. The dataset contains 23,544 technologies structured into meta and theme clusters with embedding vectors. A subset of 100 technologies, called ET100, is manually verified within this dataset. Various indices, such as Technology Awareness Index and Generality Index, are developed to assess the emerging technology landscape. Additional metadata from sources like Wikipedia and Crunchbase are used to validate the indices. A classifier is trained to distinguish between developed technologies and technology-related terms. This comprehensive approach provides new insights into the world of emerging technologies and helps in understanding the dynamics and trends shaping the technology landscape.<br /><br />Summary: <div>
arXiv:2505.10591v1 Announce Type: cross 
Abstract: This paper describes a novel methodology to map the universe of emerging technologies, utilising various source data that contain a rich diversity and breadth of contemporary knowledge to create a new dataset and multiple indices that provide new insights into these technologies. The Cosmos 1.0 dataset is a comprehensive collection of 23,544 technologies (ET23k) structured into a hierarchical model. Each technology is categorised into three meta clusters (ET3) and seven theme clusters (ET7) enhanced by 100-dimensional embedding vectors. Within the cosmos, we manually verify 100 emerging technologies called the ET100. This dataset is enriched with additional indices specifically developed to assess the landscape of emerging technologies, including the Technology Awareness Index, Generality Index, Deeptech, and Age of Tech Index. The dataset incorporates extensive metadata sourced from Wikipedia and linked data from third-party sources such as Crunchbase, Google Books, OpenAlex and Google Scholar, which are used to validate the relevance and accuracy of the constructed indices. Moreover, we trained a classifier to identify whether they are developed "technology" or technology-related "terms".
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestyBot: Detecting and Disrupting Chinese Communist Party Influence Stratagems</title>
<link>https://arxiv.org/abs/2505.10746</link>
<guid>https://arxiv.org/abs/2505.10746</guid>
<content:encoded><![CDATA[
<div> Keywords: Foreign information operations, Russian actors, Chinese actors, ChestyBot, detection and mitigation strategies

Summary:
Foreign information operations conducted by Russian and Chinese actors exploit the permissive information environment in the United States. These campaigns pose a threat to democratic institutions and the Westphalian model. Existing detection and mitigation strategies often fail to detect active information campaigns in real time. In response to this challenge, ChestyBot, a pragmatics-based language model, has been developed to detect unlabeled foreign malign influence tweets with a high accuracy rate of up to 98.34%. This model introduces a novel framework to disrupt foreign influence operations in their early stages, providing a proactive approach to countering malign influence in the online sphere. With its high accuracy and effectiveness, ChestyBot offers a promising tool for enhancing the security of democratic institutions against foreign interference. 

<br /><br />Summary: <div>
arXiv:2505.10746v1 Announce Type: cross 
Abstract: Foreign information operations conducted by Russian and Chinese actors exploit the United States' permissive information environment. These campaigns threaten democratic institutions and the broader Westphalian model. Yet, existing detection and mitigation strategies often fail to identify active information campaigns in real time. This paper introduces ChestyBot, a pragmatics-based language model that detects unlabeled foreign malign influence tweets with up to 98.34% accuracy. The model supports a novel framework to disrupt foreign influence operations in their formative stages.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Patterns and Influence of Advertising in Print Newspapers</title>
<link>https://arxiv.org/abs/2505.10791</link>
<guid>https://arxiv.org/abs/2505.10791</guid>
<content:encoded><![CDATA[
<div> advertising, newspapers, India, analysis, coverage
<br />
The paper investigates advertising practices in print newspapers in India using image processing and OCR techniques to extract data from digital versions. The dataset compiled from five newspapers in multiple languages reveals consistent print advertising levels despite declining circulation, with company ads dominating prominent pages and government ads contributing disproportionately to revenue. The study also shows that advertising in newspapers influences the coverage an advertiser receives, with regression analyses indicating a correlation between increased advertising and more favorable media coverage for corporate advertisers. This relationship remains consistent over time and across different levels of advertiser popularity.
<br /><br />Summary: <div>
arXiv:2505.10791v1 Announce Type: cross 
Abstract: This paper investigates advertising practices in print newspapers across India using a novel data-driven approach. We develop a pipeline employing image processing and OCR techniques to extract articles and advertisements from digital versions of print newspapers with high accuracy. Applying this methodology to five popular newspapers that span multiple regions and three languages, English, Hindi, and Telugu, we assembled a dataset of more than 12,000 editions containing several hundred thousand advertisements. Collectively, these newspapers reach a readership of over 100 million people. Using this extensive dataset, we conduct a comprehensive analysis to answer key questions about print advertising: who advertises, what they advertise, when they advertise, where they place their ads, and how they advertise. Our findings reveal significant patterns, including the consistent level of print advertising over the past six years despite declining print circulation, the overrepresentation of company ads on prominent pages, and the disproportionate revenue contributed by government ads. Furthermore, we examine whether advertising in a newspaper influences the coverage an advertiser receives. Through regression analyses on coverage volume and sentiment, we find strong evidence supporting this hypothesis for corporate advertisers. The results indicate a clear trend where increased advertising correlates with more favorable and extensive media coverage, a relationship that remains robust over time and across different levels of advertiser popularity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alexandria: A Library of Pluralistic Values for Realtime Re-Ranking of Social Media Feeds</title>
<link>https://arxiv.org/abs/2505.10839</link>
<guid>https://arxiv.org/abs/2505.10839</guid>
<content:encoded><![CDATA[
<div> values, social media, algorithms, library, user study

Summary:<br /><br />Social media feed ranking algorithms often prioritize engagement as their main objective, leading to various criticisms. To address this, the authors propose a library of 78 values that social media algorithms should consider, beyond just engagement. They implement these values into LLM-powered content classifiers to create a browser extension called Alexandria, which allows users to re-rank their Twitter feed based on their desired values. Two user studies were conducted to test the effectiveness of this approach, with results showing that a diverse library of values allows for more nuanced preferences and greater user control. The study argues that the missing values in current social media algorithms can be incorporated and utilized effectively through end-user tools like Alexandria. <div>
arXiv:2505.10839v1 Announce Type: cross 
Abstract: Social media feed ranking algorithms fail when they too narrowly focus on engagement as their objective. The literature has asserted a wide variety of values that these algorithms should account for as well -- ranging from well-being to productive discourse -- far more than can be encapsulated by a single topic or theory. In response, we present a $\textit{library of values}$ for social media algorithms: a pluralistic set of 78 values as articulated across the literature, implemented into LLM-powered content classifiers that can be installed individually or in combination for real-time re-ranking of social media feeds. We investigate this approach by developing a browser extension, $\textit{Alexandria}$, that re-ranks the X/Twitter feed in real time based on the user's desired values. Through two user studies, both qualitative (N=12) and quantitative (N=257), we found that diverse user needs require a large library of values, enabling more nuanced preferences and greater user control. With this work, we argue that the values criticized as missing from social media ranking algorithms can be operationalized and deployed today through end-user tools.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Differentially Private Graph Clustering via the Power Iteration Method</title>
<link>https://arxiv.org/abs/2505.11169</link>
<guid>https://arxiv.org/abs/2505.11169</guid>
<content:encoded><![CDATA[
<div> Graph clustering, locally differentially private, power iteration method, spectral clustering, randomized response

Summary:
This paper introduces a locally differentially private graph clustering algorithm that addresses limitations of previous methods. The algorithm is based on the power iteration method and is interactive. By eliminating the noise introduced by the largest eigenvector constant, the algorithm achieves local differential privacy with a constant privacy budget for well-clustered graphs with a minimum degree of $\tilde{\Omega}(\sqrt{n})$. This is a significant improvement compared to randomized response methods, which require a privacy budget in $\Omega(\log n)$. Experimental results show that the proposed algorithm performs better than spectral clustering applied to randomized response results. <div>
arXiv:2505.11169v1 Announce Type: cross 
Abstract: We propose a locally differentially private graph clustering algorithm. Previous works have explored this problem, including approaches that apply spectral clustering to graphs generated via the randomized response algorithm. However, these methods only achieve accurate results when the privacy budget is in $\Omega(\log n)$, which is unsuitable for many practical applications. In response, we present an interactive algorithm based on the power iteration method. Given that the noise introduced by the largest eigenvector constant can be significant, we incorporate a technique to eliminate this constant. As a result, our algorithm attains local differential privacy with a constant privacy budget when the graph is well-clustered and has a minimum degree of $\tilde{\Omega}(\sqrt{n})$. In contrast, while randomized response has been shown to produce accurate results under the same minimum degree condition, it is limited to graphs generated from the stochastic block model. We perform experiments to demonstrate that our method outperforms spectral clustering applied to randomized response results.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using co-sharing to identify use of mainstream news for promoting potentially misleading narratives</title>
<link>https://arxiv.org/abs/2308.06459</link>
<guid>https://arxiv.org/abs/2308.06459</guid>
<content:encoded><![CDATA[
<div> Keywords: online misinformation, social media, reliable sources, narratives, fact-checking<br />
Summary:<br />
- Research on online misinformation often focuses on unreliable sources, but this study shows how users can use information from reliable sources to spread misleading narratives.
- By analyzing Twitter data from 2018 to 2021 matched with voter information, the study finds that misinformation narratives are more likely to be present in articles co-shared with fake news on social media.
- Users share factually true information from reliable sources alongside fake news to enhance the credibility and reach of misleading claims.
- This form of misinformation, where true information is repurposed for false narratives, may be more prevalent than previously thought.
- Understanding how users manipulate information from reliable sources to propagate misinformation is crucial in combatting the spread of false narratives online.<br /><br />Summary: <div>
arXiv:2308.06459v2 Announce Type: replace 
Abstract: Much of the research quantifying volume and spread of online misinformation measures the construct at the source level, identifying a set of specific unreliable domains that account for a relatively small share of news consumption. This source-level dichotomy obscures the potential for users to repurpose factually true information from reliable sources to advance misleading narratives. We demonstrate this potentially far more prevalent form of misinformation by identifying articles from reliable sources that are frequently co-shared with (shared by users who also shared) "fake" news on social media, and concurrently extracting narratives present in fake news content and claims fact-checked as false. Specifically in this study, we use Twitter/X data from May 2018 to November 2021 matched to a U.S. voter file. We find that narratives present in misinformation content are significantly more likely to occur in co-shared articles than in articles from the same reliable sources that are not co-shared, consistent with users using information from mainstream sources to enhance the credibility and reach of potentially misleading claims.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Staying Fresh: Efficient Algorithms for Timely Social Information Distribution</title>
<link>https://arxiv.org/abs/2308.13260</link>
<guid>https://arxiv.org/abs/2308.13260</guid>
<content:encoded><![CDATA[
<div> NP-hard, Urban Sensing Network, Online Social Network, Combinatorial Optimization, Matrix Computations <br />
<br />
Summary: 
The study focuses on the interplay between urban sensing networks and online social networks in location-based social networks. The problem of selecting hotspots to enhance point-of-interest (PoI) sharing is proven to be NP-hard. Existing approximation solutions are not feasible, leading to the development of a polynomial-time algorithm with a guaranteed approximation ratio. The PoI-sharing process is transformed into matrix computations, enabling the derivation of a closed-form objective with desirable properties. An augmentation-adaptive algorithm is proposed for selected users to move around and sense more PoI information. The theoretical findings are supported by simulation results using synthetic and real-world datasets. <div>
arXiv:2308.13260v3 Announce Type: replace 
Abstract: In location-based social networks (LBSNs), users sense urban point-of-interest (PoI) information in the vicinity and share such information with friends in online social networks. Given users' limited social connections and severe lags in disseminating fresh PoI to all, major LBSNs aim to enhance users' social PoI sharing by selecting $k$ out of $m$ users as hotspots and broadcasting their fresh PoI information to the entire user community. This motivates us to study a new combinatorial optimization problem that involves the interplay between an urban sensing network and an online social network. We prove that this problem is NP-hard and also renders existing approximation solutions not viable. Through analyzing the interplay effects between the two networks, we successfully transform the involved PoI-sharing process across two networks to matrix computations for deriving a closed-form objective to hold desirable properties (e.g., submodularity and monotonicity). This finding enables us to develop a polynomial-time algorithm that guarantees a ($1-\frac{m-2}{m}(\frac{k-1}{k})^k$) approximation of the optimum. Furthermore, we allow each selected user to move around and sense more PoI information to share and propose an augmentation-adaptive algorithm with decent performance guarantees. Finally, our theoretical results are corroborated by our simulation findings using both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Generation of Preference Data for Recommendation Analysis</title>
<link>https://arxiv.org/abs/2407.16594</link>
<guid>https://arxiv.org/abs/2407.16594</guid>
<content:encoded><![CDATA[
<div> Data generation, recommendation system, user preferences, synthetic data, user behavior <br />
<br />
Summary: 
The article introduces HYDRA, a novel preferences data generation model for simulating recommendation systems in a controlled environment. HYDRA considers three main factors - user-item interaction level, item popularity, and user engagement level - to mimic real datasets. It can generate user communities with similar item adoptions to reflect real-world social influences and trends. By incorporating mixtures of probability distributions for item popularity and user engagement, HYDRA can simulate diverse scenarios realistically, capturing the complexity and variability of actual user behavior. The model's effectiveness is demonstrated through experiments on benchmark datasets, showing its capability to replicate real-world data patterns. The code for the experiments is publicly available, enabling further research and development of recommendation systems. <br /><br />Summary: <div>
arXiv:2407.16594v2 Announce Type: replace-cross 
Abstract: Simulating a recommendation system in a controlled environment, to identify specific behaviors and user preferences, requires highly flexible synthetic data generation models capable of mimicking the patterns and trends of real datasets. In this context, we propose HYDRA, a novel preferences data generation model driven by three main factors: user-item interaction level, item popularity, and user engagement level. The key innovations of the proposed process include the ability to generate user communities characterized by similar item adoptions, reflecting real-world social influences and trends. Additionally, HYDRA considers item popularity and user engagement as mixtures of different probability distributions, allowing for a more realistic simulation of diverse scenarios. This approach enhances the model's capacity to simulate a wide range of real-world cases, capturing the complexity and variability found in actual user behavior. We demonstrate the effectiveness of HYDRA through extensive experiments on well-known benchmark datasets. The results highlight its capability to replicate real-world data patterns, offering valuable insights for developing and testing recommendation systems in a controlled and realistic manner. The code used to perform the experiments is publicly available at https://github.com/SimoneMungari/HYDRA.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoder Graph Quantile Neural Networks for Geographic Data</title>
<link>https://arxiv.org/abs/2409.18865</link>
<guid>https://arxiv.org/abs/2409.18865</guid>
<content:encoded><![CDATA[
<div> Positional Encoder Graph Neural Networks, PE-GNNs, predictive distributions, poorly calibrated, uncertainty quantification <br />
<br />
Summary: The study introduces a novel framework called Positional Encoder Graph Quantile Neural Network (PE-GQNN) that combines PE-GNNs with Quantile Neural Networks to enhance predictive accuracy and uncertainty quantification. The PE-GQNN allows for flexible conditional density estimation without strict assumptions about the target distribution and can be applied to tasks beyond spatial data. Empirical results demonstrate superior performance compared to existing methods in terms of predictive accuracy and uncertainty quantification, without added computational cost. The PE-GQNN also offers theoretical insights and identifies key special cases such as the PE-GNN, showcasing the versatility and effectiveness of the proposed framework. <div>
arXiv:2409.18865v2 Announce Type: replace-cross 
Abstract: Positional Encoder Graph Neural Networks (PE-GNNs) are among the most effective models for learning from continuous spatial data. However, their predictive distributions are often poorly calibrated, limiting their utility in applications that require reliable uncertainty quantification. We propose the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel framework that combines PE-GNNs with Quantile Neural Networks, partially monotonic neural blocks, and post-hoc recalibration techniques. The PE-GQNN enables flexible and robust conditional density estimation with minimal assumptions about the target distribution, and it extends naturally to tasks beyond spatial data. Empirical results on benchmark datasets show that the PE-GQNN outperforms existing methods in both predictive accuracy and uncertainty quantification, without incurring additional computational cost. We also provide theoretical insights and identify important special cases arising from our formulation, including the PE-GNN.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena</title>
<link>https://arxiv.org/abs/2501.03266</link>
<guid>https://arxiv.org/abs/2501.03266</guid>
<content:encoded><![CDATA[
<div> safety, ethical alignment, content moderation, user satisfaction, refusal <br />
Summary:
Ethical considerations and content moderation in Large Language Models (LLMs) are crucial topics, but the impact on user satisfaction is not well understood. This study examines user responses to model refusals in Chatbot Arena, a platform for comparing LLM responses. By analyzing nearly 50,000 comparisons, the study reveals that users are significantly less satisfied when models refuse to answer due to ethical concerns compared to technical limitations. However, the dissatisfaction varies based on the sensitivity of the prompt and the clarity of the refusal. Refusals are better received when the prompt involves illegal content and when the refusal is detailed and contextually aligned. These findings highlight the challenge in balancing safety measures with user expectations in LLM design, emphasizing the need for adaptive moderation strategies considering context and presentation. <br /><br />Summary: <div>
arXiv:2501.03266v2 Announce Type: replace-cross 
Abstract: LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</title>
<link>https://arxiv.org/abs/2501.13977</link>
<guid>https://arxiv.org/abs/2501.13977</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Artificial Intelligence, Recommendation Algorithms, Social Media, Harmful Content<br />
<br />
Summary: 
The study proposes a novel re-ranking approach using Large Language Models (LLMs) to address the challenges in moderating harmful content on social media platforms. The current moderation efforts struggle with scalability and adapting to new forms of harm due to reliance on classifiers trained with extensive human-annotated data. The proposed method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Two new metrics are introduced to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models, and across three configurations, the LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation. <div>
arXiv:2501.13977v2 Announce Type: replace-cross 
Abstract: Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling</title>
<link>https://arxiv.org/abs/2505.09665</link>
<guid>https://arxiv.org/abs/2505.09665</guid>
<content:encoded><![CDATA[
<div> Keywords: wildfires, social media, Reddit, crisis discourse analysis, public health concerns

Summary:
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, focusing on the Palisades and Eaton fires. Utilizing topic modeling methods and a hierarchical framework, the researchers categorize topics into Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA topics peaks within the first 2-5 days, aligning with fire progress. Public health and safety, loss and damage, and emergency resources are frequently discussed topics, with a focus on health-related issues like environmental and occupational health. Grief signals and mental health risks make up a significant portion of CN instances, with peaks occurring at night. The study provides the first annotated social media dataset on the 2025 LA fires and offers insights for more empathetic disaster response, public health communication, and future research on climate-related disasters.

<br /><br />Summary: 
- Analysis of Reddit discourse during 2025 LA wildfires 
- Topics categorized into SA and CN 
- SA topics peak within 2-5 days of fire progression 
- Public health and safety, loss, and damage are frequently discussed 
- Grief signals and mental health risks prominent in CN discussions <div>
arXiv:2505.09665v1 Announce Type: new 
Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion</title>
<link>https://arxiv.org/abs/2505.10197</link>
<guid>https://arxiv.org/abs/2505.10197</guid>
<content:encoded><![CDATA[
<div> Community detection, Graph Convolutional Networks (GCNs), Topological and Attributive Similarity-based Community detection (TAS-Com), Leiden algorithm, modularity

Summary:
The article introduces TAS-Com, a novel method for community detection in social networks that addresses shortcomings in existing techniques. TAS-Com leverages the Leiden algorithm and a new loss function to detect community structures with optimal modularity. It refines human-labeled communities to ensure connectivity within each community, striking a balance between modularity and compliance with human labels. Experimental results demonstrate TAS-Com's superior performance compared to state-of-the-art algorithms. This approach overcomes the challenge of suboptimal solutions in GCNs and the risk of grouping disconnected nodes based solely on attributes. By emphasizing both topological and attribute similarities, TAS-Com improves the accuracy and effectiveness of community detection in social networks. <div>
arXiv:2505.10197v1 Announce Type: new 
Abstract: Community detection, a vital technology for real-world applications, uncovers cohesive node groups (communities) by leveraging both topological and attribute similarities in social networks. However, existing Graph Convolutional Networks (GCNs) trained to maximize modularity often converge to suboptimal solutions. Additionally, directly using human-labeled communities for training can undermine topological cohesiveness by grouping disconnected nodes based solely on node attributes. We address these issues by proposing a novel Topological and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com introduces a novel loss function that exploits the highly effective and scalable Leiden algorithm to detect community structures with global optimal modularity. Leiden is further utilized to refine human-labeled communities to ensure connectivity within each community, enabling TAS-Com to detect community structures with desirable trade-offs between modularity and compliance with human labels. Experimental results on multiple benchmark networks confirm that TAS-Com can significantly outperform several state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Fact-Checks Do Not Break Follower Loyalty</title>
<link>https://arxiv.org/abs/2505.10254</link>
<guid>https://arxiv.org/abs/2505.10254</guid>
<content:encoded><![CDATA[
<div> fact-checking, social media, follower base, misinformation, engagement

Summary:<br /><br />This study examines the impact of community-based fact-checking on social media platforms in addressing misinformation. The research focuses on whether users lose followers after their posts are corrected by community fact-checks. Through analysis of time-series data on 3516 fact-checked posts, it is found that users who post misleading content do not experience significant declines in follower counts after fact-checks. This suggests that followers of users sharing misinformation tend to remain loyal and unaffected by fact-checks. The study highlights the need for additional interventions to effectively discourage the spread of misinformation on social media platforms. <div>
arXiv:2505.10254v1 Announce Type: new 
Abstract: Major social media platforms increasingly adopt community-based fact-checking to address misinformation on their platforms. While previous research has largely focused on its effect on engagement (e.g., reposts, likes), an understanding of how fact-checking affects a user's follower base is missing. In this study, we employ quasi-experimental methods to causally assess whether users lose followers after their posts are corrected via community fact-checks. Based on time-series data on follower counts for N=3516 community fact-checked posts from X, we find that community fact-checks do not lead to meaningful declines in the follower counts of users who post misleading content. This suggests that followers of spreaders of misleading posts tend to remain loyal and do not view community fact-checks as a sufficient reason to disengage. Our findings underscore the need for complementary interventions to more effectively disincentivize the production of misinformation on social media.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing AI-Generated Misinformation on Social Media</title>
<link>https://arxiv.org/abs/2505.10266</link>
<guid>https://arxiv.org/abs/2505.10266</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated misinformation, social media, X platform, viral, sentiment

Summary: 
AI-generated misinformation, including deepfakes, is a growing concern for information integrity on social media. This study analyzes a dataset of 91,452 misleading posts on the X platform, both AI-generated and non-AI-generated. The findings reveal that AI-generated misinformation is more focused on entertaining content with a positive sentiment, commonly originates from smaller user accounts, has a higher likelihood of going viral, and is slightly less believable and harmful compared to conventional misinformation. These unique characteristics of AI-generated misinformation underscore the importance of addressing this issue on social media platforms and urge for further research to combat its spread.<br /><br />Summary: <div>
arXiv:2505.10266v1 Announce Type: new 
Abstract: AI-generated misinformation (e.g., deepfakes) poses a growing threat to information integrity on social media. However, prior research has largely focused on its potential societal consequences rather than its real-world prevalence. In this study, we conduct a large-scale empirical analysis of AI-generated misinformation on the social media platform X. Specifically, we analyze a dataset comprising N=91,452 misleading posts, both AI-generated and non-AI-generated, that have been identified and flagged through X's Community Notes platform. Our analysis yields four main findings: (i) AI-generated misinformation is more often centered on entertaining content and tends to exhibit a more positive sentiment than conventional forms of misinformation, (ii) it is more likely to originate from smaller user accounts, (iii) despite this, it is significantly more likely to go viral, and (iv) it is slightly less believable and harmful compared to conventional misinformation. Altogether, our findings highlight the unique characteristics of AI-generated misinformation on social media. We discuss important implications for platforms and future research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Approximate Biclique Counting over Large Bipartite Graphs</title>
<link>https://arxiv.org/abs/2505.10471</link>
<guid>https://arxiv.org/abs/2505.10471</guid>
<content:encoded><![CDATA[
<div> Keywords: $(p,q)$-bicliques, bipartite graphs, graph coloring, dynamic programming, sampling algorithm

Summary: 
$(p,q)$-bicliques in bipartite graphs are important for various applications but exact counting is computationally challenging. A new method is proposed using $(p,q)$-brooms, special spanning trees of $(p,q)$-bicliques, to approximate counts efficiently. By utilizing graph coloring and dynamic programming, the method can provide unbiased estimates with error guarantees. An efficient sampling algorithm is introduced to derive approximate counts from $(p,q)$-broom results. Empirical results show the method outperforms existing techniques in accuracy and runtime on real-world networks, offering up to 8 times error reduction and 50 times speedup. This approach provides a scalable solution for large-scale $(p,q)$-biclique counting. 

<br /><br />Summary: 
$(p,q)$-bicliques in bipartite graphs are important for various applications but exact counting is computationally challenging. A new method is proposed using $(p,q)$-brooms, special spanning trees of $(p,q)$-bicliques, to approximate counts efficiently. By utilizing graph coloring and dynamic programming, the method can provide unbiased estimates with error guarantees. An efficient sampling algorithm is introduced to derive approximate counts from $(p,q)$-broom results. Empirical results show the method outperforms existing techniques in accuracy and runtime on real-world networks, offering up to 8 times error reduction and 50 times speedup. This approach provides a scalable solution for large-scale $(p,q)$-biclique counting.  <div>
arXiv:2505.10471v1 Announce Type: new 
Abstract: Counting $(p,q)$-bicliques in bipartite graphs is crucial for a variety of applications, from recommendation systems to cohesive subgraph analysis. Yet, it remains computationally challenging due to the combinatorial explosion to exactly count the $(p,q)$-bicliques. In many scenarios, e.g., graph kernel methods, however, exact counts are not strictly required. To design a scalable and high-quality approximate solution, we novelly resort to $(p,q)$-broom, a special spanning tree of the $(p,q)$-biclique, which can be counted via graph coloring and efficient dynamic programming. Based on the intermediate results of the dynamic programming, we propose an efficient sampling algorithm to derive the approximate $(p,q)$-biclique count from the $(p,q)$-broom counts. Theoretically, our method offers unbiased estimates with provable error guarantees. Empirically, our solution outperforms existing approximation techniques in both accuracy (up to 8$\times$ error reduction) and runtime (up to 50$\times$ speedup) on nine real-world bipartite networks, providing a scalable solution for large-scale $(p,q)$-biclique counting.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Signed Network Coordination Games</title>
<link>https://arxiv.org/abs/2505.09799</link>
<guid>https://arxiv.org/abs/2505.09799</guid>
<content:encoded><![CDATA[
<div> pairwise-separable network games, coordinating behaviors, anti-coordinating behaviors, directed signed graph, Nash equilibria<br />
Summary:<br />
The article explores binary-action pairwise-separable network games that involve coordinating and anti-coordinating actions. The model is based on a directed signed graph with weighted interactions and individual bias terms. It focuses on a scenario where a cohesive subset of players is present, connected by positive weights or forming balanced adversarial subcommunities. The study guarantees the existence of Nash equilibria characterized by consensus or polarization within the cohesive group, with stability under best response transitions. These results are based on the supermodular properties of coordination games and the concept of graph cohesiveness, providing robustness in game settings involving complex network structures. <div>
arXiv:2505.09799v1 Announce Type: cross 
Abstract: We study binary-action pairwise-separable network games that encompass both coordinating and anti-coordinating behaviors. Our model is grounded in an underlying directed signed graph, where each link is associated with a weight that describes the strenght and nature of the interaction. The utility for each agent is an aggregation of pairwise terms determined by the weights of the signed graph in addition to an individual bias term. We consider a scenario that assumes the presence of a prominent 'cohesive' subset of players, who are either connected exclusively by positive weights, or forms a structurally balanced subset that can be bipartitioned into two adversarial subcommunities with positive intra-community and negative inter-community edges. Given the properties of the game restricted to the remaining players, our results guarantee the existence of Nash equilibria characterized by a consensus or, respectively, a polarization within the first group, as well as their stability under best response transitions. Our results can be interpreted as robustness results, building on the supermodular properties of coordination games and on a novel use of the concept of graph cohesiveness.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence</title>
<link>https://arxiv.org/abs/2505.09854</link>
<guid>https://arxiv.org/abs/2505.09854</guid>
<content:encoded><![CDATA[
<div> edge devices, distributed learning, federated learning, decentralized FL, gossip learning <br />
<br />
Summary: Chisme introduces protocols for intelligent services at the network edge, catering to heterogeneous data distributions, intermittent connectivity, and limited infrastructure. It offers synchronous decentralized learning (Chisme-DFL) and asynchronous gossip learning (Chisme-GL) to enable collaborative model training considering data diversity. The use of a data similarity heuristic allows agents to infer affinity and optimize model updates in both DFL and GL paradigms. Chisme-DFL scales linearly with network size, while Chisme-GL has a constant resource requirement. Experimental results show that Chisme methods outperform traditional approaches in model training over distributed and varied data in networks with different connectivities. <div>
arXiv:2505.09854v1 Announce Type: cross 
Abstract: As demand for intelligent services rises and edge devices become more capable, distributed learning at the network edge has emerged as a key enabling technology. While existing paradigms like federated learning (FL) and decentralized FL (DFL) enable privacy-preserving distributed learning in many scenarios, they face potential challenges in connectivity and synchronization imposed by resource-constrained and infrastructure-less environments. While more robust, gossip learning (GL) algorithms have generally been designed for homogeneous data distributions and may not suit all contexts. This paper introduces Chisme, a novel suite of protocols designed to address the challenges of implementing robust intelligence in the network edge, characterized by heterogeneous data distributions, episodic connectivity, and lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. We introduce a data similarity heuristic that allows agents to opportunistically infer affinity with each other using the existing communication of model updates in decentralized FL and GL. We leverage the heuristic to extend DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration. While Chisme-DFL is a synchronous decentralized approach whose resource utilization scales linearly with network size, Chisme-GL is fully asynchronous and has a lower, constant resource requirement independent of network size. We demonstrate that Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirically evaluating commonsense intelligence in large language models with large-scale human judgments</title>
<link>https://arxiv.org/abs/2505.10309</link>
<guid>https://arxiv.org/abs/2505.10309</guid>
<content:encoded><![CDATA[
<div> heterogeneity, common sense, artificial intelligence, language models, evaluation<br />
<br />
Summary: 
The article discusses the assessment of common sense in artificial intelligence models, focusing on language models. It highlights the variability in human perceptions of common sense, challenging the assumption of homogeneity in human common sense. A novel evaluation method is proposed, considering the diversity among humans by comparing a model's judgments to those of a human population. The study finds that most large language models fall below the human median in common sense competence when treated as individual survey respondents. Additionally, when used as simulators of a population, language models exhibit only modest agreement with real humans on common sense statements. Surprisingly, smaller, open-weight models outperform larger, proprietary frontier models in this evaluation framework. The framework highlights the importance of adapting AI models to different human collectivities with varying social stocks of knowledge. <div>
arXiv:2505.10309v1 Announce Type: cross 
Abstract: Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a novel method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducing the first and second moment of empirical degree distributions</title>
<link>https://arxiv.org/abs/2505.10373</link>
<guid>https://arxiv.org/abs/2505.10373</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic models, complex networks, Exponential Random Graphs, variance, non-linear ERGs 

Summary:
Probabilistic models are increasingly used to analyze complex networks, with Exponential Random Graphs (ERGs) being a popular choice. While linear ERGs have been widely studied, they are unable to account for the variance in the empirical degree distribution. Non-linear ERGs are necessary to address this limitation. The traditional mean-field approximation fails to capture the degree-corrected version of the two-star model, leading to degeneration. To overcome this, a fitness-induced variant of the model is introduced, known as the 'softened' model. This model successfully reproduces the sample variance while maintaining the explanatory power of the linear counterpart. By incorporating non-linear ERGs, researchers can better study the structural organization of real-world complex networks using a canonical framework. <div>
arXiv:2505.10373v1 Announce Type: cross 
Abstract: The study of probabilistic models for the analysis of complex networks represents a flourishing research field. Among the former, Exponential Random Graphs (ERGs) have gained increasing attention over the years. So far, only linear ERGs have been extensively employed to gain insight into the structural organisation of real-world complex networks. None, however, is capable of accounting for the variance of the empirical degree distribution. To this aim, non-linear ERGs must be considered. After showing that the usual mean-field approximation forces the degree-corrected version of the two-star model to degenerate, we define a fitness-induced variant of it. Such a `softened' model is capable of reproducing the sample variance, while retaining the explanatory power of its linear counterpart, within a purely canonical framework.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A classification of overlapping clustering schemes for hypergraphs</title>
<link>https://arxiv.org/abs/2404.03332</link>
<guid>https://arxiv.org/abs/2404.03332</guid>
<content:encoded><![CDATA[
<div> hypergraphs, overlapping clustering, representability, excisive, functorial

Summary: 
The study delves into the problem of identifying overlapping clusterings of hypergraphs, building upon previous research. It introduces the concept of representability in overlapping clustering schemes, establishing that any such scheme is excisive and functorial. Moreover, it demonstrates that an excisive and functorial clustering scheme is isomorphic to a representable one. The study also highlights that representable clustering schemes can be computed in polynomial time for simple graphs with bounded expansion, with the exponent determined by the maximum independence number of a graph in the representing set. This finding extends to non-overlapping representable clustering schemes as well, proving to be valuable independently. <div>
arXiv:2404.03332v2 Announce Type: replace-cross 
Abstract: Community detection in graphs is a problem that is likely to be relevant whenever network data appears, and consequently the problem has received much attention with many different methods and algorithms applied. However, many of these methods are hard to study theoretically, and they optimise for somewhat different goals. A general and rigorous account of the problem and possible methods remains elusive.
  We study the problem of finding overlapping clusterings of hypergraphs, continuing the line of research started by Carlsson and M\'emoli (2013) of classifying clustering schemes as functors. We extend their notion of representability to the overlapping case, showing that any representable overlapping clustering scheme is excisive and functorial, and any excisive and functorial clustering scheme is isomorphic to a representable clustering scheme.
  We also note that, for simple graphs, any representable clustering scheme is computable in polynomial time on graphs of bounded expansion, with an exponent determined by the maximum independence number of a graph in the representing set. This result also applies to non-overlapping representable clustering schemes, and so may be of independent interest.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2409.08946</link>
<guid>https://arxiv.org/abs/2409.08946</guid>
<content:encoded><![CDATA[
<div> active graph domain adaptation, topological relationships, message passing mechanism, semantic information, distribution discrepancy 

Summary:
The paper introduces the problem of active graph domain adaptation, where a small set of informative nodes on the target graph are selected for extra annotation. The proposed approach, named DELTA, consists of two subnetworks that explore topological semantics from different perspectives - edge-oriented and path-oriented. The edge-oriented subnetwork leverages message passing to learn neighborhood information, while the path-oriented subnetwork explores high-order relationships. Informative candidate nodes are selected based on consistency across the subnetworks, and local semantics are aggregated from their K-hop subgraphs for topological uncertainty estimation. Target node-source node discrepancy scores help account for distribution shifts. Experimental results demonstrate that DELTA outperforms existing approaches on benchmark datasets. The code implementation of DELTA is publicly available at the provided GitHub repository. <br /><br />Summary: <div>
arXiv:2409.08946v2 Announce Type: replace-cross 
Abstract: Graph domain adaptation has recently enabled knowledge transfer across different graphs. However, without the semantic information on target graphs, the performance on target graphs is still far from satisfactory. To address the issue, we study the problem of active graph domain adaptation, which selects a small quantitative of informative nodes on the target graph for extra annotation. This problem is highly challenging due to the complicated topological relationships and the distribution discrepancy across graphs. In this paper, we propose a novel approach named Dual Consistency Delving with Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA consists of an edge-oriented graph subnetwork and a path-oriented graph subnetwork, which can explore topological semantics from complementary perspectives. In particular, our edge-oriented graph subnetwork utilizes the message passing mechanism to learn neighborhood information, while our path-oriented graph subnetwork explores high-order relationships from sub-structures. To jointly learn from two subnetworks, we roughly select informative candidate nodes with the consideration of consistency across two subnetworks. Then, we aggregate local semantics from its K-hop subgraph based on node degrees for topological uncertainty estimation. To overcome potential distribution shifts, we compare target nodes and their corresponding source nodes for discrepancy scores as an additional component for fine selection. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms various state-of-the-art approaches. The code implementation of DELTA is available at https://github.com/goose315/DELTA.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDL-Pool: Adaptive Multilevel Graph Pooling Based on Minimum Description Length</title>
<link>https://arxiv.org/abs/2409.10263</link>
<guid>https://arxiv.org/abs/2409.10263</guid>
<content:encoded><![CDATA[
<div> Graph pooling, deep graph representation learning, MDL-Pool, minimum description length, interdependencies, hierarchical structures <br />
Summary: 
MDL-Pool is introduced as a novel graph pooling operator based on the minimum description length (MDL) principle. It addresses the limitations of current approaches by considering interdependencies between different hierarchical levels in graphs and adapting to datasets with varying graph sizes that require pooling at different depths. The MDL loss formulation facilitates direct comparison between multiple pooling alternatives with different depths, enhancing model complexity and goodness-of-fit balance. MDL-Pool outperforms various baselines in an empirical evaluation on standard graph classification datasets, showcasing its competitive performance and efficiency in summarizing topological properties and features of graphs for graph-level tasks like classification and regression. <div>
arXiv:2409.10263v2 Announce Type: replace-cross 
Abstract: Graph pooling compresses graphs and summarises their topological properties and features in a vectorial representation. It is an essential part of deep graph representation learning and is indispensable in graph-level tasks like classification or regression. Current approaches pool hierarchical structures in graphs by iteratively applying shallow pooling operators up to a fixed depth. However, they disregard the interdependencies between structures at different hierarchical levels and do not adapt to datasets that contain graphs with different sizes that may require pooling with various depths. To address these issues, we propose MDL-Pool, a pooling operator based on the minimum description length (MDL) principle, whose loss formulation explicitly models the interdependencies between different hierarchical levels and facilitates a direct comparison between multiple pooling alternatives with different depths. MDP-Pool builds on the map equation, an information-theoretic objective function for community detection, which naturally implements Occam's razor and balances between model complexity and goodness-of-fit via the MDL. We demonstrate MDL-Pool's competitive performance in an empirical evaluation against various baselines across standard graph classification datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalar embedding of temporal network trajectories</title>
<link>https://arxiv.org/abs/2412.02715</link>
<guid>https://arxiv.org/abs/2412.02715</guid>
<content:encoded><![CDATA[
<div> embedding, temporal network, trajectory, time series analysis, signal processing

Summary:
This paper discusses the concept of embedding temporal networks, which are dynamic networks where links change over time, into a low-dimensional Euclidean space for the purpose of studying their complex dynamics using time series analysis and signal processing. The focus is on preserving the relative graph distances between network snapshots in the embedding, rather than the topological structure of each snapshot. This approach utilizes techniques like Multidimensional Scaling (MDS) and Principal Component Analysis (PCA) to reduce dimensionality and capture the essential dynamics of the network trajectory. By applying this methodology to various network trajectory models and empirical data, the study confirms that important dynamical properties of temporal networks can be preserved in their scalar embeddings, enabling effective time series analysis on these networks. <div>
arXiv:2412.02715v2 Announce Type: replace-cross 
Abstract: A temporal network -- a collection of snapshots recording the evolution of a network whose links appear and disappear dynamically -- can be interpreted as a trajectory in graph space. In order to characterize the complex dynamics of such trajectory via the tools of time series analysis and signal processing, it is sensible to preprocess the trajectory by embedding it in a low-dimensional Euclidean space. Here we argue that, rather than the topological structure of each network snapshot, the main property of the trajectory that needs to be preserved in the embedding is the relative graph distance between snapshots. This idea naturally leads to dimensionality reduction approaches that explicitly consider relative distances, such as Multidimensional Scaling (MDS) or identifying the distance matrix as a feature matrix in which to perform Principal Component Analysis (PCA). This paper provides a comprehensible methodology that illustrates this approach. Its application to a suite of generative network trajectory models and empirical data certify that nontrivial dynamical properties of the network trajectories are preserved already in their scalar embeddings, what enables the possibility of performing time series analysis in temporal networks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?</title>
<link>https://arxiv.org/abs/2505.08797</link>
<guid>https://arxiv.org/abs/2505.08797</guid>
<content:encoded><![CDATA[
<div> Keywords: digital social relations, symbolic capital, visibility, influence, reputation

Summary: 
This study delves into the dynamics of visibility and influence in digital social relations to understand the emergence of a new form of symbolic capital. Through a mixed-methods approach involving interviews with digitally active individuals and quantitative social media data analysis, key predictors of digital symbolic capital were identified. The research found that visibility is shaped by content quality, network size, and engagement strategies, while influence is tied to credibility, authority, and trust. The study highlights a distinct form of symbolic capital based on online visibility, influence, and reputation, separate from traditional forms. Ethical implications of these dynamics were discussed, and suggestions were made for future research, stressing the importance of updating social theories to accommodate digital transformations.<br /><br />Summary: <div>
arXiv:2505.08797v1 Announce Type: new 
Abstract: This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation</title>
<link>https://arxiv.org/abs/2505.09081</link>
<guid>https://arxiv.org/abs/2505.09081</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based modeling, language models, social network simulation, temporal stability, memory system

Summary:
1. The paper introduces a new approach, SALM, that integrates language models (LMs) into agent-based modeling for social systems.
2. SALM achieves exceptional temporal stability in multi-agent scenarios, surpassing traditional rule-based approaches.
3. It utilizes a hierarchical prompting architecture that allows stable simulation across a large number of timesteps while reducing token usage significantly.
4. An attention-based memory system in SALM achieves high cache hit rates with minimal memory growth.
5. The framework provides formal bounds on personality stability, a crucial factor in social simulations.
6. Extensive validation against SNAP ego networks confirms SALM's capability to model long-term social phenomena with validated behavioral fidelity.

<br /><br />Summary: SALM introduces a novel approach to agent-based modeling by incorporating language models, which enhances temporal stability, memory efficiency, and behavioral fidelity. The framework's hierarchical prompting architecture and attention-based memory system enable stable simulation over long periods, with formal guarantees on personality stability. Extensive validation demonstrates SALM's effectiveness in modeling complex social networks. <div>
arXiv:2505.09081v1 Announce Type: new 
Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial public goods games with queueing and reputation</title>
<link>https://arxiv.org/abs/2505.09154</link>
<guid>https://arxiv.org/abs/2505.09154</guid>
<content:encoded><![CDATA[
<div> Keywords: public goods game, spatial model, M/M/1 queueing system, reputation mechanism, cooperation <br />
Summary: <br />
The study introduces a spatial public goods game that incorporates an M/M/1 queueing system to model the dynamic interactions among players. Traditional public goods games do not account for the asynchrony of player strategies. This model addresses this limitation by simulating the flow of interactions using a queueing system where players arrive following a Poisson process with exponentially distributed service times. Additionally, a reputation mechanism is included, giving preference to players who have cooperated in the past. The results demonstrate that a high arrival rate, low service rate, and reputation mechanism work together to promote cooperation among individuals in the network. This approach offers a new perspective on how public goods can be efficiently provisioned in social and economic systems. <br /> <div>
arXiv:2505.09154v1 Announce Type: new 
Abstract: In real-world social and economic systems, the provisioning of public goods generally entails continuous interactions among individuals, with decisions to cooperate or defect being influenced by dynamic factors such as timing, resource availability, and the duration of engagement. However, the traditional public goods game ignores the asynchrony of the strategy adopted by players in the game. To address this problem, we propose a spatial public goods game that integrates an M/M/1 queueing system to simulate the dynamic flow of player interactions. We use a birth-death process to characterize the stochastic dynamics of this queueing system, with players arriving following a Poisson process and service times being exponentially distributed under a first-come-first-served basis with finite queue capacity. We also incorporate reputation so that players who have cooperated in the past are more likely to be chosen for future interactions. Our research shows that a high arrival rate, low service rate, and the reputation mechanism jointly facilitate the emergence of cooperative individuals in the network, which thus provides an interesting and new perspective for the provisioning of public goods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving towards informative and actionable social media research</title>
<link>https://arxiv.org/abs/2505.09254</link>
<guid>https://arxiv.org/abs/2505.09254</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, complexity, societal impacts, causality, experimental methods 

Summary: 
Social media's societal impacts remain contentious, with observational studies highlighting concerns while randomized controlled trials (RCTs) yield conflicting findings. The complexity of social systems, with feedback loops and non-linearity, complicates assessing causality. Large-scale experiments may show null effects due to system complexities rather than true absence of impact. Eliminating social media is impractical, necessitating a focus on specific platform design choices. Progress requires a complexity-minded approach integrating experimental, observational, and theoretical methods to understand the net impacts of social media on individuals and society. <br /><br />Summary: <div>
arXiv:2505.09254v1 Announce Type: new 
Abstract: Social media is nearly ubiquitous in modern life, and concerns have been raised about its putative societal impacts, ranging from undermining mental health and exacerbating polarization to fomenting violence and disrupting democracy. Despite extensive research, consensus on these effects remains elusive, with observational studies often highlighting concerns while randomized controlled trials (RCTs) yield conflicting or null findings. This review examines how the complexity inherent in social systems can account for such discrepancies, emphasizing that emergent societal and long-term outcomes cannot be readily inferred from individual-level effects. In complex systems, such as social networks, feedback loops, hysteresis, multi-scale dynamics, and non-linearity limit the utility of approaches for assessing causality that are otherwise robust in simpler contexts. Revisiting large-scale experiments, we explore how null or conflicting findings may reflect these complexities rather than a true absence of effects. Even in cases where the methods are appropriate, assessing the net impacts of social media provides little actionable insight given that eliminating social media is not a realistic option for whole populations. We argue that progress will require a complexity-minded approach focused on specific design choices of online platforms that triangulates experimental, observational and theoretical methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Niche Connectivity Paradox: Multichrome Contagions Overcome Vaccine Hesitancy more effectively than Monochromacy</title>
<link>https://arxiv.org/abs/2505.09605</link>
<guid>https://arxiv.org/abs/2505.09605</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine hesitancy, Twitter users, multichrome contagions, intervention, pro-vaccine attitudes

Summary:
The article examines the complexity of vaccine hesitancy in individuals, highlighting the fluctuating nature of attitudes towards vaccines. By analyzing a large dataset of Twitter users, the study identifies multichrome contagions as potential targets for intervention. These individuals exhibit variability in their vaccine attitudes and engage with a diverse range of topics, including progressive issues such as climate change. The research shows that interventions targeting multichrome contagions can promote pro-vaccine attitudes by leveraging the synergistic effect of fragmented, non-overlapping communities. Through data-driven simulations, the study demonstrates the effectiveness of such interventions in driving desired attitude and behavior changes in network-based settings, particularly for addressing vaccine hesitancy. Our work offers valuable insights into harnessing the unique characteristics of multichrome contagions for enhancing public health outcomes.<br /><br />Summary: <div>
arXiv:2505.09605v1 Announce Type: new 
Abstract: The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations. While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic. It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another. Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals). We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination. We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks. Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes. Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes. Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
<link>https://arxiv.org/abs/2505.09017</link>
<guid>https://arxiv.org/abs/2505.09017</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic graph representation learning, HiPPO algorithm, Graph Convolution Networks, Gated Recurrent Unit, State Space Model

Summary: 
The proposed method, DyGSSM, addresses limitations in dynamic graph representation learning by combining Graph Convolution Networks and Gated Recurrent Unit for local and global feature extraction in each snapshot. A cross-attention mechanism integrates the features, while a State Space Model based on the HiPPO algorithm manages long-term dependencies in parameter updates. Experiments on public datasets demonstrate that DyGSSM outperforms existing methods in a majority of cases, showcasing its effectiveness in capturing both global and local information simultaneously and managing temporal dependencies for enhanced performance in dynamic graph representation learning.<br /><br />Summary: <div>
arXiv:2505.09017v1 Announce Type: cross 
Abstract: Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instant AoI Optimization through Relay Location Selection in Disaster Multi-hop Communication</title>
<link>https://arxiv.org/abs/2505.09386</link>
<guid>https://arxiv.org/abs/2505.09386</guid>
<content:encoded><![CDATA[
<div> Keywords: meteorological disasters, communication infrastructures, multi-hop wireless communication, Age of Information, UAV-relayed network

Summary: 
This paper explores the impact of meteorological disasters on communication infrastructures and proposes the use of multi-hop wireless communication, involving IoT devices like UAVs and rescue robots, as an alternative solution. The focus is on analyzing the Age of Information (AoI) metric in this context, specifically in a UAV-relayed wireless network model. By formulating the end-to-end instant AoI and deriving the optimal relay UAV location through mathematical analysis, the study aims to minimize AoI and enhance communication performance in search and rescue operations. Simulations demonstrate that the proposed relay location consistently achieves optimal AoI levels, surpassing other schemes in effectiveness. Through this research, the potential of multi-hop wireless communication in mitigating communication disruptions caused by meteorological disasters is highlighted, emphasizing the importance of efficient communication strategies in emergency response scenarios. 

<br /><br />Summary: <div>
arXiv:2505.09386v1 Announce Type: cross 
Abstract: Meteorological disasters such as typhoons, forest fires, and floods can damage the communication infrastructures, which will further disable the communication capabilities of cellular networks. The multi-hop wireless communication based on IoT devices (e.g., rescue robots, UAVs, and mobile devices) becomes an available and rapidly deployable communication approach for search and rescue operations. However, Age of Information (AoI), an emerging network performance metric, has not been comprehensively investigated in this multi-hop model. In this paper, we first construct a UAV-relayed wireless network model and formulate the end-to-end instant AoI. Then we derive the optimal location of the relay UAV to achieve the minimum instant AoI by mathematical analysis. Simulations show that the derived relay location can always guarantee the optimal AoI and outperform other schemes.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wormhole Detection Based on Z-Score And Neighbor Table Comparison</title>
<link>https://arxiv.org/abs/2505.09405</link>
<guid>https://arxiv.org/abs/2505.09405</guid>
<content:encoded><![CDATA[
<div> Wormhole attacks, disaster rescue opportunity networks, third-party auditor, Z-Score data processing method, detection method<br />
<br />
Summary: Wormhole attacks can disrupt network topology in disaster rescue networks, leading to traffic analysis, DoS, and packet loss attacks. This study proposes a detection method using rescue equipment as a third-party auditor and the Z-Score data processing method for statistical analysis. Simulations validate the effectiveness of the approach, which does not rely on GPS or timers, making it economically valuable and practical for disaster relief. <div>
arXiv:2505.09405v1 Announce Type: cross 
Abstract: Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.
  By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks. This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.) as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks. Finally, we perform a large number of simulations to evaluate the proposed method. Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Asymptotically Optimal Approximation Algorithm for Multiobjective Submodular Maximization at Scale</title>
<link>https://arxiv.org/abs/2505.09525</link>
<guid>https://arxiv.org/abs/2505.09525</guid>
<content:encoded><![CDATA[
<div> algorithm, multiobjective submodular maximization, approximation guarantee, fair centrality maximization, experimental evaluation

Summary:
This work addresses the problem of maximizing multiple submodular functions simultaneously, known as multiobjective submodular maximization. Existing algorithms either provide weak guarantees or rely on expensive evaluations, making them impractical. The authors introduce a scalable algorithm that achieves the best-known approximation guarantee for this problem. They also propose a novel application  fair centrality maximization  which can be solved through multiobjective submodular maximization. Experimental results demonstrate that the new algorithm outperforms existing ones in both objective value and running time. Overall, this work fills a gap in the field of combinatorial optimization by providing a practical and efficient solution for maximizing the minimum over multiple submodular functions.<br /><br />Summary: <div>
arXiv:2505.09525v1 Announce Type: cross 
Abstract: Maximizing a single submodular set function subject to a cardinality constraint is a well-studied and central topic in combinatorial optimization. However, finding a set that maximizes multiple functions at the same time is much less understood, even though it is a formulation which naturally occurs in robust maximization or problems with fairness considerations such as fair influence maximization or fair allocation.
  In this work, we consider the problem of maximizing the minimum over many submodular functions, which is known as multiobjective submodular maximization. All known polynomial-time approximation algorithms either obtain a weak approximation guarantee or rely on the evaluation of the multilinear extension. The latter is expensive to evaluate and renders such algorithms impractical. We bridge this gap and introduce the first scalable and practical algorithm that obtains the best-known approximation guarantee. We furthermore introduce a novel application fair centrality maximization and show how it can be addressed via multiobjective submodular maximization. In our experimental evaluation, we show that our algorithm outperforms known algorithms in terms of objective value and running time.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Susceptibility Paradox in Online Social Influence</title>
<link>https://arxiv.org/abs/2406.11553</link>
<guid>https://arxiv.org/abs/2406.11553</guid>
<content:encoded><![CDATA[
<div> Keywords: susceptibility, online influence, social networks, peer influence dynamics, Generalized Friendship Paradox

Summary: 
- The study explores susceptibility to online influence within social networks, comparing influence-driven and spontaneous behaviors in content adoption.
- Influence-driven adoption shows high homophily, indicating that individuals prone to influence tend to connect with similarly susceptible peers, reinforcing peer influence dynamics.
- The Generalized Friendship Paradox is extended to influence-driven behaviors, revealing that users' friends are generally more susceptible to influence than the users themselves, establishing the Susceptibility Paradox online.
- Susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional user metadata.
- The findings shed light on the interplay between user engagement and characteristics in content adoption, offering insights for designing more effective moderation strategies to protect vulnerable audiences. 

<br /><br />Summary: <div>
arXiv:2406.11553v3 Announce Type: replace 
Abstract: Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics, whereas spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree Matrix Comparison for Graph Alignment</title>
<link>https://arxiv.org/abs/2411.07475</link>
<guid>https://arxiv.org/abs/2411.07475</guid>
<content:encoded><![CDATA[
<div> method, graph alignment, degree matrix comparison, unsupervised, geometric alignment

Summary:
Degree Matrix Comparison (DMC) is a new unsupervised geometric alignment method for heterogeneous networks that has shown promising results. Through experiments and mathematical proofs, DMC achieves high accuracy in node alignment, up to 99% for networks with overlapping nodes and 100% for isomorphic graphs. The proposed Greedy DMC reduces time complexity, while Weighted DMC shows potential for aligning weighted graphs. Positive results from applying these variations of DMC suggest the method's validity and efficacy. The sequence of DMC methods presents a reliable solution to the graph alignment problem, offering accurate node correspondence across networks. The simplicity and effectiveness of DMC make it a valuable addition to the field of graph alignment algorithms. 

<br /><br />Summary: <div>
arXiv:2411.07475v3 Announce Type: replace 
Abstract: The graph alignment problem, which considers the optimal node correspondence across networks, has recently gained significant attention due to its wide applications. There are graph alignment methods suited for various network types, but we focus on the unsupervised geometric alignment algorithms. We propose Degree Matrix Comparison (DMC), a very simple degree-based method that has shown to be effective for heterogeneous networks. Through extensive experiments and mathematical proofs, we demonstrate the potential of this method. Remarkably, DMC achieves up to 99% correct node alignment for 90%-overlap networks and 100% accuracy for isomorphic graphs. Additionally, we propose a reduced Greedy DMC with lower time complexity and Weighted DMC that has demonstrated potential for aligning weighted graphs. Positive results from applying Greedy DMC and the Weighted DMC furthermore speaks to the validity and potential of the DMC. The sequence of DMC methods could significantly impact graph alignment, offering reliable solutions for the task.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Politics: Prevalence, Spreaders, and Emotional Reception of AI-Generated Political Images on X</title>
<link>https://arxiv.org/abs/2502.11248</link>
<guid>https://arxiv.org/abs/2502.11248</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, social media discourse, Twitter, political images, platform governance <br />
Summary: 
This study examines the prevalence, spreaders, and reception of AI-generated political images on Twitter related to the 2024 U.S. Presidential Election. Approximately 12% of shared images are AI-generated, with 10% of users responsible for sharing the majority of them. AIGC superspreaders, who share a high volume of AI-generated images and receive significant engagement, are more likely to be X Premium subscribers with a right-leaning orientation and engage in automated behavior. They elicit more positive and less toxic responses than non-AI image tweets. The study sheds light on the role generative AI plays in shaping online socio-political environments and highlights the implications for platform governance. <br /> <div>
arXiv:2502.11248v2 Announce Type: replace 
Abstract: Despite widespread concerns about the risks of AI-generated content (AIGC) to the integrity of social media discourse, little is known about its scale and scope, the actors responsible for its dissemination online, and the user responses it elicits. In this work, we measure and characterize the prevalence, spreaders, and emotional reception of AI-generated political images. Analyzing a large-scale dataset from Twitter/X related to the 2024 U.S. Presidential Election, we find that approximately 12% of shared images are detected as AI-generated, and around 10% of users are responsible for sharing 80% of AI-generated images. AIGC superspreaders--defined as the users who not only share a high volume of AI-generated images but also receive substantial engagement through retweets--are more likely to be X Premium subscribers, have a right-leaning orientation, and exhibit automated behavior. Their profiles contain a higher proportion of AI-generated images than non-superspreaders, and some engage in extreme levels of AIGC sharing. Moreover, superspreaders' AI image tweets elicit more positive and less toxic responses than their non-AI image tweets. This study serves as one of the first steps toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Differential Privacy-Preserving Spectral Clustering for General Graphs</title>
<link>https://arxiv.org/abs/2309.06867</link>
<guid>https://arxiv.org/abs/2309.06867</guid>
<content:encoded><![CDATA[
<div> flipping probability, spectral clustering, differential privacy, stability, network clustering <br />
Summary: <br />
This study examines the stability of spectral clustering algorithms under local differential privacy for general graphs, departing from the assumption that networks are generated from the stochastic block model. The research focuses on the edge flipping method for privacy protection and finds that when edges in an n-vertex graph are flipped with a probability of O(log n/n), clustering outcomes remain consistent. Empirical tests support these theoretical results. It is shown that spectral clustering on well-clustered graphs may yield unstable results for flipping probabilities exceeding (log n/n), suggesting a privacy budget of (log n) for general graphs. This analysis provides valuable insights into the privacy-preserving capabilities of spectral clustering algorithms in real-world network data. <br /> <div>
arXiv:2309.06867v2 Announce Type: replace-cross 
Abstract: Spectral clustering is a widely used algorithm to find clusters in networks. Several researchers have studied the stability of spectral clustering under local differential privacy with the additional assumption that the underlying networks are generated from the stochastic block model (SBM). However, we argue that this assumption is too restrictive since social networks do not originate from the SBM. Thus, we delve into an analysis for general graphs in this work. Our primary focus is the edge flipping method -- a common technique for protecting local differential privacy. We show that, when the edges of an $n$-vertex graph satisfying some reasonable well-clustering assumptions are flipped with a probability of $O(\log n/n)$, the clustering outcomes are largely consistent. Empirical tests further corroborate these theoretical findings. Conversely, although clustering outcomes have been stable for non-sparse and well-clustered graphs produced from the SBM, we show that in general, spectral clustering may yield highly erratic results on certain well-clustered graphs when the flipping probability is $\omega(\log n/n)$. This indicates that the best privacy budget obtainable for general graphs is $\Theta(\log n)$.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analytical Emotion Framework of Rumour Threads on Social Media</title>
<link>https://arxiv.org/abs/2502.16560</link>
<guid>https://arxiv.org/abs/2502.16560</guid>
<content:encoded><![CDATA[
<div> Keywords: online social media, rumours, emotions, contagion, causality

Summary:
Rumours in online social media can have significant repercussions, necessitating a deeper understanding of their development. This study explores the relationship between emotions and rumours in threaded discussions, revealing a comprehensive emotion framework with multi-aspect detection. Through analysis of rumour and non-rumour threads, it is observed that rumours elicit more negative emotions like anger, fear, and pessimism, while non-rumours evoke positivity. Emotions are found to be contagious, with rumours spreading negativity and non-rumours spreading positivity. Causal analysis identifies surprise as a bridge between rumours and other emotions, with pessimism linked to sadness and fear, and optimism to joy and love. This research sheds light on the emotional dynamics of online social media threads, highlighting the impact of emotions on the spread of rumours. <div>
arXiv:2502.16560v2 Announce Type: replace-cross 
Abstract: Rumours in online social media pose significant risks to modern society, motivating the need for better understanding of how they develop. We focus specifically on the interface between emotion and rumours in threaded discourses, building on the surprisingly sparse literature on the topic which has largely focused on single aspect of emotions within the original rumour posts themselves, and largely overlooked the comparative differences between rumours and non-rumours. In this work, we take one step further to provide a comprehensive analytical emotion framework with multi-aspect emotion detection, contrasting rumour and non-rumour threads and provide both correlation and causal analysis of emotions. We applied our framework on existing widely-used rumour datasets to further understand the emotion dynamics in online social media threads. Our framework reveals that rumours trigger more negative emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive ones. Emotions are contagious, rumours spread negativity, non-rumours spread positivity. Causal analysis shows surprise bridges rumours and other emotions; pessimism comes from sadness and fear, while optimism arises from joy and love.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition</title>
<link>https://arxiv.org/abs/2505.08052</link>
<guid>https://arxiv.org/abs/2505.08052</guid>
<content:encoded><![CDATA[
<div> Keywords: computational model, Persian poetry, similarity network, poet influence, digital humanities 

Summary: 
This study presents a computational model to explore the dynamics of influence among classical Persian poets by creating a multi-dimensional similarity network. By utilizing various features such as semantic, lexical, stylistic, thematic, and metrical elements from Ganjoor's corpus, poets are represented within weighted similarity matrices. The aggregate graph generated from these matrices illustrates the interconnection of poets and their influence on each other. Key poets, style hubs, and bridging poets are identified through network investigation using centrality measures. Additionally, the Louvain community detection algorithm helps identify clusters of poets with shared stylistic and thematic coherence, mapping to recognized literary schools. This data-driven approach offers insights into both canonical figures and lesser-known poets holding structural significance in Persian literature. By combining computational linguistics with literary study, this research provides a scalable model for analyzing poetic tradition, facilitating retrospective analysis and future research within the arena of digital humanities. 

Summary: <br /><br /> <div>
arXiv:2505.08052v1 Announce Type: new 
Abstract: This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection on Noisy Stochastic Block Models</title>
<link>https://arxiv.org/abs/2505.08251</link>
<guid>https://arxiv.org/abs/2505.08251</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Noisy stochastic block models, Geometric noise, Erdos-Renyi model censoring

Summary:
DuoSpec algorithm addresses community detection in noisy stochastic block models, focusing on geometric noise and Erdos-Renyi model censoring. It aims to de-noise networks for better community recovery. The algorithm outperforms existing methods on noisy models, as demonstrated on synthetic data. Testing on the Amazon metadata dataset showed promising results for community detection. DuoSpec provides a solution for effectively identifying communities in networks affected by noise, showcasing its potential for practical application in various real-world scenarios. <br /><br />Summary: <div>
arXiv:2505.08251v1 Announce Type: new 
Abstract: We study the problem of community detection in noisy stochastic block models. We focus on two types of noise: (1) geometric noise where a latent-space kernel affects edge formation, and (2) Erdos-Renyi model censoring where edges are masked independently. We present a new algorithm DuoSpec that de-noises the network to a pristine stochastic block model structure for better community recovery. We demonstrate on synthetic data that our algorithm outperforms existing community detection methods on noisy models. We test our algorithm on the Amazon metadata dataset and demonstrate strong results on community detection.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Information Diffusion Beyond Explicit Social Ties: A Study of Implicit-Link Diffusion on Twitter</title>
<link>https://arxiv.org/abs/2505.08354</link>
<guid>https://arxiv.org/abs/2505.08354</guid>
<content:encoded><![CDATA[
<div> implicit-link diffusion, information propagation, social media, user engagement, information dissemination

Summary:
- Information diffusion on social media platforms goes beyond explicit social connections.
- Implicit links play a significant role in disseminating content across diverse communities.
- Users farther from the original source in the social network are more likely to engage in diffusion through implicit links.
- Implicit links contribute less to overall diffusion size than explicit links but have a distinct role.
- User groups exhibit strong patterns of social homophily in their choice of diffusion channel.

<br /><br />Summary: <div>
arXiv:2505.08354v1 Announce Type: new 
Abstract: Information diffusion on social media platforms is often assumed to occur primarily through explicit social connections, such as follower or friend relationships. However, information frequently propagates beyond these observable ties -- via external websites, search engines, or algorithmic recommendations -- forming implicit links between users who are not directly connected. Despite their potential impact, the mechanisms and characteristics of such implicit-link diffusion remain underexplored. In this study, we investigate the dynamics of nontrivial information diffusion mediated by implicit links on Twitter, using four large-scale datasets. We define implicit-link diffusion as the reposting of content by users who are not explicitly connected to the original poster. Our analysis reveals that users located farther from the original source in the social network are more likely to engage in diffusion through implicit links, suggesting that such links often arise from sources outside direct social relationships. Moreover, while implicit links contribute less to the overall diffusion size than explicit links, they play a distinct role in disseminating content across diverse and topologically distant communities. We further identify user groups who predominantly engage in diffusion through either explicit or implicit links, and demonstrate that the choice of diffusion channel exhibits strong patterns of social homophily. These findings underscore the importance of incorporating implicit-link dynamics into models of information diffusion and social influence.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A political cartography of news sharing: Capturing story, outlet and content level of news circulation on Twitter</title>
<link>https://arxiv.org/abs/2505.08359</link>
<guid>https://arxiv.org/abs/2505.08359</guid>
<content:encoded><![CDATA[
<div> political leaning, news sharing, digital platforms, online news circulation, research pipeline

Summary:
The article discusses the impact of news sharing on digital platforms and the importance of studying online news circulation among users with different political leanings. It critiques existing approaches for their simplified measures of political leaning and limited analysis of news sources and content. The researchers propose a new research pipeline to systematically map news sharing based on both source and content. They demonstrate through a proof of concept that news sharing diversifies along a second political dimension, outlets vary in their topics, and some outlets cater different news items to different audiences. This methodological contribution provides valuable insights that were previously overlooked in the study of news sharing behavior on digital platforms. 

<br /><br />Summary: <div>
arXiv:2505.08359v1 Announce Type: new 
Abstract: News sharing on digital platforms shapes the digital spaces millions of users navigate. Trace data from these platforms also enables researchers to study online news circulation. In this context, research on the types of news shared by users of differential political leaning has received considerable attention. We argue that most existing approaches (i) rely on an overly simplified measurement of political leaning, (ii) consider only the outlet level in their analyses, and/or (iii) study news circulation among partisans by making ex-ante distinctions between partisan and non-partisan news. In this methodological contribution, we introduce a research pipeline that allows a systematic mapping of news sharing both with respect to source and content. As a proof of concept, we demonstrate insights that otherwise remain unnoticed: Diversification of news sharing along the second political dimension; topic-dependent sharing of outlets; some outlets catering different items to different audiences.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News</title>
<link>https://arxiv.org/abs/2505.08532</link>
<guid>https://arxiv.org/abs/2505.08532</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, multi-agent system, large language models, debate process, interpretability

Summary: 
TruEDebate (TED) is a new approach for detecting fake news using a multi-agent system with large language models. The system simulates a formal debate setting where agents organize into teams to support or challenge the truth of news. This process includes opening statements, cross-examination, rebuttal, and closing statements, allowing for a thorough evaluation of news content. The DebateFlow Agents handle the debate process, while the InsightFlow Agents provide a synthesis and analysis of the debates. The Synthesis Agent summarizes the debates to provide an overarching viewpoint, while the Analysis Agent uses role embeddings and a debate graph to model interactions between debate roles and arguments, ultimately providing a final judgment on the news' truthfulness. TED aims to enhance interpretability and effectiveness in fake news detection by leveraging the reasoning abilities of large language models through a structured debate process. 

<br /><br />Summary: TruEDebate (TED) utilizes a multi-agent system with large language models for fake news detection. The system employs a debate process with DebateFlow and InsightFlow Agents to thoroughly evaluate news content. DebateFlow Agents organize agents into supporting and challenging teams, while InsightFlow Agents provide synthesis and analysis. The Synthesis Agent offers an overarching viewpoint, and the Analysis Agent uses role embeddings and a debate graph to make a final judgment. TED enhances interpretability in fake news detection by leveraging structured debate processes and the reasoning abilities of large language models. <div>
arXiv:2505.08532v1 Announce Type: new 
Abstract: In today's digital environment, the rapid propagation of fake news via social networks poses significant social challenges. Most existing detection methods either employ traditional classification models, which suffer from low interpretability and limited generalization capabilities, or craft specific prompts for large language models (LLMs) to produce explanations and results directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the saying that "truth becomes clearer through debate," our study introduces a novel multi-agent system with LLMs named TruEDebate (TED) to enhance the interpretability and effectiveness of fake news detection. TED employs a rigorous debate process inspired by formal debate settings. Central to our approach are two innovative components: the DebateFlow Agents and the InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where one supports and the other challenges the truth of the news. These agents engage in opening statements, cross-examination, rebuttal, and closing statements, simulating a rigorous debate process akin to human discourse analysis, allowing for a thorough evaluation of news content. Concurrently, the InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent and the Analysis Agent. The Synthesis Agent summarizes the debates and provides an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The Analysis Agent, which includes a role-aware encoder and a debate graph, integrates role embeddings and models the interactions between debate roles and arguments using an attention mechanism, providing the final judgment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer</title>
<link>https://arxiv.org/abs/2505.08330</link>
<guid>https://arxiv.org/abs/2505.08330</guid>
<content:encoded><![CDATA[
<div> Keywords: anomalous edges, dynamic graphs, structural-temporal coupling, graph transformer model, anomaly detection

Summary: 
This paper presents a novel approach for detecting anomalous edges in dynamic graphs by incorporating structural and temporal features through a dynamic graph transformer model. The lack of structural-temporal coupling information in existing methods is overcome by integrating features from two levels to capture anomaly-aware graph evolutionary patterns. The dynamic graph transformer, enhanced with positional encoding, effectively captures discrimination and contextual consistency signals. Experimental results on six datasets show that the proposed method outperforms current state-of-the-art models in anomaly detection. A case study further demonstrates the effectiveness of the approach in a real-world application. The model's ability to distinguish anomalies from normal instances in evolving triple-based data sets, such as social networks and transaction management, makes it a valuable tool for various applications requiring anomaly detection in dynamic graphs. 

<br /><br />Summary: <div>
arXiv:2505.08330v1 Announce Type: cross 
Abstract: Detecting anomalous edges in dynamic graphs is an important task in many applications over evolving triple-based data, such as social networks, transaction management, and epidemiology. A major challenge with this task is the absence of structural-temporal coupling information, which decreases the ability of the representation to distinguish anomalies from normal instances. Existing methods focus on handling independent structural and temporal features with embedding models, which ignore the deep interaction between these two types of information. In this paper, we propose a structural-temporal coupling anomaly detection architecture with a dynamic graph transformer model. Specifically, we introduce structural and temporal features from two integration levels to provide anomaly-aware graph evolutionary patterns. Then, a dynamic graph transformer enhanced by two-dimensional positional encoding is implemented to capture both discrimination and contextual consistency signals. Extensive experiments on six datasets demonstrate that our method outperforms current state-of-the-art models. Finally, a case study illustrates the strength of our method when applied to a real-world task.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2505.08464</link>
<guid>https://arxiv.org/abs/2505.08464</guid>
<content:encoded><![CDATA[
<div> Keywords: Stance detection, Large Language Models, Contextual understanding, Multimodal analysis, Benchmark datasets

Summary: 
This review article explores the use of Large Language Models (LLMs) in stance detection across various platforms. It examines foundational concepts, methodologies, datasets, and applications of LLMs in stance detection. A taxonomy for LLM-based approaches is presented, focusing on learning methods, data modalities, and target relationships. Evaluation techniques, benchmark datasets, and performance trends are analyzed. Key applications include misinformation detection, political analysis, and social media moderation. Challenges such as implicit stance expression and cultural biases are identified. Promising future directions like explainable stance reasoning and low-resource adaptation are discussed, along with the need for real-time deployment frameworks. The survey provides insights into emerging trends, open challenges, and future directions for researchers and practitioners in developing advanced stance detection systems powered by LLMs. 

<br /><br />Summary: <div>
arXiv:2505.08464v1 Announce Type: cross 
Abstract: Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Big Data and the Computational Social Science of Entrepreneurship and Innovation</title>
<link>https://arxiv.org/abs/2505.08706</link>
<guid>https://arxiv.org/abs/2505.08706</guid>
<content:encoded><![CDATA[
<div> Keywords: large-scale social data, machine-learning methods, innovation, entrepreneurship, artificial intelligence

Summary: 
This chapter discusses the challenges and opportunities faced by scholars of entrepreneurship and innovation with the explosion of large-scale social data and advancements in machine-learning methods. It explores the difficulties in identifying technological and commercial novelty, documenting new venture origins, and forecasting competition between new technologies and commercial forms. The chapter suggests leveraging new text, network, image, audio, and video data to advance research in innovation and entrepreneurship. It highlights the use of machine-learning models and big data to create precision measurements as observatories of innovation and entrepreneurship on a societal level. Additionally, artificial intelligence models fueled by big data can generate 'digital doubles' of technology and business for virtual experimentation. By combining big data with big models, the chapter argues for the advancement of theory development and testing in entrepreneurship and innovation. <br /><br />Summary: <div>
arXiv:2505.08706v1 Announce Type: cross 
Abstract: As large-scale social data explode and machine-learning methods evolve, scholars of entrepreneurship and innovation face new research opportunities but also unique challenges. This chapter discusses the difficulties of leveraging large-scale data to identify technological and commercial novelty, document new venture origins, and forecast competition between new technologies and commercial forms. It suggests how scholars can take advantage of new text, network, image, audio, and video data in two distinct ways that advance innovation and entrepreneurship research. First, machine-learning models, combined with large-scale data, enable the construction of precision measurements that function as system-level observatories of innovation and entrepreneurship across human societies. Second, new artificial intelligence models fueled by big data generate 'digital doubles' of technology and business, forming laboratories for virtual experimentation about innovation and entrepreneurship processes and policies. The chapter argues for the advancement of theory development and testing in entrepreneurship and innovation by coupling big data with big models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance</title>
<link>https://arxiv.org/abs/2402.16670</link>
<guid>https://arxiv.org/abs/2402.16670</guid>
<content:encoded><![CDATA[
<div> market, attention, advertising, detrimental, principles

Summary:
The article discusses how attention has become a valuable commodity in the economic market, with web platforms using various techniques to capture it on a large scale. This has led to negative consequences such as polarizing opinions, spreading false information, and jeopardizing public health and democracies. The paper combines insights from psychology, sociology, and neuroscience to analyze the current practices and their effects. It proposes a set of principles and calls for action to address the detrimental impact of attention-capturing practices on the web. The aim is to prevent the wastage of attention on a global scale, as it is unsustainable for society to allow such practices to continue unchecked. <div>
arXiv:2402.16670v2 Announce Type: replace 
Abstract: Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral and Topological Heterogeneities in Network Versions of Schelling's Segregation Model</title>
<link>https://arxiv.org/abs/2408.05623</link>
<guid>https://arxiv.org/abs/2408.05623</guid>
<content:encoded><![CDATA[
<div> agent-based models, residential segregation, heterogeneity, preferences, topologies
Summary:<br />
- Agent-based models are used to study residential segregation dynamics. 
- Previous studies have focused on the impact of individual preferences and social network structures on segregation levels. 
- Combining heterogeneous preferences and network topologies in simulations leads to reduced segregation levels. 
- Increased representation of both types of heterogeneities results in a wider range of segregation outcomes. 
- A new dynamic of segregation emerges with highly tolerant nodes clustering in dense areas and intolerant nodes relocating to sparse regions, resembling an urban-rural divide.<br /> <div>
arXiv:2408.05623v4 Announce Type: replace-cross 
Abstract: Agent-based models of residential segregation have been of persistent interest to various research communities since their origin with James Sakoda and popularization by Thomas Schelling. Frequently, these models have sought to elucidate the extent to which the collective dynamics of individual preferences may cause segregation to emerge. This open question has sustained relevance in U.S. jurisprudence. Previous investigation of heterogeneity of behaviors (preferences) has shown reductions in segregation. Meanwhile, previous investigation of heterogeneity of social network topologies has shown no significant impact to observed segregation levels. In the present study, we examined the effects of the concurrent presence of both behavioral and topological heterogeneities in network segregation models. Simulations were conducted using both homogeneous and heterogeneous preference models on 2D lattices with varied levels of densification to create topological heterogeneities (i.e., clusters, hubs). Results show a richer variety of outcomes, including novel differences in resultant segregation levels and hub composition. Notably, with concurrent increased representations of heterogeneous preferences and heterogeneous topologies, reduced levels of segregation emerge. Simultaneously, we observe a novel dynamic of segregation between tolerance levels as highly tolerant nodes take residence in dense areas and push intolerant nodes to sparse areas mimicking the urban-rural divide.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenSky Report 2025: Improving Crowdsourced Flight Trajectories with ADS-C Data</title>
<link>https://arxiv.org/abs/2505.06254</link>
<guid>https://arxiv.org/abs/2505.06254</guid>
<content:encoded><![CDATA[
<div> ADS-B, ADS-C, OpenSky Network, air traffic surveillance data, trajectory reconstruction

Summary: 
The OpenSky Network has expanded its data collection to include ADS-C messages via satellite communication, enhancing coverage over oceans and remote regions. By combining ADS-B and ADS-C data, detailed long-haul flight paths, particularly for transatlantic and African routes, can be accurately reconstructed. This integration improves trajectory accuracy, leading to better estimates of fuel consumption and emissions. The combined data also showcases flight patterns in previously underrepresented regions in Africa. Despite coverage limitations, this advancement in providing open access to global flight trajectory data opens up new research possibilities in areas such as air traffic management, environmental impact assessment, and aviation safety.<br /><br />Summary: <div>
arXiv:2505.06254v1 Announce Type: new 
Abstract: The OpenSky Network has been collecting and providing crowdsourced air traffic surveillance data since 2013. The network has primarily focused on Automatic Dependent Surveillance--Broadcast (ADS-B) data, which provides high-frequency position updates over terrestrial areas. However, the ADS-B signals are limited over oceans and remote regions, where ground-based receivers are scarce. To address these coverage gaps, the OpenSky Network has begun incorporating data from the Automatic Dependent Surveillance--Contract (ADS-C) system, which uses satellite communication to track aircraft positions over oceanic regions and remote areas. In this paper, we analyze a dataset of over 720,000 ADS-C messages collected in 2024 from around 2,600 unique aircraft via the Alphasat satellite, covering Europe, Africa, and parts of the Atlantic Ocean. We present our approach to combining ADS-B and ADS-C data to construct detailed long-haul flight paths, particularly for transatlantic and African routes. Our findings demonstrate that this integration significantly improves trajectory reconstruction accuracy, allowing for better fuel consumption and emissions estimates. We illustrate how combined data captures flight patterns across previously underrepresented regions across Africa. Despite coverage limitations, this work marks an important advancement in providing open access to global flight trajectory data, enabling new research opportunities in air traffic management, environmental impact assessment, and aviation safety.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation</title>
<link>https://arxiv.org/abs/2505.06612</link>
<guid>https://arxiv.org/abs/2505.06612</guid>
<content:encoded><![CDATA[
<div> Keywords: social recommendation, multi-semantic modeling, graph convolutional network, tensor convolutional network, Bayesian posterior probability

<br /><br />Summary: In today's rapidly evolving social media landscape, social recommendation systems are increasingly being used as hybrid recommendation solutions. Traditional methods often focus on user similarity, which can lead to the exclusion of relevant relationships and reduce accuracy. This study aims to improve social recommendations by examining the interplay of semantic information across social networks and user-item interaction networks. To this end, a novel model named Burger is introduced, which features robust graph denoising-augmentation fusion and multi-semantic modeling. The approach begins with constructing a social tensor to enhance training efficiency. It employs both graph convolutional and tensor convolutional networks to distinguish between user-item preferences and social preferences. A bi-semantic coordination loss is introduced to capture the mutual influence of semantic information from different domains. Furthermore, Bayesian posterior probability is used to identify potential social relations and mitigate the impact of irrelevant connections. Finally, the model utilizes a sliding window mechanism to keep the social tensor updated for subsequent iterations. Experiments across three real datasets demonstrate that Burger outperforms existing state-of-the-art models in terms of accuracy and effectiveness. <div>
arXiv:2505.06612v1 Announce Type: new 
Abstract: In the era of rapid development of social media, social recommendation systems as hybrid recommendation systems have been widely applied. Existing methods capture interest similarity between users to filter out interest-irrelevant relations in social networks that inevitably decrease recommendation accuracy, however, limited research has a focus on the mutual influence of semantic information between the social network and the user-item interaction network for further improving social recommendation. To address these issues, we introduce a social \underline{r}ecommendation model with ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly propose to construct a social tensor in order to smooth the training process of the model. Then, a graph convolutional network and a tensor convolutional network are employed to capture user's item preference and social preference, respectively. Considering the different semantic information in the user-item interaction network and the social network, a bi-semantic coordination loss is proposed to model the mutual influence of semantic information. To alleviate the interference of interest-irrelevant relations on multi-semantic modeling, we further use Bayesian posterior probability to mine potential social relations to replace social noise. Finally, the sliding window mechanism is utilized to update the social tensor as the input for the next iteration. Extensive experiments on three real datasets show Burger has a superior performance compared with the state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and Study of t , Peak and Effective Diameter in Temporal Networks</title>
<link>https://arxiv.org/abs/2505.06719</link>
<guid>https://arxiv.org/abs/2505.06719</guid>
<content:encoded><![CDATA[
<div> keywords: temporal networks, diameter, effective diameter, peak diameter, t-Diameter<br />
Summary:<br />
Understanding the spread of information, diseases, and influence in temporal networks is a complex challenge. This study introduces a mathematical framework to analyze diameter in temporal networks, presenting three new metrics: Effective Diameter, Peak Diameter, and t-Diameter. The study combines theoretical analysis with empirical validation using real-world datasets. Results show high accuracy of the model, with effective diameter decreasing with increasing average degree and increasing with network size. t-Diameter and Peak Diameter are more sensitive to node removal, making them valuable for epidemic modeling. The framework bridges formal modeling and empirical data, offering insights into the temporal dynamics of networked systems and providing tools for assessing robustness, controlling information spread, and optimizing interventions. <br /><br /> <div>
arXiv:2505.06719v1 Announce Type: new 
Abstract: Understanding how information, diseases, or influence spread across networks is a fundamental challenge in complex systems. While network diameter has been extensively studied in static networks, its definition and behavior in temporal networks remain underexplored due to their dynamic nature. In this study, we present a formal mathematical framework for analyzing diameter in temporal networks and introduce three time-aware metrics: Effective Diameter , Peak Diameter (*D), and t-Diameter (tD), each capturing distinct temporal aspects of connectivity and diffusion. Our approach combines theoretical analysis with empirical validation using four real-world datasets: high school, hospital, conference, and workplace contact networks. We simulate flow propagation on temporal networks and compare the observed diameters with the proposed theoretical Equations. Across all datasets, our model demonstrates high accuracy, with low RMSE and absolute error values. Furthermore, we observe that the effective diameter decreases with increasing average degree and increases with network size. The results also show that tD and *D are more sensitive to node removal, highlighting their relevance for applications such as epidemic modeling. By bridging formal modeling and empirical data, our framework offers new insights into the temporal dynamics of networked systems and provides tools for assessing robustness, controlling information spread, and optimizing interventions in time-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Robustness and Reducibility of Multiplex Networks with Embedding-Aided Interlayer Similarities</title>
<link>https://arxiv.org/abs/2505.06998</link>
<guid>https://arxiv.org/abs/2505.06998</guid>
<content:encoded><![CDATA[
<div> Keywords: interlayer similarity, multiplex networks, EATSim, network robustness, structural similarity

<br /><br />Summary: The article discusses the significance of interlayer similarity in multiplex networks to understand the complexities of interconnected systems. This study reveals how variations in one network layer influence others, with implications in transportation, social, and biological contexts. Existing algorithms for measuring interlayer similarity are limited as they capture only partial information, hindering a complete understanding of multiplex networks. To overcome this, the authors introduce a new approach called Embedding Aided inTerlayer Similarity (EATSim), which integrates both intralayer structural similarity and cross-layer anchor node alignment consistency. This comprehensive framework offers a better analysis of interconnected networks. Extensive experiments conducted on synthetic and real-world datasets indicate that EATSim effectively captures the geometry of similarities between layers, leading to improved accuracy in interlayer similarity measurement. Furthermore, EATSim showcases state-of-the-art performance in two applications: predicting network robustness and network reducibility. These results demonstrate the approach's potential in enhancing the understanding and management of complex systems, making EATSim a valuable tool for researchers studying multiplex networks. <div>
arXiv:2505.06998v1 Announce Type: new 
Abstract: The study of interlayer similarity of multiplex networks helps to understand the intrinsic structure of complex systems, revealing how changes in one layer can propagate and affect others, thus enabling broad implications for transportation, social, and biological systems. Existing algorithms that measure similarity between network layers typically encode only partial information, which limits their effectiveness in capturing the full complexity inherent in multiplex networks. To address this limitation, we propose a novel interlayer similarity measuring approach named Embedding Aided inTerlayer Similarity (EATSim). EATSim concurrently incorporates intralayer structural similarity and cross-layer anchor node alignment consistency, providing a more comprehensive framework for analyzing interconnected systems. Extensive experiments on both synthetic and real-world networks demonstrate that EATSim effectively captures the underlying geometric similarities between interconnected networks, significantly improving the accuracy of interlayer similarity measurement. Moreover, EATSim achieves state-of-the-art performance in two downstream applications: predicting network robustness and network reducibility, showing its great potential in enhancing the understanding and management of complex systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Language of Influence: Sentiment, Emotion, and Hate Speech in State Sponsored Influence Operations</title>
<link>https://arxiv.org/abs/2505.07212</link>
<guid>https://arxiv.org/abs/2505.07212</guid>
<content:encoded><![CDATA[
<div> sentiment, emotion, abusive speech, social media, influence operations <br />
Summary: <br />
This study examines state-sponsored influence operations (SIOs) on social media from China, Iran, and Russia. Analyzing 1.5 million tweets, the research uncovers different patterns in sentiment, emotion, and abusive speech used by these campaigns. Russian SIOs predominantly employ negative sentiment and toxic language to polarize audiences. Iranian operations balance negative and positive tones to provoke hostility and garner support. Meanwhile, Chinese campaigns focus on positive messaging to promote favorable narratives. These findings highlight the diverse strategies employed by countries in influencing public opinion through social media platforms. <div>
arXiv:2505.07212v1 Announce Type: new 
Abstract: State-sponsored influence operations (SIOs) on social media have become an instrumental tool for manipulating public opinion and spreading unverified information. This study analyzes the sentiment, emotion, and abusive speech in tweets circulated by influence campaigns originating from three distinct countries: China, Iran, and Russia. We examined 1.5 million tweets to uncover patterns in the content of the influence operations using the dataset provided by Twitter. Our findings reveal distinct patterns of sentiment, emotion, and abusive nature in different SIOs. Our experimental result shows that Russian influence operations predominantly employ negative sentiment and toxic language to polarize audiences, Iranian operations balance negative and positive tones to provoke hostility while fostering support, and Chinese campaigns focus on positive messaging to promote favorable narratives.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Ethics in the Fediverse: Analyzing the Role of Instance Policies in Mastodon Research</title>
<link>https://arxiv.org/abs/2505.07606</link>
<guid>https://arxiv.org/abs/2505.07606</guid>
<content:encoded><![CDATA[
<div> Keywords: Mastodon, data collection, research ethics, policy adherence, decentralized platforms  

<br /><br />Summary: This article examines the disconnection between the individual policies of Mastodon instances, many of which explicitly ban data collection for research purposes, and the actual practices observed in academic research that utilizes Mastodon data. The authors conducted a systematic analysis of 29 studies that sourced data from Mastodon, discovering a notable lack of compliance with the declared policies of these instances, despite researchers generally being aware of their existence. The findings highlight the ethical dilemmas surrounding the use of data collected from decentralized social media platforms, pointing to a gap in the adherence to ethical guidelines. Furthermore, the article calls for a more extensive discourse on researchers' ethical responsibilities when conducting research on alternative social media platforms like Mastodon. It emphasizes the importance of respecting user privacy and instance-specific rules, advocating for a reevaluation of current practices to foster ethical research approaches that align with community guidelines. This dialogue is crucial for ensuring that the academic use of data from decentralized social networks does not undermine the very principles of privacy and trust that these platforms aim to uphold. <div>
arXiv:2505.07606v1 Announce Type: new 
Abstract: This article addresses the disconnect between the individual policy documents of Mastodon instances--many of which explicitly prohibit data collection for research purposes--and the actual data handling practices observed in academic research involving Mastodon. We present a systematic analysis of 29 works that used Mastodon as a data source, revealing limited adherence to instance--level policies despite researchers' general awareness of their existence. Our findings underscore the need for broader discussion about ethical obligations in research on alternative, decentralized social media platforms.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'Congratulations, morons': Dynamics of Toxicity and Interaction Polarization in the Covid Vaccination and Ukraine War Twitter Debates</title>
<link>https://arxiv.org/abs/2505.07646</link>
<guid>https://arxiv.org/abs/2505.07646</guid>
<content:encoded><![CDATA[
<div> polarization, echo chambers, social media, Twitter datasets, toxicity <br />
Summary: 
The study explores polarization dynamics in social media discussions around Covid-19 vaccination and the Ukraine war by analyzing two large Twitter datasets. The analysis focuses on influencer preferences and toxicity of post contents. By examining retweet behavior and clustering users based on ideological preferences, the study identifies ideological opposition and temporal associations between toxicity and structural divergence. The findings suggest that polarization is a multifaceted dynamic phenomenon that can manifest within a single ideological camp. The study highlights the importance of considering polarization as an evolving process that may lead to unexpected outcomes in information diffusion spaces. The research provides insights into understanding how polarization influences partisan behavior on social media platforms. <br /> <div>
arXiv:2505.07646v1 Announce Type: new 
Abstract: The existence of polarization and echo chambers has been noted in social media discussions of public concern such as the Covid-19 pandemic, foreign election interference, and regional conflicts. However, measuring polarization and assessing the manner in which polarization contributes to partisan behavior is not always possible to evaluate with static network or affect measurements. To address this, we conduct an analysis of two large Twitter datasets collected around Covid-19 vaccination and the Ukraine war to investigate polarization in terms of the evolution in influencer preferences and toxicity of post contents. By reducing retweet behavior in each sample to several key dimensions, we identify clusters that reflect ideological preferences, along with geographic or linguistic separation for some cases. By tracking the central retweet tendency of these clusters over time, we observe differences in the relative position of ideologically unaligned clusters compared to aligned ones, which we interpret as reflecting polarization dynamics in the information diffusion space. We then measure the toxicity of posts and test if toxicity in one cluster can be temporally dependent on its structural closeness to (or toxicity of) another. We find evidence of ideological opposition among clusters of users in both samples, and a temporal association between toxicity and structural divergence for at least two ideologically opposed clusters in our samples. These observations support the importance of analyzing polarization as a multifaceted dynamic phenomenon where polarization dynamics may also manifest in unexpected ways such as within a single ideological camp.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity</title>
<link>https://arxiv.org/abs/2505.06313</link>
<guid>https://arxiv.org/abs/2505.06313</guid>
<content:encoded><![CDATA[
<div> sentiments, NATO unity, NATO Article 5 trust, GPT models, qualitative analytics

Summary:
The paper explores the use of GPT models with retrieval-augmented generation (RAG) to analyze sentiments, NATO unity, and NATO Article 5 trust opinions in various web sources. Utilizing the GPT-4.1 model, qualitative news summaries and quantitative opinion scores were generated for NATO-related topics. Bayesian regression was employed to analyze the opinion score trends, revealing a downward trend in opinions related to NATO unity. The study does not serve as political analysis but showcases AI-based approaches for further analytics. Additionally, a dynamic model based on neural ordinary differential equations was considered for understanding evolving public opinions. The results indicate that utilizing GPT models for news analysis can offer valuable qualitative and quantitative insights for comprehensive analytical approaches. <div>
arXiv:2505.06313v1 Announce Type: cross 
Abstract: The paper considers the use of GPT models with retrieval-augmented generation (RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity and NATO Article 5 trust opinion scores in different web sources: news sites found via Google Search API, Youtube videos with comments, and Reddit discussions. A RAG approach using GPT-4.1 model was applied to analyse news where NATO related topics were discussed. Two levels of RAG analytics were used: on the first level, the GPT model generates qualitative news summaries and quantitative opinion scores using zero-shot prompts; on the second level, the GPT model generates the summary of news summaries. Quantitative news opinion scores generated by the GPT model were analysed using Bayesian regression to get trend lines. The distributions found for the regression parameters make it possible to analyse an uncertainty in specified news opinion score trends. Obtained results show a downward trend for analysed scores of opinion related to NATO unity.
  This approach does not aim to conduct real political analysis; rather, it consider AI based approaches which can be used for further analytics
  as a part of a complex analytical approach. The obtained results demonstrate that the use of GPT models for news analysis can give informative qualitative and quantitative analytics, providing important insights.
  The dynamic model based on neural ordinary differential equations was considered for modelling public opinions. This approach makes it possible to analyse different scenarios for evolving public opinions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK</title>
<link>https://arxiv.org/abs/2505.06782</link>
<guid>https://arxiv.org/abs/2505.06782</guid>
<content:encoded><![CDATA[
<div> approaches, regulation, electronic cigarettes, Australia, UK <br />
Summary:<br />
- Australia and the UK have contrasting approaches to the regulation of electronic cigarettes, with Australia being more restrictive and the UK more permissive.<br />
- Both countries developed their policies based on the same evidence base.<br />
- A Large Language Model-based classifier was used to analyze electronic cigarette-related policy documents from Australia and the UK.<br />
- The classifier showed that Australian documents contain more harmful statements about e-cigarettes, while UK documents emphasize the benefits.<br />
- This study highlights how different jurisdictions manage and present evidence in shaping their policies on electronic cigarettes, demonstrating the potential of using LLM-based methods in health policy formation. <br />Summary: <div>
arXiv:2505.06782v1 Announce Type: cross 
Abstract: Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When cardinals strategize: An agent-based model of influence and ideology for the papal conclave</title>
<link>https://arxiv.org/abs/2505.07014</link>
<guid>https://arxiv.org/abs/2505.07014</guid>
<content:encoded><![CDATA[
<div> agent-based models, papal conclaves, social influence, strategic voting, ideological alignment
<br />
In this study, two agent-based models are proposed to explore the dynamics of papal conclaves. The models consider how social influence, strategic voting, and ideological alignment influence the time taken to elect a pope. The first model includes mechanisms where cardinals imitate peers' choices and shift support to the most voted candidate from the previous round. Strategic behavior is introduced through "useful voting", where agents switch to the most viable alternative if their preferred candidate receives insufficient votes. The second model incorporates ideological blocs, with cardinals and candidates grouped as progressives or conservatives. Numerical simulations show that ideological polarization can prolong the election process. However, the quick outcome of the 2025 conclave suggests that informal consensus-building before voting could expedite convergence. These results emphasize the importance of strategic flexibility and ideological structure in collective decision-making during papal conclaves. 
<br /><br />Summary: <div>
arXiv:2505.07014v1 Announce Type: cross 
Abstract: We propose and analyze two agent-based models to investigate the dynamics of papal conclaves, focusing on how social influence, strategic voting, and ideological alignment affect the time required to elect a pope. In the first model, cardinals interact through two mechanisms: with probability $p$, they imitate the choice of a randomly selected peer, and with probability $q$, they shift support to the most voted candidate from the previous round. Additionally, strategic behavior is introduced via ``useful voting'', where agents abandon their preferred candidate if he receives less than a certain fraction of the votes, switching instead to the most viable alternative. A candidate must secure a qualified majority of two-thirds to be elected. After that, we extend the framework by incorporating ideological blocs, assigning each cardinal and candidate to one of two groups (e.g., progressives and conservatives). Cardinals initially vote for candidates from their own group, but may cross ideological lines due to strategic considerations. We initialize the population with $20\%$ conservative cardinals, reflecting the current composition shaped by papal appointments. Numerical simulations show that ideological polarization can delay the election, increasing the average number of voting rounds. Our results highlight the crucial role of both strategic flexibility and ideological structure in collective decision-making under conditions found in papal conclaves. The recent rapid outcome of the 2025 conclave, despite a polarized electorate, suggests that informal consensus-building - possibly prior to voting - may play a decisive role in accelerating convergence, complementing the mechanisms explored in our model.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Unconstrained Local Search for Partitioning Irregular Graphs</title>
<link>https://arxiv.org/abs/2308.15494</link>
<guid>https://arxiv.org/abs/2308.15494</guid>
<content:encoded><![CDATA[
<div> Keywords: balanced graph partitioning, local search, solution quality, parallel scalability, irregular graphs <br />
Summary: <br />
In this paper, new refinement heuristics for the balanced graph partitioning problem are introduced, challenging the traditional rule of only allowing moves that maintain balanced block sizes. By permitting temporary balance violations, significant improvements in solution quality, especially for irregular instances like social networks, are achieved. Efficient implementations involve carefully selecting candidates for unconstrained moves and developing algorithms for rebalancing later on. The study explores various design choices to achieve high parallel scalability. Experimental results show that the parallel unconstrained local search techniques outperform existing solvers by a large margin, finding 75% of the best solutions on irregular graphs. Additionally, a 9.6% improvement in edge cut over the next best competitor is achieved, while being only 7.7% slower in the geometric mean. <br />

Summary: <br /> <div>
arXiv:2308.15494v3 Announce Type: replace 
Abstract: We present new refinement heuristics for the balanced graph partitioning problem that break with an age-old rule. Traditionally, local search only permits moves that keep the block sizes balanced (below a size constraint). In this work, we demonstrate that admitting large temporary balance violations drastically improves solution quality. The effects are particularly strong on irregular instances such as social networks. Designing efficient implementations of this general idea involves both careful selection of candidates for unconstrained moves as well as algorithms for rebalancing the solution later on. We explore a wide array of design choices to achieve this, in addition to our third goal of high parallel scalability. We present compelling experimental results, demonstrating that our parallel unconstrained local search techniques outperform the prior state of the art by a substantial margin. Compared with four state-of-the-art solvers, our new technique finds 75\% of the best solutions on irregular graphs. We achieve a 9.6\% improvement in edge cut over the next best competitor, while being only 7.7\% slower in the geometric mean.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dynamics of Collective Creativity in Human-AI Social Networks</title>
<link>https://arxiv.org/abs/2502.17962</link>
<guid>https://arxiv.org/abs/2502.17962</guid>
<content:encoded><![CDATA[
<div> AI, generative AI, collective creativity, experimental social networks, creative writing task <br />
Summary: This study explores the impact of generative AI on collective creativity in experimental social networks. Large-scale online experiments involving 879 participants and AI agents in a creative writing task revealed that AI-only networks initially displayed greater creativity and diversity than human-only and human-AI networks. However, over time, hybrid human-AI networks surpassed AI-only networks in diversity. This shift was attributed to AI agents retaining little from the original stories, while human-only networks maintained continuity. The findings emphasize the significance of human-AI interactions in shaping creativity within experimental social networks and shed light on the dynamic nature of human-AI hybrid societies.<br /><br /> <div>
arXiv:2502.17962v2 Announce Type: replace 
Abstract: Generative AI is reshaping modern culture, enabling individuals to create high-quality outputs across domains such as images, text, and music. However, we know little about the impact of generative AI on collective creativity. This study investigates how human-AI interactions shape collective creativity within experimental social networks. We conducted large-scale online experiments with 879 participants and AI agents in a creative writing task. Participants (either humans or AI) joined 5x5 grid-based networks, and were asked to iteratively select, modify, and share stories. Initially, AI-only networks showed greater creativity (rated by a separate group of 94 human raters) and diversity than human-only and human-AI networks. However, over time, hybrid human-AI networks became more diverse in their creations than AI-only networks. In part, this is because AI agents retained little from the original stories, while human-only networks preserved continuity. These findings highlight the value of experimental social networks in understanding human-AI hybrid societies.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global decomposition of networks into multiple cores formed by local hubs</title>
<link>https://arxiv.org/abs/2407.00355</link>
<guid>https://arxiv.org/abs/2407.00355</guid>
<content:encoded><![CDATA[
<div> Keywords: network decomposition, core-periphery structure, hub centrality, edge pruning, hierarchical structure

Summary: 
The article introduces a novel network decomposition scheme that uncovers multiscale core-periphery structures within networks by utilizing locally defined nodal hub centrality and edge-pruning techniques. The method identifies breaking points in network decomposition by removing locally least connected nodes, revealing an onion-like hierarchical structure. Unlike the traditional k-core decomposition method, this approach focuses on relative information within local structures, effectively highlighting locally crucial substructures. Additionally, the method can detect multiple core-periphery structures and decompose coarse-grained supernode networks by integrating it with network community detection. This innovative approach provides a more detailed and insightful analysis of network structures, offering a valuable tool for studying complex networks in various fields. 

<br /><br />Summary: <div>
arXiv:2407.00355v3 Announce Type: replace-cross 
Abstract: Networks are ubiquitous in various fields, representing systems where nodes and their interconnections constitute their intricate structures. We introduce a network decomposition scheme to reveal multiscale core-periphery structures lurking inside, using the concept of locally defined nodal hub centrality and edge-pruning techniques built upon it. We demonstrate that the hub-centrality-based edge pruning reveals a series of breaking points in network decomposition, which effectively separates a network into its backbone and shell structures. Our local-edge decomposition method iteratively identifies and removes locally least connected nodes, and uncovers an onion-like hierarchical structure as a result. Compared with the conventional $k$-core decomposition method, our method based on relative information residing in local structures exhibits a clear advantage in terms of discovering locally crucial substructures. As an application of the method, we present a scheme to detect multiple core-periphery structures and the decomposition of coarse-grained supernode networks, by combining the method with the network community detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models</title>
<link>https://arxiv.org/abs/2505.05816</link>
<guid>https://arxiv.org/abs/2505.05816</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy-preserving, spectral clustering, community detection, stochastic block models, edge differential privacy <br />
<br />
Summary: <br />
In this study, researchers investigate privacy-preserving spectral clustering for community detection in stochastic block models. They specifically focus on edge differential privacy and propose private algorithms for community recovery. The research delves into the trade-offs between the privacy budget and accurate community label recovery. The team establishes information-theoretic conditions that ensure the accuracy of their methods, offering theoretical guarantees for successful community recovery under edge DP. This work contributes to the field by addressing the crucial issue of preserving privacy while maintaining the accuracy of community detection algorithms. <div>
arXiv:2505.05816v1 Announce Type: new 
Abstract: We investigate privacy-preserving spectral clustering for community detection within stochastic block models (SBMs). Specifically, we focus on edge differential privacy (DP) and propose private algorithms for community recovery. Our work explores the fundamental trade-offs between the privacy budget and the accurate recovery of community labels. Furthermore, we establish information-theoretic conditions that guarantee the accuracy of our methods, providing theoretical assurances for successful community recovery under edge DP.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection</title>
<link>https://arxiv.org/abs/2505.05965</link>
<guid>https://arxiv.org/abs/2505.05965</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping structures, graph autoencoder, semi-supervised, attribute noise<br />
Summary:<br />
The article introduces a novel approach for detecting overlapping communities in networks by proposing a semi-supervised graph autoencoder. This model combines graph multi-head attention and modularity maximization to effectively identify communities that share common nodes. It learns semantic representations by integrating structural, attribute, and prior knowledge while addressing noise in node features. Key features of the model include a noise-resistant architecture and a design that incorporates modularity constraints for community quality optimization. Experimental results demonstrate the superior performance of the model compared to existing methods, showing improvements in NMI and F1-score metrics. The model also exhibits exceptional robustness to attribute noise, maintaining stable performance even when 60% of node features are corrupted. These findings underscore the significance of incorporating attribute semantics and structural patterns for accurate community detection in complex networks.<br /><br />Summary: <div>
arXiv:2505.05965v1 Announce Type: new 
Abstract: Community detection in networks with overlapping structures remains a significant challenge, particularly in noisy real-world environments where integrating topology, node attributes, and prior information is critical. To address this, we propose a semi-supervised graph autoencoder that combines graph multi-head attention and modularity maximization to robustly detect overlapping communities. The model learns semantic representations by fusing structural, attribute, and prior knowledge while explicitly addressing noise in node features. Key innovations include a noise-resistant architecture and a semantic semi-supervised design optimized for community quality through modularity constraints. Experiments demonstrate superior performance the model outperforms state-of-the-art methods in overlapping community detection (improvements in NMI and F1-score) and exhibits exceptional robustness to attribute noise, maintaining stable performance under 60\% feature corruption. These results highlight the importance of integrating attribute semantics and structural patterns for accurate community discovery in complex networks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
<link>https://arxiv.org/abs/2505.06184</link>
<guid>https://arxiv.org/abs/2505.06184</guid>
<content:encoded><![CDATA[
<div> Keywords: social media user profiling, large language model, interpretability, adaptability, Persian political Twitter dataset

Summary: 
Our novel approach leverages a large language model to analyze social media user profiles through domain-defining statements, creating abstractive and extractive user profiles without the need for large labeled datasets. By using semi-supervised filtering with a domain-specific knowledge base, our method generates interpretable natural language profiles that condense user data for downstream tasks. We introduce a Persian political Twitter dataset (X) and an evaluation framework with human validation to showcase the effectiveness of our approach. Experimental results demonstrate a 9.8% improvement over state-of-the-art methods, highlighting the flexibility, adaptability, and interpretability of our approach. <div>
arXiv:2505.06184v1 Announce Type: new 
Abstract: Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm</title>
<link>https://arxiv.org/abs/2505.05511</link>
<guid>https://arxiv.org/abs/2505.05511</guid>
<content:encoded><![CDATA[
<div> random forest model, energy storage system, operational costs, power load balancing, economic performance

Summary:
The study analyzes the economic performance of various parks under different conditions, focusing on operational costs and power load balancing. Initially, parks without energy storage were analyzed using a random forest model, indicating a correlation between cost and electricity purchase. Simulations after deploying a 50kW/100kWh energy storage system showed decreased wind and solar power curtailment and reduced operational costs. Using a genetic algorithm, the energy storage configuration of each park was optimized, leading to improved economic indicators for Parks A, B, and C. The research highlights the importance of optimizing energy storage configurations to reduce costs, enhance economic benefits, and promote sustainable development in the power system. 

Summary: <div>
arXiv:2505.05511v1 Announce Type: cross 
Abstract: This study aims to analyze the economic performance of various parks under different conditions, particularly focusing on the operational costs and power load balancing before and after the deployment of energy storage systems. Firstly, the economic performance of the parks without energy storage was analyzed using a random forest model. Taking Park A as an example, it was found that the cost had the greatest correlation with electricity purchase, followed by photovoltaic output, indicating that solar and wind power output are key factors affecting economic performance. Subsequently, the operation of the parks after the configuration of a 50kW/100kWh energy storage system was simulated, and the total cost and operation strategy of the energy storage system were calculated. The results showed that after the deployment of energy storage, the amount of wind and solar power curtailment in each park decreased, and the operational costs were reduced. Finally, a genetic algorithm was used to optimize the energy storage configuration of each park. The energy storage operation strategy was optimized through fitness functions, crossover operations, and mutation operations. After optimization, the economic indicators of Parks A, B, and C all improved. The research results indicate that by optimizing energy storage configuration, each park can reduce costs, enhance economic benefits, and achieve sustainable development of the power system.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equalizing Closeness Centralities via Edge Additions</title>
<link>https://arxiv.org/abs/2505.06222</link>
<guid>https://arxiv.org/abs/2505.06222</guid>
<content:encoded><![CDATA[
<div> Graph modification problems, closeness centralities, algorithmic fairness, NP-hard, approximation algorithms<br />
Summary:<br />
The article explores graph modification problems focused on equalizing the network positions of nodes, motivated by social capital fairness. Two formalized problems are discussed: Closeness Ratio Improvement to maximize closeness centrality ratio between two nodes and Closeness Gap Minimization to minimize the difference. Both problems are proven to be NP-hard, with a quasilinear-time approximation for Closeness Ratio Improvement. However, Closeness Gap Minimization lacks a multiplicative approximation unless P=NP. The study concludes with suggestions for future research in this problem domain, including potential generalizations. <div>
arXiv:2505.06222v1 Announce Type: cross 
Abstract: Graph modification problems with the goal of optimizing some measure of a given node's network position have a rich history in the algorithms literature. Less commonly explored are modification problems with the goal of equalizing positions, though this class of problems is well-motivated from the perspective of equalizing social capital, i.e., algorithmic fairness. In this work, we study how to add edges to make the closeness centralities of a given pair of nodes more equal. We formalize two versions of this problem: Closeness Ratio Improvement, which aims to maximize the ratio of closeness centralities between two specified nodes, and Closeness Gap Minimization, which aims to minimize the absolute difference of centralities. We show that both problems are $\textsf{NP}$-hard, and for Closeness Ratio Improvement we present a quasilinear-time $\frac{6}{11}$-approximation, complemented by a bicriteria inapproximability bound. In contrast, we show that Closeness Gap Minimization admits no multiplicative approximation unless $\textsf{P} = \textsf{NP}$. We conclude with a discussion of open directions for this style of problem, including several natural generalizations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Spread of Online Incivility in Brazilian Politics</title>
<link>https://arxiv.org/abs/2504.08960</link>
<guid>https://arxiv.org/abs/2504.08960</guid>
<content:encoded><![CDATA[
<div> Keywords: incivility, online discourse, Brazilian politics, social media, political influencers <br />
Summary: <br />
This study introduces a multidimensional framework for understanding online political incivility, highlighting impoliteness, physical harm, hate speech, and threats to democratic values. Analyzing 5 million tweets from 2,307 political influencers during the 2022 Brazilian general election, the research reveals that impoliteness peaks during election campaigns, while other forms of incivility are triggered by specific violent events. Left-aligned influencers are identified as primary spreaders of online incivility, both direct and indirect. They disseminate content from various sources, creating a diffusion pattern involving direct and two-step communication flows. This study sheds light on the diverse nature of incivility in Brazilian politics, providing insights that can be applied to other political contexts. <br /> <div>
arXiv:2504.08960v2 Announce Type: replace 
Abstract: Incivility refers to behaviors that violate collective norms and disrupt cooperation within the political process. Although large-scale online data and automated techniques have enabled the quantitative analysis of uncivil discourse, prior research has predominantly focused on impoliteness or toxicity, often overlooking other behaviors that undermine democratic values. To address this gap, we propose a multidimensional conceptual framework encompassing Impoliteness, Physical Harm and Violent Political Rhetoric, Hate Speech and Stereotyping, and Threats to Democratic Institutions and Values. Using this framework, we measure the spread of online political incivility in Brazil using approximately 5 million tweets posted by 2,307 political influencers during the 2022 Brazilian general election. Through statistical modeling and network analysis, we examine the dynamics of uncivil posts at different election stages, identify key disseminators and audiences, and explore the mechanisms driving the spread of uncivil information online. Our findings indicate that impoliteness is more likely to surge during election campaigns. In contrast, the other dimensions of incivility are often triggered by specific violent events. Moreover, we find that left-aligned individual influencers are the primary disseminators of online incivility in the Brazilian Twitter/X sphere and that they disseminate not only direct incivility but also indirect incivility when discussing or opposing incivility expressed by others. They relay those content from politicians, media agents, and individuals to reach broader audiences, revealing a diffusion pattern mixing the direct and two-step flows of communication theory. This study offers new insights into the multidimensional nature of incivility in Brazilian politics and provides a conceptual framework that can be extended to other political contexts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-Learn: Label-Efficient Graph Open-Set Learning</title>
<link>https://arxiv.org/abs/2410.16386</link>
<guid>https://arxiv.org/abs/2410.16386</guid>
<content:encoded><![CDATA[
<div> Graph open-set learning, out-of-distribution detection, label-efficient, graph neural network, K-Medoids.

Summary:
LEGO-Learn is introduced to address the challenge of open-set node classification on graphs with limited labels. It utilizes a GNN-based filter to identify and exclude out-of-distribution nodes, selecting informative in-distribution nodes using K-Medoids algorithm. A C+1 classifier is implemented to differentiate known classes from OOD nodes, employing a weighted cross-entropy loss to balance OOD removal with retaining informative examples. Experimental results on real-world datasets show LEGO-Learn outperforms existing methods, improving ID classification accuracy by up to 6.62% and AUROC for OOD detection by 7.49%. The framework demonstrates significant advancements in graph-based models for recognizing unseen classes while minimizing labeling costs, crucial for high-stakes applications in finance, security, and healthcare.<br /><br />Summary: <div>
arXiv:2410.16386v2 Announce Type: replace-cross 
Abstract: How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to many labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that tackles open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then select highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the C known ID classes and an additional class representing OOD nodes (hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, with up to a 6.62% improvement in ID classification accuracy and a 7.49% increase in AUROC for OOD detection.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community and hyperedge inference in multiple hypergraphs</title>
<link>https://arxiv.org/abs/2505.04967</link>
<guid>https://arxiv.org/abs/2505.04967</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, interconnections, stochastic block model, community detection, edge prediction 

<br /><br />Summary: Hypergraphs effectively model high-order interactions in biological and social systems. This study focuses on the interconnections between multiple hypergraphs to synthesize integrated information and enhance the understanding of underlying structures. A model is proposed based on the stochastic block model, allowing the integration of data from multiple hypergraphs to reveal latent high-order structures. It addresses the phenomenon of preferential attachment in hyperedges, where certain nodes significantly influence hyperedge formation, introducing a hyperedge internal degree to quantify these contributions. The model demonstrates capabilities in community mining, predicting missing hyperedges of various sizes, and inferring inter-hypergraph edges. The effectiveness of the model is validated through application to high-order datasets, showcasing strong performance in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Additionally, it supports the analysis of multiple hypergraphs of different types and allows for the analysis of a single hypergraph without inter-hypergraph edges. This work presents a practical and flexible tool for analyzing multiple hypergraphs, contributing significantly to the comprehension of organization within real-world high-order systems. <div>
arXiv:2505.04967v1 Announce Type: new 
Abstract: Hypergraphs, capable of representing high-order interactions via hyperedges, have become a powerful tool for modeling real-world biological and social systems. Inherent relationships within these real-world systems, such as the encoding relationship between genes and their protein products, drive the establishment of interconnections between multiple hypergraphs. Here, we demonstrate how to utilize those interconnections between multiple hypergraphs to synthesize integrated information from multiple higher-order systems, thereby enhancing understanding of underlying structures. We propose a model based on the stochastic block model, which integrates information from multiple hypergraphs to reveal latent high-order structures. Real-world hyperedges exhibit preferential attachment, where certain nodes dominate hyperedge formation. To characterize this phenomenon, our model introduces hyperedge internal degree to quantify nodes' contributions to hyperedge formation. This model is capable of mining communities, predicting missing hyperedges of arbitrary sizes within hypergraphs, and inferring inter-hypergraph edges between hypergraphs. We apply our model to high-order datasets to evaluate its performance. Experimental results demonstrate strong performance of our model in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Moreover, we show that our model enables analysis of multiple hypergraphs of different types and supports the analysis of a single hypergraph in the absence of inter-hypergraph edges. Our work provides a practical and flexible tool for analyzing multiple hypergraphs, greatly advancing the understanding of the organization in real-world high-order systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks</title>
<link>https://arxiv.org/abs/2505.04628</link>
<guid>https://arxiv.org/abs/2505.04628</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social capabilities, benchmark, HSII-Dataset, chain of thought

<br /><br />Summary:  
The article addresses the need for large language models (LLMs) to operate independently in complex social settings, rather than just serving as auxiliary assistants for individual communication. It highlights the current lack of systematic measurement for LLM social capabilities and introduces the How Social Is It (HSII) benchmark. This benchmark aims to assess LLMs' abilities in multi-user, multi-turn tasks grounded in sociological principles. HSII includes four evaluation stages: format parsing, target selection, target switching conversation, and stable conversation, which together measure communication and task completion in realistic social scenarios through the HSII-Dataset. The dataset is methodically derived from news sources. The researchers conducted an ablation study utilizing clustering techniques and also explored the impact of the chain of thought (COT) method on LLM social performance. Recognizing the computational cost of COT, they introduced a new metric called COT-complexity to balance correctness and efficiency in evaluating specific social tasks. Overall, the experiments indicate that the HSII benchmark effectively evaluates LLMs' social skills. <div>
arXiv:2505.04628v1 Announce Type: cross 
Abstract: Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections</title>
<link>https://arxiv.org/abs/2505.05459</link>
<guid>https://arxiv.org/abs/2505.05459</guid>
<content:encoded><![CDATA[
<div> Keywords: misleading narratives, elections, taxonomy, UKElectionNarratives, language models  

<br /><br />Summary: This article highlights the significant role of misleading narratives in shaping public opinion during elections, particularly how they affect voters' perceptions of candidates and political parties. To effectively address this issue, the authors introduce the first taxonomy categorizing common misleading narratives observed in recent European elections. Utilizing this taxonomy, they create UKElectionNarratives, a pioneering dataset of human-annotated misleading narratives from the UK General Elections conducted in 2019 and 2024. Furthermore, the study benchmarks various Pre-trained and Large Language Models, with a special focus on GPT-4o, to analyze their performance in detecting election-related misleading narratives. The findings reveal the effectiveness of these models, paving the way for enhanced detection methods. In addition, the authors discuss a range of potential use cases that stem from their research and provide recommendations for future directions, relying on the proposed codebook and dataset to inform further inquiry into misleading narratives during elections. This foundational work aims to improve public discourse and foster more informed voting behavior by addressing the prevalence of misinformation in electoral contexts. <div>
arXiv:2505.05459v1 Announce Type: cross 
Abstract: Misleading narratives play a crucial role in shaping public opinion during elections, as they can influence how voters perceive candidates and political parties. This entails the need to detect these narratives accurately. To address this, we introduce the first taxonomy of common misleading narratives that circulated during recent elections in Europe. Based on this taxonomy, we construct and analyse UKElectionNarratives: the first dataset of human-annotated misleading narratives which circulated during the UK General Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language Models (focusing on GPT-4o), studying their effectiveness in detecting election-related misleading narratives. Finally, we discuss potential use cases and make recommendations for future research directions using the proposed codebook and dataset.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Multipolar Polarization</title>
<link>https://arxiv.org/abs/2405.16352</link>
<guid>https://arxiv.org/abs/2405.16352</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, ideological polarization, multipolar systems, generalized Euclidean distance, quantifying polarization

<br /><br />Summary: The study emphasizes the importance of analyzing social networks to accurately define ideological polarization. It highlights a common limitation in existing methods that often rely on a two-dimensional opinion space, which is inadequate for modeling multipolar systems such as multi-party political environments. This limitation restricts the effectiveness of quantifying polarization in a more nuanced context. The paper presents an experimental comparison of various methods for measuring multipolar polarization in networks. The findings reveal that the average pairwise distance extension of generalized Euclidean distance exhibits several desirable properties when quantifying polarization. This metric proves to be advantageous over other methods due to its empirical accuracy and intuitive understanding. By utilizing this enhanced metric, researchers can more effectively investigate multipolar polarized systems, expanding the scope and robustness of polarization studies in social networks. Overall, the research contributes to a deeper understanding of ideological divides in complex social structures by fostering analytical approaches that accommodate multiple opinion poles rather than relying solely on binary frameworks. <div>
arXiv:2405.16352v2 Announce Type: replace 
Abstract: Studying and understanding social networks is crucial for accurately defining ideological polarization, since they enable precise modeling of social structures. One of the limitations of many methods for quantifying polarization on networks is the assumption of a two-dimensional opinion space. This prevents accurate study of multipolar systems like multi-party political systems, where modeling more than two opinion poles is beneficial. Here, I experimentally compare methods for quantifying multipolar polarization on a network and find that the average pairwise distance extension of generalized Euclidean distance conforms to several desired properties, showing its advantages over other methods. This allows the study of multipolar polarized systems based on an empirically and intuitively good metric.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$k$-local Graphs</title>
<link>https://arxiv.org/abs/2410.00601</link>
<guid>https://arxiv.org/abs/2410.00601</guid>
<content:encoded><![CDATA[
<div> Keywords: locality, coloured graphs, clustering, NP-complete, algorithm

<br /><br />Summary: In 2017, Day et al. introduced the concept of locality as a structural complexity measure for patterns in pattern matching, a field initiated by Angluin in 1980. Subsequently, Casel et al. established in 2019 that determining the locality of an arbitrary pattern is an NP-complete problem. This study expands on these ideas by applying the concept of locality to coloured graphs. The goal is to find an enumeration of colours that allows stepwise colouring of the graph while minimizing the number of clusters created. The authors provide initial theoretical results concerning graph classes and introduce a priority search algorithm designed to compute the $k$-locality of a graph efficiently. This algorithm is optimal in the number of marking prefix expansions and is significantly faster than exhaustive search methods. To demonstrate the practical application and advantages of $k$-locality in knowledge discovery, the authors conduct a case study on a subgraph from the DBLP database. The findings suggest that the proposed approach could enhance clustering in coloured graphs and contribute to various applications in data analysis and pattern recognition. <div>
arXiv:2410.00601v2 Announce Type: replace-cross 
Abstract: In 2017 Day et al. introduced the notion of locality as a structural complexity-measure for patterns in the field of pattern matching established by Angluin in 1980. In 2019 Casel et al. showed that determining the locality of an arbitrary pattern is NP-complete. Inspired by hierarchical clustering, we extend the notion to coloured graphs, i.e., given a coloured graph determine an enumeration of the colours such that colouring the graph stepwise according to the enumeration leads to as few clusters as possible. Next to first theoretical results on graph classes, we propose a priority search algorithm to compute the $k$-locality of a graph. The algorithm is optimal in the number of marking prefix expansions, and is faster by orders of magnitude than an exhaustive search. Finally, we perform a case study on a DBLP subgraph to demonstrate the potential of $k$-locality for knowledge discovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning</title>
<link>https://arxiv.org/abs/2501.01031</link>
<guid>https://arxiv.org/abs/2501.01031</guid>
<content:encoded><![CDATA[
<div> Keywords: cultural values, Large Language Models, Retrieval-Augmented Generation, ValuesRAG, World Values Survey

<br /><br />Summary: Ensuring alignment with cultural values in Large Language Models (LLMs) is crucial, as they often reflect Western-centric biases, resulting in misrepresentation and fairness issues. Traditional methods such as role assignment and few-shot learning face limitations due to their dependency on pre-trained knowledge and their inability to encapsulate nuanced cultural values. To tackle these challenges, the authors present ValuesRAG, a novel framework that employs Retrieval-Augmented Generation (RAG) combined with In-Context Learning (ICL) to dynamically integrate cultural and demographic knowledge during text generation. ValuesRAG utilizes the World Values Survey (WVS) dataset to produce summaries of individual values and employs curated regional datasets for testing. The framework retrieves relevant summaries based on demographic features, followed by a reranking process to select the top-k summaries. In evaluations across six diverse regional datasets, ValuesRAG outperformed baseline methods, including zero-shot and few-shot approaches, in various experimental settings. The results emphasize ValuesRAG's effectiveness in creating culturally aligned and inclusive AI systems, highlighting the potential of dynamic retrieval-based techniques to reconcile global LLM capabilities with localized cultural values. <div>
arXiv:2501.01031v3 Announce Type: replace-cross 
Abstract: Ensuring cultural values alignment in Large Language Models (LLMs) remains a critical challenge, as these models often embed Western-centric biases from their training data, leading to misrepresentations and fairness concerns in cross-cultural applications. Existing approaches such as role assignment and few-shot learning struggle to address these limitations effectively due to their reliance on pre-trained knowledge, limited scalability, and inability to capture nuanced cultural values. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. We subsequently curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show that it consistently outperforms baselines: including zero-shot, role-assignment, few-shot, and hybrid methods, both in main experiments and ablation settings. Notably, ValuesRAG achieves the best overall performance over prior methods, demonstrating its effectiveness in fostering culturally aligned and inclusive AI systems. Our findings underscore the potential of dynamic retrieval-based methods to bridge the gap between global LLM capabilities and localized cultural values.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework</title>
<link>https://arxiv.org/abs/2505.03746</link>
<guid>https://arxiv.org/abs/2505.03746</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, Cyberbullying detection, Machine Learning, Large Language Models, Explainability dashboard

Summary:
Our proposed solution for cyberbullying detection utilizes stream-based Machine Learning models combined with Large Language Models for feature engineering. By processing incoming samples incrementally, our system addresses the evolving nature of online abusive and hate speech. An explainability dashboard enhances trustworthiness and reliability, promoting accountability. Experimental results show promising performance close to 90% across all evaluation metrics, surpassing existing works in the literature. This real-time solution contributes to the safety of online communities by detecting abusive behavior promptly, preventing long-lasting harassment, and reducing negative consequences in society. 

<br /><br />Summary: <div>
arXiv:2505.03746v1 Announce Type: new 
Abstract: Social media platforms enable instant and ubiquitous connectivity and are essential to social interaction and communication in our technological society. Apart from its advantages, these platforms have given rise to negative behaviors in the online community, the so-called cyberbullying. Despite the many works involving generative Artificial Intelligence (AI) in the literature lately, there remain opportunities to study its performance apart from zero/few-shot learning strategies. Accordingly, we propose an innovative and real-time solution for cyberbullying detection that leverages stream-based Machine Learning (ML) models able to process the incoming samples incrementally and Large Language Models (LLMS) for feature engineering to address the evolving nature of abusive and hate speech online. An explainability dashboard is provided to promote the system's trustworthiness, reliability, and accountability. Results on experimental data report promising performance close to 90 % in all evaluation metrics and surpassing those obtained by competing works in the literature. Ultimately, our proposal contributes to the safety of online communities by timely detecting abusive behavior to prevent long-lasting harassment and reduce the negative consequences in society.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing</title>
<link>https://arxiv.org/abs/2505.03769</link>
<guid>https://arxiv.org/abs/2505.03769</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-platform social media, user engagement, text-visual content, title rewriting, quantitative analysis

Summary: 
This study explores the impact of rewriting Reddit post titles adapted from YouTube video titles on user engagement for multimodal content. An analysis of a large dataset of Reddit posts sharing YouTube videos reveals that modified titles significantly improve engagement. A controlled experiment isolates the effects of textual variations and identifies key factors that enhance engagement, such as emotional resonance, lexical richness, and alignment with community norms. Statistical tests confirm the impact of effective title rewrites on engagement, with a fine-tuned BERT classifier achieving high accuracy in predicting user preferences. By combining quantitative rigor with qualitative insights, this study provides valuable insights into engagement dynamics and presents a robust framework for enhancing cross-platform, multimodal content strategies. 

<br /><br />Summary: <div>
arXiv:2505.03769v1 Announce Type: new 
Abstract: In today's cross-platform social media landscape, understanding factors that drive engagement for multimodal content, especially text paired with visuals, remains complex. This study investigates how rewriting Reddit post titles adapted from YouTube video titles affects user engagement. First, we build and analyze a large dataset of Reddit posts sharing YouTube videos, revealing that 21% of post titles are minimally modified. Statistical analysis demonstrates that title rewrites measurably improve engagement. Second, we design a controlled, multi-phase experiment to rigorously isolate the effects of textual variations by neutralizing confounding factors like video popularity, timing, and community norms. Comprehensive statistical tests reveal that effective title rewrites tend to feature emotional resonance, lexical richness, and alignment with community-specific norms. Lastly, pairwise ranking prediction experiments using a fine-tuned BERT classifier achieves 74% accuracy, significantly outperforming near-random baselines, including GPT-4o. These results validate that our controlled dataset effectively minimizes confounding effects, allowing advanced models to both learn and demonstrate the impact of textual features on engagement. By bridging quantitative rigor with qualitative insights, this study uncovers engagement dynamics and offers a robust framework for future cross-platform, multimodal content strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Content Moderation Lead Users Away from Fringe Movements? Evidence from a Recovery Community</title>
<link>https://arxiv.org/abs/2505.03772</link>
<guid>https://arxiv.org/abs/2505.03772</guid>
<content:encoded><![CDATA[
<div> community, exredpill, Reddit, Manosphere, content moderation

Summary:<br />
The study examines the impact of banning and quarantining radical communities within the Manosphere on the exredpill recovery community on Reddit. Banning these communities led to increased participation in exredpill, while quarantining had no effect. The effect of banning was stronger than real-world events related to the Manosphere. Moderation actions did not result in a spike in toxicity or malicious activity in exredpill, indicating that content moderation acts as a deradicalization catalyst. The findings suggest that sanctions on fringe movements linked to hate speech, violence, and terrorism can contribute to individuals abandoning these movements. <div>
arXiv:2505.03772v1 Announce Type: new 
Abstract: Online platforms have sanctioned individuals and communities associated with fringe movements linked to hate speech, violence, and terrorism, but can these sanctions contribute to the abandonment of these movements? Here, we investigate this question through the lens of exredpill, a recovery community on Reddit meant to help individuals leave movements within the Manosphere, a conglomerate of fringe Web based movements focused on men's issues. We conduct an observational study on the impact of sanctioning some of Reddit's largest Manosphere communities on the activity levels and user influx of exredpill, the largest associated recovery subreddit. We find that banning a related radical community positively affects participation in exredpill in the period following the ban. Yet, quarantining the community, a softer moderation intervention, yields no such effects. We show that the effect induced by banning a radical community is stronger than for some of the widely discussed real-world events related to the Manosphere and that moderation actions against the Manosphere do not cause a spike in toxicity or malicious activity in exredpill. Overall, our findings suggest that content moderation acts as a deradicalization catalyst.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media and Academia: How Gender Influences Online Scholarly Discourse</title>
<link>https://arxiv.org/abs/2505.03773</link>
<guid>https://arxiv.org/abs/2505.03773</guid>
<content:encoded><![CDATA[
<div> AI innovation, gender, social media, communication, academics
Summary:<br /><br />This study examines gender differences in online communication patterns of academics in computer science at the top 20 USA universities on the social media platform X. Men tend to post more about AI innovation, society, and machine learning, while women focus more on engaging AI events. Women express stronger emotions in their tweets, with certain emotions linked to specific topics. Female academics display more empathy and discuss personal experiences. However, both genders show consistency in factors like self-praise and politeness. Female academics receive more toxic and threatening replies online, indicating a need for a more inclusive environment for scholarly engagement. This research underscores the influence of gender on shaping academics' online communication and highlights the importance of addressing disparities in online interactions. 
Summary: <div>
arXiv:2505.03773v1 Announce Type: new 
Abstract: This study investigates gender-based differences in online communication patterns of academics, focusing on how male and female academics represent themselves and how users interact with them on the social media platform X (formerly Twitter). We collect historical Twitter data of academics in computer science at the top 20 USA universities and analyze their tweets, retweets, and replies to uncover systematic patterns such as discussed topics, engagement disparities, and the prevalence of negative language or harassment. The findings indicate that while both genders discuss similar topics, men tend to post more tweets about AI innovation, current USA society, machine learning, and personal perspectives, whereas women post slightly more on engaging AI events and workshops. Women express stronger positive and negative sentiments about various events compared to men. However, the average emotional expression remains consistent across genders, with certain emotions being more strongly associated with specific topics. Writing-style analysis reveals that female academics show more empathy and are more likely to discuss personal problems and experiences, with no notable differences in other factors, such as self-praise, politeness, and stereotypical comments. Analyzing audience responses indicates that female academics are more frequently subjected to severe toxic and threatening replies. Our findings highlight the impact of gender in shaping the online communication of academics and emphasize the need for a more inclusive environment for scholarly engagement.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics</title>
<link>https://arxiv.org/abs/2505.03795</link>
<guid>https://arxiv.org/abs/2505.03795</guid>
<content:encoded><![CDATA[
<div> Keywords: Human networks, strategic game, modeling methods, community-aware behavior, user study

Summary:
The study examines various methods for modeling human behavior in a strategic network game, the Junior High Game (JHG). By comparing different modeling approaches, the researchers find that a model called hCAB, which considers community-aware behavior and population distribution, outperforms other methods. When applied to small groups, hCAB closely reflects the dynamics of human populations. A user study shows that participants could not distinguish hCAB agents from real humans, indicating that hCAB effectively mirrors human behavior in the game. Understanding human networks is crucial for societal outcomes, and the study provides insights into how to promote favorable outcomes by modeling human behavior accurately in strategic games. <div>
arXiv:2505.03795v1 Announce Type: new 
Abstract: Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG). These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior vs. community-aware behavior) and the statistical moments they model (mean vs. distribution). Results show that the highest-performing method models the population's distribution rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies (6-11 individuals), this learned model, called hCAB, closely mirrors the population dynamics of human groups (with some differences). Additionally, a user study reveals that human participants were unable to distinguish hCAB agents from other humans, thus illustrating that individual hCAB behavior plausibly mirrors human behavior in this strategic network game.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Climate Change Landscape on TikTok</title>
<link>https://arxiv.org/abs/2505.03813</link>
<guid>https://arxiv.org/abs/2505.03813</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, climate action discourse, social media, lifestyle choices, dietary choices <br />
Summary: 
This study focuses on the impact of social media platforms, specifically TikTok, on climate action discourse. The researchers collected a dataset of 590K videos from 14K creators on TikTok discussing climate topics. Using topic modeling, they identified that creators mainly address climate issues through the lens of lifestyle and dietary choices. By creating a climate taxonomy, they mapped the topics discussed on the platform and discovered non-climate "gateway" topics that could attract new audiences to climate discussions. This study highlights the importance of understanding how social media platforms influence the conversation around climate change and the potential for leveraging TikTok to engage a younger, environmentally-conscious audience. <br /><br /> <div>
arXiv:2505.03813v1 Announce Type: new 
Abstract: Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikTok's climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate "gateway" topics that could draw new audiences into climate discussions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries</title>
<link>https://arxiv.org/abs/2505.03816</link>
<guid>https://arxiv.org/abs/2505.03816</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban transportation, NYC Taxi Trip dataset, Pathao Food Trip dataset, demand patterns, geospatial analysis 

Summary: 
This study utilizes the NYC Taxi Trip dataset from New York City and the Pathao Food Trip dataset from Dhaka, Bangladesh to analyze transportation patterns. Through Exploratory Data Analysis (EDA), the study aims to identify key trends in demand, peak times, and important geographical hotspots. Geospatial analysis is conducted to map out high-demand and low-demand regions, while the SARIMAX model is used for time series analysis to forecast demand patterns. Clustering techniques are applied to identify significant areas of high and low demand. The findings offer valuable insights for optimizing fleet management and resource allocation in both passenger transport and food delivery services. These insights can help enhance urban transportation systems, improve service efficiency, and better meet customer needs in diverse urban environments.<br /><br />Summary: This study analyzes transportation patterns using datasets from New York City and Dhaka, focusing on demand trends, peak times, and geographical hotspots. Through EDA, geospatial analysis, time series analysis, and clustering techniques, the study provides insights to optimize fleet management and resource allocation in urban transportation services, ultimately enhancing service efficiency and urban transportation systems. <div>
arXiv:2505.03816v1 Announce Type: new 
Abstract: Urban transportation plays a vital role in modern city life, affecting how efficiently people and goods move around. This study analyzes transportation patterns using two datasets: the NYC Taxi Trip dataset from New York City and the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify key trends in demand, peak times, and important geographical hotspots. We start with Exploratory Data Analysis (EDA) to understand the basic characteristics of the datasets. Next, we perform geospatial analysis to map out high-demand and low-demand regions. We use the SARIMAX model for time series analysis to forecast demand patterns, capturing seasonal and weekly variations. Lastly, we apply clustering techniques to identify significant areas of high and low demand. Our findings provide valuable insights for optimizing fleet management and resource allocation in both passenger transport and food delivery services. These insights can help improve service efficiency, better meet customer needs, and enhance urban transportation systems in diverse urban environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-aware analysis of cross-city visitor flows using large language models and social media data</title>
<link>https://arxiv.org/abs/2505.03847</link>
<guid>https://arxiv.org/abs/2505.03847</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-city visitor flows, public events, social media, machine learning, transport policies <br />
Summary: <br />
This study introduces a framework to analyze cross-city visitor flows during public events using large language models and social media data. By extracting event features from online information, a machine learning model predicts visitor flows with high accuracy. The research focuses on Hong Kong and explores the impacts of different event types on visitor numbers, highlighting the importance of promotional and word-of-mouth popularity. The study emphasizes the need for coordinated measures across government agencies and specialized transport policies to manage surges in travel demand during events. Promotional and word-of-mouth popularity have varying effects on visitor flows, particularly among metro and high-speed rail users. These findings can inform strategies such as shuttle services and traffic management to optimize transportation during public events. <br /> <div>
arXiv:2505.03847v1 Announce Type: new 
Abstract: Public events, such as music concerts and fireworks displays, can cause irregular surges in cross-city travel demand, leading to potential overcrowding, travel delays, and public safety concerns. To better anticipate and accommodate such demand surges, it is essential to estimate cross-city visitor flows with awareness of public events. Although prior studies typically focused on the effects of a single mega event or disruptions around a single venue, this study introduces a generalizable framework to analyze visitor flows under diverse and concurrent events. We propose to leverage large language models (LLMs) to extract event features from multi-source online information and massive user-generated content on social media platforms. Specifically, social media popularity metrics are designed to capture the effects of online promotion and word-of-mouth in attracting visitors. An event-aware machine learning model is then adopted to uncover the specific impacts of different event features and ultimately predict visitor flows for upcoming events. Using Hong Kong as a case study, the framework is applied to predict daily flows of mainland Chinese visitors arriving at the city, achieving a testing R-squared of over 85%. We further investigate the heterogeneous event impacts on visitor numbers across different event types and major travel modes. Both promotional popularity and word-of-mouth popularity are found to be associated with increased visitor flows, but the specific effects vary by the event type. This association is more pronounced among visitors arriving by metro and high-speed rail, while it has less effect on air travelers. The findings can facilitate coordinated measures across government agencies and guide specialized transport policies, such as shuttle transit services to event venues, and comprehensive on-site traffic management strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Appeal and Scope of Misinformation Spread by AI Agents and Humans</title>
<link>https://arxiv.org/abs/2505.04028</link>
<guid>https://arxiv.org/abs/2505.04028</guid>
<content:encoded><![CDATA[
<div> misinformation, AI agents, social network platforms, COVID-19 vaccine discourse, tweet engagement

Summary: 
- The study examines the impact of misinformation and AI agents on social network platforms, focusing on COVID-19 vaccine discourse.
- Two new metrics, Appeal and Scope, are proposed to quantify the influence of misinformation based on tweet engagement and user network position.
- Analysis of 5.8 million misinformation tweets across three time periods reveals higher prevalence of misinformation during the Pre-Vaccine and Vaccine Launch periods.
- Human-generated misinformation tweets show higher appeal and scope compared to bot-generated ones.
- Tweedie regression analysis highlights human-generated misinformation as more concerning during the Vaccine Launch week, while bot-generated misinformation reaches peak appeal and scope during the Pre-Vaccine period. 

<br /><br />Summary: <div>
arXiv:2505.04028v1 Announce Type: new 
Abstract: This work examines the influence of misinformation and the role of AI agents, called bots, on social network platforms. To quantify the impact of misinformation, it proposes two new metrics based on attributes of tweet engagement and user network position: Appeal, which measures the popularity of the tweet, and Scope, which measures the potential reach of the tweet. In addition, it analyzes 5.8 million misinformation tweets on the COVID-19 vaccine discourse over three time periods: Pre-Vaccine, Vaccine Launch, and Post-Vaccine. Results show that misinformation was more prevalent during the first two periods. Human-generated misinformation tweets tend to have higher appeal and scope compared to bot-generated ones. Tweedie regression analysis reveals that human-generated misinformation tweets were most concerning during Vaccine Launch week, whereas bot-generated misinformation reached its highest appeal and scope during the Pre-Vaccine period.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delegation and Participation in Decentralized Governance: An Epistemic View</title>
<link>https://arxiv.org/abs/2505.04136</link>
<guid>https://arxiv.org/abs/2505.04136</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized governance, epistemic tests, transfer delegation, direct participation, DAOs 

Summary:
Decentralized governance methods are evaluated based on epistemic tests, focusing on the ability to reach correct outcomes. Partial abstention emerges as a strong governance method, outperforming transfer delegation where voters transfer voting rights to others. Multi-step transfer delegation shows promise but is not without epistemic weaknesses. Enhanced direct participation can have negative epistemic impacts unless certain conditions are met. Additional direct participation can be beneficial under specific governance conditions, increasing the likelihood of correct decisions. The study also suggests the potential use of prediction markets, auctions, and AI agents to enhance decentralized governance outcomes. These findings are crucial for decentralized autonomous organizations (DAOs) looking to compete with centralized organizations, highlighting the importance of epistemic performance in decision-making processes. 

<br /><br />Summary: 
Decentralized governance methods, including partial abstention and transfer delegation, are examined through epistemic tests to assess their ability to reach correct outcomes. While partial abstention proves to be a strong method, transfer delegation has inherent epistemic weaknesses. Enhanced direct participation can have both positive and negative epistemic impacts, depending on governance conditions. Additional direct participation can improve decision-making under certain circumstances. To enhance outcomes, the study suggests the potential use of prediction markets, auctions, and AI agents in decentralized governance. These findings are essential for DAOs aiming to compete with centralized organizations, emphasizing the significance of epistemic performance in governance processes. <div>
arXiv:2505.04136v1 Announce Type: new 
Abstract: We develop and apply epistemic tests to various decentralized governance methods as well as to study the impact of participation. These tests probe the ability to reach a correct outcome when there is one. We find that partial abstention is a strong governance method from an epistemic standpoint compared to alternatives such as various forms of ``transfer delegation" in which voters explicitly transfer some or all of their voting rights to others. We make a stronger case for multi-step transfer delegation than is present in previous work but also demonstrate that transfer delegation has inherent epistemic weaknesses. We show that enhanced direct participation, voters exercising their own voting rights, can have a variety of epistemic impacts, some very negative. We identify governance conditions under which additional direct participation is guaranteed to do no epistemic harm and is likely to increase the probability of making correct decisions. In light of the epistemic challenges of voting-based decentralized governance, we consider the possible supplementary use of prediction markets, auctions, and AI agents to improve outcomes. All these results are significant because epistemic performance matters if entities such as DAOs (decentralized autonomous organizations) wish to compete with organizations that are more centralized.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random walks with resetting on hypergraph</title>
<link>https://arxiv.org/abs/2505.04215</link>
<guid>https://arxiv.org/abs/2505.04215</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph, random walks with resetting, spectral theory, eigenvalues, node ranking

Summary:
Random walks with resetting on hypergraphs are analyzed using spectral theory, with key parameters like occupation probability and mean first passage time expressed in terms of transition matrix eigenvalues. An optimal reset probability condition and a condition for its existence are derived. The relationship between random walks on hypergraphs and simple random walks is established, showing that hypergraph eigenvalues can be represented using graph eigenvalues. A new research framework is proposed that considers the intrinsic structure of hypergraphs, improving node ranking accuracy over traditional methods by assigning proper weights to neighboring nodes. The impact of resetting mechanisms on cover time is explored, offering potential solutions for optimizing search efficiency.	Extensive experiments demonstrate the effectiveness of this framework in producing reliable results. 

<br /><br />Summary: <div>
arXiv:2505.04215v1 Announce Type: new 
Abstract: Hypergraph has been selected as a powerful candidate for characterizing higher-order networks and has received
  increasing attention in recent years. In this article, we study random walks with resetting on hypergraph by utilizing
  spectral theory. Specifically, we derive exact expressions for some fundamental yet key parameters, including occupation
  probability, stationary distribution, and mean first passage time, all of which are expressed in terms of the eigenvalues
  and eigenvectors of the transition matrix. Furthermore, we provide a general condition for determining the optimal
  reset probability and a sufficient condition for its existence. In addition, we build up a close relationship between
  random walks with resetting on hypergraph and simple random walks. Concretely, the eigenvalues and eigenvectors
  of the former can be precisely represented by those of the latter. More importantly, when considering random walks,
  we abandon the traditional approach of converting hypergraph into a graph and propose a research framework that
  preserves the intrinsic structure of hypergraph itself, which is based on assigning proper weights to neighboring nodes.
  Through extensive experiments, we show that the new framework produces distinct and more reliable results than
  the traditional approach in node ranking. Finally, we explore the impact of the resetting mechanism on cover time,
  providing a potential solution for optimizing search efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flowers to Fascism? The Cottagecore to Tradwife Pipeline on Tumblr</title>
<link>https://arxiv.org/abs/2505.04561</link>
<guid>https://arxiv.org/abs/2505.04561</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, aesthetic-based radicalization, Tradwife, Cottagecore, extremism 

Summary:
The study examined the potential for aesthetic-based radicalization on social media platforms, specifically focusing on the intersection of Cottagecore and Tradwife content. While explicit radicalization was not found, there was evidence of a mainstreaming effect as the two communities overlapped. Surprisingly, there was unexpected interaction between queer identities and Tradwives, with some Tradwives even incorporating queer individuals and denouncing racism within their community. This could indicate a re-branding of extremist content to align with platform norms. A temporal analysis showed a shift in the central tags used by Tradwives towards reactionary ideals post-2021, moving from aesthetics and hobbies to focus on religion, traditional gender roles, and homesteading. This suggests a potential shift in the ideologies promoted within the community. Overall, the study highlights the complex dynamics at play within online communities and the need for continued monitoring of extremist content. 

<br /><br />Summary: <div>
arXiv:2505.04561v1 Announce Type: new 
Abstract: In this work we collected and analyzed social media posts to investigate aesthetic-based radicalization where users searching for Cottagecore content may find Tradwife content co-opted by white supremacists, white nationalists, or other far-right extremist groups. Through quantitative analysis of over 200,000 Tumblr posts and qualitative coding of about 2,500 Tumblr posts, we did not find evidence of a explicit radicalization. We found that problematic Tradwife posts found in the literature may be confined to Tradwife-only spaces, while content in the Cottagecore tag generally did not warrant extra moderation. However, we did find evidence of a mainstreaming effect in the overlap between the Tradwife and Cottagecore communities. In our qualitative analysis there was more interaction between queer and Tradwife identities than expected based on the literature, and some Tradwives even explicitly included queer people and disavowed racism in the Tradwife community on Tumblr. This could be genuine, but more likely it was an example of extremists re-branding their content and following platform norms to spread ideologies that would otherwise be rejected by Tumblr users. Additionally, through temporal analysis we observed a change in the central tags used by Tradwives in the Cottagecore tag pre- and post- 2021. Initially these posts focused on aesthetics and hobbies like baking and gardening, but post-2021 the central tags focused more on religion, traditional gender roles, and homesteading, all markers of reactionary ideals.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation</title>
<link>https://arxiv.org/abs/2505.03774</link>
<guid>https://arxiv.org/abs/2505.03774</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, OOD detection, heterogeneous graphs, energy propagation, meta-path-based<br />
Summary:<br />
- The study addresses the challenge of out-of-distribution (OOD) node detection in heterogeneous graphs, which are common in real-world scenarios.<br />
- A novel methodology, OODHG, is proposed to detect OOD nodes and classify in-distribution (ID) nodes based on the detection results.<br />
- OODHG utilizes energy values and a meta-path-based energy propagation mechanism to differentiate between ID and OOD nodes effectively.<br />
- The approach is shown to outperform baseline models in OOD detection tasks and accurately classify ID nodes in heterogeneous graphs.<br />
- The method's simplicity and effectiveness make it a promising solution for OOD detection in complex, real-world graph data. <br /> <div>
arXiv:2505.03774v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node and structural information from graph data. While current GNNs perform well in node classification tasks within in-distribution (ID) settings, real-world scenarios often present distribution shifts, leading to the presence of out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and challenging task. Most existing research focuses on homogeneous graphs, but real-world graphs are often heterogeneous, consisting of diverse node and edge types. This heterogeneity adds complexity and enriches the informational content. To the best of our knowledge, OOD detection in heterogeneous graphs remains an underexplored area. In this context, we propose a novel methodology for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the first task's results. Specifically, we learn representations for each node in the heterogeneous graph, calculate energy values to determine whether nodes are OOD, and then classify ID nodes. To leverage the structural information of heterogeneous graphs, we introduce a meta-path-based energy propagation mechanism and an energy constraint to enhance the distinction between ID and OOD nodes. Extensive experimental findings substantiate the simplicity and effectiveness of OODHG, demonstrating its superiority over baseline models in OOD detection tasks and its accuracy in ID node classification.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Causal Effects in Networks with Cluster-Based Bandits</title>
<link>https://arxiv.org/abs/2505.04200</link>
<guid>https://arxiv.org/abs/2505.04200</guid>
<content:encoded><![CDATA[
<div> Keywords: causal effects, randomized controlled trial, A/B testing, multi-armed bandit, social networks<br />
Summary:<br />
The article discusses the challenges of estimating causal effects in the presence of interference, particularly in social networks. It highlights the importance of adapting strategies over time to efficiently learn the total treatment effect while balancing exploration and exploitation. Two cluster-based multi-armed bandit algorithms are introduced to address these challenges and maximize expected rewards. The performance of these algorithms is compared with vanilla MAB algorithms and traditional RCT methods on semi-synthetic data with simulated interference. The results show that the cluster-based MAB algorithms outperform the vanilla MAB algorithms, offering a higher reward-action ratio without sacrificing accuracy in treatment effect estimation. This research provides valuable insights into optimizing A/B testing strategies in network settings with interference. <br />Summary: <div>
arXiv:2505.04200v1 Announce Type: cross 
Abstract: The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population. However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome. Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it. Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network. We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation. We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference. The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover. The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.04461</link>
<guid>https://arxiv.org/abs/2505.04461</guid>
<content:encoded><![CDATA[
<div> Graphs, Temporal interaction, Representation learning, Downstream tasks, Temporal dependencies  
Summary:  
Temporal interaction graphs (TIGs) are essential in modeling complex dynamic system behaviors. Temporal interaction graph representation learning (TIGRL) plays a crucial role in embedding nodes in TIGs into low-dimensional representations that preserve structural and temporal information. This enhances performance in classification, prediction, and clustering tasks within evolving data environments. This paper introduces TIG concepts, highlights temporal dependencies' significance, and categorizes state-of-the-art TIGRL methods based on the information utilized during learning. It also provides datasets and benchmarks sources for empirical investigations. Key open challenges and promising research directions in TIGRL are discussed, setting the stage for future advancements in the field.  
<br /><br />Summary: <div>
arXiv:2505.04461v1 Announce Type: cross 
Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Garden city: A synthetic dataset and sandbox environment for analysis of pre-processing algorithms for GPS human mobility data</title>
<link>https://arxiv.org/abs/2412.00913</link>
<guid>https://arxiv.org/abs/2412.00913</guid>
<content:encoded><![CDATA[
<div> Keywords: Human mobility datasets, sparsity, processing algorithms, synthetic trajectory simulator, open-source code

Summary: 
The article discusses the challenges associated with human mobility datasets, particularly regarding the high sparsity in commercial datasets that can lead to errors in processing algorithms. These errors may impact the accuracy of downstream results derived from such datasets, making it crucial to validate and calibrate the algorithms effectively. To address these issues, the authors propose a synthetic trajectory simulator and sandbox environment that replicates key features of commercial datasets to test processing algorithms effectively. By comparing algorithm outputs with "ground-truth" synthetic trajectories and mobility diaries, researchers can enhance the robustness of their analyses. The open-source code provided by the authors facilitates the use of this simulator, making it accessible to the wider research community for evaluation and validation of processing algorithms.<br /><br />Summary: <div>
arXiv:2412.00913v2 Announce Type: replace 
Abstract: Human mobility datasets have seen increasing adoption in the past decade, enabling diverse applications that leverage the high precision of measured trajectories relative to other human mobility datasets. However, there are concerns about whether the high sparsity in some commercial datasets can introduce errors due to lack of robustness in processing algorithms, which could compromise the validity of downstream results. The scarcity of "ground-truth" data makes it particularly challenging to evaluate and calibrate these algorithms. To overcome these limitations and allow for an intermediate form of validation of common processing algorithms, we propose a synthetic trajectory simulator and sandbox environment meant to replicate the features of commercial datasets that could cause errors in such algorithms, and which can be used to compare algorithm outputs with "ground-truth" synthetic trajectories and mobility diaries. Our code is open-source and is publicly available alongside tutorial notebooks and sample datasets generated with it.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race</title>
<link>https://arxiv.org/abs/2501.17831</link>
<guid>https://arxiv.org/abs/2501.17831</guid>
<content:encoded><![CDATA[
<div> Algorithmic audit, TikTok, political biases, partisan content recommendations, social media platforms

Summary: 
The study focuses on investigating TikTok's recommendation algorithm for political biases by conducting 323 independent algorithmic audit experiments. The research reveals significant disparities in content distribution, with Republican-seeded accounts receiving more party-aligned recommendations compared to Democratic-seeded accounts. Furthermore, Democratic-seeded accounts were exposed to more opposite-party recommendations on average. These disparities exist across different states and persist even when accounting for engagement metrics. The study highlights the influence of negative partisanship content on the recommendation algorithm during a critical election period, raising concerns about platform neutrality. <div>
arXiv:2501.17831v2 Announce Type: replace 
Abstract: TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of "sock puppet" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-IDS: Doubly Disentangled Dynamic Intrusion Detection</title>
<link>https://arxiv.org/abs/2307.11079</link>
<guid>https://arxiv.org/abs/2307.11079</guid>
<content:encoded><![CDATA[
<div> Keywords: Network-based intrusion detection system, feature disentanglement, dynamic graph diffusion, unknown threats, explainability<br />
Summary: 
The article introduces a novel method called 3D-IDS for enhancing the performance of Network-based Intrusion Detection Systems (NIDS). Existing methods are found to be inconsistent in detecting various known and unknown attacks due to entangled distributions of flow features. 3D-IDS addresses these issues through two-step feature disentanglements and a dynamic graph diffusion scheme. The method first disentangles traffic features using mutual information optimization, then uses a memory model to generate representations highlighting attack-specific features. A novel graph diffusion method is employed for spatial-temporal aggregation in evolving data streams. Experimental results demonstrate the effectiveness of 3D-IDS in identifying various attacks, including unknown threats and hard-to-detect known attacks. The two-step feature disentanglements also contribute to improving the explainability of NIDS. <div>
arXiv:2307.11079v3 Announce Type: replace-cross 
Abstract: Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approximate-Master-Equation Formulation of the Watts Threshold Model on Hypergraphs</title>
<link>https://arxiv.org/abs/2503.04020</link>
<guid>https://arxiv.org/abs/2503.04020</guid>
<content:encoded><![CDATA[
<div> Keywords: behavioral dynamics, social networks, hypergraphs, Watts threshold model, approximate master equations

Summary:
This study explores behavioral dynamics on social networks, considering interactions among groups of individuals in addition to pairs. By extending the Watts threshold model to hypergraphs and using approximate master equations, the researchers develop a continuous-time model with high accuracy. They simplify the model to a system of three differential equations for better computational efficiency and interpretability. Through linearization, they identify conditions for large spreading events. Applying the model to real-world networks, such as a French primary school and computer-science coauthorships hypergraph, demonstrates its accuracy. The study suggests that incorporating structural correlations into future models will enhance accuracy for real-world networks. The research provides insights into polyadic interactions in social dynamics, highlighting the importance of considering group dynamics in addition to pairwise interactions. 

<br /><br />Summary: <div>
arXiv:2503.04020v2 Announce Type: replace-cross 
Abstract: In traditional models of behavioral or opinion dynamics on social networks, researchers suppose that all interactions occur between pairs of individuals. However, in reality, social interactions also occur in groups of three or more individuals. A common way to incorporate such polyadic interactions is to study dynamical processes on hypergraphs. In a hypergraph, interactions can occur between any number of the individuals in a network. The Watts threshold model (WTM) is a well-known model of a simplistic social spreading process. Very recently, Chen et al. extended the WTM from dyadic networks (i.e., graphs) to polyadic networks (i.e., hypergraphs). In the present paper, we extend their discrete-time model to continuous time using approximate master equations (AMEs). By using AMEs, we are able to model the system with very high accuracy. We then reduce the high-dimensional AME system to a system of three coupled differential equations without any detectable loss of accuracy. This much lower-dimensional system is more computationally efficient to solve numerically and is also easier to interpret. We linearize the reduced AME system and calculate a cascade condition, which allows us to determine when a large spreading event occurs. We then apply our model to a social contact network of a French primary school and to a hypergraph of computer-science coauthorships. We find that the AME system is accurate in modelling the polyadic WTM on these empirical networks; however, we expect that future work that incorporates structural correlations between nearby nodes and groups into the model for the dynamics will lead to more accurate theory for real-world networks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Troika algorithm: approximate optimization for accurate clique partitioning and clustering of weighted networks</title>
<link>https://arxiv.org/abs/2505.03573</link>
<guid>https://arxiv.org/abs/2505.03573</guid>
<content:encoded><![CDATA[
<div> clique partitioning, network clustering, Troika, community detection, portfolio analysis
<br />
Summary:
Troika is a new approximation algorithm designed for clique partitioning in network clustering tasks. It efficiently solves this NP-hard problem for small to mid-sized networks, delivering solutions with guaranteed proximity to global optimality. Compared to alternatives like integer programming solvers and heuristics, Troika is faster and more accurate, offering reliable results within a user-specified optimality gap tolerance. The algorithm's applications extend to community detection and portfolio analysis, where it outperforms modularity-based algorithms and showcases dynamic changes in portfolio networks during significant events like the financial crisis of 2008 and the COVID-19 pandemic. With successful performance on benchmark and real networks, Troika emerges as a dependable method for solving clique partitioning instances on standard hardware. 
<br /> <div>
arXiv:2505.03573v1 Announce Type: new 
Abstract: Clique partitioning is a fundamental network clustering task, with applications in a wide range of computational sciences. It involves identifying an optimal partition of the nodes for a real-valued weighted graph according to the edge weights. An optimal partition is one that maximizes the sum of within-cluster edge weights over all possible node partitions. This paper introduces a novel approximation algorithm named Troika to solve this NP-hard problem in small to mid-sized networks for instances of theoretical and practical relevance. Troika uses a branch-and-cut scheme for branching on node triples to find a partition that is within a user-specified optimality gap tolerance. Troika offers advantages over alternative methods like integer programming solvers and heuristics for clique partitioning. Unlike existing heuristics, Troika returns solutions within a guaranteed proximity to global optimality. And our results indicate that Troika is faster than using the state-of-the-art integer programming solver Gurobi for most benchmark instances. Besides its advantages for solving the clique partitioning problem, we demonstrate the applications of Troika in community detection and portfolio analysis. Troika returns partitions with higher proximity to optimal compared to eight modularity-based community detection algorithms. When used on networks of correlations among stocks, Troika reveals the dynamic changes in the structure of portfolio networks including downturns from the 2008 financial crisis and the reaction to the COVID-19 pandemic. Our comprehensive results based on benchmarks from the literature and new real and random networks point to Troika as a reliable and accurate method for solving clique partitioning instances with up to 5000 edges on standard hardware.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Impact of Group Interactions on Climate-related Opinion Change in Reddit</title>
<link>https://arxiv.org/abs/2505.02989</link>
<guid>https://arxiv.org/abs/2505.02989</guid>
<content:encoded><![CDATA[
<div> climate change, opinion dynamics, social media, hypergraph model, Reddit 
Summary: 
- The study focuses on opinion dynamics models in social networks, particularly on social media platforms like Reddit.
- Traditional dyadic models are limited in capturing group dynamics in online discussions, prompting the use of a temporal hypergraph model.
- The hypergraph model accurately predicts shifts in stance towards climate issues at the individual user level.
- The approach is tested against a large language model to validate its predictions, showing superior performance compared to dyadic models.
- The research sheds light on the complexity of opinion formation and evolution in online spaces, highlighting the challenges in capturing nuances of opinions in group interactions. 

<br /><br />Summary: <div>
arXiv:2505.02989v1 Announce Type: cross 
Abstract: Opinion dynamics models describe the evolution of behavioral changes within social networks and are essential for informing strategies aimed at fostering positive collective changes, such as climate action initiatives. When applied to social media interactions, these models typically represent social exchanges in a dyadic format to allow for a convenient encoding of interactions into a graph where edges represent the flow of information from one individual to another. However, this structural assumption fails to adequately reflect the nature of group discussions prevalent on many social media platforms. To address this limitation, we present a temporal hypergraph model that effectively captures the group dynamics inherent in conversational threads, and we apply it to discussions about climate change on Reddit. This model predicts temporal shifts in stance towards climate issues at the level of individual users. In contrast to traditional studies in opinion dynamics that typically rely on simulations or limited empirical validation, our approach is tested against a comprehensive ground truth estimated by a large language model at the level of individual user comments. Our findings demonstrate that using hypergraphs to model group interactions yields superior predictions of the microscopic dynamics of opinion formation, compared to state-of-the-art models based on dyadic interactions. Although our research contributes to the understanding of these complex social systems, significant challenges remain in capturing the nuances of how opinions are formed and evolve within online spaces.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coevolution of Actions and Opinions in Networks of Coordinating and Anti-Coordinating Agents</title>
<link>https://arxiv.org/abs/2505.03078</link>
<guid>https://arxiv.org/abs/2505.03078</guid>
<content:encoded><![CDATA[
<div> coevolutionary model, agent dynamics, opinions, game theory, network structure 
<br />
Summary: 
In this paper, the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions are explored. For coordinating agents, convergence to a Nash equilibrium is guaranteed, with conditions for consensus configurations and regions of attraction for equilibria identified. In the scenario of anti-coordinating agents, all trajectories converge to a Nash equilibrium using potential game theory. Analytical conditions on the network structure and model parameters are established to ensure the existence of consensus and polarized equilibria, characterizing their regions of attraction. This study offers insights into how agents interact and reach equilibrium in different scenarios within a social network context. <div>
arXiv:2505.03078v1 Announce Type: cross 
Abstract: In this paper, we investigate the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions. In the model, the individuals of a population interact on a two-layer network, sharing their opinions and observing others' action, while revising their own opinions and actions according to a game-theoretic mechanism, grounded in the social psychology literature. First, we consider the scenario of coordinating agents, where convergence to a Nash equilibrium (NE) is guaranteed. We identify conditions for reaching consensus configurations and establish regions of attraction for these equilibria. Second, we study networks of anti-coordinating agents. In this second scenario, we prove that all trajectories converge to a NE by leveraging potential game theory. Then, we establish analytical conditions on the network structure and model parameters to guarantee the existence of consensus and polarized equilibria, characterizing their regions of attraction.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Exploratory Learning-Aided Community Detection Under Topological Uncertainty</title>
<link>https://arxiv.org/abs/2304.04497</link>
<guid>https://arxiv.org/abs/2304.04497</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping communities, graph neural networks, network exploration, network inference

Summary: 
META-CODE is a framework designed to detect overlapping communities in social networks where the network structure is uncertain or only partially known. It consists of iterative steps including node-level community-affiliation embeddings, network exploration through community-affiliation-based node queries, and network inference using a neural network model. The framework outperforms benchmark community detection methods, achieving significant improvements in normalized mutual information on real-world datasets. The individual modules of META-CODE contribute to its effectiveness, with node queries playing a crucial role. Empirical evaluations and theoretical findings support the efficacy of node queries in the framework. The inferred network converges to provide accurate community detection results. Overall, META-CODE offers a powerful and efficient solution for community detection in social networks with uncertain or incomplete network structures. 

<br /><br />Summary: <div>
arXiv:2304.04497v4 Announce Type: replace 
Abstract: In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often uncertain, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities via exploratory learning aided by easy-to-collect node metadata when networks are topologically unknown (or only partially known). Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through extensive experiments on three real-world datasets including two large networks, we demonstrate: (a) the superiority of META-CODE over benchmark community detection methods, achieving remarkable gains up to 65.55% on the Facebook dataset over the best competitor among our selected competitive methods in terms of normalized mutual information (NMI), (b) the impact of each module in META-CODE, (c) the effectiveness of node queries in META-CODE based on empirical evaluations and theoretical findings, and (d) the convergence of the inferred network.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Opinions Prediction Utilizes Fusing Dynamics Equation with LLM-based Agents</title>
<link>https://arxiv.org/abs/2409.08717</link>
<guid>https://arxiv.org/abs/2409.08717</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion dynamics, Large Language Model, Cellular Automata, Agent-Based Modeling

Summary:
The study introduces the Fusing Dynamics Equation-Large Language Model (FDE-LLM) algorithm for simulating and predicting user opinions on social media. It addresses the limitations of traditional algorithms by incorporating real-world social data and aligning with Large Language Models. The FDE-LLM divides users into opinion leaders and followers, utilizing Cellular Automata (CA) and the Susceptible-Infectious-Recovered (SIR) model to capture the dynamics of opinion evolution. Experiments on Weibo datasets show that the FDE-LLM outperforms traditional Agent-Based Modeling (ABM) algorithms and LLM-based approaches. The algorithm accurately depicts opinion decay and recovery over time, highlighting the potential of LLMs in enhancing understanding of social media dynamics.
<br /><br />Summary: <div>
arXiv:2409.08717v4 Announce Type: replace 
Abstract: In the context where social media emerges as a pivotal platform for social movements and shaping public opinion, accurately simulating and predicting the dynamics of user opinions is of significant importance. Such insights are vital for understanding social phenomena, informing policy decisions, and guiding public opinion. Unfortunately, traditional algorithms based on idealized models and disregarding social data often fail to capture the complexity and nuance of real-world social interactions. This study proposes the Fusing Dynamics Equation-Large Language Model (FDE-LLM) algorithm. This innovative approach aligns the actions and evolution of opinions in Large Language Models (LLMs) with the real-world data on social networks. The FDE-LLM devides users into two roles: opinion leaders and followers. Opinion leaders use LLM for role-playing and employ Cellular Automata(CA) to constrain opinion changes. In contrast, opinion followers are integrated into a dynamic system that combines the CA model with the Susceptible-Infectious-Recovered (SIR) model. This innovative design significantly improves the accuracy of the simulation. Our experiments utilized four real-world datasets from Weibo. The result demonstrates that the FDE-LLM significantly outperforms traditional Agent-Based Modeling (ABM) algorithms and LLM-based algorithms. Additionally, our algorithm accurately simulates the decay and recovery of opinions over time, underscoring LLMs potential to revolutionize the understanding of social media dynamics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective</title>
<link>https://arxiv.org/abs/2403.16137</link>
<guid>https://arxiv.org/abs/2403.16137</guid>
<content:encoded><![CDATA[
<div> knowledge-based, graph self-supervised learning, graph foundation models, taxonomy, pretext tasks 
Summary:
Graph self-supervised learning is essential for pre-training graph foundation models by utilizing various knowledge patterns present in graph data. Existing surveys of GFMs have shortcomings in comprehensiveness, categorization, and perspective. To address this, a knowledge-based taxonomy categorizing self-supervised graph models by specific graph knowledge used is proposed. The taxonomy includes microscopic, mesoscopic, and macroscopic knowledge categories, encompassing 9 knowledge categories and over 25 pretext tasks for pre-training GFMs. Various downstream task generalization strategies are also covered. This approach allows for a clearer examination of graph models with new architectures, such as graph language models, and provides deeper insights into constructing GFMs. <div>
arXiv:2403.16137v3 Announce Type: replace-cross 
Abstract: Graph self-supervised learning (SSL) is now a go-to method for pre-training graph foundation models (GFMs). There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial to learning generalized representations for GFMs. However, existing surveys of GFMs have several shortcomings: they lack comprehensiveness regarding the most recent progress, have unclear categorization of self-supervised methods, and take a limited architecture-based perspective that is restricted to only certain types of graph models. As the ultimate goal of GFMs is to learn generalized graph knowledge, we provide a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective. We propose a knowledge-based taxonomy, which categorizes self-supervised graph models by the specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and more than 25 pretext tasks for pre-training GFMs, as well as various downstream task generalization strategies. Such a knowledge-based taxonomy allows us to re-examine graph models based on new architectures more clearly, such as graph language models, as well as provide more in-depth insights for constructing GFMs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs</title>
<link>https://arxiv.org/abs/2505.01698</link>
<guid>https://arxiv.org/abs/2505.01698</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media, content generation, influence, network structure <br />
Summary: <br />
The paper explores the potential of Large Language Models (LLMs) in generating personalized influential content to enhance a user's presence on social media. It highlights the limitations of current content generation techniques in addressing real-world social media challenges. By incorporating network information into content prompts, the research aims to boost post influence by leveraging underlying network structures. Multiple content-centric and structure-aware prompts are designed and evaluated through empirical experiments across LLMs. The findings demonstrate the effectiveness of injecting network information into prompt for content generation, shedding light on strategies that can significantly improve post influence. The research provides insights on enhancing visibility and influence on social media through innovative content generation approaches. The code for the study is accessible on GitHub for further exploration and experimentation. <br /> <div>
arXiv:2505.01698v1 Announce Type: new 
Abstract: The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDTok: A Dataset for Eating Disorder Content on TikTok</title>
<link>https://arxiv.org/abs/2505.02250</link>
<guid>https://arxiv.org/abs/2505.02250</guid>
<content:encoded><![CDATA[
<div> Keywords: eating disorders, TikTok, social media, digital health, mental health

Summary:
Eating disorders, such as anorexia nervosa and bulimia nervosa, have seen an increase during the COVID-19 pandemic, potentially exacerbated by exposure to idealized body images online. TikTok, a popular platform with a large adolescent user base, has become a notable space for the sharing of eating disorder content, raising concerns about its impact on vulnerable populations. A dataset of 43,040 TikTok videos related to eating disorders collected from January 2019 to June 2024 offers insights into content spread, moderation, user engagement, and the pandemic's influence on eating disorder trends. This dataset fills research gaps and can inform strategies to reduce the risks associated with harmful content. It contributes valuable insights to the study of digital health and the role of social media in shaping mental health. <div>
arXiv:2505.02250v1 Announce Type: new 
Abstract: Eating disorders, which include anorexia nervosa and bulimia nervosa, have been exacerbated by the COVID-19 pandemic, with increased diagnoses linked to heightened exposure to idealized body images online. TikTok, a platform with over a billion predominantly adolescent users, has become a key space where eating disorder content is shared, raising concerns about its impact on vulnerable populations. In response, we present a curated dataset of 43,040 TikTok videos, collected using keywords and hashtags related to eating disorders. Spanning from January 2019 to June 2024, this dataset, offers a comprehensive view of eating disorder-related content on TikTok. Our dataset has the potential to address significant research gaps, enabling analysis of content spread and moderation, user engagement, and the pandemic's influence on eating disorder trends. This work aims to inform strategies for mitigating risks associated with harmful content, contributing valuable insights to the study of digital health and social media's role in shaping mental health.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A longitudinal analysis of misinformation, polarization and toxicity on Bluesky after its public launch</title>
<link>https://arxiv.org/abs/2505.02317</link>
<guid>https://arxiv.org/abs/2505.02317</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, decentralized platform, user activity, political leanings, moderation efforts

Summary: 
Bluesky is a decentralized social media platform similar to Twitter that recently opened to the public, leading to a surge in user activity. Analysis of user behavior revealed a balanced distribution of original and reshared content, with low toxicity levels on the platform. Most Bluesky users lean left politically and share content from reliable sources. The influx of new users after the public launch, particularly those posting in English and Japanese, increased platform activity, but some accounts exhibited suspicious behavior and were flagged for spam or suspended, indicating effective moderation efforts. The study also highlighted misinformation dynamics and engagement in harmful conversations, showing that Bluesky maintains a relatively positive and credible environment for social interactions. 

<br /><br />Summary: <div>
arXiv:2505.02317v1 Announce Type: new 
Abstract: Bluesky is a decentralized, Twitter-like social media platform that has rapidly gained popularity. Following an invite-only phase, it officially opened to the public on February 6th, 2024, leading to a significant expansion of its user base. In this paper, we present a longitudinal analysis of user activity in the two months surrounding its public launch, examining how the platform evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an activity distribution comparable to more established social platforms, yet it features a higher volume of original content relative to reshared posts and maintains low toxicity levels. We further investigate the political leanings of its user base, misinformation dynamics, and engagement in harmful conversations. Our findings indicate that Bluesky users predominantly lean left politically and tend to share high-credibility sources. After the platform's public launch, an influx of new users, particularly those posting in English and Japanese, contributed to a surge in activity. Among them, several accounts displayed suspicious behaviors, such as mass-following users and sharing content from low-credibility news sources. Some of these accounts have already been flagged as spam or suspended, suggesting that Bluesky's moderation efforts have been effective.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability</title>
<link>https://arxiv.org/abs/2505.02343</link>
<guid>https://arxiv.org/abs/2505.02343</guid>
<content:encoded><![CDATA[
<div> Keywords: social correction, social media, online experiment, credibility evaluations, commenting behavior 

Summary: 
An online experiment focused on the phenomenon of Social Correction, examining how users' credibility evaluations and confidence, combined with online reputational concerns, influence their commenting behavior on social media posts. Results showed that users tended to be more cautious and conservative when giving disputing comments compared to endorsing ones. However, participants were more discerning and critical in their disputing comments, highlighting a cautious approach towards correcting misinformation. These findings contribute to understanding the dynamics of social correction on social media, shedding light on the factors that influence users' commenting behavior and the reliability of their comments. The study underscores the importance of considering the credibility evaluations of social media users and the impact of online reputational concerns in the context of combating misinformation. 

<br /><br />Summary: <div>
arXiv:2505.02343v1 Announce Type: new 
Abstract: Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs</title>
<link>https://arxiv.org/abs/2505.02741</link>
<guid>https://arxiv.org/abs/2505.02741</guid>
<content:encoded><![CDATA[
<div> algorithm, spectral sparsification, dynamic graphs, random walk, GPU-based

Summary:
The work introduces dyGRASS, a dynamic algorithm for spectral sparsification of large undirected graphs with streaming edge updates. It utilizes random-walk-based methods to estimate node-to-node distances efficiently in both the original graph and its sparsifier for incremental and decremental updates. dyGRASS identifies spectrally critical edges among updates for capturing structural changes and recovers important edges during deletions. The algorithm leverages GPU-based non-backtracking random walks for parallel operation, enhancing performance and scalability. Experimental results demonstrate a 10x speedup over the state-of-the-art algorithm inGRASS, eliminating setup overhead and improving solution quality. dyGRASS excels in fully dynamic graph sparsification, accommodating both edge inserts and deletes across diverse graph instances from various domains like integrated circuits, finite element analysis, and social networks. <br /><br />Summary: <div>
arXiv:2505.02741v1 Announce Type: new 
Abstract: This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
<div> Trust dynamics, HAIG framework, evolving relationships, AI systems, human-AI<br />
<br />
Summary: This paper introduces the HAIG framework to analyze trust dynamics in evolving human-AI relationships. It addresses the limitations of current categorical frameworks in capturing the evolving nature of AI systems from tools to partners. The HAIG framework operates on three levels: dimensions, continua, and thresholds, focusing on maintaining appropriate trust relationships while maximizing utility and ensuring safeguards. It takes a trust-utility orientation rather than risk-based or principle-based approaches. The analysis highlights how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution in various contexts. Case studies in healthcare and European regulation demonstrate the framework's effectiveness in complementing existing models and anticipating governance challenges. <div>
arXiv:2505.01651v1 Announce Type: cross 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2505.02027</link>
<guid>https://arxiv.org/abs/2505.02027</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph In-Context Learning, Prompt Generation, Prompt Selection, Prompt Augmentation, Pre-trained Models

Summary: 
Graph In-Context Learning has gained attention for adapting pre-trained graph models to new graphs without updating parameters. Existing methods use random prompts, leading to noise and lower performance. GraphPrompter introduces a multi-stage adaptive prompt optimization approach, enhancing in-context learning. The Prompt Generator highlights informative edges for prompt construction, reducing noise. The Prompt Selector dynamically selects relevant prompts using a $k$-nearest neighbors algorithm. The Prompt Augmenter enhances model generalization with a cache replacement strategy. GraphPrompter outperforms baselines by over 8% on average. The code is available at https://github.com/karin0018/GraphPrompter. 

<br /><br />Summary: <div>
arXiv:2505.02027v1 Announce Type: cross 
Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities</title>
<link>https://arxiv.org/abs/2505.02208</link>
<guid>https://arxiv.org/abs/2505.02208</guid>
<content:encoded><![CDATA[
<div> Grassroots Democratic Federation, large-scale digital communities, fair democratic governance, sortition, federation<br />
Summary:<br />
The article discusses the concept of Grassroots Democratic Federation for large-scale digital communities, aiming to achieve egalitarian formation and fair democratic governance. The federation evolves through grassroots formation and consensual federation of digital communities based on various criteria. Small communities govern themselves, while larger ones are governed by assemblies elected by sortition. The article focuses on the dynamic evolution of the federation, adapting fairness conditions to this setting. It emphasizes fair representation and participation, ensuring these conditions hold as the federation grows. A protocol is presented to meet these fairness requirements, aiming to stabilize the federation structure over time. The approach addresses the dynamic nature of the federation, striving for inclusive and democratic governance of digital communities. <br />Summary: <div>
arXiv:2505.02208v1 Announce Type: cross 
Abstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.
  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
<link>https://arxiv.org/abs/2505.02573</link>
<guid>https://arxiv.org/abs/2505.02573</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, federated graph learning, condensed graph, FedGM, communication efficiency<br />
<br />
Summary:
The article introduces FedGM, a novel approach for federated graph learning that addresses data heterogeneity and privacy risks. It utilizes condensed graphs to aggregate knowledge from distributed graphs, reducing communication costs and privacy risks. Experiment results on six datasets demonstrate FedGM's superiority over existing methods, highlighting its potential as a new FGL paradigm. <div>
arXiv:2505.02573v1 Announce Type: cross 
Abstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models</title>
<link>https://arxiv.org/abs/2401.00688</link>
<guid>https://arxiv.org/abs/2401.00688</guid>
<content:encoded><![CDATA[
<div> Hypergraphs, community structure, mixed-membership stochastic block models, node attributes, dimensionality reduction <br />
Summary: 
The study proposes the HyperNEO framework, combining mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods to infer community structure. This approach aims to simplify the visualization and interpretation of community structure in hypergraphs by generating a node layout that preserves node community memberships. Testing on synthetic and empirical hypergraphs with node attributes, the framework shows promise in broadening the exploration of higher-order community structure in complex systems. <div>
arXiv:2401.00688v2 Announce Type: replace 
Abstract: Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy-access online social media metrics can foster the identification of misinformation sharing users</title>
<link>https://arxiv.org/abs/2408.15186</link>
<guid>https://arxiv.org/abs/2408.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, social media, user identification, factuality, online metrics <br />
Summary: <br />
Researchers have long studied the challenge of misinformation, but identifying primary sharers is difficult and time-consuming. This study proposes a low-barrier method to differentiate social media users likely to share misinformation by analyzing easily accessible online metrics. The research suggests that high tweet frequency and newer account age are associated with sharing low factuality content. Additionally, the number of accounts followed and the number of tweets produced may impact the spread of misinformation, depending on the user's follower count. By utilizing these simple social network metrics, platforms like Twitter can effectively identify users who are prone to spreading misinformation, aiding in combating the issue on social media. <div>
arXiv:2408.15186v2 Announce Type: replace 
Abstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drilling into Erasmus learning mobility flows between countries 2014-2024</title>
<link>https://arxiv.org/abs/2505.00889</link>
<guid>https://arxiv.org/abs/2505.00889</guid>
<content:encoded><![CDATA[
<div> Keywords: Erasmus, mobility network, weighted networks, visualization, clustering

Summary:
The study focuses on analyzing the Erasmus mobility network, highlighting typical issues and methods in examining weighted networks. Various alternative exploratory perspectives are proposed for the dense network of 35 countries with a wide range of visit weights. Transformation techniques are employed to address the vast weight range. Skeleton reduction methods reveal Spain as a key node in the network, along with dominant roles of Germany, France, and Italy. Matrix representations unveil block patterns showcasing clustering of countries into developed and less developed clusters. Balassa normalization matrices indicate deviations from expected visit patterns, with certain clusters exceeding or falling below expectations. Overall, the study offers insights into network structure, highlighting key players and patterns of mobility flow within the Erasmus network.<br /><br />Summary: <div>
arXiv:2505.00889v1 Announce Type: new 
Abstract: Analyzing the Erasmus mobility network, we illustrate typical problems and approaches in analyzing weighted networks. We propose alternative exploratory views on the network "Erasmus+ learning mobility flows since 2014". The network has 35 nodes (countries), is very dense, and the range of link weights (number of visits) is huge (from 1 to 217003). An increasing transformation is used to reduce the range. The traditional graph-based visualization is unreadable. To gain insight into the structure of a dense network, it can be reduced to a skeleton by removing less essential links and/or nodes. We have determined the 1-neighbors and 2-neighbors subnetworks. The 1-neighbors skeleton highlights Spain as the main attractor in the network. The 2-neighbors skeleton shows the dominant role of Spain, Germany, France, and Italy. The hubs and authorities, Pathfinder and Ps cores methods confirm these observations.
  Using the "right" order of the nodes in a matrix representation can reveal the network structure as block patterns in the displayed matrix. The clustering of network nodes based on corrected Salton dissimilarity again shows the dominant role of Spain, Germany, France, and Italy, but also two main clusters of the division into developed/less developed countries. The Balassa normalization (log(measured/expected) visits) matrix shows that most visits within the two main clusters are above expected, while most visits between them are below expected; within the clusters of Balkan countries, Baltic countries, {SK, CZ, HU}, {IS, DK, NO} visits are much above expected, etc.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 1. Networks and knowledge graphs</title>
<link>https://arxiv.org/abs/2505.00912</link>
<guid>https://arxiv.org/abs/2505.00912</guid>
<content:encoded><![CDATA[
<div> Keywords: network, knowledge graph, RDF, Semantic Web, network analysis<br />
Summary:<br />
The article explores the relationship between networks and knowledge graphs, identifying knowledge graphs as a specialized form of network. It discusses how a knowledge graph can be transformed into various networks and subject to network analysis procedures. RDF is highlighted as a formalization of the knowledge graph idea within the context of the Semantic Web, with potential applicability to general network descriptions. The discussion underscores the interchangeability of concepts between knowledge graphs and networks, emphasizing the utility of knowledge graphs in generating diverse network structures. The article suggests that analysis techniques developed for knowledge graphs can be extended to network analysis, indicating a cross-pollination of methodologies in these domains. <div>
arXiv:2505.00912v1 Announce Type: new 
Abstract: The relationship between the concepts of network and knowledge graph is explored. A knowledge graph can be considered a special type of network. When using a knowledge graph, various networks can be obtained from it, and network analysis procedures can be applied to them. RDF is a formalization of the knowledge graph concept for the Semantic Web, but some of its solutions are also extensible to a format for describing general networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 2. Format elements</title>
<link>https://arxiv.org/abs/2505.00921</link>
<guid>https://arxiv.org/abs/2505.00921</guid>
<content:encoded><![CDATA[
<div> networks, common format, key elements, describing, discussed <br />
Summary: 
This article delves into the essential components that a standardized format for describing networks should encompass. The discussion highlights the significance of establishing a common structure to accurately convey network information. Key elements identified for inclusion in such a format involve comprehensive descriptions of network configurations and characteristics. The necessity of incorporating specific details regarding network components, connections, and functionalities is emphasized to enhance the clarity and utility of network descriptions. The article underscores the importance of standardizing the language and terminology used to define network attributes to facilitate accurate communication and understanding among stakeholders. In conclusion, the article advocates for a systematic approach towards developing a universal format that can effectively capture the complexities and nuances of diverse network systems. <div>
arXiv:2505.00921v1 Announce Type: new 
Abstract: The key elements that a common format for describing networks should include are discussed.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Persistent Homology Distinguishes Simple and Complex Contagions with High Accuracy</title>
<link>https://arxiv.org/abs/2505.00958</link>
<guid>https://arxiv.org/abs/2505.00958</guid>
<content:encoded><![CDATA[
<div> classification, regression, extended persistent homology, simple contagion, complex contagion

Summary: The study explores distinguishing simple and complex contagions using extended persistent homology (EPH) in the context of network dynamics. Traditional methods struggle due to confounding factors and individual heterogeneity. EPH, applied to simulated contagion dynamics on real-world networks, effectively differentiates between simple and complex contagion processes and predicts their parameters. The models exhibit high predictive performance across various contagion parameters, even with noise and partial observability. EPH captures the influence of cycles of different lengths on contagion dynamics, providing a valuable metric for model classification and parameter prediction. The findings suggest that topological data analysis tools can aid in solving network optimization problems like seeding and vaccination strategies, as well as network inference and reconstruction challenges. <div>
arXiv:2505.00958v1 Announce Type: new 
Abstract: The social contagion literature makes a distinction between simple (independent cascade or bond percolation processes that pass infections through edges) and complex contagions (bootstrap percolation or threshold processes that require local reinforcement to spread). However, distinguishing simple and complex contagions using observational data poses a significant challenge in practice. Estimating population-level activation functions from observed contagion dynamics is hindered by confounding factors that influence adoptions (other than neighborhood interactions), as well as heterogeneity in individual behaviors and modeling variations that make it difficult to design appropriate null models for inferring contagion types. Here, we show that a new tool from topological data analysis (TDA), called extended persistent homology (EPH), when applied to contagion processes over networks, can effectively detect simple and complex contagion processes, as well as predict their parameters. We train classification and regression models using EPH-based topological summaries computed on simulated simple and complex contagion dynamics on three real-world network datasets and obtain high predictive performance over a wide range of contagion parameters and under a variety of informational constraints, including uncertainty in model parameters, noise, and partial observability of contagion dynamics. EPH captures the role of cycles of varying lengths in the observed contagion dynamics and offers a useful metric to classify contagion models and predict their parameters. Analyzing geometrical features of network contagion using TDA tools such as EPH can find applications in other network problems such as seeding, vaccination, and quarantine optimization, as well as network inference and reconstruction problems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-moderation in the decentralized era: decoding blocking behavior on Bluesky</title>
<link>https://arxiv.org/abs/2505.01174</link>
<guid>https://arxiv.org/abs/2505.01174</guid>
<content:encoded><![CDATA[
<div> Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking

Summary:
This study focuses on self-moderation through blocking behavior on the decentralized social networking platform Bluesky. By analyzing user activity over three months, the research aims to understand the connection between online behavior and the likelihood of being blocked. The study defines user profiles based on various features related to user activity, content characteristics, and network interactions. The research addresses two primary questions: whether users' blocking likelihood can be predicted from their behavior, and which behavioral features are linked to a higher chance of being blocked. The findings provide valuable insights into moderation on decentralized social networks and offer a robust analytical framework for future research in this area.<br /><br />Summary: Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking <div>
arXiv:2505.01174v1 Announce Type: new 
Abstract: Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell me who its founders are and I'll tell you what your online community looks like: Online community founders' personality and community attributes</title>
<link>https://arxiv.org/abs/2505.01219</link>
<guid>https://arxiv.org/abs/2505.01219</guid>
<content:encoded><![CDATA[
<div> personality traits, online communities, founders, sustainability, engagement

Summary:<br />
This study focuses on the personality traits of online community founders and their impact on community sustainability and attributes. By analyzing the Big Five personality traits of 35,164 founders in 8,625 Reddit communities, the study finds that founder traits play a significant role in determining community engagement, social network structure, and founder activity within the community. The research highlights the importance of considering behavioral and psychological aspects of community members and leaders in understanding and predicting community outcomes. The findings suggest that founder traits can serve as predictors of community success and offer valuable insights into the factors that contribute to the growth and longevity of online communities. <div>
arXiv:2505.01219v1 Announce Type: new 
Abstract: Online communities are an increasingly important stakeholder for firms, and despite the growing body of research on them, much remains to be learned about them and about the factors that determine their attributes and sustainability. Whereas most of the literature focuses on predictors such as community activity, network structure, and platform interface, there is little research about behavioral and psychological aspects of community members and leaders. In the present study we focus on the personality traits of community founders as predictors of community attributes and sustainability. We develop a tool to estimate community members' Big Five personality traits from their social media text and use it to estimate the traits of 35,164 founders in 8,625 Reddit communities. We find support for most of our predictions about the relationships between founder traits and community sustainability and attributes, including the level of engagement within the community, aspects of its social network structure, and whether the founders themselves remain active in it.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation</title>
<link>https://arxiv.org/abs/2505.00839</link>
<guid>https://arxiv.org/abs/2505.00839</guid>
<content:encoded><![CDATA[
<div> Keywords: auditory stimuli, affective computing, biometric signals, deep learning, stress monitoring <br />
Summary: 
This study examines the emotional and physiological effects of spiritual meditation, music, and natural silence on individuals using a new dataset called SMSAT. The researchers developed a deep learning model to extract features from the acoustic time series data, achieving high accuracy in classifying affective states. They also introduced the Calmness Analysis Model (CAM), a deep learning framework that combines handcrafted and learned features to classify affective states with 99.99% accuracy. The study found significant differences in cardiac response characteristics among the different auditory conditions, with spiritual meditation inducing the most pronounced physiological fluctuations. The proposed models outperformed existing methods in affective state classification tasks, indicating potential applications in stress monitoring, mental well-being, and therapeutic audio-based interventions. <br /><br />Summary: <div>
arXiv:2505.00839v1 Announce Type: cross 
Abstract: Understanding how auditory stimuli influence emotional and physiological states is fundamental to advancing affective computing and mental health technologies. In this paper, we present a multimodal evaluation of the affective and physiological impacts of three auditory conditions, that is, spiritual meditation (SM), music (M), and natural silence (NS), using a comprehensive suite of biometric signal measures. To facilitate this analysis, we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT) dataset, a novel benchmark comprising acoustic time series (ATS) signals recorded under controlled exposure protocols, with careful attention to demographic diversity and experimental consistency. To model the auditory induced states, we develop a contrastive learning based SMSAT audio encoder that extracts highly discriminative embeddings from ATS data, achieving 99.99% classification accuracy in interclass and intraclass evaluations. Furthermore, we propose the Calmness Analysis Model (CAM), a deep learning framework integrating 25 handcrafted and learned features for affective state classification across auditory conditions, attaining robust 99.99% classification accuracy. In contrast, pairwise t tests reveal significant deviations in cardiac response characteristics (CRC) between SM analysis via ANOVA inducing more significant physiological fluctuations. Compared to existing state of the art methods reporting accuracies up to 90%, the proposed model demonstrates substantial performance gains (up to 99%). This work contributes a validated multimodal dataset and a scalable deep learning framework for affective computing applications in stress monitoring, mental well-being, and therapeutic audio-based interventions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Data-centric Directed Graph Learning: An Entropy-driven Approach</title>
<link>https://arxiv.org/abs/2505.00983</link>
<guid>https://arxiv.org/abs/2505.00983</guid>
<content:encoded><![CDATA[
<div> Keywords: DiGraph Neural Networks, knowledge distillation, hierarchical encoding theory, topology, graph datasets

Summary: 
The paper introduces EDEN, a novel approach for data-centric learning in directed graphs. EDEN leverages hierarchical knowledge trees constructed from directed structural measurements to refine knowledge flow and enhance data-centric knowledge distillation during model training. By quantifying mutual information between node profiles, EDEN significantly improves the predictive performance of (Di)Graph Neural Networks across various graph datasets and downstream tasks. The proposed framework not only achieves state-of-the-art results but also demonstrates strong enhancements for existing (Di)GNN models. This approach paves the way for a deeper exploration of correlations between directed edges and node profiles in complex topology systems, highlighting the importance of data-centric perspectives for enhancing model-centric neural networks.

<br /><br />Summary: <div>
arXiv:2505.00983v1 Announce Type: cross 
Abstract: The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a data-centric digraph learning paradigm or a model-agnostic hot-and-plug data-centric Knowledge Distillation (KD) module. The core idea is to achieve data-centric ML, guided by our proposed hierarchical encoding theory for structured data. Specifically, EDEN first utilizes directed structural measurements from a topology perspective to construct a coarse-grained Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual information of node profiles to refine knowledge flow in the HKT, enabling data-centric KD supervision within model training. As a general framework, EDEN can also naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets (homophily and heterophily) and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering complementary information sharing in spider monkey collective foraging using higher-order spatial networks</title>
<link>https://arxiv.org/abs/2505.01167</link>
<guid>https://arxiv.org/abs/2505.01167</guid>
<content:encoded><![CDATA[
<div> Collectives, distributed processing, fission-fusion dynamics, foraging information, simplicial complexes <br />
Summary:<br />
The study focuses on how collectives can process information in a distributed manner through fission-fusion dynamics. By analyzing the overlaps between individual core ranges that represent seasonal knowledge, the research identifies sets of individuals with balanced overlap between redundantly and uniquely known areas. Using simplicial complexes, higher-order interactions are represented, revealing complementarity in shared foraging information. The complex spatial networks from fission-fusion dynamics enable adaptive collective processing of foraging information in dynamic environments. <div>
arXiv:2505.01167v1 Announce Type: cross 
Abstract: Collectives are often able to process information in a distributed fashion, surpassing each individual member's processing capacity. In fission-fusion dynamics, where group members come together and split from others often, sharing complementary information about uniquely known foraging areas could allow a group to track a heterogenous foraging environment better than any group member on its own. We analyse the partial overlaps between individual core ranges, which we assume represent the knowledge of an individual during a given season. We identify sets of individuals whose overlap shows a balance between redundantly and uniquely known portions and we use simplicial complexes to represent these higher-order interactions. The structure of the simplicial complexes shows holes in various dimensions, revealing complementarity in the foraging information that is being shared. We propose that the complex spatial networks arising from fission-fusion dynamics allow for adaptive, collective processing of foraging information in dynamic environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings</title>
<link>https://arxiv.org/abs/2503.12994</link>
<guid>https://arxiv.org/abs/2503.12994</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, abuse detection, representation learning, textual content, conversational graphs

Summary:
The article addresses the common issue of abusive behavior on online social networks and proposes a novel approach using representation learning methods to generate embeddings of both textual content and conversational graphs. Two methods are proposed to learn whole-graph representations using edge directions, weights, signs, and vertex attributes. The study experiments with various textual and graph embedding methods on a dataset annotated for abuse detection, achieving high F-measure scores of 81.02 using text alone and 80.61 using graphs alone. Combining both modalities through fusion strategies significantly improves abuse detection performance, increasing the F-measure to 87.06. The study also identifies specific engineered features captured by the embedding methods, shedding light on the discriminative information considered by the representation learning methods. <div>
arXiv:2503.12994v2 Announce Type: replace 
Abstract: Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn wholegraph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief System Dynamics as Network of Single Layered Neural Network</title>
<link>https://arxiv.org/abs/2505.00005</link>
<guid>https://arxiv.org/abs/2505.00005</guid>
<content:encoded><![CDATA[
<div> belief propagation, social network, polarization, misinformation, neural network

Summary:
In this study, a modified model of the Friedkin-Johnsen model was proposed to investigate belief propagation on social networks. The model treated individuals as single-layer neural networks, with confidence levels on evidence as inputs and belief as the output. The research reaffirmed Madison's remedy for factionalism and found that a network with a giant component reduced belief distribution variance more than a network with two communities, despite creating more social pressure. Additionally, a community structure decreased sensitivity of belief distribution variance to individual confidence levels. The model's insights have implications for political polarization, misinformation, economic conflicts, as well as applications in personality theory and behavioral psychology. <div>
arXiv:2505.00005v1 Announce Type: new 
Abstract: As problems in political polarization and the spread of misinformation become serious, belief propagation on a social network becomes an important question to explore. Previous breakthroughs have been made in algorithmic approaches to understanding how group consensus or polarization can occur in a population. This paper proposed a modified model of the Friedkin-Johnsen model that tries to explain the underlying stubbornness of individual as well as possible back fire effect by treating each individual as a single layer neural network on a set of evidence for a particular statement with input being confidence level on each evidence, and belief of the statement is the output of this neural network.
  In this papar, we reafirmed the importance of Madison's cure for the mischief of faction, and found that when structure of understanding is polarized, a network with a giant component can decrease the variance in the belief distribution more than a network with two communities, but creates more social pressure by doing so. We also found that when community structure is formed, variance in the belief distribution become less sensitive to confidence level of individuals. The model can have various applications to political and historical problems caused by misinfomation and conflicting economic interest as well as applications to personality theory and behavior psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams</title>
<link>https://arxiv.org/abs/2505.00242</link>
<guid>https://arxiv.org/abs/2505.00242</guid>
<content:encoded><![CDATA[
<div> Interpretable, Partial Differential Equation, Tensor Decomposition, Time-Varying, Forecasting <br />
<br />
Summary: 
The paper introduces D-Tracker, a method for capturing time-varying temporal patterns in social activity tensor data streams and forecasting future activities. D-Tracker utilizes a tensor decomposition framework incorporating partial differential equations to interpret trends, seasonality, and interest diffusion between locations. It automatically models tensor data streams without the need for hyperparameters and is computationally scalable. Experiments using web search volume and COVID-19 infection data demonstrate D-Tracker's superior forecasting accuracy and efficiency compared to existing methods, highlighting its ability to extract location-based interest diffusion information. The D-Tracker source code and datasets are freely available for access, enabling further research and applications in analyzing and predicting social activity patterns.  <br /><br />Summary: <div>
arXiv:2505.00242v1 Announce Type: new 
Abstract: Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-Tracker, a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a) Interpretable: it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b) Automatic: it has no hyperparameters and continuously models tensor data streams fully automatically; (c) Scalable: the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at {https://github.com/Higashiguchi-Shingo/D-Tracker.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avatar Communication Provides More Efficient Online Social Support Than Text Communication</title>
<link>https://arxiv.org/abs/2505.00287</link>
<guid>https://arxiv.org/abs/2505.00287</guid>
<content:encoded><![CDATA[
<div> avatar communication, online social support, social relationships, offline social resources, metaverse societies
<br />
Summary: 
The study investigates the differences in online social support between avatar communication service users and text communication service users. Avatar communication users received more online social support, had more stable relationships, and had fewer offline social resources compared to text communication users. However, the positive association between online and offline social support was stronger for avatar communication users. The study emphasizes the importance of realistic online communication experiences through avatars, including nonverbal and real-time interactions. It also highlights the challenges faced by avatar communication users in the physical world, such as the lack of offline social resources. Enhancing online social support through avatars could help address these issues and potentially improve social resource problems in both online and offline settings in future metaverse societies. 
<br /> <div>
arXiv:2505.00287v1 Announce Type: new 
Abstract: Online communication via avatars provides a richer online social experience than text communication. This reinforces the importance of online social support. Online social support is effective for people who lack social resources because of the anonymity of online communities. We aimed to understand online social support via avatars and their social relationships to provide better social support to avatar users. Therefore, we administered a questionnaire to three avatar communication service users (Second Life, ZEPETO, and Pigg Party) and three text communication service users (Facebook, X, and Instagram) (N=8,947). There was no duplication of users for each service. By comparing avatar and text communication users, we examined the amount of online social support, stability of online relationships, and the relationships between online social support and offline social resources (e.g., offline social support). We observed that avatar communication service users received more online social support, had more stable relationships, and had fewer offline social resources than text communication service users. However, the positive association between online and offline social support for avatar communication users was more substantial than for text communication users. These findings highlight the significance of realistic online communication experiences through avatars, including nonverbal and real-time interactions with co-presence. The findings also highlighted avatar communication service users' problems in the physical world, such as the lack of offline social resources. This study suggests that enhancing online social support through avatars can address these issues. This could help resolve social resource problems, both online and offline in future metaverse societies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Sexual Predation and Victimization Through Warnings and Awareness among High-Risk Users</title>
<link>https://arxiv.org/abs/2505.00293</link>
<guid>https://arxiv.org/abs/2505.00293</guid>
<content:encoded><![CDATA[
<div> Keywords: online sexual predators, prevention strategy, high-risk individuals, machine learning, randomized controlled trial

Summary:
This study focused on preventing online sexual predation by targeting high-risk individuals through warnings and awareness-building messages based on criminal psychology theories. Using a machine learning model, high-risk users on an avatar-based communication platform were identified and divided into intervention and control groups. The intervention successfully reduced violations and victimization among women for a significant period, highlighting the effectiveness of targeted interventions in preventing online sexual abuse. However, the impact on men was not as pronounced, indicating a need for gender-specific prevention strategies. These findings contribute to the ongoing efforts to combat online sexual predators and enhance understanding of criminal psychology in the digital age. 

<br /><br />Summary: <div>
arXiv:2505.00293v1 Announce Type: new 
Abstract: Online sexual predators target children by building trust, creating dependency, and arranging meetings for sexual purposes. This poses a significant challenge for online communication platforms that strive to monitor and remove such content and terminate predators' accounts. However, these platforms can only take such actions if sexual predators explicitly violate the terms of service, not during the initial stages of relationship-building. This study designed and evaluated a strategy to prevent sexual predation and victimization by delivering warnings and raising awareness among high-risk individuals based on the routine activity theory in criminal psychology. We identified high-risk users as those with a high probability of committing or being subjected to violations, using a machine learning model that analyzed social networks and monitoring data from the platform. We conducted a randomized controlled trial on a Japanese avatar-based communication application, Pigg Party. High-risk players in the intervention group received warnings and awareness-building messages, while those in the control group did not receive the messages, regardless of their risk level. The trial involved 12,842 high-risk players in the intervention group and 12,844 in the control group for 138 days. The intervention successfully reduced violations and being violated among women for 12 weeks, although the impact on men was limited. These findings contribute to efforts to combat online sexual abuse and advance understanding of criminal psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> virtual model, U.S. congresspersons, language models, digital twin, Tweets 

Summary: 
This paper presents a virtual model of U.S. congresspersons using language models, which qualifies as a digital twin. A dataset containing Tweets from congresspersons is analyzed, and language models simulate their Tweets accurately. These generated Tweets can predict voting behavior and the likelihood of bipartisanship, aiding in resource allocation and legislative dynamics. The study discusses the analysis's limitations and potential extensions. <div>
arXiv:2505.00006v1 Announce Type: cross 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)</title>
<link>https://arxiv.org/abs/2505.00393</link>
<guid>https://arxiv.org/abs/2505.00393</guid>
<content:encoded><![CDATA[
<div> semantic, subgraph similarity search, keyword set, aggregated neighbor difference, indexing mechanism  
Summary:  
- This paper introduces the Subgraph Similarity Search under Aggregated Neighbor Difference Semantics (S$^3$AND) problem, which aims to find subgraphs in a data graph that are similar to a query graph by considering keywords and graph structures. 
- The authors propose two pruning methods, namely keyword set and aggregated neighbor difference lower bound pruning, to reduce the search space by eliminating false alarms of candidate vertices/subgraphs. 
- An effective indexing mechanism is designed to support the efficient S$^3$AND query answering algorithm. 
- Extensive experiments show the effectiveness and efficiency of the S$^3$AND approach on both real and synthetic graphs across various parameter settings.  
Summary: <div>
arXiv:2505.00393v1 Announce Type: cross 
Abstract: For the past decades, the \textit{subgraph similarity search} over a large-scale data graph has become increasingly important and crucial in many real-world applications, such as social network analysis, bioinformatics network analytics, knowledge graph discovery, and many others. While previous works on subgraph similarity search used various graph similarity metrics such as the graph isomorphism, graph edit distance, and so on, in this paper, we propose a novel problem, namely \textit{subgraph similarity search under aggregated neighbor difference semantics} (S$^3$AND), which identifies subgraphs $g$ in a data graph $G$ that are similar to a given query graph $q$ by considering both keywords and graph structures (under new keyword/structural matching semantics). To efficiently tackle the S$^3$AND problem, we design two effective pruning methods, \textit{keyword set} and \textit{aggregated neighbor difference lower bound pruning}, which rule out false alarms of candidate vertices/subgraphs to reduce the S$^3$AND search space. Furthermore, we construct an effective indexing mechanism to facilitate our proposed efficient S$^3$AND query answering algorithm. Through extensive experiments, we demonstrate the effectiveness and efficiency of our S$^3$AND approach over both real and synthetic graphs under various parameter settings.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Cultural and Digital Divides: A Low-Latency JackTrip Framework for Equitable Music Education in the Global South</title>
<link>https://arxiv.org/abs/2505.00550</link>
<guid>https://arxiv.org/abs/2505.00550</guid>
<content:encoded><![CDATA[
<div> Keywords: digital technologies, music education, JackTrip framework, Global South, cultural preservation

Summary: 
This paper introduces a low-latency JackTrip framework that addresses infrastructural and cultural challenges in music education in the Global South. The framework utilizes an open-source UDP-based audio streaming protocol to overcome technical constraints like limited bandwidth and high latency prevalent in rural and underserved regions. A comparison with conventional platforms like Zoom shows that JackTrip achieves sub-30 ms latency under simulated low-resource conditions while maintaining intricate audio details crucial for non-Western musical traditions. Spectral analysis confirms JackTrip's ability to handle microtonal scales, complex rhythms, and harmonic textures, providing an authentic medium for real-time ensemble performance and music education. These results highlight the potential of decentralized, edge-computing solutions in promoting technological equity and cultural preservation among educators and musicians in the Global South.<br /><br />Summary: <div>
arXiv:2505.00550v1 Announce Type: cross 
Abstract: The rapid expansion of digital technologies has transformed educational landscapes worldwide, yet significant infrastructural and cultural challenges persist in the Global South. This paper introduces a low-latency JackTrip framework designed to bridge both the cultural and digital divides in music education. By leveraging an open-source, UDP-based audio streaming protocol originally developed at Stanford's CCRMA, the framework is tailored to address technical constraints such as intermittent connectivity, limited bandwidth, and high latency that characterize many rural and underserved regions. The study systematically compares the performance of JackTrip with conventional platforms like Zoom, demonstrating that JackTrip achieves sub-30~ms latency under simulated low-resource conditions while preserving the intricate audio details essential for non-Western musical traditions. Spectral analysis confirms that JackTrip's superior handling of microtonal scales, complex rhythms, and harmonic textures provides a culturally authentic medium for real-time ensemble performance and music education. These findings underscore the transformative potential of decentralized, edge-computing solutions in empowering educators and musicians across the Global South, promoting both technological equity and cultural preservation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new sociology of humans and machines</title>
<link>https://arxiv.org/abs/2402.14410</link>
<guid>https://arxiv.org/abs/2402.14410</guid>
<content:encoded><![CDATA[
<div> Keywords: fake social media accounts, generative artificial intelligence, complex social systems, human-machine interactions, collective decision-making

Summary:<br /><br />
The article discusses the proliferation of robots, bots, and algorithms in various aspects of society and the need to study the interactions between humans and intelligent machines. It reviews research on competition, coordination, cooperation, contagion, and collective decision-making in complex social systems. Examples are provided from high-frequency trading markets, social media platforms, open collaboration communities, and discussion forums. The importance of a new sociology of humans and machines is emphasized, highlighting the need for researchers to use complex system methods, engineers to design AI for human-machine and machine-machine interactions, and regulators to govern the development of human-machine communities. By understanding and addressing the dynamics and patterns in human-machine interactions, we can ensure the resilience and robustness of these communities in the face of increasing technological integration. <div>
arXiv:2402.14410v3 Announce Type: replace 
Abstract: From fake social media accounts and generative artificial intelligence chatbots to trading algorithms and self-driving vehicles, robots, bots and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions and transportation arteries. Networks of multiple interdependent and interacting humans and intelligent machines constitute complex social systems for which the collective outcomes cannot be deduced from either human or machine behaviour alone. Under this paradigm, we review recent research and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open collaboration community and a discussion forum. To ensure more robust and resilient human-machine communities, we require a new sociology of humans and machines. Researchers should study these communities using complex system methods; engineers should explicitly design artificial intelligence for human-machine and machine-machine interactions; and regulators should govern the ecological diversity and social co-development of humans and machines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of households as entities in social networks</title>
<link>https://arxiv.org/abs/2502.14764</link>
<guid>https://arxiv.org/abs/2502.14764</guid>
<content:encoded><![CDATA[
<div> household, individual, social network, entitativity, data analysis  
Summary:  
This article discusses the differences between household and individual social networks and the importance of choosing the correct network for study design and data analysis. The study explores how the results of social network analysis can vary depending on whether the household or individual network is studied, impacting findings on assortativity, influence-maximizing nodes, and information spread within households. The authors propose systematic recommendations for determining the relevant network representation to study, considering entitativity criteria and cultural or experimental contexts. They highlight the illusion of entitativity as a factor where grouping individuals into households may not adequately capture social dynamics. Understanding which network to study is crucial for researchers and practitioners analyzing social network data, and this work aims to provide guidance for making informed decisions in data collection and analysis.  
<br /><br />Summary: <div>
arXiv:2502.14764v2 Announce Type: replace 
Abstract: Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omitted Labels Induce Nontransitive Paradoxes in Causality</title>
<link>https://arxiv.org/abs/2311.06840</link>
<guid>https://arxiv.org/abs/2311.06840</guid>
<content:encoded><![CDATA[
<div> omitted label contexts, training data, specialized human experts, Simpson's paradox, nontransitive structures <br />
Summary: 
The article discusses the concept of omitted label contexts in training data, common in specialized fields or focused studies. It explores how adjustments in such contexts may require non-exchangeable treatment and control groups. Through studying Simpson's paradox, the article identifies the existence of nontransitivity in networks of conclusions drawn from different contexts. It demonstrates that the space of possible nontransitive structures in these networks corresponds to structures formed from aggregating ranked-choice votes. Overall, the study sheds light on the complexities of analyzing datasets with limited label contexts and the implications of nontransitivity in drawing conclusions from diverse sets of data. <div>
arXiv:2311.06840v4 Announce Type: replace-cross 
Abstract: We explore "omitted label contexts," in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific, focused studies. By studying Simpson's paradox, we observe that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Updating Katz centrality by counting walks</title>
<link>https://arxiv.org/abs/2411.19560</link>
<guid>https://arxiv.org/abs/2411.19560</guid>
<content:encoded><![CDATA[
<div> Efficient, effective, update, Katz centralities, node removal, edge removal, loss of walks, network, algorithms, F-avoiding first-passage walks, total network communicability, numerical experiments, synthetic networks, real-world networks.

Summary:
This article introduces strategies for updating Katz centralities in simple graphs following node and edge removal. Formulas for measuring the "loss of walks" in a network due to these removals are provided, based on the concept of F-avoiding first-passage walks. The article also presents algorithms informed by these formulas and derives bounds on changes in total network communicability. Extensive numerical experiments on both synthetic and real-world networks validate the theoretical findings. The study emphasizes efficient and effective approaches for maintaining centrality measures in networks undergoing structural changes. <div>
arXiv:2411.19560v2 Announce Type: replace-cross 
Abstract: We develop efficient and effective strategies for the update of Katz centralities after node and edge removal in simple graphs. We provide explicit formulas for the ``loss of walks" a network suffers when nodes/edges are removed, and use these to inform our algorithms. The theory builds on the newly introduced concept of $\cF$-avoiding first-passage walks. Further, bounds on the change of total network communicability are also derived. Extensive numerical experiments on synthetic and real-world networks complement our theoretical results.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Small Communities in the Planted Partition Model</title>
<link>https://arxiv.org/abs/2504.01663</link>
<guid>https://arxiv.org/abs/2504.01663</guid>
<content:encoded><![CDATA[
<div> community recovery, planted partition model, correlation coefficient, Diamond Percolation, power-law distribution<br />
Summary:<br />
In this paper, the authors analyze community recovery in the planted partition model (PPM) with a focus on scenarios with a large number of communities. They redefine recovery regimes using the correlation coefficient to accommodate varying community sizes. The Diamond Percolation algorithm is introduced as an effective method for recovering communities with minimal constraints on community numbers and sizes. The algorithm shows promising results under mild assumptions on edge probabilities. Additionally, the study considers unbalanced partitions, particularly when community sizes follow a power-law distribution, which is common in real-world networks. These findings present valuable insights into community recovery techniques in complex network structures. <div>
arXiv:2504.01663v2 Announce Type: replace-cross 
Abstract: We analyze community recovery in the planted partition model (PPM) in regimes where the number of communities is arbitrarily large. We examine the three standard recovery regimes: exact recovery, almost exact recovery, and weak recovery. When communities vary in size, traditional accuracy- or alignment-based metrics become unsuitable for assessing the correctness of a predicted partition. To address this, we redefine these recovery regimes using the correlation coefficient, a more versatile metric for comparing partitions. We then demonstrate that $\textit{Diamond Percolation}$, an algorithm based on common-neighbors, successfully recovers communities under mild assumptions on edge probabilities, with minimal restrictions on the number and sizes of communities. As a key application, we consider the case where community sizes follow a power-law distribution, a characteristic frequently found in real-world networks. To the best of our knowledge, we provide the first recovery results for such unbalanced partitions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection</title>
<link>https://arxiv.org/abs/2504.21357</link>
<guid>https://arxiv.org/abs/2504.21357</guid>
<content:encoded><![CDATA[
<div> Keywords: information cocoon, social media debates, double-layer network, graph auto-encoder, community detection algorithms

Summary: 
This paper addresses the issue of information cocoons in social media debates resulting from homogeneous viewpoints and preferences clustering users into sub-networks. The authors propose a double-layer network model considering relational ties and feature-based user similarity. They develop graph auto-encoder based community detection algorithms to identify and break information cocoons. Testing on real and synthetic datasets shows the proposed algorithms outperform existing methods in partitioning user communities. An intervention strategy based on influence is introduced, showing how the algorithms can effectively reduce polarization and information cocoon formation with minimal intervention. The Markov states transition model is used to simulate intervention effects, demonstrating the effectiveness of the proposed algorithms in mitigating information cocoons. <br /><br />Summary: <div>
arXiv:2504.21357v1 Announce Type: new 
Abstract: With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Machine Learning for characterizing social networks Agent-based models</title>
<link>https://arxiv.org/abs/2504.21609</link>
<guid>https://arxiv.org/abs/2504.21609</guid>
<content:encoded><![CDATA[
<div> Keywords: social media networks, agent-based modeling, High Performance Computing, Machine Learning, user behaviors

Summary: 
Agent-based modeling (ABM) is a valuable tool for studying social media networks, allowing for the simulation of individual behaviors and system-level evolution. However, the complexity of modeling social networks requires superior data processing and storage capabilities, which can be provided by High Performance Computing (HPC). By leveraging Machine Learning (ML) methods, researchers can efficiently analyze vast amounts of data from social media users to better understand behaviors, preferences, and trends. This proposal aims to use ML to characterize user attributes and develop a general user model for ABM simulations of social networks on HPC systems. By combining ABM, HPC, and ML, researchers can gain valuable insights into social network dynamics and user interactions. <div>
arXiv:2504.21609v1 Announce Type: new 
Abstract: Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual users, abusive language detection, code-mixed text, low-resource languages, NLP<br />
Summary:<br />
- Detecting abusive language in code-mixed text poses challenges due to linguistic blending and context dependency.
- A manually annotated dataset of Telugu-English and Nepali-English code-mixed comments was introduced for abusive language detection.
- Different machine learning and deep learning models were experimented with, including Logistic Regression, Neural Networks, and Large Language Models.
- Performance was optimized through hyperparameter tuning and evaluated using 10-fold cross-validation.
- The study provides insights into the difficulties of detecting abusive language in code-mixed settings and establishes benchmarks for abusive language detection in low-resource languages like Telugu and Nepali. This can aid in the development of more robust moderation strategies for multilingual social media environments.<br /> <div>
arXiv:2504.21026v1 Announce Type: cross 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
<link>https://arxiv.org/abs/2504.21060</link>
<guid>https://arxiv.org/abs/2504.21060</guid>
<content:encoded><![CDATA[
<div> Keywords: government-led narratives, mass media, Innovation-Driven Development Strategy, total factor productivity, economic growth

Summary:
The article presents the "Narratives-Construct-Commitment (NCC)" framework, which explores how government-led narratives evolve from framing expectations to becoming sustainable pillars for growth. By analyzing the Innovation-Driven Development Strategy of 2016 as a case study, the study identifies the impact of narrative shocks and their influence on investment incentives, R&amp;D resources, and total factor productivity (TFP). The findings highlight the role of credible narratives in shaping expectations, driving economic growth, and institutionalizing vision for sustained improvements. The research provides insights into the transformation of visions into tangible economic outcomes through the strategic use of narratives in policy-making and development initiatives. <div>
arXiv:2504.21060v1 Announce Type: cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ``Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Manipulated Contents Using Knowledge-Grounded Inference</title>
<link>https://arxiv.org/abs/2504.21165</link>
<guid>https://arxiv.org/abs/2504.21165</guid>
<content:encoded><![CDATA[
<div> Fake news, manipulated content, detection, zero-day, mainstream search engines <br />
<br />
Summary: 
The article introduces Manicod, a tool designed to detect zero-day manipulated content by utilizing contextual information from mainstream search engines. By sourcing real-time context and using a large language model (LLM) with retrieval-augmented generation (RAG), Manicod can determine if a piece of content is truthful or manipulated, providing an explanation for its decision. The tool is validated using a dataset of 4270 manipulated fake news articles and achieves an overall F1 score of 0.856, outperforming existing methods in fact-checking and claim verification. Manicod addresses the challenge of zero-day manipulated content, offering a promising solution for identifying fake news in real-time scenarios. <div>
arXiv:2504.21165v1 Announce Type: cross 
Abstract: The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Metric Dimension of Sparse Random Graphs</title>
<link>https://arxiv.org/abs/2504.21244</link>
<guid>https://arxiv.org/abs/2504.21244</guid>
<content:encoded><![CDATA[
<div> bounds, metric dimension, random graphs, Erds-Rnyi, connectivity transition <br />
Summary: 
The article presents upper and lower bounds on the likely metric dimension of Erds-Rnyi random graphs. Previous research had provided bounds for random graphs with expected degrees greater than or equal to log^5 n, leaving a gap for sparser graphs with lower expected degrees. The new bounds cover the range just above the connectivity transition, where the expected degree is a constant multiple of the logarithm of n, up to log^5 n. The lower bound is based on an entropic argument, offering a more general approach compared to previous methodologies, while the upper bound is similar to existing results. These findings contribute to understanding the metric properties of random graphs across a wide range of densities. <br /><br />Summary: <div>
arXiv:2504.21244v1 Announce Type: cross 
Abstract: In 2013, Bollob\'as, Mitsche, and Pralat at gave upper and lower bounds for the likely metric dimension of random Erd\H{o}s-R\'enyi graphs $G(n,p)$ for a large range of expected degrees $d=pn$. However, their results only apply when $d \ge \log^5 n$, leaving open sparser random graphs with $d < \log^5 n$. Here we provide upper and lower bounds on the likely metric dimension of $G(n,p)$ from just above the connectivity transition, i.e., where $d=pn=c \log n$ for some $c > 1$, up to $d=\log^5 n$. Our lower bound technique is based on an entropic argument which is more general than the use of Suen's inequality by Bollob\'as, Mitsche, and Pralat, whereas our upper bound is similar to theirs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defense Against Shortest Path Attacks</title>
<link>https://arxiv.org/abs/2305.19083</link>
<guid>https://arxiv.org/abs/2305.19083</guid>
<content:encoded><![CDATA[
<div> defense, shortest paths, graph manipulation, Stackelberg game, NP-hard

Summary:
This paper addresses the issue of defending against malicious manipulation of graphs to control traffic flow between nodes. The proposed defense strategy involves modifying edge weights in order to recommend shortest paths to users while maintaining the integrity of the original graph. The defender aims to minimize the probability of attacks from malicious actors while minimizing negative impacts on benign users. The defense is formulated as a Stackelberg game, with the defender taking the leading role. The problem is proven to be NP-hard, and heuristic solutions are proposed for both zero-sum and non-zero-sum scenarios. By formulating a linear program for local optimization, the defense strategy achieves results close to the lower bound of the defender's cost. Experimental results with synthetic and real networks demonstrate the effectiveness of the proposed methods in defending against graph manipulation attacks. 

<br /><br />Summary: <div>
arXiv:2305.19083v2 Announce Type: replace 
Abstract: Identifying shortest paths between nodes in a network is an important task in many applications. Recent work has shown that a malicious actor can manipulate a graph to make traffic between two nodes of interest follow their target path. In this paper, we develop a defense against such attacks by modifying the edge weights that users observe. The defender must balance inhibiting the attacker against any negative effects on benign users. Specifically, the defender's goals are: (a) recommend the shortest paths to users, (b) make the lengths of the shortest paths in the published graph close to those of the same paths in the true graph, and (c) minimize the probability of an attack. We formulate the defense as a Stackelberg game in which the defender is the leader and the attacker is the follower. We also consider a zero-sum version of the game in which the defender's goal is to minimize cost while achieving the minimum possible attack probability. We show that the defense problem is NP-hard and propose heuristic solutions for both the zero-sum and non-zero-sum settings. By relaxing some constraints of the original problem, we formulate a linear program for local optimization around a feasible point. We present defense results with both synthetic and real networks and show that our methods often reach the lower bound of the defender's cost.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOPIM: Bayesian Optimization for influence maximization on temporal networks</title>
<link>https://arxiv.org/abs/2308.04700</link>
<guid>https://arxiv.org/abs/2308.04700</guid>
<content:encoded><![CDATA[
<div> Bayesian Optimization, Influence Maximization, Temporal Networks, Gaussian Process Regression, Expected Improvement <br />
Summary:<br />
The study introduces BOPIM, a Bayesian Optimization approach for Influence Maximization on temporal networks. The challenges addressed include constructing kernel functions based on Hamming distance and Jaccard coefficient, and optimizing the acquisition function using Expected Improvement with noise adjustment. Numerical experiments on real-world networks show that BOPIM outperforms other methods and achieves comparable influence spreads to a gold-standard greedy algorithm, with a significantly faster runtime. Surprisingly, the Hamming kernel performs better than the Jaccard kernel. The study also explores ways to quantify uncertainty in optimal seed sets, a novel approach in Influence Maximization research. <div>
arXiv:2308.04700v4 Announce Type: replace 
Abstract: The goal of influence maximization (IM) is to select a small set of seed nodes which maximizes the spread of influence on a network. In this work, we propose BOPIM, a Bayesian Optimization (BO) algorithm for IM on temporal networks. The IM task is well-suited for a BO solution due to its expensive and complicated objective function. There are at least two key challenges, however, that must be overcome, primarily due to the inputs coming from a cardinality-constrained, non-Euclidean, combinatorial space. The first is constructing the kernel function for the Gaussian Process regression. We propose two kernels, one based on the Hamming distance between seed sets and the other leveraging the Jaccard coefficient between node's neighbors. The second challenge is the acquisition function. For this, we use the Expected Improvement function, suitably adjusting for noise in the observations, and optimize it using a greedy algorithm to account for the cardinality constraint. In numerical experiments on real-world networks, we prove that BOPIM outperforms competing methods and yields comparable influence spreads to a gold-standard greedy algorithm while being as much as ten times faster. In addition, we find that the Hamming kernel performs favorably compared to the Jaccard kernel in nearly all settings, a somewhat surprising result as the former does not explicitly account for the graph structure. Finally, we demonstrate two ways that the proposed method can quantify uncertainty in optimal seed sets. To our knowledge, this is the first attempt to look at uncertainty in the seed sets for IM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, HateSieve, Hateful Memes, Contrastive Meme Generator, Memes 

Summary:<br /><br />Amidst the increasing use of Large Multimodal Models (LMMs) in creating and interpreting complex content, the threat of spreading biased and harmful memes persists. Current safety protocols often struggle to uncover hate speech subtly embedded in "Confounder Memes." To tackle this issue, the researchers introduce HateSieve, a new framework focused on improving the detection and segmentation of hateful elements in memes. HateSieve employs a unique Contrastive Meme Generator to create semantically paired memes, a tailored triplet dataset for contrastive learning, and an Image-Text Alignment module to generate context-aware embeddings for precise meme segmentation. Experimental results using the Hateful Meme Dataset demonstrate that HateSieve outperforms existing LMMs in accuracy with fewer parameters while providing a reliable method for pinpointing and isolating hate speech within memes. Viewer discretion is advised due to the academic discussions of hate speech. <div>
arXiv:2408.05794v2 Announce Type: replace-cross 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction</title>
<link>https://arxiv.org/abs/2504.20492</link>
<guid>https://arxiv.org/abs/2504.20492</guid>
<content:encoded><![CDATA[
<div> Link prediction, TriHetGCN, Graph Convolutional Networks, topological indicators, connection probability <br />
<br />
Summary: TriHetGCN is proposed to enhance link prediction in complex networks. It integrates topological indicators like triadic closure and degree heterogeneity into the Graph Convolutional Networks (GCNs) framework. The model consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. By incorporating node features, TriHetGCN improves global structure perception and effectively captures intrinsic structural relationships between node pairs. Evaluated on various real-world datasets, TriHetGCN outperforms existing methods and demonstrates strong generalizability across different network types. This work bridges statistical physics and graph deep learning, offering a promising framework for diverse applications in link prediction. <div>
arXiv:2504.20492v1 Announce Type: new 
Abstract: Link prediction aims to estimate the likelihood of connections between pairs of nodes in complex networks, which is beneficial to many applications from friend recommendation to metabolic network reconstruction. Traditional heuristic-based methodologies in the field of complex networks typically depend on predefined assumptions about node connectivity, limiting their generalizability across diverse networks. While recent graph neural network (GNN) approaches capture global structural features effectively, they often neglect node attributes and intrinsic structural relationships between node pairs. To address this, we propose TriHetGCN, an extension of traditional Graph Convolutional Networks (GCNs) that incorporates explicit topological indicators -- triadic closure and degree heterogeneity. TriHetGCN consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. The topology feature module constructs node features using shortest path distances to anchor nodes, enhancing global structure perception. The graph structural module integrates topological indicators into the GCN framework to model triadic closure and heterogeneity. The connection probability module uses deep learning to predict links. Evaluated on nine real-world datasets, from traditional networks without node attributes to large-scale networks with rich features, TriHetGCN achieves state-of-the-art performance, outperforming mainstream methods. This highlights its strong generalization across diverse network types, offering a promising framework that bridges statistical physics and graph deep learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts</title>
<link>https://arxiv.org/abs/2504.20065</link>
<guid>https://arxiv.org/abs/2504.20065</guid>
<content:encoded><![CDATA[
<div> Plato, Aristotle, references, network analysis, historical works
Summary: The study used computational methods to analyze references in 2,245 philosophical texts from 550 BCE to 1940 AD. It mapped over 294,970 references between authors to measure how philosophical ideas spread over time. Plato and Aristotle accounted for nearly 10% of all references, indicating their significant influence. The analysis supported the view of St. Thomas Aquinas as a synthesizer between Aristotelian and Christian philosophy. The results were presented through an interactive visualization tool, allowing users to explore the networks dynamically. The methodology demonstrated the value of applying network analysis to study the intellectual lineages of philosophical scholarship through textual references. <div>
arXiv:2504.20065v1 Announce Type: cross 
Abstract: We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication in Agile Software Development - A Mapping Study</title>
<link>https://arxiv.org/abs/2504.20186</link>
<guid>https://arxiv.org/abs/2504.20186</guid>
<content:encoded><![CDATA[
<div> Keywords: Software industry, Agile software development, Communication, Review, Research gaps

Summary: 
In the fast-paced software industry, Agile software development (ASD) is crucial to ensure fast and efficient development processes. However, despite ASD being prevalent for over two decades, there are still many unknowns related to it. This study focuses on the critical factor of communication within ASD. Through a review of 14 studies, the areas of interest and research gaps in ASD communication were identified. The community's interest in communication within ASD was highlighted, shedding light on the importance of effective communication in agile development processes. Addressing these research gaps can lead to a better understanding of how communication impacts the success of ASD initiatives. Overall, this study emphasizes the significance of clear and efficient communication in Agile software development practices. 

Summary: <div>
arXiv:2504.20186v1 Announce Type: cross 
Abstract: Software industry is a fast-moving industry and to keep up with this pace the development process also needs to be fast and efficient and Agile software development (ASD) is the answer to this problem. Even though ASD has been in there for over two decades there are still multiple unknown questions tied to ASD that need to be addressed. In this study we are going to address one of the most critical factors of ASD i.e. Communication. We conducted a review of 14 studies and found the areas under ASD communication that the community is interested in as well as research gaps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection in multi-layer networks by regularized debiased spectral clustering</title>
<link>https://arxiv.org/abs/2409.07956</link>
<guid>https://arxiv.org/abs/2409.07956</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, multi-layer networks, regularized Laplacian matrix, stochastic block model, modularity

Summary:<br /><br />
This study introduces the regularized debiased sum of squared adjacency matrices (RDSoS) method for community detection in multi-layer networks. RDSoS extends the classical regularized Laplacian matrix to handle multi-layer networks, showing potential in various applications such as gene function prediction and fraud detection. The method is consistent under the multi-layer stochastic block model and its degree-corrected version. A new metric, sum of squared adjacency matrices modularity (SoS-modularity), is introduced to assess community quality and estimate the number of communities. Experimental results demonstrate the method's superiority over state-of-the-art techniques, insensitivity to regularizer selection, and ability to reveal the assortative property of real networks. SoS-modularity provides a more accurate evaluation of community quality compared to traditional metrics. This work opens up new possibilities for community detection in complex multi-layer networks. <div>
arXiv:2409.07956v2 Announce Type: replace-cross 
Abstract: Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events</title>
<link>https://arxiv.org/abs/2504.18837</link>
<guid>https://arxiv.org/abs/2504.18837</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme weather events, sentiment analysis, social media, climate change, wildfires 

Summary: 
Sentiment analysis plays a crucial role in understanding public perception of extreme weather events driven by climate change, such as wildfires and floods, on social media platforms. The survey explores various methods for sentiment analysis, including lexicon-based, machine learning models, and large language models. It also discusses challenges and ethical considerations related to analyzing sentiment during real-time, high-impact situations like the 2025 Los Angeles forest fires. Data collection and annotation techniques, such as weak supervision and real-time event tracking, are important for accurate sentiment analysis. Open problems include misinformation detection, multimodal sentiment extraction, and ensuring alignment with human values. The goal of the survey is to provide guidance for researchers and practitioners in effectively understanding sentiment during the climate crisis era. 

Summary: <div>
arXiv:2504.18837v1 Announce Type: new 
Abstract: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Substructure Discovery Algorithm For Homogeneous Multilayer Networks</title>
<link>https://arxiv.org/abs/2504.19328</link>
<guid>https://arxiv.org/abs/2504.19328</guid>
<content:encoded><![CDATA[
<div> Keywords: substructure discovery, graph mining, multilayer networks, decoupling approach, distributed processing<br />
Summary:<br />
Graph mining focuses on finding core substructures in real-world graphs. Substructure discovery involves identifying meaningful patterns in large datasets. Multilayer networks (MLNs) are effective for modeling complex datasets with multiple entity types and relationships. This paper proposes a novel decoupling-based approach for substructure discovery in homogeneous MLNs. The approach processes each layer independently and then composes results from multiple layers to identify substructures in the entire network. The algorithm is implemented using the Map/Reduce paradigm for scalability. Experimental analysis on synthetic and real-world datasets demonstrates the correctness, speedup, and response time of the algorithm. <div>
arXiv:2504.19328v1 Announce Type: new 
Abstract: Graph mining analyzes real-world graphs to find core substructures (connected subgraphs) in applications modeled as graphs. Substructure discovery is a process that involves identifying meaningful patterns, structures, or components within a large data set. These substructures can be of various types, such as frequent patterns, motifs, or other relevant features within the data.
  To model complex data sets -- with multiple types of entities and relationships -- multilayer networks (or MLNs) have been shown to be more effective as compared to simple and attributed graphs. Analysis algorithms on MLNs using the decoupling approach have been shown to be both efficient and accurate. Hence, this paper focuses on substructure discovery in homogeneous multilayer networks (one type of MLN) using a novel decoupling-based approach. In this approach, each layer is processed independently, and then the results from two or more layers are composed to identify substructures in the entire MLN. The algorithm is designed and implemented, including the composition part, using one of the distributed processing frameworks (the Map/Reduce paradigm) to provide scalability.
  After establishing the correctness, we analyze the speedup and response time of the proposed algorithm and approach through extensive experimental analysis on large synthetic and real-world data sets with diverse graph characteristics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleScope: A Longitudinal Dataset for Investigating Online Discourse and Information Interaction on Telegram</title>
<link>https://arxiv.org/abs/2504.19536</link>
<guid>https://arxiv.org/abs/2504.19536</guid>
<content:encoded><![CDATA[
<div> Keyword: Telegram, dataset, social media, analysis, research <br />
Summary:<br />
This paper introduces TeleScope, a comprehensive dataset suite for analyzing Telegram channels. The dataset includes metadata for 500K channels and message data for 71K public channels, totaling 120M messages. It also provides channel connections and user interaction data for studying information spread and message forwarding patterns. Enrichments like language detection and message posting periods enhance the dataset for in-depth discourse analysis. The dataset enables diverse applications and reproducible social media studies, similar to those on platforms like Twitter. 

<br /><br />Summary: <div>
arXiv:2504.19536v1 Announce Type: new 
Abstract: Telegram is a globally popular instant messaging platform known for its strong emphasis on security, privacy, and unique social networking features. It has recently emerged as the host for various cross-domain analysis and research works, such as social media influence, propaganda studies, and extremism. This paper introduces TeleScope, an extensive dataset suite that, to our knowledge, is the largest of its kind. It comprises metadata for about 500K Telegram channels and downloaded message metadata for about 71K public channels, accounting for around 120M crawled messages. We also release channel connections and user interaction data built using Telegram's message-forwarding feature to study multiple use cases, such as information spread and message forwarding patterns. In addition, we provide data enrichments, such as language detection, active message posting periods for each channel, and Telegram entities extracted from messages, that enable online discourse analysis beyond what is possible with the original data alone. The dataset is designed for diverse applications, independent of specific research objectives, and sufficiently versatile to facilitate the replication of social media studies comparable to those conducted on platforms like X (formerly Twitter)
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Italian Telegram Ecosystem</title>
<link>https://arxiv.org/abs/2504.19594</link>
<guid>https://arxiv.org/abs/2504.19594</guid>
<content:encoded><![CDATA[
<div> Keywords: Telegram, Italian, network analysis, toxicity, extremism<br />
Summary: <br />
This study analyzes the Italian Telegram sphere, examining the spread of misinformation, extremism, and toxicity within the platform's unmoderated environment. Using a dataset of 186 million messages from 13,151 chats collected in 2023, the research employs network analysis and Large Language Models to explore thematic communities and ideological alignment. Results reveal strong thematic and ideological homophily, with far-left and far-right rhetoric coexisting in mixed ideological communities on certain issues. Toxicity is found to be normalized within highly toxic communities, with Italians primarily targeting Black people, Jews, and gay individuals. Additionally, intra-national hostility is observed, reflecting regional and intra-regional cultural conflicts rooted in historical divisions. This comprehensive analysis provides valuable insights into the dynamics of the Italian Telegram ecosystem and sheds light on online toxicity in various cultural and linguistic contexts. <br /> <div>
arXiv:2504.19594v1 Announce Type: new 
Abstract: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observational Learning with a Budget</title>
<link>https://arxiv.org/abs/2504.19396</link>
<guid>https://arxiv.org/abs/2504.19396</guid>
<content:encoded><![CDATA[
<div> Bayesian learning, observational model, signal, central planner, budget allocation <br />
Summary: <br />
The article discusses a Bayesian observational learning model in which agents receive private signals about a binary state and make decisions based on their signals and previous observations. A central planner aims to enhance signal quality across agents by allocating a limited budget. The budget allocation problem is formulated and analyzed, and two optimal strategies are proposed. One of these strategies maximizes the likelihood of achieving a correct information cascade. <div>
arXiv:2504.19396v1 Announce Type: cross 
Abstract: We consider a model of Bayesian observational learning in which a sequence of agents receives a private signal about an underlying binary state of the world. Each agent makes a decision based on its own signal and its observations of previous agents. A central planner seeks to improve the accuracy of these signals by allocating a limited budget to enhance signal quality across agents. We formulate and analyze the budget allocation problem and propose two optimal allocation strategies. At least one of these strategies is shown to maximize the probability of achieving a correct information cascade.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation</title>
<link>https://arxiv.org/abs/2504.19489</link>
<guid>https://arxiv.org/abs/2504.19489</guid>
<content:encoded><![CDATA[
<div> community search algorithms, cohesiveness measures, online social networks, group cohesion, CHASE framework

Summary: This paper evaluates the effectiveness of community search algorithms in online social networks based on cohesiveness measures. While current methods primarily use structural or attribute-based approaches to measure cohesiveness, this study introduces five psychology-informed measures based on group cohesion theory from social psychology. The novel CHASE framework is proposed to evaluate eight representative algorithms on these measures. The analysis reveals a lack of correlation between structural and psychological cohesiveness, highlighting the challenge in identifying psychologically cohesive communities in online social networks. This study provides valuable insights for the development of future community search methods. <br /><br /> <div>
arXiv:2504.19489v1 Announce Type: cross 
Abstract: Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t.these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding</title>
<link>https://arxiv.org/abs/2504.19734</link>
<guid>https://arxiv.org/abs/2504.19734</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialogue data, Large Language Models, Automated coding, Communicative acts, Contextual complexity

Summary:
- The study introduces a novel LLM-assisted automated coding approach for dialogue data.
- Code prediction for utterances is based on dialogue-specific characteristics using separate prompts.
- Multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek are engaged in collaborative code prediction.
- Contextual consistency checking using GPT-4o substantially improved accuracy.
- Accuracy of act predictions was consistently higher than that of event predictions.

<br /><br />Summary: This study presents a new methodological framework for improving the precision of automated coding of dialogue data by leveraging Large Language Models. It introduces a novel approach where code prediction for utterances is based on specific dialogue characteristics through separate prompts. Multiple LLMs are used for collaborative code prediction, and a contextual consistency checking method significantly enhances accuracy. The study highlights the importance of understanding communicative acts and events in dialogue analysis, with act predictions consistently outperforming event predictions. This innovative framework provides a scalable solution for addressing contextual challenges in dialogue analysis. <div>
arXiv:2504.19734v1 Announce Type: cross 
Abstract: Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> DeFi, lending, liquidity pools, interest rate curves, decentralized

Summary:
The article presents a dynamic model for the lending market in Decentralized Finance (DeFi) that addresses inefficiencies and risks in current platforms like Aave and Compound. The proposed model incorporates evolving demand and supply curves along with an adaptive interest rate controller that reacts in real-time to market changes. By using a Recursive Least Squares algorithm, the controller ensures stable utilization and manages default and liquidation risks. The algorithm offers theoretical guarantees on interest rate convergence and utilization stability while reducing vulnerability to adversarial manipulation compared to static curves. Two approaches are suggested to counter adversarial manipulation, including a detection method for extreme fluctuations and a market-based strategy to enhance elasticity. The model's performance is validated through Aave data, demonstrating low best-fit error, and improved utilization and liquidation management compared to static curve protocols. 
<br /><br />Summary: <div>
arXiv:2410.13105v4 Announce Type: replace 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
<div> Keywords: Misinformation, Datasets, Evaluation Quality Assurance, Detection model, Research

Summary:
This study addresses the challenging issue of misinformation by curating a comprehensive collection of datasets, totaling 75, for empirical research. The evaluation of these datasets reveals flaws in many, such as spurious correlations and ambiguous examples, impacting their reliability. State-of-the-art baselines are provided, highlighting the limitations of categorical labels in assessing detection model performance accurately. The authors propose Evaluation Quality Assurance (EQA) as a tool to guide the field towards systemic solutions and improve research quality in misinformation detection. The ultimate goal of this guide is to promote higher quality data and grounded evaluations to enhance the field's understanding and combat misinformation effectively.
<br /><br />Summary: <div>
arXiv:2411.05060v3 Announce Type: replace 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Participation in Collective Action from Social Media</title>
<link>https://arxiv.org/abs/2501.07368</link>
<guid>https://arxiv.org/abs/2501.07368</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, collective action, text classifiers, Reddit, computational social science

Summary:
Social media is crucial for mobilizing collective action and understanding individual engagement in global challenges. However, research in this area lacks granular data on participation levels. To address this gap, this study presents text classifiers that can identify participation expressions across different levels, from recognizing issues to active involvement. By training BERT and Llama3 models on Reddit data, the study demonstrates the effectiveness of smaller language models in detecting participation nuances. Applying this methodology to Reddit enables a more robust characterization of online communities compared to existing methods. This framework provides reliable annotations for Computational Social Science research to analyze the dynamics of collective action in online spaces. <div>
arXiv:2501.07368v2 Announce Type: replace 
Abstract: Social media play a key role in mobilizing collective action, holding the potential for studying the pathways that lead individuals to actively engage in addressing global challenges. However, quantitative research in this area has been limited by the absence of granular and large-scale ground truth about the level of participation in collective action among individual social media users. To address this limitation, we present a novel suite of text classifiers designed to identify expressions of participation in collective action from social media posts, in a topic-agnostic fashion. Grounded in the theoretical framework of social movement mobilization, our classification captures participation and categorizes it into four levels: recognizing collective issues, engaging in calls-to-action, expressing intention of action, and reporting active involvement. We constructed a labeled training dataset of Reddit comments through crowdsourcing, which we used to train BERT classifiers and fine-tune Llama3 models. Our findings show that smaller language models can reliably detect expressions of participation (weighted F1=0.71), and rival larger models in capturing nuanced levels of participation. By applying our methodology to Reddit, we illustrate its effectiveness as a robust tool for characterizing online communities in innovative ways compared to topic modeling, stance detection, and keyword-based methods. Our framework contributes to Computational Social Science research by providing a new source of reliable annotations useful for investigating the social dynamics of collective action.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Unknown Social Networks for Discovering Hidden Nodes</title>
<link>https://arxiv.org/abs/2501.12571</link>
<guid>https://arxiv.org/abs/2501.12571</guid>
<content:encoded><![CDATA[
<div> Hidden-node discovery, social networks, graph exploration, machine learning, node embeddings
<br />
Summary:
In this paper, the authors address the challenge of discovering hidden nodes in unknown social networks by formulating three types of hidden-node discovery problems: Sybil-node discovery, peripheral-node discovery, and influencer discovery. They employ a graph exploration framework grounded in machine learning to tackle these problems, constructing prediction models based on the subgraph structure obtained during exploration. Empirical investigations on real social graphs demonstrate the efficiency of graph exploration strategies in uncovering hidden nodes, with query cost multipliers of 1.2 for discovering 10% and 1.4 for discovering 90% of hidden nodes compared to the known topology case. The use of node embeddings for hidden-node discovery is found to be effective in certain scenarios but can degrade efficiency in others. The authors propose a bandit algorithm to combine prediction models using node embeddings with those that do not, showing that this approach achieves efficient node discovery across various settings.
<br /><br />Summary: <div>
arXiv:2501.12571v2 Announce Type: replace 
Abstract: In this paper, we address the challenge of discovering hidden nodes in unknown social networks, formulating three types of hidden-node discovery problems, namely, Sybil-node discovery, peripheral-node discovery, and influencer discovery. We tackle these problems by employing a graph exploration framework grounded in machine learning. Leveraging the structure of the subgraph gradually obtained from graph exploration, we construct prediction models to identify target hidden nodes in unknown social graphs. Through empirical investigations of real social graphs, we investigate the efficiency of graph exploration strategies in uncovering hidden nodes. Our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known. Specifically, the query cost of discovering 10% of the hidden nodes is at most only 1.2 times that when the topology is known, and the query-cost multiplier for discovering 90% of the hidden nodes is at most only 1.4. Furthermore, our results suggest that using node embeddings, which are low-dimensional vector representations of nodes, for hidden-node discovery is a double-edged sword: it is effective in certain scenarios but sometimes degrades the efficiency of node discovery. Guided by this observation, we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not, and our analysis shows that the bandit-based graph exploration strategy achieves efficient node discovery across a wide array of settings.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Wisdom of Intellectually Humble Networks</title>
<link>https://arxiv.org/abs/2502.02015</link>
<guid>https://arxiv.org/abs/2502.02015</guid>
<content:encoded><![CDATA[
<div> Keywords: collective wisdom, intellectual humility, social networks, agent-based modeling, polarization

Summary: 
This paper examines the impact of intellectual humility on collective wisdom within social networks. Using agent-based modeling and data-calibrated simulations, the researchers demonstrate that intellectual humility can lead to more accurate estimations and reduce polarization in social networks. By fostering a mindset of openness to perspectives and willingness to revise beliefs, intellectual humility helps individuals avoid cognitive and social biases that can hinder collective wisdom. The study shows that interventions promoting intellectual humility can enhance the overall accuracy of group estimations while maintaining cohesion within social networks. The findings are robust across different task settings and network structures, suggesting the potential for practical applications in improving decision-making processes and policy outcomes in democratic societies. By understanding how intellectual humility influences group dynamics, policymakers and stakeholders can leverage this trait to enhance the quality of collective decision-making. 

<br /><br />Summary: <div>
arXiv:2502.02015v2 Announce Type: replace 
Abstract: People's collectively shared beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dynamics of leadership and success in software development teams</title>
<link>https://arxiv.org/abs/2404.18833</link>
<guid>https://arxiv.org/abs/2404.18833</guid>
<content:encoded><![CDATA[
<div> Keywords: teamwork, software development, collaborative processes, leadership change, success growth

Summary: 
Teams in software development projects were analyzed using fine-grained temporal data from Rust, JavaScript, and Python ecosystems. The study found that workload distribution within teams is uneven, with greater heterogeneity leading to higher success rates. A lead developer often emerges early on, shouldering the majority of work. A significant number of projects undergo a change in lead developer, with this transition more likely in projects led by inexperienced users. Leadership changes are associated with faster growth in project success. This research provides insights into the dynamics of online collaborative projects, highlighting the importance of understanding team evolution and its impact on success in collaborative processes.<br /><br />Summary: <div>
arXiv:2404.18833v3 Announce Type: replace-cross 
Abstract: From science to industry, teamwork plays a crucial role in knowledge production and innovation. Most studies consider teams as static groups of individuals, thereby failing to capture how the micro-dynamics of collaborative processes and organizational changes determine team success. Here, we leverage fine-grained temporal data on software development teams from three software ecosystems -- Rust, JavaScript, and Python -- to gain insights into the dynamics of online collaborative projects. Our analysis reveals an uneven workload distribution in teams, with stronger heterogeneity correlated with higher success, and the early emergence of a lead developer carrying out the majority of work. Moreover, we find that a sizeable fraction of projects experience a change of lead developer, with such a transition being more likely in projects led by inexperienced users. Finally, we show that leadership change is associated with faster success growth. Our work contributes to a deeper understanding of the link between team evolution and success in collaborative processes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models for newspaper sentiment analysis during COVID-19: The Guardian</title>
<link>https://arxiv.org/abs/2405.13056</link>
<guid>https://arxiv.org/abs/2405.13056</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, sentiment analysis, newspapers, The Guardian, public response

Summary:<br /><br />During the COVID-19 pandemic, The Guardian newspaper was analyzed for sentiment trends using large language models. The study focused on various stages of the pandemic, including initial transmission, lockdowns, and vaccination. The analysis revealed a shift in public sentiment from urgent crisis response to concerns about health and the economy. The study found a predominance of negative sentiments, such as sadness, annoyance, anxiety, and denial, in The Guardian's coverage, both before and during the pandemic. This contrasts with social media sentiment analyses, which showed a more diverse emotional reflection. Overall, The Guardian portrayed a grim narrative with negative sentiments prevailing across news sections for countries like Australia, the UK, and the world. Sentiment analysis of newspaper sources during COVID-19 can offer insights into how the media covered the pandemic and captured the evolving public response. <br /><br />Summary: <div>
arXiv:2405.13056v2 Announce Type: replace-cross 
Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks</title>
<link>https://arxiv.org/abs/2407.04465</link>
<guid>https://arxiv.org/abs/2407.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: biological networks, scale-free structures, Compounded Burr distribution, heavy-tailed degree distributions, maximum likelihood estimation <br />
Summary: <br />
Complex biological networks often exhibit scale-free structures, but empirical studies show deviations from ideal power law behavior. The Compounded Burr (CBurr) distribution, a novel four parameter family, is proposed to accurately model network degree distributions, with a focus on biological networks. Statistical properties of the CBurr distribution are rigorously derived, and an efficient maximum likelihood estimation framework is developed. The CBurr model showcases broad applicability in various domains, outperforming classical models like power-law and log-normal on biological network datasets. By providing a statistically grounded framework, the CBurr model enhances our understanding of the structural heterogeneity of biological networks. <div>
arXiv:2407.04465v3 Announce Type: replace-cross 
Abstract: Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Collective Accuracy in Socially Connected Networks</title>
<link>https://arxiv.org/abs/2411.08625</link>
<guid>https://arxiv.org/abs/2411.08625</guid>
<content:encoded><![CDATA[
<div> network-mediated influence, collective decision-making, social networks, binary choices, group performance 

Summary:
In this study, the accuracy of collective decision-making in socially connected populations was analyzed. Agents in the network update binary choices based on private signals that are slightly biased towards the correct alternative. Through local interactions on the network, social influence plays a crucial role in aggregating these signals. The research found that in large-population scenarios, the probability of a correct majority converges to a specific mathematical expression involving the regularized incomplete beta function. Surprisingly, the collective accuracy exceeds that of individual agents when private signals are better than random, indicating that network-mediated influence can improve group performance. These results have implications for designing resilient decision-making systems in various networks, including social, biological, and engineered systems, where accuracy relies on the interactions of interdependent and noisy agents.<br /><br />Summary: <div>
arXiv:2411.08625v2 Announce Type: replace-cross 
Abstract: We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Egocentric Temporal Neighborhoods to probe for spatial correlations in temporal networks and infer their topology</title>
<link>https://arxiv.org/abs/2501.16070</link>
<guid>https://arxiv.org/abs/2501.16070</guid>
<content:encoded><![CDATA[
<div> Keywords: motifs, temporal networks, triangles, edge-centered motifs, graph tiling theory

Summary:
Motifs are fundamental components of social face-to-face interaction temporal networks, with traditional motifs lacking in inclusivity or efficiency. This study introduces edge-centered motifs that encompass triangles and can be efficiently mined in any temporal network. Analytical comparisons with Egocentric Temporal Neighborhoods motifs show that edge-centered motifs provide relevant information. Empirical data analysis supports the significance of edge-centered motifs in probing spatial correlations in network dynamics. The distribution of edge-centered motifs in social face-to-face interaction networks is approximated. Exploration of using edge-centered motif statistics to infer complete network topology leads to the development of graph tiling theory, a new mathematical framework. The study highlights the importance of considering triangles and spatial correlations in understanding social systems through temporal networks. 

<br /><br />Summary: <div>
arXiv:2501.16070v2 Announce Type: replace-cross 
Abstract: Motifs are thought to be some fundamental components of social face-to-face interaction temporal networks. However, the motifs previously considered are either limited to a handful of nodes and edges, or do not include triangles, which are thought to be of critical relevance to understand the dynamics of social systems. Thus, we introduce a new class of motifs, that include these triangles, are not limited in their number of nodes or edges, and yet can be mined efficiently in any temporal network. Referring to these motifs as the edge-centered motifs, we show analytically how they subsume the Egocentric Temporal Neighborhoods motifs of the literature. We also confirm in empirical data that the edge-centered motifs bring relevant information with respect to the Egocentric motifs by using a principle of maximum entropy. Then, we show how mining for the edge-centered motifs in a network can be used to probe for spatial correlations in the underlying dynamics that have produced that network. We deduce an approximate formula for the distribution of the edge-centered motifs in empirical networks of social face-to-face interactions. In the last section of this paper, we explore how the statistics of the edge-centered motifs can be used to infer the complete topology of the network they were sampled from. This leads to the needs of mathematical development, that we inaugurate here under the name of graph tiling theory.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration</title>
<link>https://arxiv.org/abs/2503.13238</link>
<guid>https://arxiv.org/abs/2503.13238</guid>
<content:encoded><![CDATA[
<div> Keywords: Arab Spring, scholarly attention, academic discourse, funding, migration networks 

Summary: 
The study examines the impact of the Arab Spring on scholarly attention in 10 countries in the Middle East and North Africa region. Using a difference-in-difference statistical framework on over 25 million articles published from 2002 to 2019, the researchers found that most target countries experienced a significant increase in scholarly attention post-Arab Spring compared to other regions, with Egypt garnering the most attention. The study also delves into the role of funding and migration networks in shaping scholarly attention, highlighting Saudi Arabia as a key player in attracting researchers and funding projects in the region. This research sheds light on the evolving academic landscape in the aftermath of the Arab Spring and underscores the importance of analyzing the influence of socio-political movements on scholarly discourse in the region. 

<br /><br />Summary: <div>
arXiv:2503.13238v2 Announce Type: replace-cross 
Abstract: The Arab Spring is a major socio-political movement that reshaped democratic aspirations in the Middle East and North Africa, attracting global attention through news, social media, and academic discourse. However, its consequences on the academic landscape in the region are still unclear. Here, we conduct the first study of scholarly attention toward 10 target countries affected by the Arab Spring by analyzing more than 25 million articles published from 2002 to 2019. Using a difference-in-difference statistical framework, we find that most target countries have experienced a significant increase in scholarly attention post-Arab Spring compared to the rest of the world, with Egypt attracting the most attention. We investigate how funding and migration networks relate to scholarly attention and reveal that Saudi Arabia has emerged as a key player among Western nations by attracting researchers and funding projects that shape research on the region.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community</title>
<link>https://arxiv.org/abs/2503.22066</link>
<guid>https://arxiv.org/abs/2503.22066</guid>
<content:encoded><![CDATA[
<div> GitHub, pull request, Rust, diversity, social network analysis
Summary:
The study examines the demographic composition and interaction patterns of contributors in the Rust programming language ecosystem, focusing on key projects like Rust, Rust Analyzer, and Cargo. Using GitHub pull request data and social network analysis, disparities in gender and geographic representation among pivotal contributors are revealed, highlighting the need for more inclusive practices. The results suggest that while the Rust community is globally active, the contributor base lacks diversity, indicating a disconnect with the wider user community. To encourage broader participation and ensure alignment with the diverse global community, inclusive measures are necessary within the Rust ecosystem. <div>
arXiv:2503.22066v3 Announce Type: replace-cross 
Abstract: Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning on Large Graphs using a Densifying Regularity Lemma</title>
<link>https://arxiv.org/abs/2504.18273</link>
<guid>https://arxiv.org/abs/2504.18273</guid>
<content:encoded><![CDATA[
<div> Keywords: large graphs, Intersecting Block Graph (IBG), weak regularity lemma, graph neural network, node classification<br />
Summary: 
Large graphs pose challenges for traditional Message Passing Neural Networks due to computational and memory costs scaling linearly with the number of edges. The Intersecting Block Graph (IBG) is introduced as a low-rank factorization of large directed graphs, based on intersecting bipartite components. By weighting non-edges less, any graph can be efficiently approximated by a dense IBG. A constructive version of the weak regularity lemma is proven, showing that any graph can be approximated by a dense IBG with rank depending only on the chosen accuracy. A graph neural network architecture operating on the IBG representation shows competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion with linear memory and computational complexity in the number of nodes. This approach offers a more efficient solution for learning on large graphs compared to traditional methods. <br /><br /> <div>
arXiv:2504.18273v1 Announce Type: new 
Abstract: Learning on large graphs presents significant challenges, with traditional Message Passing Neural Networks suffering from computational and memory costs scaling linearly with the number of edges. We introduce the Intersecting Block Graph (IBG), a low-rank factorization of large directed graphs based on combinations of intersecting bipartite components, each consisting of a pair of communities, for source and target nodes. By giving less weight to non-edges, we show how to efficiently approximate any graph, sparse or dense, by a dense IBG. Specifically, we prove a constructive version of the weak regularity lemma, showing that for any chosen accuracy, every graph, regardless of its size or sparsity, can be approximated by a dense IBG whose rank depends only on the accuracy. This dependence of the rank solely on the accuracy, and not on the sparsity level, is in contrast to previous forms of the weak regularity lemma. We present a graph neural network architecture operating on the IBG representation of the graph and demonstrating competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion, while having memory and computational complexity linear in the number of nodes rather than edges.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and Short-term Validity of Findings</title>
<link>https://arxiv.org/abs/2504.18140</link>
<guid>https://arxiv.org/abs/2504.18140</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, algorithmic audits, reproducibility, generalizability, TikTok

Summary:
In this study, the researchers examine the reproducibility of previous sockpuppeting audits on TikTok's recommender systems and the generalizability of their findings. They encounter challenges due to changes in the platform and content evolution, as well as limitations in the previous research methods. The experiments reveal that one-shot audit findings may only hold in the short term, emphasizing the importance of reproducible audits to track changes over time. The audit reproducibility relies heavily on methodological choices and the state of algorithms and content on the platform. This highlights the need for systematic social media algorithmic audits to ensure users are not confined to filter bubbles and not exposed to problematic content. <div>
arXiv:2504.18140v1 Announce Type: cross 
Abstract: Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Freshness in Dynamic Gossip Networks</title>
<link>https://arxiv.org/abs/2504.18504</link>
<guid>https://arxiv.org/abs/2504.18504</guid>
<content:encoded><![CDATA[
<div> version age of information, time-varying connections, network topology, continuous time Markov chain, information freshness

Summary:
- The article examines a source sharing updates with a network of gossiping nodes where the network's topology switches between two arbitrary topologies based on a continuous time Markov chain (CTMC).
- It assesses the impact of time-varying connections on information freshness using the version age of information metric.
- If the two networks have differing static long-term average version ages, the version age of the varying-topologies network is influenced by the transition rates in the CTMC.
- When the CTMC transition rates exceed the faster of the two static network's average version ages, the average version age of the varying-topologies network aligns with the faster average version age.
- The behavior of a small fraction of nodes can significantly affect the network's long-term average version age negatively, leading to the definition of a typical set of nodes.
- The study also evaluates the impact of fast and slow CTMC transition rates on this typical set of nodes.<br /><br />Summary: <div>
arXiv:2504.18504v1 Announce Type: cross 
Abstract: We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media</title>
<link>https://arxiv.org/abs/2410.17390</link>
<guid>https://arxiv.org/abs/2410.17390</guid>
<content:encoded><![CDATA[
<div> transparency, social networks, recommendation algorithms, shadow banning, public discourse <br />
<br />
Summary: This paper explores the impact of visibility alterations on Twitter discussions regarding the Ukraine-Russia conflict and the 2024 US Presidential Elections. The study, based on two large datasets, reveals that tweets containing external links are systematically penalized in terms of visibility, regardless of their ideological stance or reliability. The algorithm also appears to favor or penalize content based on the specific accounts producing it, as evident in the comparison between tweets from different sources or political figures. The findings underscore the critical need for transparency in content moderation and recommendation systems to safeguard public discourse integrity and provide fair access to online platforms. <div>
arXiv:2410.17390v2 Announce Type: replace 
Abstract: In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used to suppress dissenting voices. This prompts the need for greater transparency and a better understanding of this practice.
  In this paper, we investigate the presence of visibility alterations through a large-scale quantitative analysis of two Twitter/X datasets comprising over 40 million tweets from more than 9 million users, focused on discussions surrounding the Ukraine-Russia conflict and the 2024 US Presidential Elections. We use view counts to detect patterns of reduced or inflated visibility and examine how these correlate with user opinions, social roles, and narrative framings. Our analysis shows that the algorithm systematically penalizes tweets containing links to external resources, reducing their visibility by up to a factor of eight, regardless of the ideological stance or source reliability. Rather, content visibility may be penalized or favored depending on the specific accounts producing it, as observed when comparing tweets from the Kyiv Independent and RT.com or tweets by Donald Trump and Kamala Harris. Overall, our work highlights the importance of transparency in content moderation and recommendation systems in protecting the integrity of public discourse and ensuring equitable access to online platforms.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensics and security issues in the Internet of Things</title>
<link>https://arxiv.org/abs/2309.02707</link>
<guid>https://arxiv.org/abs/2309.02707</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT, forensics, security, smart home system, blockchain-based authentication mechanism

Summary: 
IoT devices face security challenges due to the lack of standardized measures, making them vulnerable to cyberattacks. Unauthorized access can lead to data compromises and control over critical infrastructure. To address these issues, a FLIP-based smart home system can be developed with security-conscious design. Implementing a blockchain-based authentication mechanism with a multi-chain structure can enhance security between trust domains. Deep learning can aid in creating a network forensics framework for effective detection and tracking of cyberattacks. Limiting data usage in big data applications for IoT-based systems is crucial. Researchers are encouraged to explore solutions to these challenges to advance the IoT field.<br /><br />Summary: <div>
arXiv:2309.02707v2 Announce Type: replace-cross 
Abstract: Given the exponential expansion of the internet, the possibilities of security attacks and cybercrimes have increased accordingly. However, poorly implemented security mechanisms in the Internet of Things (IoT) devices make them susceptible to cyberattacks, which can directly affect users. IoT forensics is thus needed to investigate and mitigate such attacks. While many works have examined IoT applications and challenges, only a few have focused on both the forensic and security issues in IoT. Therefore, this paper reviews forensic and security issues associated with IoT in different fields. Prospects and challenges in IoT research and development are also highlighted. As the literature demonstrates, most IoT devices are vulnerable to attacks due to a lack of standardized security measures. Unauthorized users could get access, compromise data, and even benefit from control of critical infrastructure. To fulfill the security-conscious needs of consumers, IoT can be used to develop a smart home system by designing the security-conscious needs of consumers; IoT can be used to create a smart home system by designing an IoT can be used to develop a smart home system by designing a FLIP-based system that is highly scalable and adaptable. A blockchain-based authentication mechanism with a multi-chain structure can provide additional security protection between different trust domains. Deep learning can be utilized to develop a network forensics framework with a high-performing system for detecting and tracking cyberattack incidents. Moreover, researchers should consider limiting the amount of data created and delivered when using big data to develop IoT-based smart systems. The findings of this review will stimulate academics to seek potential solutions for the identified issues, thereby advancing the IoT field.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-driven embedding of networks in hyperbolic space</title>
<link>https://arxiv.org/abs/2406.10711</link>
<guid>https://arxiv.org/abs/2406.10711</guid>
<content:encoded><![CDATA[
<div> Bayesian hyperbolic random graph model, MCMC algorithm, uncertainty quantification, embeddings, network properties <br />
<br />
Summary: BIGUE is a novel MCMC algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model, allowing for uncertainty quantification in inferred network coordinates. This algorithm can provide credible intervals for coordinates and network properties, offering a more comprehensive analysis than current algorithms. It highlights that some networks may have multiple plausible embeddings, a factor that traditional optimization methods might overlook. BIGUE's exploration of the posterior distribution aligns with existing algorithms while enhancing the understanding of hyperbolic network models. <div>
arXiv:2406.10711v2 Announce Type: replace-cross 
Abstract: Hyperbolic models are known to produce networks with properties observed empirically in most network datasets, including heavy-tailed degree distribution, high clustering, and hierarchical structures. As a result, several embeddings algorithms have been proposed to invert these models and assign hyperbolic coordinates to network data. Current algorithms for finding these coordinates, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that the samples are consistent with current algorithms while providing added credible intervals for the coordinates and all network properties. We also show that some networks admit two or more plausible embeddings, a feature that an optimization algorithm can easily overlook.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Vec: Self-Supervised Geospatial Embeddings</title>
<link>https://arxiv.org/abs/2504.16942</link>
<guid>https://arxiv.org/abs/2504.16942</guid>
<content:encoded><![CDATA[
<div> S2Vec, geospatial embeddings, S2 Geometry library, self-supervised framework, rasterization, masked autoencoding

Summary: 
S2Vec is introduced as a self-supervised framework for creating geospatial embeddings using the S2 Geometry library to partition areas into cells, rasterizing feature vectors within cells as images, and applying masked autoencoding for encoding. These embeddings capture local feature characteristics and spatial relationships for various geospatial tasks. The framework is evaluated on socioeconomic prediction tasks, demonstrating competitive performance against image-based embeddings. Combining S2Vec with image-based embeddings through multimodal fusion shows improved performance. The results showcase S2Vec's ability to learn effective general-purpose geospatial representations and complement other data modalities in geospatial artificial intelligence.<br /><br />Summary: <div>
arXiv:2504.16942v1 Announce Type: new 
Abstract: Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on three large-scale socioeconomic prediction tasks, showing its competitive performance against state-of-the-art image-based embeddings. We also explore the benefits of combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our results highlight how S2Vec can learn effective general-purpose geospatial representations and how it can complement other data modalities in geospatial artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burning some myths on privacy properties of social networks against active attacks</title>
<link>https://arxiv.org/abs/2504.16944</link>
<guid>https://arxiv.org/abs/2504.16944</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, privacy, active attacks, (k,l)-anonymity, graph theory <br />
Summary: <br />
The research focuses on challenging the common belief that social networks have no privacy. It discusses active attacks on social network privacy and the (k,l)-anonymity measure to quantify privacy against such attacks. Utilizing the concept of k-metric antidimensional graphs, the study highlights that social networks may not be as insecure as believed. Computational experiments on random and real networks reveal that only a small number of graphs are vulnerable to privacy breaches. The research also explores mathematical properties of k-metric antidimensional graphs and proposes operations to obscure privacy vulnerabilities in graphs. This work contributes to reevaluating the privacy risks in social networks and offers insights into enhancing privacy protection against active attacks. <br /> <div>
arXiv:2504.16944v1 Announce Type: new 
Abstract: This work focuses on showing some arguments addressed to dismantle the extended idea about that social networks completely lacks of privacy properties. We consider the so-called active attacks to the privacy of social networks and the counterpart $(k,\ell)$-anonymity measure, which is used to quantify the privacy satisfied by a social network against active attacks. To this end, we make use of the graph theoretical concept of $k$-metric antidimensional graphs for which the case $k=1$ represents those graphs achieving the worst scenario in privacy whilst considering the $(k,\ell)$-anonymity measure.
  As a product of our investigation, we present a large number of computational results stating that social networks might not be as insecure as one often thinks. In particular, we develop a large number of experiments on random graphs which show that the number of $1$-metric antidimensional graphs is indeed ridiculously small with respect to the total number of graphs that can be considered. Moreover, we search on several real networks in order to check if they are $1$-metric antidimensional, and obtain that none of them are such. Along the way, we show some theoretical studies on the mathematical properties of the $k$-metric antidimensional graphs for any suitable $k\ge 1$. In addition, we also describe some operations on graphs that are $1$-metric antidimensional so that they get embedded into another larger graphs that are not such, in order to obscure their privacy properties against active attacks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
<div> simulation framework, urban mobility, behavioral choices, transportation modes, population groups
<br />
The article introduces a new simulation framework for generative agents to simulate realistic urban behaviors. It addresses the limitations of existing methods by incorporating multiple functional buildings and transportation modes in a virtual city. Extensive surveys were conducted to model behavioral choices and mobility preferences among population groups, resulting in a scalable framework capable of simulating over 4,000 agents. The framework captures the complexity of urban mobility while enabling in-depth analyses of crowd density prediction and vehicle preferences across agent demographics. The realism of the generated behaviors was assessed through micro and macro-level analyses, showcasing the framework's ability to simulate large-scale urban scenarios with accuracy and efficiency.
<br /><br />Summary: <div>
arXiv:2504.16946v1 Announce Type: new 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices in modern cities, and require prohibitive computational resources for large-scale population simulation. To address these limitations, we first present a virtual city that features multiple functional buildings and transportation modes. Then, we conduct extensive surveys to model behavioral choices and mobility preferences among population groups. Building on these insights, we introduce a simulation framework that captures the complexity of urban mobility while remaining scalable, enabling the simulation of over 4,000 agents. To assess the realism of the generated behaviors, we perform a series of micro and macro-level analyses. Beyond mere performance comparison, we explore insightful experiments, such as predicting crowd density from movement patterns and identifying trends in vehicle preferences across agent demographics.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments</title>
<link>https://arxiv.org/abs/2504.16947</link>
<guid>https://arxiv.org/abs/2504.16947</guid>
<content:encoded><![CDATA[
<div> Framework, prediction, social computing, community responses, sentiment<br />
<br />
Summary: <br />
SCRAG is a prediction framework inspired by social computing, designed to forecast community responses to social media posts. It integrates large language models with a Retrieval-Augmented Generation technique to overcome challenges like reliance on static training data and hallucinations. By retrieving historical responses and external knowledge, SCRAG can accurately predict how a community will respond to new narratives. Extensive experiments on the X platform show over 10% improvements in key metrics. The framework is effective in capturing diverse ideologies and nuances, providing concrete insights into community responses for applications like public sentiment prediction and crisis management. <div>
arXiv:2504.16947v1 Announce Type: new 
Abstract: This paper introduces SCRAG, a prediction framework inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Sampling: An Overview and Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.17701</link>
<guid>https://arxiv.org/abs/2504.17701</guid>
<content:encoded><![CDATA[
<div> node-based, edge-based, exploration-based, network sampling, temporal networks

Summary:
- Network sampling is essential for analyzing large or partially observable networks.
- Different sampling methods vary in effectiveness depending on the context.
- Empirical comparison of node-based, edge-based, and exploration-based sampling methods.
- Advanced methods show better accuracy on static networks, while simpler techniques are more effective for temporal networks.
- The choice of sampling strategy should consider both network structure and analytical objectives. 

<br /><br />Summary: <div>
arXiv:2504.17701v1 Announce Type: new 
Abstract: Network sampling is a crucial technique for analyzing large or partially observable networks. However, the effectiveness of different sampling methods can vary significantly depending on the context. In this study, we empirically compare representative methods from three main categories: node-based, edge-based, and exploration-based sampling. We used two real-world datasets for our analysis: a scientific collaboration network and a temporal message-sending network. Our results indicate that no single sampling method consistently outperforms the others in both datasets. Although advanced methods tend to provide better accuracy on static networks, they often perform poorly on temporal networks, where simpler techniques can be more effective. These findings suggest that the best sampling strategy depends not only on the structural characteristics of the network but also on the specific metrics that need to be preserved or analyzed. Our work offers practical insights for researchers in choosing sampling approaches that are tailored to different types of networks and analytical objectives.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Percolation as Decision Threshold for Risk Management in Cross-Country Thermal Soaring</title>
<link>https://arxiv.org/abs/2504.16945</link>
<guid>https://arxiv.org/abs/2504.16945</guid>
<content:encoded><![CDATA[
<div> Keywords: soaring, updrafts, graph percolation, risk management, flight logs

Summary:
Graph percolation theory is used to analyze the risk of failure to find updrafts for long-range flight by fixed-wing aircraft without propulsion systems. By evaluating flight logs from human soaring pilots, it is found that pilots rarely fly in conditions that do not satisfy graph percolation, indicating a desired minimum node degree. Pilots accept reduced climb rates to maintain percolation, showcasing the importance of this risk measure in optimizing speed and managing risk in soaring flight. The uncertainty of updraft locations underscores the necessity of determining when to exploit updrafts for continued flight, crucial for successful long-distance soaring. The study highlights the practical utility of graph percolation as a tool for evaluating and improving in-flight decision-making in soaring aircraft, aiding pilots in maximizing their efficiency and safety during flight.<br /><br />Summary: <div>
arXiv:2504.16945v1 Announce Type: cross 
Abstract: Long range flight by fixed-wing aircraft without propulsion systems can be accomplished by "soaring" -- exploiting randomly located updrafts to gain altitude which is expended in gliding flight. As the location of updrafts is uncertain and cannot be determined except through in situ observation, aircraft exploiting this energy source are at risk of failing to find a subsequent updraft. Determining when an updraft must be exploited to continue flight is essential to managing risk and optimizing speed. Graph percolation offers a theoretical explanation for this risk, and a framework for evaluating it using information available to the operator of a soaring aircraft in flight. The utility of graph percolation as a risk measure is examined by analyzing flight logs from human soaring pilots. This analysis indicates that in sport soaring pilots rarely operate in a condition which does not satisfy graph percolation, identifies an apparent desired minimum node degree, and shows that pilots accept reduced climb rates in order to maintain percolation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social sustainability through engagement in a training context with tools such as the Native Podcast and Facebook social network</title>
<link>https://arxiv.org/abs/2504.16964</link>
<guid>https://arxiv.org/abs/2504.16964</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainability, social dimension, EUTIC 2023 symposium, engagement process, digital tools

Summary:
The article addresses the previously neglected social dimension of sustainability within literature, with a particular focus on the upcoming EUTIC 2023 symposium. It introduces an engagement process designed to promote sustainable development using digital tools that are inspired by everyday life and cater to lifelong learning in training contexts. Rooted in information and communication sciences, the work advocates for a multi-disciplinary approach that can be applied across various disciplines. The authors aim to challenge current perspectives and generate insights on the intersection of digital tools, sustainability, and lifelong learning. This innovative approach holds promise for addressing sustainability concerns and promoting social well-being through interactive and accessible digital solutions. <div>
arXiv:2504.16964v1 Announce Type: cross 
Abstract: The social dimension of sustainability seems to have been a notion rarely addressed in the literature (Dubois et al., 2001) until the early 2000s. The EUTIC 2023 symposium provides an opportunity to take up this topical issue. To this end, we are presenting an engagement process that is part of a sustainable development dynamic, based on digital tools inspired by everyday life, for applications in the context of training, with a view to lifelong learning. Our work, which stems from the information and communication sciences, is rooted in a multi-disciplinary approach that we believe can be echoed in a variety of disciplines, but which it is interesting to challenge, hence the purpose of this contribution.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.17099</link>
<guid>https://arxiv.org/abs/2504.17099</guid>
<content:encoded><![CDATA[
<div> geometric information, entity representations, knowledge graphs, RDF2Vec, location-aware embeddings
Summary:
- The paper introduces a new approach that incorporates geometric information to learn location-aware entity embeddings.
- Existing methods for learning entity representations do not consider geometries stored in knowledge graphs.
- The approach expands nodes by flooding the graph from geographic nodes to ensure all reachable nodes are considered.
- A modified version of RDF2Vec, biased with spatial weights from the flooded graph, is then applied to learn location-aware embeddings.
- Evaluations on benchmark datasets show that the proposed approach outperforms non-location-aware RDF2Vec and GeoTransE.<br /><br />Summary: <div>
arXiv:2504.17099v1 Announce Type: cross 
Abstract: Many knowledge graphs contain a substantial number of spatial entities, such as cities, buildings, and natural landmarks. For many of these entities, exact geometries are stored within the knowledge graphs. However, most existing approaches for learning entity representations do not take these geometries into account. In this paper, we introduce a variant of RDF2Vec that incorporates geometric information to learn location-aware embeddings of entities. Our approach expands different nodes by flooding the graph from geographic nodes, ensuring that each reachable node is considered. Based on the resulting flooded graph, we apply a modified version of RDF2Vec that biases graph walks using spatial weights. Through evaluations on multiple benchmark datasets, we demonstrate that our approach outperforms both non-location-aware RDF2Vec and GeoTransE.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences</title>
<link>https://arxiv.org/abs/2504.17146</link>
<guid>https://arxiv.org/abs/2504.17146</guid>
<content:encoded><![CDATA[
<div> Keywords: network statistics, Google Trends data, COVID-19 disease progression, dynamic time warping, infodemiology

Summary: 
The study utilized network statistics derived from Google Trends data to predict COVID-19 progression in Metro Manila, Philippines. By applying dynamic time warping (DTW), the temporal alignment between network metrics and COVID-19 case trajectories was measured. The study explored various parameters and found that network density and data preprocessing methods significantly influenced alignment quality. The optimal configuration, using network density with Rescaling Daily Data transformation, achieved substantial temporal alignment with COVID-19 confirmed cases data. The findings suggest that online search behavior can be a useful indicator for epidemic surveillance in urban areas like Metro Manila, leveraging the Philippines' high online usage during the pandemic. This approach offers a valuable tool for early disease spread detection and complements public health monitoring in resource-limited settings. 

<br /><br />Summary: <div>
arXiv:2504.17146v1 Announce Type: cross 
Abstract: The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online posting effects: Unveiling the non-linear journeys of users in depression communities on Reddit</title>
<link>https://arxiv.org/abs/2311.17684</link>
<guid>https://arxiv.org/abs/2311.17684</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, depression, online self-disclosure, mental health discourse, Reddit

Summary:
Through an analysis of user interactions on Reddit boards focused on depression, this study investigates the consequences of online self-disclosure on mental health discourse. Utilizing a data-informed framework, the research identifies 4 distinct clusters representing different psychological states among users. The findings reveal that online exposure to peers' emotional and semantic content can lead to transitions in users' psychological states. These transitions do not follow a linear progression but rather a spiral journey through positive and negative phases. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. The insights gained from this research can contribute to understanding the impact of online social platforms as self-help forums for individuals seeking support for mental health issues. 

<br /><br />Summary: Through an analysis of user interactions on Reddit boards focused on depression, this study identified 4 distinct clusters representing different psychological states. Online exposure to peers' content can lead to transitions in users' psychological states in a spiral journey. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. <div>
arXiv:2311.17684v3 Announce Type: replace 
Abstract: Social media platforms have become pivotal as self-help forums, enabling individuals to share personal experiences and seek support. However, on topics as sensitive as depression, what are the consequences of online self-disclosure? Here, we delve into the dynamics of mental health discourse on various Reddit boards focused on depression. To this aim, we introduce a data-informed framework reconstructing online dynamics from 303k users interacting over two years. Through user-generated content, we identify 4 distinct clusters representing different psychological states. Our analysis unveils online posting effects: a user can transition to another psychological state after online exposure to peers' emotional/semantic content. As described by conditional Markov chains and different levels of social exposure, users' transitions reveal navigation through both positive and negative phases in a spiral rather than a linear progression. Interpreted in light of psychological literature, related particularly to the Patient Health Engagement (PHE) model, our findings can provide evidence that the type and layout of online social interactions have an impact on users' "journeys" when posting about depression.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election</title>
<link>https://arxiv.org/abs/2410.22716</link>
<guid>https://arxiv.org/abs/2410.22716</guid>
<content:encoded><![CDATA[
<div> Russian interference, coordinated information operations, social media, cross-platform, influence campaigns <br />
<br />
Summary: <br />
This study examines coordinated information operations on social media platforms, focusing on interactions across Twitter (now $\mathbb{X$), Facebook, and Telegram related to the 2024 U.S. Election. By analyzing similarity networks, the research identifies coordinated communities engaging in suspicious sharing behaviors within and between platforms. The study uncovers potential foreign interference, with Russian-affiliated media systematically promoted on Telegram and $\mathbb{X$. In addition, the analysis reveals significant intra- and cross-platform coordinated inauthentic activity driving the dissemination of highly partisan, low-credibility, and conspiratorial content. The findings emphasize the need for regulatory measures that go beyond individual platforms to effectively combat the challenges posed by cross-platform coordinated influence campaigns. <div>
arXiv:2410.22716v4 Announce Type: replace 
Abstract: Coordinated information operations remain a persistent challenge on social media, despite platform efforts to curb them. While previous research has primarily focused on identifying these operations within individual platforms, this study shows that coordination frequently transcends platform boundaries. Leveraging newly collected data of online conversations related to the 2024 U.S. Election across $\mathbb{X}$ (formerly, Twitter), Facebook, and Telegram, we construct similarity networks to detect coordinated communities exhibiting suspicious sharing behaviors within and across platforms. Proposing an advanced coordination detection model, we reveal evidence of potential foreign interference, with Russian-affiliated media being systematically promoted across Telegram and $\mathbb{X}$. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic activity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These findings highlight the urgent need for regulatory measures that extend beyond individual platforms to effectively address the growing challenge of cross-platform coordinated influence campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
<div> demographic targeting, fairness, large language models, climate change communication, social media

Summary:
Large language models (LLMs) were used to analyze microtargeting practices in climate change communication on social media, focusing on demographic targeting and fairness. The study achieved an 88.55% accuracy in predicting demographic targets and revealed distinct messaging strategies for different audiences. Young adults were targeted with activism and environmental themes, while women were engaged through caregiving and advocacy topics. LLMs provided transparent explanations for their classifications, highlighting the thematic elements used. A fairness analysis uncovered biases in predicting senior citizens and male audiences, underscoring the need for accountability and inclusivity in social media-driven climate campaigns. This study's framework demonstrates the effectiveness of LLMs in dissecting targeted communication strategies and emphasizes the importance of addressing biases for a more transparent and inclusive climate communication approach.

<br /><br />Summary: <div>
arXiv:2410.05401v2 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating DAO Sustainability and Longevity Through On-Chain Governance Metrics</title>
<link>https://arxiv.org/abs/2504.11341</link>
<guid>https://arxiv.org/abs/2504.11341</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralised Autonomous Organisations, governance efficiency, financial robustness, decentralisation, community engagement

Summary: 
Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. This paper addresses sustainability challenges faced by DAOs, such as limited user participation and concentrated voting power. By introducing a framework of Key Performance Indicators (KPIs) focusing on governance efficiency, financial robustness, decentralisation, and community engagement, the study identifies recurring governance patterns in real-world DAOs. Analysis of a custom dataset using non-parametric methods highlights issues like low participation rates and high proposer concentration. The proposed KPIs provide a data-driven method for assessing and improving DAO governance structures, supporting a comprehensive evaluation of decentralised systems. This research offers practical tools for enhancing the resilience and effectiveness of DAO-based governance models.

Summary: <div>
arXiv:2504.11341v2 Announce Type: replace-cross 
Abstract: Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. However, many DAOs face sustainability challenges linked to limited user participation, concentrated voting power, and technical design constraints. This paper addresses these issues by identifying research gaps in DAO evaluation and introducing a framework of Key Performance Indicators (KPIs) that capture governance efficiency, financial robustness, decentralisation, and community engagement. We apply the framework to a custom-built dataset of real-world DAOs constructed from on-chain data and analysed using non-parametric methods. The results reveal recurring governance patterns, including low participation rates and high proposer concentration, which may undermine long-term viability. The proposed KPIs offer a replicable, data-driven method for assessing DAO governance structures and identifying potential areas for improvement. These findings support a multidimensional approach to evaluating decentralised systems and provide practical tools for researchers and practitioners working to improve the resilience and effectiveness of DAO-based governance models.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schelling segregation dynamics in densely-connected social network graphs</title>
<link>https://arxiv.org/abs/2504.16307</link>
<guid>https://arxiv.org/abs/2504.16307</guid>
<content:encoded><![CDATA[
<div> network model, segregation, polarization, agent-based, social<br />
Summary: <br />
- Schelling segregation model used to study segregation dynamics in dense social networks.
- Dense networks show less segregation compared to sparse networks.
- Agents do not end up more segregated than they desire to be.
- Polarization is difficult in networks with one smaller group and unstable with an extremely small group.
- Mixed evidence for the "paradox of weak minority preferences" in a densely connected social network. <br /> <div>
arXiv:2504.16307v1 Announce Type: new 
Abstract: Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.
  We found that a dense social network does not become as strongly segregated as a sparse network, and that agents' numbers of same-group neighbours do not greatly exceed their desired numbers (i.e. they do not end up more segregated than they desire to be). Furthermore, we found that the network was very difficult to polarise when one group was somewhat smaller than the other, and that it became unstable when one group was extremely small, both of which provide insights into real-world polarisation dynamics. Finally, we tested the question of whether an increase in the minority group's desire for same-group neighbours created more segregation than a similar increase for the majority group -- the "paradox of weak minority preferences" -- and found mixed evidence for this effect in a densely connected social network.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Social Computing Tools for Undergraduate Research Community Building</title>
<link>https://arxiv.org/abs/2504.16366</link>
<guid>https://arxiv.org/abs/2504.16366</guid>
<content:encoded><![CDATA[
<div> impostor syndrome, undergraduate students, social computing tools, research community, sense of belonging <br />
Summary: <br />
The article proposes the use of spontaneous online social networks (SOSNs) to overcome barriers faced by new members, particularly undergraduate students, in research communities. By integrating a photo sharing bot inspired by SOSNs like BeReal into a computing research lab, the study aimed to enhance sense of belonging, peripheral awareness, and feelings of togetherness. Through surveys and interviews with 17 lab members over 2 weeks, an increase in sense of togetherness was observed. The approach facilitated greater awareness of peers' personal lives, fostering a stronger sense of community and reducing feelings of disconnectedness. Overall, the integration of social computing tools in small research communities shows promise in promoting a more inclusive and supportive environment for new members. <br /> <div>
arXiv:2504.16366v1 Announce Type: new 
Abstract: Many barriers exist when new members join a research community, including impostor syndrome. These barriers can be especially challenging for undergraduate students who are new to research. In our work, we explore how the use of social computing tools in the form of spontaneous online social networks (SOSNs) can be used in small research communities to improve sense of belonging, peripheral awareness, and feelings of togetherness within an existing CS research community. Inspired by SOSNs such as BeReal, we integrated a Wizard-of-Oz photo sharing bot into a computing research lab to foster community building among members. Through a small sample of lab members (N = 17) over the course of 2 weeks, we observed an increase in participants' sense of togetherness based on pre- and post-study surveys. Our surveys and semi-structured interviews revealed that this approach has the potential to increase awareness of peers' personal lives, increase feelings of community, and reduce feelings of disconnectedness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.16323</link>
<guid>https://arxiv.org/abs/2504.16323</guid>
<content:encoded><![CDATA[
<div> Keywords: digital media, Media Content Atlas, multimodal large language models, content analysis, screen data

Summary: 
The study introduces the Media Content Atlas (MCA), a pipeline for investigating large-scale screen data using multimodal large language models (MLLMs). MCA allows for moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Tested on 1.12 million smartphone screenshots from 112 adults over a month, MCA supports open-ended exploration and hypothesis generation, as well as hypothesis-driven investigations. Expert evaluators found MCA to be usable and promising for research and intervention design, with high relevance and accuracy ratings for clustering results and descriptions. By combining methodological possibilities with specific research needs, MCA enables both inductive and deductive inquiry, offering new opportunities for media and HCI research.<br /><br />Summary: <div>
arXiv:2504.16323v1 Announce Type: cross 
Abstract: As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, GPT-4o, Llama 3

Summary:
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral are being explored for countering conspiracy theories online through counterspeech. However, the study found that these models often generate generic or superficial responses when provided with expert-crafted counterspeech prompts. They also tend to over-acknowledge fear and frequently fabricate facts or sources, posing challenges for practical application. Unlike hate speech, there is a lack of datasets pairing conspiracy theory comments with expert counterspeech, highlighting the need for further research in this area. The study underscores the limitations of using prompt-based approaches with LLMs for countering harmful online content, indicating the importance of developing more effective strategies for addressing conspiracy theories. 

<br /><br />Summary: 
- Large Language Models like GPT-4o, Llama 3, and Mistral are explored for countering conspiracy theories through counterspeech. 
- These models often generate generic or superficial responses to expert-crafted counterspeech prompts. 
- They tend to over-acknowledge fear and frequently fabricate facts or sources, challenging their practical application. 
- Lack of datasets pairing conspiracy theory comments with expert counterspeech highlights the need for further research in this area. 
- Limitations of using prompt-based approaches with LLMs for countering harmful online content are underscored, emphasizing the need for more effective strategies. <div>
arXiv:2504.16604v1 Announce Type: cross 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Science (SecSci), Basic Concepts and Mathematical Foundations</title>
<link>https://arxiv.org/abs/2504.16617</link>
<guid>https://arxiv.org/abs/2504.16617</guid>
<content:encoded><![CDATA[
<div> Keywords: textbook, security courses, lecture notes, research problems, advanced courses

Summary: 
This textbook is a compilation of lecture notes from security courses taught at various universities, including Oxford, Royal Holloway, and Hawaii. The early chapters are designed for a first course in security, while the middle chapters have been used in advanced courses. Towards the end of the textbook, there are also research problems included. The material covered in the textbook spans different levels of expertise, making it suitable for a range of students from beginners to more advanced learners. The inclusion of research problems at the end of the textbook provides an opportunity for students to engage with the material in a more in-depth and practical manner. Overall, this textbook offers a comprehensive overview of security concepts and is a valuable resource for students studying the subject. 

<br /><br />Summary: <div>
arXiv:2504.16617v1 Announce Type: cross 
Abstract: This textbook compiles the lecture notes from security courses taught at Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii. The early chapters are suitable for a first course in security. The middle chapters have been used in advanced courses. Towards the end there are also some research problems.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Traps and Algorithmic Interventions</title>
<link>https://arxiv.org/abs/2406.04462</link>
<guid>https://arxiv.org/abs/2406.04462</guid>
<content:encoded><![CDATA[
<div> philosophical literature, pessimism traps, information cascades, economics, mathematics <br />
Summary: <br />
This paper explores the relationship between philosophical concepts of pessimism traps and formal models of information cascades in economics and mathematics. Pessimism traps describe how individuals in a community mimic sub-optimal actions of others in uncertain situations, similar to how information cascades involve agents making decisions based on private signals and public histories. The study shows that information cascades inevitably occur in many scenarios, and populations can easily fall into incorrect cascades based on the strength of signals. Once formed, cascades are difficult to break without external intervention. The paper proposes an intervention strategy to redirect populations from incorrect to correct cascades, demonstrating both theoretical and empirical analyses of its efficacy. <div>
arXiv:2406.04462v3 Announce Type: replace 
Abstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-Aware Dense Subgraph Discovery</title>
<link>https://arxiv.org/abs/2412.02604</link>
<guid>https://arxiv.org/abs/2412.02604</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense subgraph discovery, Fairness, Graph mining, Subgraph density, Tractable formulations <br />
Summary: 
This study introduces two tractable formulations for fair Dense subgraph discovery (DSD), focusing on promoting fairness in detecting dense subgraphs that represent diverse subgroups within the vertex set. Existing methods for fair DSD have limitations in NP-hard formulations and lack flexibility in defining fairness levels. The proposed methods offer structured approaches to incorporate fairness with varying levels and introduce a measure of fairness-induced relative loss in subgraph density. Results from extensive experiments on real-world datasets demonstrate that the new methods outperform existing solutions, with significantly lower subgraph density loss in some cases, particularly excelling in scenarios with extreme subgroup imbalances. By addressing these limitations, the study contributes to a better understanding of the trade-off between density and fairness in DSD. <br /> <div>
arXiv:2412.02604v2 Announce Type: replace 
Abstract: Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerable Connectivity Caused by Local Communities in Spatial Networks</title>
<link>https://arxiv.org/abs/2412.14513</link>
<guid>https://arxiv.org/abs/2412.14513</guid>
<content:encoded><![CDATA[
<div> Keywords: local communities, spatial networks, robustness, infrastructure, long-distance links

Summary:
This study examines the impact of local communities on the robustness of connectivity in spatial networks with a focus on planar infrastructure. The research reveals that networks with strong local communities have weaker robustness against malicious attacks. The presence of concentrated nodes connected with short links within local communities leads to vulnerabilities in the network's connectivity. However, the study also suggests that introducing long-distance links can help mitigate the negative effects of local communities on network robustness. By incorporating long-distance connections, the network's ability to withstand attacks and maintain connectivity can be improved. Overall, the findings emphasize the importance of considering the structure of local communities in spatial networks when assessing network robustness and the potential benefits of incorporating longer links to enhance resilience. 

<br /><br />Summary: <div>
arXiv:2412.14513v4 Announce Type: replace 
Abstract: Local communities by concentration of nodes connected with short links are widely observed in spatial networks. However, how such structure affects robustness of connectivity against malicious attacks remains unclear. This study investigates the impact of local communities on the robustness by modeling planar infrastructure reveals that the robustness is weakened by strong local communities in spatial networks. These results highlight the potential of long-distance links in mitigating the negative effects of local community on the robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-based Anchor Embedding for Exact Subgraph Matching</title>
<link>https://arxiv.org/abs/2502.00031</link>
<guid>https://arxiv.org/abs/2502.00031</guid>
<content:encoded><![CDATA[
<div> Keyword: subgraph matching, graph neural network, anchor embedding, exact matching, query plan
Summary:
The paper introduces a novel graph neural network (GNN)-based anchor embedding framework (GNN-AE) to address the subgraph matching problem with exact results. Unlike other approaches that provide only approximate solutions, the GNN-AE approach leverages anchor concepts and embeddings to ensure no false dismissals in subgraph matching. By transforming the problem into a search task in the embedding space, the proposed method guarantees exact matching. An efficient matching growth algorithm is developed to retrieve all exact matches in parallel, supported by a cost-model-based DFS query plan for enhanced performance. Experimental results on various datasets validate the effectiveness and efficiency of the GNN-AE approach for exact subgraph matching.<br /><br />Summary: <div>
arXiv:2502.00031v3 Announce Type: replace 
Abstract: Subgraph matching query is a classic problem in graph data management and has a variety of real-world applications, such as discovering structures in biological or chemical networks, finding communities in social network analysis, explaining neural networks, and so on. To further solve the subgraph matching problem, several recent advanced works attempt to utilize deep-learning-based techniques to handle the subgraph matching query. However, most of these works only obtain approximate results for subgraph matching without theoretical guarantees of accuracy. In this paper, we propose a novel and effective graph neural network (GNN)-based anchor embedding framework (GNN-AE), which allows exact subgraph matching. Unlike GNN-based approximate subgraph matching approaches that only produce inexact results, in this paper, we pioneer a series of concepts related to anchor (including anchor, anchor graph/path, etc.) in subgraph matching and carefully devise the anchor (graph) embedding technique based on GNN models. We transform the subgraph matching problem into a search problem in the embedding space via the anchor (graph & path) embedding techniques. With the proposed anchor matching mechanism, GNN-AE can guarantee subgraph matching has no false dismissals. We design an efficient matching growth algorithm, which can retrieve the locations of all exact matches in parallel. We also propose a cost-model-based DFS query plan to enhance the parallel matching growth algorithm. Through extensive experiments on 6 real-world and 3 synthetic datasets, we confirm the effectiveness and efficiency of our GNN-AE approach for exact subgraph matching.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic hashtag recommendation in social media with trend shift detection and adaptation</title>
<link>https://arxiv.org/abs/2504.00044</link>
<guid>https://arxiv.org/abs/2504.00044</guid>
<content:encoded><![CDATA[
<div> recommendation systems, hashtags, social media, trend shifts, Apache Storm

Summary:
Hashtag recommendation systems play a crucial role in enhancing content categorization and search on social media platforms. However, existing static models face challenges in adapting to the dynamic nature of social media conversations where new hashtags constantly emerge and existing ones evolve semantically. To address this issue, the H-ADAPTS methodology is introduced, which incorporates a trend-aware mechanism to detect shifts in hashtag usage, reflecting evolving trends in social media discussions. By leveraging the Apache Storm framework for scalable and fault-tolerant analysis of high-velocity social data, H-ADAPTS can efficiently adapt its recommendation model based on recent posts. Experimental results from real-world case studies, such as the COVID-19 pandemic and the 2020 US presidential election, demonstrate that H-ADAPTS outperforms existing solutions by providing timely and relevant hashtag recommendations that align with emerging trends in social media conversations. <br /><br />Summary: <div>
arXiv:2504.00044v2 Announce Type: replace 
Abstract: Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition</title>
<link>https://arxiv.org/abs/2307.14530</link>
<guid>https://arxiv.org/abs/2307.14530</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping community, stochastic block model, estimation error, lower bound

Summary: 
Community detection in networks is a crucial issue with applications in various fields. This study focuses on the Mixed-Membership Stochastic Block Model (MMSB) for overlapping community detection. The goal is to reconstruct relationships between communities based on network observations. Different approaches are compared, and a new estimator is proposed that matches the minimax lower bound on estimation error. Theoretical results are established under general model conditions, and experiments are conducted to demonstrate the theory. This work contributes to advancing the understanding and techniques for identifying overlapping communities in networks.<br /><br />Summary: <div>
arXiv:2307.14530v2 Announce Type: replace-cross 
Abstract: Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics</title>
<link>https://arxiv.org/abs/2504.15927</link>
<guid>https://arxiv.org/abs/2504.15927</guid>
<content:encoded><![CDATA[
<div> community detection, semi-supervised, crystallization kinetics, CLANN, Transitive Annealer

Summary:
The paper introduces CLANN, a novel semi-supervised community detection method inspired by crystallization kinetics. Traditional methods face challenges with scalability and unreasonable starting points. CLANN integrates annealing principles to optimize the identification of community cores, mimicking a crystal subgrain expanding into a complete grain. Through a Transitive Annealer, neighboring cliques are merged, and the community core is repositioned for spontaneous growth, improving scalability. Extensive experiments across 43 network settings show CLANN outperforms existing methods on multiple real-world datasets, demonstrating its effectiveness and efficiency in community detection. <br /><br />Summary: <div>
arXiv:2504.15927v1 Announce Type: new 
Abstract: Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Widely Known Findings Easier to Retract?</title>
<link>https://arxiv.org/abs/2504.15504</link>
<guid>https://arxiv.org/abs/2504.15504</guid>
<content:encoded><![CDATA[
<div> Keywords: retraction, failures, science, citation, Altmetric

Summary: 
Retraction failures are common in science, and this study aims to understand why they occur and what factors make findings harder or easier to retract. Utilizing data from Microsoft Academic Graph, Retraction Watch, and Altmetric, the study tests the hypothesis that the social spread of scientific information contributes to retraction failures. Surprisingly, the study finds that widely known or well-established results are actually easier to retract, as their retractions are more relevant to a larger number of scientists. Highly cited papers experience more significant reductions in citations post-retraction and attract more attention to their retractions. These findings suggest that the popularity and impact of a study can influence the ease of retraction, highlighting the importance of social spread and scientific dissemination in the retractions process. 

Summary: <div>
arXiv:2504.15504v1 Announce Type: cross 
Abstract: Failures of retraction are common in science. Why do these failures occur? And, relatedly, what makes findings harder or easier to retract? We use data from Microsoft Academic Graph, Retraction Watch, and Altmetric -- including retracted papers, citation records, and Altmetric scores and mentions -- to test recently proposed answers to these questions. A recent previous study by LaCroix et al. employ simple network models to argue that the social spread of scientific information helps explain failures of retraction. One prediction of their models is that widely known or well established results, surprisingly, should be easier to retract, since their retraction is more relevant to more scientists. Our results support this conclusion. We find that highly cited papers show more significant reductions in citation after retraction and garner more attention to their retractions as they occur.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing well-balanced spanning trees of unweighted networks</title>
<link>https://arxiv.org/abs/2205.06628</link>
<guid>https://arxiv.org/abs/2205.06628</guid>
<content:encoded><![CDATA[
<div> algorithm, spanning tree, network, breadth-first search, graph

Summary:<br /><br />
Spanning trees are crucial for simplifying and sampling networks. Prim's and Kruskal's algorithms are commonly used for weighted networks, but for unweighted networks, breadth-first search may be a better choice as it preserves network structure better and produces more compact and well-balanced spanning trees. The study conducted experiments on synthetic and real networks to compare the performance of different algorithms. The results suggest that breadth-first search algorithm outperforms priority-first search algorithms in terms of preserving network structure and producing efficient spanning trees. This research provides valuable insights into the impact of different algorithms on spanning tree construction and highlights the importance of choosing the right algorithm based on the network's characteristics. <div>
arXiv:2205.06628v2 Announce Type: replace 
Abstract: A spanning tree of a network or graph is a subgraph that connects all nodes with the least number or weight of edges. The spanning tree is one of the most straightforward techniques for network simplification and sampling, and for discovering its backbone or skeleton. Prim's algorithm and Kruskal's algorithm are well-known algorithms for computing a spanning tree of a weighted network, and are therefore also the default procedure for unweighted networks in the most popular network libraries. In this paper, we empirically study the performance of these algorithms on unweighted networks and compare them with different priority-first search algorithms. We show that the structure of a network, such as the distances between the nodes, is better preserved by a simpler algorithm based on breadth-first search. The spanning trees are also most compact and well-balanced as measured by classical graph indices. We support our findings with experiments on synthetic graphs and more than a thousand real networks, and demonstrate practical applications of the computed spanning trees. We conclude that if a spanning tree is to maintain the structure of an unweighted network, the breadth-first search algorithm should be the preferred choice.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem</title>
<link>https://arxiv.org/abs/2410.22897</link>
<guid>https://arxiv.org/abs/2410.22897</guid>
<content:encoded><![CDATA[
<div> Connected services, autonomous driving, data collection, privacy concerns, ontology<br />
Summary:<br />
The automotive industry is undergoing a transformation towards connected services and autonomous driving, leading to increased data collection and sharing in vehicles. This shift raises privacy concerns, prompting the need for understanding how modern vehicles handle data exchange among different parties. A high-level conceptual graph-based model was developed using the ontology 101 methodology, incorporating information from various sources including privacy policy analysis and literature review. The model provides insights into data sharing practices and can be expanded for diverse contexts. Realistic examples demonstrate the model's effectiveness in uncovering privacy issues related to vehicle-related data sharing. Future research directions include exploring advanced ontology languages for reasoning, conducting topological analysis to identify data privacy risks, and developing tools for comparative analysis. These efforts aim to enhance understanding of the vehicle-centric data sharing ecosystem. <div>
arXiv:2410.22897v3 Announce Type: replace 
Abstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the accurate computation of expected modularity in probabilistic networks</title>
<link>https://arxiv.org/abs/2408.07161</link>
<guid>https://arxiv.org/abs/2408.07161</guid>
<content:encoded><![CDATA[
<div> modularity, probabilistic networks, expected value, FPWP, computation <br />
Summary: <br />
Modularity is a key measure for assessing community structures in networks. In probabilistic networks with uncertain edge existence, computing the expected modularity is challenging. The proposed FPWP technique efficiently computes the probability distribution and expected value of modularity. Comparison with other methods reveals that removing low-probability edges or treating probabilities as weights leads to inaccuracies. Monte Carlo sampling has variable convergence based on network parameters. Brute-force computation, while accurate, is slow. In contrast, FPWP is fast and guarantees precise results. Comprehensive experiments on real-world and synthetic networks showcase the accuracy and time efficiency of the FPWP technique. <div>
arXiv:2408.07161v3 Announce Type: replace-cross 
Abstract: Modularity is one of the most widely used measures for evaluating communities in networks. In probabilistic networks, where the existence of edges is uncertain and uncertainty is represented by probabilities, the expected value of modularity can be used instead. However, efficiently computing expected modularity is challenging. To address this challenge, we propose a novel and efficient technique (FPWP) for computing the probability distribution of modularity and its expected value. In this paper, we implement and compare our method and various general approaches for expected modularity computation in probabilistic networks. These include: (1) translating probabilistic networks into deterministic ones by removing low-probability edges or treating probabilities as weights, (2) using Monte Carlo sampling to approximate expected modularity, and (3) brute-force computation. We evaluate the accuracy and time efficiency of FPWP through comprehensive experiments on both real-world and synthetic networks with diverse characteristics. Our results demonstrate that removing low-probability edges or treating probabilities as weights produces inaccurate results, while the convergence of the sampling method varies with the parameters of the network. Brute-force computation, though accurate, is prohibitively slow. In contrast, our method is much faster than brute-force computation, but guarantees an accurate result.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model of Silence, or the Probability of "Un Ange Passe"</title>
<link>https://arxiv.org/abs/2504.13931</link>
<guid>https://arxiv.org/abs/2504.13931</guid>
<content:encoded><![CDATA[
<div> Keywords: silence, conversation dynamics, Markov chain model, phase-transition-like phenomenon, intercultural communication 

Summary: 
The study introduces the concept of "Un ange passe" or "An angel passes," a universal phenomenon of sudden silence in a co-present group. The significance and interpretation of silence vary across cultures, impacting organizational productivity, creativity, and medical settings. Mathematical modeling of silence dynamics is relatively unexplored, prompting the development of a Markov chain model to analyze silence behavior. Results indicate a phase-transition-like occurrence where silence abruptly ends once individuals lose awareness of the conversation, underscoring the necessity of mutual awareness for silence to arise. This model not only enhances understanding of conversational dynamics but also holds potential for enhancing intercultural communication, organizational efficiency, and medical practices. 

<br /><br />Summary: <div>
arXiv:2504.13931v1 Announce Type: new 
Abstract: In French, the phrase "Un ange passe" ("An angel passes") refers to the sudden silence that falls over a co-present group -- that is, a group of people sharing the same physical space. As evidenced by the presence of similar expressions across languages and cultures, this phenomenon represents a universal feature of human conversation. At the same time, the meaning attributed to silence can differ greatly across national, cultural, and interpersonal contexts. Consequently, a wide range of studies have focused on the impact of silence on organizational productivity, its relationship to ideas and creativity, and its potential effectiveness in medical settings. Despite the important role that silence plays, very few studies have attempted to characterize its features using mathematical modeling. In this study, we propose a Markov chain model to describe the dynamics of silence in a co-present group and attempt to analyze its behavior. Our results reveal a phase-transition-like phenomenon, where the probability of silence abruptly drops to zero once individuals' awareness of the surrounding conversation falls below a critical threshold. In other words, such silence can emerge only when individuals retain a minimal degree of mutual awareness of those around them. The model proposed in this study not only offers a deeper understanding of conversational dynamics, but also holds potential for contributing to intercultural communication, organizational productivity, and medical practice.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Writing Patterns Reveal a Hidden Division of Labor in Scientific Teams</title>
<link>https://arxiv.org/abs/2504.14093</link>
<guid>https://arxiv.org/abs/2504.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: individual contributions, coauthored papers, author order, writing signatures, labor specialization

Summary:
This study proposes a behavior-based approach to identifying individual contributions in scientific papers by analyzing author-specific LaTeX macros used as writing signatures in over 730,000 arXiv papers. The research covers the period from 1991 to 2023 and includes over half a million scientists. The method is validated against self-reports, author order, disciplinary norms, and Overleaf records to reliably infer author-level writing activity. The findings reveal a hidden division of labor in collaborative research, with first authors focusing on technical sections and last authors contributing to conceptual sections. This empirical evidence of labor specialization at scale provides new tools to improve credit allocation in scientific collaborations. <div>
arXiv:2504.14093v1 Announce Type: new 
Abstract: The recognition of individual contributions is central to the scientific reward system, yet coauthored papers often obscure who did what. Traditional proxies like author order assume a simplistic decline in contribution, while emerging practices such as self-reported roles are biased and limited in scope. We introduce a large-scale, behavior-based approach to identifying individual contributions in scientific papers. Using author-specific LaTeX macros as writing signatures, we analyze over 730,000 arXiv papers (1991-2023), covering over half a million scientists. Validated against self-reports, author order, disciplinary norms, and Overleaf records, our method reliably infers author-level writing activity. Section-level traces reveal a hidden division of labor: first authors focus on technical sections (e.g., Methods, Results), while last authors primarily contribute to conceptual sections (e.g., Introduction, Discussion). Our findings offer empirical evidence of labor specialization at scale and new tools to improve credit allocation in collaborative research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking mob Dynamics in online social networks Using epidemiology model based on Mobility Equations</title>
<link>https://arxiv.org/abs/2504.14172</link>
<guid>https://arxiv.org/abs/2504.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, epidemiological models, COVID-19 spread, Twitter activity, mobility dynamics

Summary:
<br />
This research focuses on analyzing the social behavior related to the spread of COVID-19 using Twitter activity data from April to June 2020. A mathematical model is introduced to integrate mobility dynamics, derived from real data, to adjust outbreak rates based on social interactions. The model considers mobility as a time-varying parameter, accounting for fluctuations in contact rates influenced by factors like personal behavior and external factors such as lockdowns and quarantines. The study identifies a threshold number, examines the existence of bifurcation, and establishes the stability of steady states. Numerical simulations and sensitivity analysis of relevant parameters are conducted to track public sentiment and engagement trends during the pandemic. <div>
arXiv:2504.14172v1 Announce Type: new 
Abstract: Nowadays, social media is the main tool in our new lives. The outbreak news and all related obtained from social media, and mob events affect the of spread these news fast. Recently, epidemiological models to study disease spread and analyze the behavior of mob groups by dealing with "contagions" that propagate through user networks. In this research, we introduced a mathematical model to analyze social behavior related to COVID-19 spread by examining Twitter activity from April 2020 to June 2020. The main feature of this model is the integration of mobility dynamics that be derived from the above real data, to adjust the rate of outbreak based on the response of social interactions. Consider mobility as a parameter of time-varying, and fluctuations in the rate of contact that is driven by factors like personal behavior or external affecting such as "lockdown" and "quarantine" etc., to track public sentiment and engagement trends during the pandemic. The threshold number is derived, and the existence of bifurcation and the stability of the steady states are established. Numerical simulations and sensitivity analysis of relevant parameters are also carried out.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Diffusion and Preferential Attachment in a Network of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14438</link>
<guid>https://arxiv.org/abs/2504.14438</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, information diffusion, two-time-scale dynamical model, reputation-based preferential attachment mechanism, numerical experiments

Summary:
This paper presents a model for information diffusion in a network of Large Language Models (LLMs) that can provide answers to queries from distributed datasets, even potentially hallucinating the answer. The model incorporates a two-time-scale dynamical system where opinions evolve rapidly while network structure changes slowly. By using a mean-field approximation, the paper establishes conditions for a stable equilibrium where all LLMs remain truthful. Additionally, approximation guarantees for the mean-field and singularly perturbed approximations are provided. To address hallucination and enhance the influence of truthful nodes, a reputation-based preferential attachment mechanism is proposed to reconfigure the network based on evaluations by LLMs of their neighbors. Numerical experiments conducted on an open-source LLM validate the effectiveness of the preferential attachment mechanism and demonstrate optimization of a cost function for the two-time-scale system. <br /><br />Summary: <div>
arXiv:2504.14438v1 Announce Type: new 
Abstract: This paper models information diffusion in a network of Large Language Models (LLMs) that is designed to answer queries from distributed datasets, where the LLMs can hallucinate the answer. We introduce a two-time-scale dynamical model for the centrally administered network, where opinions evolve faster while the network's degree distribution changes more slowly. Using a mean-field approximation, we establish conditions for a locally asymptotically stable equilibrium where all LLMs remain truthful. We provide approximation guarantees for the mean-field approximation and a singularly perturbed approximation of the two-time-scale system. To mitigate hallucination and improve the influence of truthful nodes, we propose a reputation-based preferential attachment mechanism that reconfigures the network based on LLMs' evaluations of their neighbors. Numerical experiments on an open-source LLM (LLaMA-3.1-8B) validate the efficacy of our preferential attachment mechanism and demonstrate the optimization of a cost function for the two-time-scale system.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Local Separators Shape Community Structure in Large Networks</title>
<link>https://arxiv.org/abs/2504.14501</link>
<guid>https://arxiv.org/abs/2504.14501</guid>
<content:encoded><![CDATA[
<div> community detection, local separators, network structure, modularity optimization, hierarchical structures

Summary:
This study explores the use of local separators for community detection in large networks. Local 1-separators are found to effectively identify densely connected communities, outperforming traditional modularity-based methods. On the other hand, local 2-separators reveal hierarchical structures within networks, although they may lead to over-fragmentation of small clusters. These findings are particularly relevant for road networks, suggesting potential applications in transportation and infrastructure analysis. Overall, local separators offer a scalable and interpretable alternative to traditional community detection algorithms, providing a more nuanced understanding of network structure based on structural bottlenecks rather than global connectivity.<br /><br />Summary: <div>
arXiv:2504.14501v1 Announce Type: new 
Abstract: Community detection is a key tool for analyzing the structure of large networks. Standard methods, such as modularity optimization, focus on identifying densely connected groups but often overlook natural local separations in the graph. In this paper, we investigate local separator methods, which decompose networks based on structural bottlenecks rather than global connectivity. We systematically compare them with well-established community detection algorithms on large real-world networks. Our results show that local 1-separators consistently identify the densest communities, outperforming modularity-based methods in this regard, while local 2-separators reveal hierarchical structures but may over-fragment small clusters. These findings are particularly strong for road networks, suggesting practical applications in transportation and infrastructure analysis. Our study highlights local separators as a scalable and interpretable alternative for network decomposition.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform</title>
<link>https://arxiv.org/abs/2504.14904</link>
<guid>https://arxiv.org/abs/2504.14904</guid>
<content:encoded><![CDATA[
<div> benchmark, content moderation, short video platforms, user feedback, KuaiMod <br />
Summary: <br />
Exponentially growing short video platforms (SVPs) struggle with content moderation, especially concerning minors' mental health. Existing methods have limitations such as human bias, lack of understanding nuanced content, and slow update cycles. This paper introduces a new SVP content moderation benchmark with authentic user feedback. The proposed KuaiMod framework addresses these challenges through training data construction, offline adaptation, and online deployment & refinement using large vision language models and Chain-of-Thought reasoning. KuaiMod outperforms other methods in moderation performance on the benchmark, reducing user reporting rates and increasing user engagement on Kuaishou. The open-sourced benchmark aims to advance research in SVP content moderation. <br /> <div>
arXiv:2504.14904v1 Announce Type: new 
Abstract: Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis</title>
<link>https://arxiv.org/abs/2504.15072</link>
<guid>https://arxiv.org/abs/2504.15072</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion propagation dynamics, multi-dimensional Hawkes processes, Graph Neural Network, VISTA dataset 

Summary:
The article introduces an innovative approach that combines multi-dimensional Hawkes processes with Graph Neural Network to model opinion propagation dynamics in social media. The extended multi-dimensional Hawkes process captures hierarchical structures, multi-dimensional interactions, and mutual influences across various topics, forming a complex propagation network. Additionally, the authors present the VISTA dataset, comprising 159 trending topics with detailed sentiment annotations across 11 categories. This dataset allows for a comprehensive understanding of public opinion dynamics across different domains like politics, entertainment, sports, health, and medicine. The approach offers strong interpretability by linking sentiment propagation to comment hierarchy and temporal evolution, serving as a robust baseline for future research. 

<br /><br />Summary: <div>
arXiv:2504.15072v1 Announce Type: new 
Abstract: The rapid development of social media has significantly reshaped the dynamics of public opinion, resulting in complex interactions that traditional models fail to effectively capture. To address this challenge, we propose an innovative approach that integrates multi-dimensional Hawkes processes with Graph Neural Network, modeling opinion propagation dynamics among nodes in a social network while considering the intricate hierarchical relationships between comments. The extended multi-dimensional Hawkes process captures the hierarchical structure, multi-dimensional interactions, and mutual influences across different topics, forming a complex propagation network. Moreover, recognizing the lack of high-quality datasets capable of comprehensively capturing the evolution of public opinion dynamics, we introduce a new dataset, VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015 second-level comments, and 29,578 third-level comments, covering diverse domains such as politics, entertainment, sports, health, and medicine. The dataset is annotated with detailed sentiment labels across 11 categories and clearly defined hierarchical relationships. When combined with our method, it offers strong interpretability by linking sentiment propagation to the comment hierarchy and temporal evolution. Our approach provides a robust baseline for future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Binary Opinions: A Deep Reinforcement Learning-Based Approach to Uncertainty-Aware Competitive Influence Maximization</title>
<link>https://arxiv.org/abs/2504.15131</link>
<guid>https://arxiv.org/abs/2504.15131</guid>
<content:encoded><![CDATA[
<div> Keywords: Competitive Influence Maximization, Deep Reinforcement Learning, Subjective Logic, Uncertainty, Online Social Networks

Summary: <br /><br />The article introduces DRIM, a novel framework for the Competitive Influence Maximization (CIM) problem in online social networks. DRIM leverages Deep Reinforcement Learning (DRL) and Subjective Logic to model uncertainty in user opinions and preferences, enhancing decision-making in seed selection for information propagation. The framework incorporates an Uncertainty-based Opinion Model (UOM) to capture realistic user uncertainty and optimize the spread of true information while countering false information. Results demonstrate that DRIM-based CIM schemes outperform existing methods in influence spread and efficiency. Sensitivity analysis reveals that network observability and information propagation positively impact performance, while high network activity can mitigate the effects of initial user biases. Overall, DRIM offers a comprehensive approach to CIM that embraces uncertainty and improves influence maximization strategies in online social networks. <div>
arXiv:2504.15131v1 Announce Type: new 
Abstract: The Competitive Influence Maximization (CIM) problem involves multiple entities competing for influence in online social networks (OSNs). While Deep Reinforcement Learning (DRL) has shown promise, existing methods often assume users' opinions are binary and ignore their behavior and prior knowledge. We propose DRIM, a multi-dimensional uncertainty-aware DRL-based CIM framework that leverages Subjective Logic (SL) to model uncertainty in user opinions, preferences, and DRL decision-making. DRIM introduces an Uncertainty-based Opinion Model (UOM) for a more realistic representation of user uncertainty and optimizes seed selection for propagating true information while countering false information. In addition, it quantifies uncertainty in balancing exploration and exploitation. Results show that UOM significantly enhances true information spread and maintains influence against advanced false information strategies. DRIM-based CIM schemes outperform state-of-the-art methods by up to 57% and 88% in influence while being up to 48% and 77% faster. Sensitivity analysis indicates that higher network observability and greater information propagation boost performance, while high network activity mitigates the effect of users' initial biases.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An experimental study of the influence of anonymous information on social media users</title>
<link>https://arxiv.org/abs/2504.15215</link>
<guid>https://arxiv.org/abs/2504.15215</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, anonymous information, influence, opinions, agent-based modeling

Summary: 
The study aimed to investigate the impact of anonymous information on individuals' opinions. Through an online experiment with participants from the U.S., the results showed that anonymous comments can influence up to half of the participants' opinion selections, potentially altering popularity rankings. Agent-based modeling was used to understand how this influence occurs, revealing a straightforward mechanism at play. The study also found that the strength of influence lessens as participants' confidence in their selections increases. Additionally, participants with higher confidence in their initial opinions were less likely to change them based on anonymous information. This study emphasizes the significant role that anonymous information from social media can play in shaping individuals' opinions, indicating the need for caution and critical thinking when consuming such content.<br /><br />Summary: <div>
arXiv:2504.15215v1 Announce Type: new 
Abstract: Increasingly, people use social media for their day-to-day interactions and as a source of information, even though much of this information is practically anonymous. This raises the question: does anonymous information influence its recipients? We conducted an online, two-phase, preregistered experiment using a nationally representative sample of participants from the U.S. to find the answer. To avoid biases of opinions among participants, in the first phase, each participant examines ten Rorschach inkblots and chooses one of four opinions assigned to each inkblot. In the second phase, the participants are randomly assigned to one of four distinct information conditions and are asked to revisit their opinions for the same ten inkblots. Conditions ranged from repeating phase one to receiving anonymous comments about certain opinions. Results were consistent with the preregistration. Importantly, anonymous comments shown in phase two influence up to half of the participants' opinion selections. To better understand the role of anonymous comments in influencing the selections of opinions, we implemented agent-based modeling (ABM). ABM results suggest that a straightforward mechanism can explain the impact of such information. Overall, our results indicate that even anonymous information can have a significant impact on its recipients, potentially altering their popularity rankings. However, the strength of such influence weakens when recipients' confidence in their selections increases. Additionally, we found that participants' confidence in the first phase is inversely related to the number of change opinions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth</title>
<link>https://arxiv.org/abs/2504.13902</link>
<guid>https://arxiv.org/abs/2504.13902</guid>
<content:encoded><![CDATA[
<div> Electric Vehicles, Charging Infrastructure, Graph Model, Time Granularity, Causality <br />
<br />
Summary: This study explores the relationship between Electric Vehicle (EV) adoption and Charging Infrastructure (CI) growth in 137 counties across six states in the U.S. Using a graph model and different time granularities, the analysis reveals that lower levels of time granularity lead to more homogeneous clusters, showing differences in EV adoption and CI growth. Causal relationships between EV adoption and CI growth are identified, with causality more prevalent in Early Adoption scenarios compared to Late Adoption ones. However, causal effects in Early Adoption are slower than in Late Adoption. The findings highlight the importance of understanding the complex dynamics between EV adoption and CI growth to facilitate the widespread adoption of electrified vehicles and address challenges in reducing CO2 emissions and natural resource depletion. <br /> <div>
arXiv:2504.13902v1 Announce Type: cross 
Abstract: The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Conspiratorial Narratives within Arabic Online Content</title>
<link>https://arxiv.org/abs/2504.14037</link>
<guid>https://arxiv.org/abs/2504.14037</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Arabic digital spaces, Named Entity Recognition, Topic Modeling, socio-political contexts

Summary: 
This study examines the spread of conspiracy theories in Arabic online platforms by using computational analysis techniques. The researchers apply Named Entity Recognition and Topic Modeling, particularly the Top2Vec algorithm, to analyze data from Arabic blogs and Facebook. They identify and classify various conspiratorial narratives, including gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering theories. The research demonstrates how these narratives are deeply ingrained in Arabic social media discussions, influenced by regional historical, cultural, and socio-political factors. By leveraging advanced Natural Language Processing methods with Arabic content, this study fills a gap in conspiracy theory research that has traditionally focused on English-language or offline data. The findings offer fresh insights into how conspiracy theories manifest and evolve in Arabic digital spaces, contributing to a better understanding of their impact on public discourse in the Arab world.

<br /><br />Summary: <div>
arXiv:2504.14037v1 Announce Type: cross 
Abstract: This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matrix Factorization with Dynamic Multi-view Clustering for Recommender System</title>
<link>https://arxiv.org/abs/2504.14565</link>
<guid>https://arxiv.org/abs/2504.14565</guid>
<content:encoded><![CDATA[
<div> dynamic multi-view clustering, matrix factorization, recommender systems, representation learning, interpretability

Summary:<br /><br />Matrix Factorization with Dynamic Multi-view Clustering (MFDMC) is proposed as a unified framework for efficient and comprehensive representation learning in recommender systems. MFDMC addresses the challenges of large-scale applications by performing end-to-end training and leveraging dynamic multi-view clustering. This approach improves efficiency and interpretability by adaptively pruning poorly formed clusters and capturing diverse roles of entities across views. The proposed framework demonstrates superior performance in recommender systems and other representation learning domains, such as computer vision. MFDMC's scalability and versatility make it a promising solution for web-scale applications. <div>
arXiv:2504.14565v1 Announce Type: cross 
Abstract: Matrix factorization (MF), a cornerstone of recommender systems, decomposes user-item interaction matrices into latent representations. Traditional MF approaches, however, employ a two-stage, non-end-to-end paradigm, sequentially performing recommendation and clustering, resulting in prohibitive computational costs for large-scale applications like e-commerce and IoT, where billions of users interact with trillions of items. To address this, we propose Matrix Factorization with Dynamic Multi-view Clustering (MFDMC), a unified framework that balances efficient end-to-end training with comprehensive utilization of web-scale data and enhances interpretability. MFDMC leverages dynamic multi-view clustering to learn user and item representations, adaptively pruning poorly formed clusters. Each entity's representation is modeled as a weighted projection of robust clusters, capturing its diverse roles across views. This design maximizes representation space utilization, improves interpretability, and ensures resilience for downstream tasks. Extensive experiments demonstrate MFDMC's superior performance in recommender systems and other representation learning domains, such as computer vision, highlighting its scalability and versatility.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Telegram as a Battlefield: Kremlin-related Communications during the Russia-Ukraine Conflict</title>
<link>https://arxiv.org/abs/2501.01884</link>
<guid>https://arxiv.org/abs/2501.01884</guid>
<content:encoded><![CDATA[
<div> Dataset, Telegram, Russia-Ukraine conflict, content moderation, misinformation 

Summary: 
The paper introduces a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected before and after the Russian invasion of Ukraine. The dataset includes over 4 million posts from 404 pro-Kremlin channels and over 1 million posts from 114 anti-Kremlin channels. It outlines the data collection process, processing methods, and characteristics of the dataset. The study highlights the dissemination of Pro-Kremlin narratives and potential misinformation on Telegram due to its minimal content moderation policies. It also notes the spread of anti-Kremlin narratives, including war footage, troop movements, and air raid warnings. The paper concludes by discussing the research opportunities this dataset presents for scholars in various disciplines. <br /><br />Summary: <div>
arXiv:2501.01884v3 Announce Type: replace 
Abstract: Telegram emerged as a crucial platform for both parties during the conflict between Russia and Ukraine. Per its minimal policies for content moderation, Pro-Kremlin narratives and potential misinformation were spread on Telegram, while anti-Kremlin narratives with related content were also propagated, such as war footage, troop movements, maps of bomb shelters, and air raid warnings. This paper presents a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected over a period spanning a year before and a year after the Russian invasion. The dataset comprises 404 pro-Kremlin channels with 4,109,645 posts and 114 anti-Kremlin channels with 1,117,768 posts. We provide details on the data collection process, processing methods, and dataset characterization. Lastly, we discuss the potential research opportunities this dataset may enable researchers across various disciplines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences</title>
<link>https://arxiv.org/abs/2504.09428</link>
<guid>https://arxiv.org/abs/2504.09428</guid>
<content:encoded><![CDATA[
<div> Keywords: online games, friend recommendation, user features, structural information, FROG<br />
<br />
Summary: 
The article discusses the importance of friend recommendation in online games due to the popularity of mobile devices. Existing approaches have limitations in incorporating multi-modal user features and structural information from friendship graphs effectively. These limitations include ignoring high-order structural proximity between users, failing to learn pairwise relevance between users at a modality-specific level, and not capturing both local and global user preferences on different modalities. To address these issues, the paper introduces an end-to-end model called FROG that better models user preferences for potential friends. The model has been evaluated through comprehensive experiments, including offline evaluation and online deployment at Tencent, demonstrating its superiority over existing approaches. <div>
arXiv:2504.09428v2 Announce Type: replace 
Abstract: Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (e.g., images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model FROG that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at Tencent have demonstrated the superiority of FROG over existing approaches.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrustMap: Mapping Truthfulness Stance of Social Media Posts on Factual Claims for Geographical Analysis</title>
<link>https://arxiv.org/abs/2504.10511</link>
<guid>https://arxiv.org/abs/2504.10511</guid>
<content:encoded><![CDATA[
<div> TrustMap, social media, factual claims, misinformation, stance detection  
Summary:  
TrustMap is a tool that categorizes social media posts into positive, negative, or neutral stances based on the truthfulness of factual claims, using advanced language models for automatic classification. The tool analyzes regional variations in stance patterns across the U.S., allowing users to explore how public opinions differ geographically. By connecting stance detection with geographical analysis, TrustMap provides valuable insights into how people engage with factual claims on social media. This innovative approach helps to combat the spread of misinformation and understand how individuals form opinions and make decisions based on the information they encounter online. <div>
arXiv:2504.10511v2 Announce Type: replace 
Abstract: Factual claims and misinformation circulate widely on social media and affect how people form opinions and make decisions. This paper presents a truthfulness stance map (TrustMap), an application that identifies and maps public stances toward factual claims across U.S. regions. Each social media post is classified as positive, negative, or neutral/no stance, based on whether it believes a factual claim is true or false, expresses uncertainty about the truthfulness, or does not explicitly take a position on the claim's truthfulness. The tool uses a retrieval-augmented model with fine-tuned language models for automatic stance classification. The stance classification results and social media posts are grouped by location to show how stance patterns vary geographically. TrustMap allows users to explore these patterns by claim and region and connects stance detection with geographical analysis to better understand public engagement with factual claims.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Graph Rewiring and Feature Denoising via Spectral Resonance</title>
<link>https://arxiv.org/abs/2408.07191</link>
<guid>https://arxiv.org/abs/2408.07191</guid>
<content:encoded><![CDATA[
<div> Algorithm, Denoise, Rewire, Graph Data, Node Classification

Summary: 
The proposed algorithm, Jointly Denoise and Rewire (JDR), simultaneously denoises node features and rewires the graph to enhance the performance of graph neural networks for node classification. By aligning the leading spectral spaces of the graph and feature matrices, JDR effectively handles graphs with multiple classes and varying levels of homophily or heterophily. The non-convex optimization problem associated with JDR is approximately solved, leading to improved results compared to existing rewiring methods across various synthetic and real-world node classification tasks. The theoretical justification for JDR in a simplified scenario further supports its effectiveness in enhancing the accuracy of downstream GNNs. <div>
arXiv:2408.07191v4 Announce Type: replace-cross 
Abstract: When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to jointly denoise the features and rewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Another Hour on TikTok: Reverse-engineering unique identifiers to obtain a complete slice of TikTok</title>
<link>https://arxiv.org/abs/2504.13279</link>
<guid>https://arxiv.org/abs/2504.13279</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, platform, dataset, metadata, posts

Summary:
Through a newly developed method, researchers extracted a representative sample from TikTok to collect a vast amount of post data. The dataset obtained includes post metadata, video media data, and comments, providing a close to complete snapshot of TikTok activity. Critical statistics of the platform were reported, estimating that 117 million posts were produced on the day observed. This method allowed for a deeper understanding of TikTok's impact on global events and addressed issues in determining fundamental characteristics of the platform. The research sheds light on the massive scale of TikTok and its significance in today's digital landscape. <div>
arXiv:2504.13279v1 Announce Type: new 
Abstract: TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, and use it to collect >99\% of posts from a full hour on the platform, alongside a dataset of >99\% of posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media data, and comments from a close to complete slice of TikTok. Using this dataset, we report the critical statistics of the platform, notably estimating a total of 117 million posts produced on the day we looked at on TikTok.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Informed by Micro and Mesoscopic Statistical Physics Methods for Community Detection</title>
<link>https://arxiv.org/abs/2504.13538</link>
<guid>https://arxiv.org/abs/2504.13538</guid>
<content:encoded><![CDATA[
<div> machine learning, community detection, complex networks, ensemble learning, structural patterns

Summary: 
The article introduces a novel framework that combines machine learning with statistical physics to enhance community detection in complex networks. Previous methods have focused on mesoscopic structures but struggle with incorporating fine-grained node similarities. The proposed framework integrates micro-level node-pair similarities into mesoscopic community structures using ensemble learning models. Experimental evaluations on artificial and real-world networks show that the framework outperforms conventional methods by achieving higher modularity and improved accuracy in NMI and ARI metrics. When ground-truth labels are available, the framework yields the most accurate results, effectively recovering real-world community structures with minimal misclassifications. The analysis of the correlation between node-pair similarity and evaluation metrics indicates a strong relationship, emphasizing the importance of node-pair similarity in enhancing detection accuracy. Overall, the study highlights the synergistic relationship between machine learning and statistical physics in uncovering complex structural patterns in networks. 

<br /><br />Summary: <div>
arXiv:2504.13538v1 Announce Type: new 
Abstract: Community detection plays a crucial role in understanding the structural organization of complex networks. Previous methods, particularly those from statistical physics, primarily focus on the analysis of mesoscopic network structures and often struggle to integrate fine-grained node similarities. To address this limitation, we propose a low-complexity framework that integrates machine learning to embed micro-level node-pair similarities into mesoscopic community structures. By leveraging ensemble learning models, our approach enhances both structural coherence and detection accuracy. Experimental evaluations on artificial and real-world networks demonstrate that our framework consistently outperforms conventional methods, achieving higher modularity and improved accuracy in NMI and ARI. Notably, when ground-truth labels are available, our approach yields the most accurate detection results, effectively recovering real-world community structures while minimizing misclassifications. To further explain our framework's performance, we analyze the correlation between node-pair similarity and evaluation metrics. The results reveal a strong and statistically significant correlation, underscoring the critical role of node-pair similarity in enhancing detection accuracy. Overall, our findings highlight the synergy between machine learning and statistical physics, demonstrating how machine learning techniques can enhance network analysis and uncover complex structural patterns.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning Method to Factual and Counterfactual Explanations for Session-based Recommendation</title>
<link>https://arxiv.org/abs/2504.13632</link>
<guid>https://arxiv.org/abs/2504.13632</guid>
<content:encoded><![CDATA[
<div> Session-based Recommendation, Explanation, FCESR, Factual, Counterfactual<br />
Summary:<br />
Session-based Recommendation (SR) systems often lack transparency in their recommendations. The FCESR framework aims to provide explanations for SR models by highlighting both factual and counterfactual factors influencing recommendations. This novel approach uses combinatorial optimization and reinforcement learning to identify the critical sequence of items impacting recommendations. By incorporating factual and counterfactual insights into a contrastive learning paradigm, the framework enhances SR accuracy significantly. Through extensive evaluations on various datasets and SR architectures, FCESR not only improves recommendation accuracy but also enhances the quality and interpretability of explanations. This advancement in transparency paves the way for more trustworthy recommendation systems. <br />Summary: <div>
arXiv:2504.13632v1 Announce Type: new 
Abstract: Session-based Recommendation (SR) systems have recently achieved considerable success, yet their complex, "black box" nature often obscures why certain recommendations are made. Existing explanation methods struggle to pinpoint truly influential factors, as they frequently depend on static user profiles or fail to grasp the intricate dynamics within user sessions. In response, we introduce FCESR (Factual and Counterfactual Explanations for Session-based Recommendation), a novel framework designed to illuminate SR model predictions by emphasizing both the sufficiency (factual) and necessity (counterfactual) of recommended items. By recasting explanation generation as a combinatorial optimization challenge and leveraging reinforcement learning, our method uncovers the minimal yet critical sequence of items influencing recommendations. Moreover, recognizing the intrinsic value of robust explanations, we innovatively utilize these factual and counterfactual insights within a contrastive learning paradigm, employing them as high-quality positive and negative samples to fine-tune and significantly enhance SR accuracy. Extensive qualitative and quantitative evaluations across diverse datasets and multiple SR architectures confirm that our framework not only boosts recommendation accuracy but also markedly elevates the quality and interpretability of explanations, thereby paving the way for more transparent and trustworthy recommendation systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagational Proxy Voting</title>
<link>https://arxiv.org/abs/2504.13641</link>
<guid>https://arxiv.org/abs/2504.13641</guid>
<content:encoded><![CDATA[
<div> Fractional Votes, Expected Utility, Voting Matrix, Absorbing Markov Chains, Budget Allocation <br />
Summary: 
This paper introduces a novel voting process where voters can allocate fractional votes based on their expected utility across various domains. By using a voting matrix to reflect preferences, the approach allows for a more nuanced expression of preferences by calculating results and relevance within each node. The authors leverage absorbing Markov chains to determine consensus and assess influence within participating nodes. An experiment involving 69 students on budget allocation demonstrates the practical application of this method. The study showcases the effectiveness of this approach in capturing diverse preferences and providing a more detailed understanding of individual and collective decision-making processes. <div>
arXiv:2504.13641v1 Announce Type: new 
Abstract: This paper proposes a voting process in which voters allocate fractional votes to their expected utility in different domains: over proposals, other participants, and sets containing proposals and participants. This approach allows for a more nuanced expression of preferences by calculating the result and relevance within each node. We modeled this by creating a voting matrix that reflects their preference. We use absorbing Markov chains to gain the consensus, and also calculate the influence within the participating nodes. We illustrate this method in action through an experiment with 69 students using a budget allocation topic.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit</title>
<link>https://arxiv.org/abs/2504.13674</link>
<guid>https://arxiv.org/abs/2504.13674</guid>
<content:encoded><![CDATA[
<div> stigma, minority students, college, stereotype, discrimination
<br />
Summary: 
The research focuses on the unique challenges faced by minority college students, influenced by their gender/sexual orientation, race, religion, and academic environment. The study examines stigma processes experienced by minority groups, particularly in online spaces like the r/college subreddit. By utilizing a Stereotype-BERT model, the study identifies stages of stigma processes such as labeling, stereotyping, separation, status loss, and discrimination. Professional identity posts are mainly associated with stereotyping, while racial identity posts are more prevalent in status loss and discrimination. Intersectional posts, reflecting compounded vulnerabilities due to intersecting identities, show a higher frequency of status loss and discrimination. The study emphasizes the importance of intersectional approaches in identifying stigma processes to promote equity for minority groups, especially racial minorities. 
<br /><br /> <div>
arXiv:2504.13674v1 Announce Type: new 
Abstract: Minority college students face unique challenges shaped by their identities based on their gender/sexual orientation, race, religion, and academic institutions, which influence their academic and social experiences. Although research has highlighted the challenges faced by individual minority groups, the stigma process-labeling, stereotyping, separation, status loss, and discrimination-that underpin these experiences remains underexamined, particularly in the online spaces where college students are highly active. We address these gaps by examining posts on subreddit, r/college, as indicators for stigma processes, our approach applies a Stereotype-BERT model, including stance toward each stereotype. We extend the stereotype model to encompass status loss and discrimination by using semantic distance with their reference sentences. Our analyses show that professional indicated posts are primarily labeled under the stereotyping stage, whereas posts indicating racial are highly represented in status loss and discrimination. Intersectional identified posts are more frequently associated with status loss and discrimination. The findings of this study highlight the need for multifaceted intersectional approaches to identifying stigma, which subsequently serve as indicators to promote equity for minority groups, especially racial minorities and those experiencing compounded vulnerabilities due to intersecting identities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
<link>https://arxiv.org/abs/2504.13261</link>
<guid>https://arxiv.org/abs/2504.13261</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, pedagogical grammar, foreign language education, evaluation 
Summary: 

The paper introduces CPG-EVAL, a benchmark designed to assess large language models' (LLMs) knowledge of pedagogical grammar in foreign language instruction. The benchmark includes tasks to test grammar recognition, fine-grained distinctions, categorical discrimination, and resistance to linguistic interference. Smaller LLMs perform well in single language instance tasks but struggle with interference and multiple instances. Larger models show better resistance to interference but still need accuracy improvement. The study emphasizes the importance of aligning LLMs with educational goals and creating more rigorous benchmarks. CPG-EVAL aims to guide the deployment of LLMs in Chinese language teaching contexts. The evaluation provides insights for educators, policymakers, and developers to understand LLM capabilities in education and informs decisions on their integration in foreign language instruction. Further research is needed to enhance model alignment and educational suitability. 

<br /><br />Summary: <div>
arXiv:2504.13261v1 Announce Type: cross 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
<link>https://arxiv.org/abs/2504.13277</link>
<guid>https://arxiv.org/abs/2504.13277</guid>
<content:encoded><![CDATA[
<div> Interpersonal Theory of Suicide, Suicidal Ideation, Machine Learning, Natural Language Analysis, AI Chatbots
<br />
Summary: 
The study utilized the Interpersonal Theory of Suicide to analyze posts from Reddit's r/SuicideWatch, identifying dimensions and risk factors of suicidal ideation. High-risk posts often included mentions of planning, attempts, and pain. Supportive responses were analyzed, showing varied reactions at different stages of suicidal ideation posts. AI chatbots were explored for providing support, showing improved structural coherence but lacking in dynamic and personalized responses. Overall, the study highlights the importance of understanding factors affecting high-risk suicidal intent and the need for careful consideration in developing AI-driven interventions for mental health support. <div>
arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</title>
<link>https://arxiv.org/abs/2504.13284</link>
<guid>https://arxiv.org/abs/2504.13284</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet penetration, Africa, mobile Internet, Senegal, social networks

Summary:
Young people in Senegal are increasingly using the Internet, especially social networks, as Internet penetration rates rise in Africa. The availability of smartphones has further boosted mobile Internet usage among the youth. However, the limited number of operators in the market restricts the choices available, impacting the value for money for users. In this study, Twitter and Facebook comments were analyzed to understand the sentiment of young people towards the price of mobile Internet in Senegal and its perceived quality. The sentiment analysis model revealed the general feelings of the users towards the service. Overall, the study sheds light on the importance of affordable and quality mobile Internet services in Senegal to meet the demands of the younger population who heavily rely on social networks for communication and self-expression.<br /><br />Summary: <div>
arXiv:2504.13284v1 Announce Type: cross 
Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Can't believe I'm crying over an anime girl": Public Parasocial Grieving and Coping Towards VTuber Graduation and Termination</title>
<link>https://arxiv.org/abs/2504.13421</link>
<guid>https://arxiv.org/abs/2504.13421</guid>
<content:encoded><![CDATA[
<div> Keywords: Virtual YouTubers, parasocial relationships, VTuber retirements, coping methods, community dynamics

Summary:
This study explores the dynamics of viewer-VTuber relationships, focusing on how English-speaking viewers cope with the retirement of VTubers. The research categorizes different types of VTuber retirements and analyzes Reddit posts to understand viewer reactions. Findings show that viewers experience emotions like sadness, shock, and loyalty, with coping methods resembling those used when losing loved ones. As time passes, emotions like sadness and love decrease while regret and loyalty show opposite trends. Viewers' reactions also highlight the VTuber identity's place within a larger community of content creators and viewers. The study discusses design implications and their impact on the VTuber ecosystem, providing insights for future research in this emerging field.<br /><br />Summary: This study delves into how English-speaking viewers navigate the retirement of Virtual YouTubers, categorizing different types of retirements and analyzing viewer reactions on Reddit. The findings reveal a range of emotions experienced by viewers, with coping methods resembling those used in times of loss. Over time, emotions like sadness and love decrease, while regret and loyalty show contrasting trends. Viewer reactions also shed light on the VTuber identity's role within a broader community of creators and viewers. The study concludes by discussing design implications and their potential impact on the VTuber ecosystem, suggesting directions for further research in this evolving field. <div>
arXiv:2504.13421v1 Announce Type: cross 
Abstract: Despite the significant increase in popularity of Virtual YouTubers (VTubers), research on the unique dynamics of viewer-VTuber parasocial relationships is nascent. This work investigates how English-speaking viewers grieved VTubers whose identities are no longer used, an interesting context as the nakanohito (i.e., the person behind the VTuber identity) is usually alive post-retirement and might "reincarnate" as another VTuber. We propose a typology for VTuber retirements and analyzed 13,655 Reddit posts and comments spanning nearly three years using mixed-methods. Findings include how viewers coped using methods similar to when losing loved ones, alongside novel coping methods reflecting different attachment styles. Although emotions like sadness, shock, concern, disapproval, confusion, and love decreased with time, regret and loyalty showed opposite trends. Furthermore, viewers' reactions situated a VTuber identity within a community of content creators and viewers. We also discuss design implications alongside implications on the VTuber ecosystem and future research directions.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset of the Representatives Elected in France During the Fifth Republic</title>
<link>https://arxiv.org/abs/2504.02869</link>
<guid>https://arxiv.org/abs/2504.02869</guid>
<content:encoded><![CDATA[
<div> Keywords: electoral system, political representation, France, database, political change<br />
Summary:<br /> 
This article discusses the importance of the electoral system in shaping democracy, particularly in France. It highlights the challenges in accessing data on elected representatives and introduces a new relational database that compiles information on representatives in France since the Fifth Republic. This database provides a valuable resource for analyzing trends in political representation, party dynamics, gender equality, and the professionalization of politics. By offering a longitudinal view of elected representatives, the database enables researchers to study the institutional stability of the Fifth Republic and identify factors driving political change. Overall, this database enhances understanding of political processes in France and facilitates in-depth analyses of the country's political landscape over time. <br /><br />Summary: <div>
arXiv:2504.02869v2 Announce Type: replace 
Abstract: The electoral system is a cornerstone of democracy, shaping the structure of political competition, representation, and accountability. In the case of France, it is difficult to access data describing elected representatives, though, as they are scattered across a number of sources, including public institutions, but also academic and individual efforts. This article presents a unified relational database that aims at tackling this issue by gathering information regarding representatives elected in France over the whole Fifth Republic (1958-present). This database constitutes an unprecedented resource for analyzing the evolution of political representation in France, exploring trends in party system dynamics, gender equality, and the professionalization of politics. By providing a longitudinal view of French elected representatives, the database facilitates research on the institutional stability of the Fifth Republic, offering insights into the factors of political change.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title>
<link>https://arxiv.org/abs/2501.02505</link>
<guid>https://arxiv.org/abs/2501.02505</guid>
<content:encoded><![CDATA[
<div> Bayesian methodology, partial rankings, pairwise comparisons, Bradley-Terry model, inference-based methods <br />
Summary: <br />
- The article introduces a Bayesian methodology for learning partial rankings from pairwise comparison data, addressing the challenge of distinguishing between items with limited or noisy comparisons.
- Current inference-based ranking methods often assign unique ranks to each item, even when there is insufficient evidence to differentiate their performance.
- The proposed framework allows for the incorporation of ties in rankings, only distinguishing between items when there is enough evidence from the data.
- An agglomerative algorithm is developed for Maximum A Posteriori (MAP) inference of partial rankings.
- The method is evaluated on real and synthetic network datasets, demonstrating its ability to provide a more parsimonious summary of the data compared to traditional ranking methods, especially in cases of sparse observations. 

<br />
Summary: <div>
arXiv:2501.02505v2 Announce Type: replace-cross 
Abstract: A common task arising in various domains is that of ranking items based on the outcomes of pairwise comparisons, from ranking players and teams in sports to ranking products or brands in marketing studies and recommendation systems. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model of the comparison outcomes, have emerged as flexible and powerful tools to tackle the task of ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, existing inference-based ranking methods overwhelmingly choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we address this problem by developing a principled Bayesian methodology for learning partial rankings -- rankings with ties -- that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. Our framework is adaptable to any statistical ranking method in which the outcomes of pairwise observations depend on the ranks or scores of the items being compared. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Bluesky</title>
<link>https://arxiv.org/abs/2504.12902</link>
<guid>https://arxiv.org/abs/2504.12902</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, rapid growth, evolving network structure, user migrations, viral information diffusion 

Summary: 
Bluesky, a social media platform, experienced rapid growth and network evolution from August 2023 to February 2025. Multiple waves of user migrations contributed to the platform's establishment of a stable, actively engaged user base. The growth process led to the formation of a dense follower network characterized by clustering and hub features, facilitating the viral diffusion of information. These developments underscore the similarities in engagement and network structure between Bluesky and established social media platforms. This study sheds light on the dynamics of user engagement and network evolution in the context of Bluesky's growth trajectory. <div>
arXiv:2504.12902v1 Announce Type: new 
Abstract: This study investigates the rapid growth and evolving network structure of Bluesky from August 2023 to February 2025. Through multiple waves of user migrations, the platform has reached a stable, persistently active user base. The growth process has given rise to a dense follower network with clustering and hub features that favor viral information diffusion. These developments highlight engagement and structural similarities between Bluesky and established platforms.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, taxonomy, large language models, factual claims, automated construction

Summary:
LLMTaxo is a new framework that utilizes large language models to automatically generate a taxonomy of factual claims from social media. By creating topics at multiple granular levels, LLMTaxo helps stakeholders navigate the complex landscape of online discourse. The framework was tested on three diverse datasets using various models, and was evaluated using both human assessments and GPT-4. Results indicated that LLMTaxo effectively categorizes factual claims and highlights the superior performance of certain models on specific datasets. This innovative approach offers valuable insights for analyzing and comprehending online content, providing a more efficient way to classify and understand information shared on social media platforms. <div>
arXiv:2504.12325v1 Announce Type: cross 
Abstract: With the vast expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomy of factual claims from social media by generating topics from multi-level granularities. This approach aids stakeholders in more effectively navigating the social media landscapes. We implement this framework with different models across three distinct datasets and introduce specially designed taxonomy evaluation metrics for a comprehensive assessment. With the evaluations from both human evaluators and GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims from social media, and reveals that certain models perform better on specific datasets.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embeddings Track Social Group Changes Across 70 Years in China</title>
<link>https://arxiv.org/abs/2504.12327</link>
<guid>https://arxiv.org/abs/2504.12327</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese state-controlled media, word embeddings, social group representation, societal beliefs, linguistic analysis 

Summary: 
The article presents a computational analysis of Chinese state-controlled media from 1950 to 2019, focusing on how societal beliefs about social groups are reflected in language. The study utilizes diachronic word embeddings to analyze the evolution of representations of social groups in Chinese media. The findings suggest significant differences in Chinese representations compared to Western contexts, particularly in terms of ethnicity, economic status, and gender. The study reveals that stereotypes related to ethnicity, age, and body type remain stable over time, while representations of gender and economic classes undergo dramatic shifts in alignment with historical transformations. This research contributes to our understanding of how language encodes societal beliefs and social structures, emphasizing the importance of non-Western perspectives in computational social science. 

<br /><br />Summary: <div>
arXiv:2504.12327v1 Announce Type: cross 
Abstract: Language encodes societal beliefs about social groups through word patterns. While computational methods like word embeddings enable quantitative analysis of these patterns, studies have primarily examined gradual shifts in Western contexts. We present the first large-scale computational analysis of Chinese state-controlled media (1950-2019) to examine how revolutionary social transformations are reflected in official linguistic representations of social groups. Using diachronic word embeddings at multiple temporal resolutions, we find that Chinese representations differ significantly from Western counterparts, particularly regarding economic status, ethnicity, and gender. These representations show distinct evolutionary dynamics: while stereotypes of ethnicity, age, and body type remain remarkably stable across political upheavals, representations of gender and economic classes undergo dramatic shifts tracking historical transformations. This work advances our understanding of how officially sanctioned discourse encodes social structure through language while highlighting the importance of non-Western perspectives in computational social science.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool</title>
<link>https://arxiv.org/abs/2504.12337</link>
<guid>https://arxiv.org/abs/2504.12337</guid>
<content:encoded><![CDATA[
<div> mental health support, AI chatbots, user engagement, TikTok comments, ethical scrutiny 
Summary: 
This study examines user engagement with large language models (LLMs) like ChatGPT as mental health tools through the analysis of over 10,000 TikTok comments. Approximately 20% of comments reflect personal use, with users reporting positive attitudes towards LLMs for mental health support. Accessibility, emotional support, and perceived therapeutic value are commonly cited benefits. However, concerns around privacy, generic responses, and the lack of professional oversight are prominent. The study highlights the need for clinical and ethical scrutiny in the use of AI for mental health support, as user feedback does not indicate alignment with any therapeutic framework. The findings emphasize the growing relevance of AI in everyday practices while underscoring the importance of addressing privacy, ethical, and clinical considerations when utilizing AI chatbots for mental health support. 
<br /><br />Summary: <div>
arXiv:2504.12337v1 Announce Type: cross 
Abstract: The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</title>
<link>https://arxiv.org/abs/2408.09613</link>
<guid>https://arxiv.org/abs/2408.09613</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, misinformation, Sina Weibo, dataset, information spread

Summary: 
Social bots play a significant role in spreading misinformation on the Sina Weibo platform, as shown in a dataset containing annotations of both misinformation and social bots. Through extensive experiments, it is evident that the dataset is comprehensive, with distinguishable misinformation and real information, and high-quality social bot annotations. The analysis reveals that social bots are actively involved in disseminating information, contributing to echo chambers by amplifying similar content on the same topics, and generating content to manipulate public opinions. This study sheds light on the interplay between social bots and misinformation on social media platforms, highlighting the need for further research and measures to combat the spread of false information. 

<br /><br />Summary: <div>
arXiv:2408.09613v2 Announce Type: replace 
Abstract: The social media platform is an ideal medium to spread misinformation, where social bots might accelerate the spread. This paper is the first to explore the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that contains annotations of misinformation and social bots. From the misinformation perspective, this dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of real information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, where we propose a weak-supervised annotator to annotate automatically. Extensive experiments prove that the dataset is the most comprehensive, misinformation and real information are distinguishable, and social bots have high annotation quality. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governance as a complex, networked, democratic, satisfiability problem</title>
<link>https://arxiv.org/abs/2412.03421</link>
<guid>https://arxiv.org/abs/2412.03421</guid>
<content:encoded><![CDATA[
<div> modeling, governance structures, decision-making, social network, democratic governments

Summary:
The article examines different governance structures within democratic governments through a social network framework. By modeling decision-making as a satisfiability problem and information flow as a social hypergraph, the study explores various governance strategies from dictatorships to direct democracy. The research suggests that effective governance can be achieved through small overlapping decision groups that make specific decisions and share information. This approach allows even polarized populations to make coherent decisions with low coordination costs. The conceptual framework can simulate different governance strategies and their effectiveness in addressing complex societal challenges. This new perspective on governance structures offers insights into improving decision-making processes in democratic societies. 

<br /><br />Summary: <div>
arXiv:2412.03421v2 Announce Type: replace-cross 
Abstract: Democratic governments comprise a subset of a population whose goal is to produce coherent decisions, solving societal challenges while respecting the will of the people. New governance frameworks represent this as a social network rather than as a hierarchical pyramid with centralized authority. But how should this network be structured? We model the decisions a population must make as a satisfiability problem and the structure of information flow involved in decision-making as a social hypergraph. This framework allows to consider different governance structures, from dictatorships to direct democracy. Between these extremes, we find a regime of effective governance where small overlapping decision groups make specific decisions and share information. Effective governance allows even incoherent or polarized populations to make coherent decisions at low coordination costs. Beyond simulations, our conceptual framework can explore a wide range of governance strategies and their ability to tackle decision problems that challenge standard governments.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.00046</link>
<guid>https://arxiv.org/abs/2504.00046</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, disaster response, LLMs, classification techniques, generative AI 

Summary: 
This paper discusses the use of social media in disaster response and management, highlighting the need for automated, aggregated, and customized data analysis to provide actionable insights for various stakeholders. The methodology presented utilizes Language Model-based approaches to analyze social media posts during disasters, focusing on user-reported issues and challenges. By employing full-spectrum Language Models like BERT for precise classification and generative models like ChatGPT for tailored report generation, the system bridges the gap between raw feedback and stakeholder-specific reports. The comparison between standard approaches and the advanced methodology shows superior performance in both quantitative metrics and qualitative assessments. The methodology enhances coordination of relief efforts, resource distribution, and media communication, delivering precise insights for diverse stakeholders involved in disaster response. <div>
arXiv:2504.00046v2 Announce Type: replace-cross 
Abstract: In recent years, social media has emerged as a primary channel for users to promptly share feedback and issues during disasters and emergencies, playing a key role in crisis management. While significant progress has been made in collecting and analyzing social media content, there remains a pressing need to enhance the automation, aggregation, and customization of this data to deliver actionable insights tailored to diverse stakeholders, including the press, police, EMS, and firefighters. This effort is essential for improving the coordination of activities such as relief efforts, resource distribution, and media communication. This paper presents a methodology that leverages the capabilities of LLMs to enhance disaster response and management. Our approach combines classification techniques with generative AI to bridge the gap between raw user feedback and stakeholder-specific reports. Social media posts shared during catastrophic events are analyzed with a focus on user-reported issues, service interruptions, and encountered challenges. We employ full-spectrum LLMs, using analytical models like BERT for precise, multi-dimensional classification of content type, sentiment, emotion, geolocation, and topic. Generative models such as ChatGPT are then used to produce human-readable, informative reports tailored to distinct audiences, synthesizing insights derived from detailed classifications. We compare standard approaches, which analyze posts directly using prompts in ChatGPT, to our advanced method, which incorporates multi-dimensional classification, sub-event selection, and tailored report generation. Our methodology demonstrates superior performance in both quantitative metrics, such as text coherence scores and latent representations, and qualitative assessments by automated tools and field experts, delivering precise insights for diverse disaster response stakeholders.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images</title>
<link>https://arxiv.org/abs/2504.10662</link>
<guid>https://arxiv.org/abs/2504.10662</guid>
<content:encoded><![CDATA[
<div> social media, emotions, Persian community, sentiment analysis, real-world

Summary: 
- An analysis of the emotional expressions in the Persian community on social media platform X compared to real-world experiences was conducted.
- A novel pipeline using Transformers-based text and image sentiment analysis modules was designed to measure emotional similarity between online and offline interactions.
- Results showed a 28.67% similarity between images on social media and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings.
- The study included 105 participants, 393 friends providing insights, over 8,300 collected tweets, and 2,000 media images.
- Statistical analysis confirmed the disparities in sentiment proportions between social media posts and real-world emotions. 

<br /><br />Summary: <div>
arXiv:2504.10662v2 Announce Type: replace-cross 
Abstract: In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Markov stability for community detection at a scale learned based on the structure</title>
<link>https://arxiv.org/abs/2504.11621</link>
<guid>https://arxiv.org/abs/2504.11621</guid>
<content:encoded><![CDATA[
arXiv:2504.11621v1 Announce Type: new 
Abstract: Community detection, the unsupervised task of clustering nodes of a graph, finds applications across various fields. The common approaches for community detection involve optimizing an objective function to partition the nodes into communities at a single scale of granularity. However, the single-scale approaches often fall short of producing partitions that are robust and at a suitable scale. The existing algorithm, PyGenStability, returns multiple robust partitions for a network by optimizing the multi-scale Markov stability function. However, in cases where the suitable scale is not known or assumed by the user, there is no principled method to select a single robust partition at a suitable scale from the multiple partitions that PyGenStability produces. Our proposed method combines the Markov stability framework with a pre-trained machine learning model for scale selection to obtain one robust partition at a scale that is learned based on the graph structure. This automatic scale selection involves using a gradient boosting model pre-trained on hand-crafted and embedding-based network features from a labeled dataset of 10k benchmark networks. This model was trained to predicts the scale value that maximizes the similarity of the output partition to the planted partition of the benchmark network. Combining our scale selection algorithm with the PyGenStability algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale community detection algorithm that returns one robust partition at a suitable scale without the need for any assumptions, input, or tweaking from the user. We compare the performance of PO against 29 algorithms and show that it outperforms 25 other algorithms by statistically meaningful margins. Our results facilitate choosing between community detection algorithms, among which PO stands out as the accurate, robust, and hyperparameter-free method.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technological Complexity Based on Japanese Patent Data</title>
<link>https://arxiv.org/abs/2504.11932</link>
<guid>https://arxiv.org/abs/2504.11932</guid>
<content:encoded><![CDATA[
arXiv:2504.11932v1 Announce Type: new 
Abstract: As international competition intensifies in technologies, nations need to identify key technologies to foster innovation. However, the identification is difficult because a technology is independent, therefore has complex nature. Here, this study aims to assess patent technological fields by applying Technological Complexity Index from a corporate perspective, addressing its underutilization in Japan despite its potential. By utilizing carefully processed patent data from fiscal years 1981 to 2010, we analyze the bipartite network which consists of 1,938 corporations and 35 or 124 technological fields. Our findings provide quantitative characteristics of ubiquity and sophistication for patent fields, the detailed technological trends that reflect the social context, and methodological stability for policymakers and researchers, contributing to targeted innovation strategies in Japan.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2504.11699</link>
<guid>https://arxiv.org/abs/2504.11699</guid>
<content:encoded><![CDATA[
arXiv:2504.11699v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in representation learning, a challenge further amplified in self-supervised settings. We propose H$^3$GNNs, an end-to-end self-supervised learning framework that harmonizes both structural properties through two key innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified space combining linear and non-linear feature projections with K-hop structural representations via a Weighted Graph Convolution Network(WGCN). A cross-attention mechanism enhances awareness and adaptability to heterophily and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a teacher-student model, the student sees the masked input graph and predicts node features inferred by the teacher that sees the full input graph in the joint encoding space. To enhance learning difficulty, we introduce two novel node-predictive-difficulty-based masking strategies. Experiments on seven benchmarks (four heterophily datasets and three homophily datasets) confirm the effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily datasets, while retaining on-par performance to previous state-of-the-art methods on the three homophily datasets.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Patterns of Viral Content on WhatsApp</title>
<link>https://arxiv.org/abs/2407.08172</link>
<guid>https://arxiv.org/abs/2407.08172</guid>
<content:encoded><![CDATA[
arXiv:2407.08172v2 Announce Type: replace 
Abstract: This paper explores the nature and spread of viral WhatsApp content among everyday users in three diverse countries: India, Indonesia, and Colombia. By analyzing hundreds of viral messages collected with participants' consent from private WhatsApp groups, we provide one of the first cross-cultural categorizations of viral content on WhatsApp. Despite the differences in cultural and geographic settings, our findings reveal striking similarities in the types of groups users engage with and the viral content they receive, particularly in the prevalence of misinformation. Our comparative analysis shows that viral content often includes political and religious narratives, with misinformation frequently recirculated despite prior debunking by fact-checking organizations. These parallels suggest that closed messaging platforms like WhatsApp facilitate similar patterns of information dissemination across different cultural contexts. This work contributes to the broader understanding of global digital communication ecosystems and provides a foundation for future research on information flow and moderation strategies in private messaging platforms.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socio-cognitive Networks between Researchers: Investigating Scientific Dualities with the Group-Oriented Relational Hyperevent Model</title>
<link>https://arxiv.org/abs/2407.21067</link>
<guid>https://arxiv.org/abs/2407.21067</guid>
<content:encoded><![CDATA[
arXiv:2407.21067v2 Announce Type: replace 
Abstract: Understanding why researchers cite certain works remains a key question in the study of scientific networks. Prior research has identified factors such as relevance, group cohesion, and source crediting. However, the interplay between cognitive and social dimensions in citation behavior - often conceptualized as a socio-cognitive network - is frequently overlooked, particularly regarding the intermediary steps that lead to a citation. Since a citation first requires a work to be published by a set of authors, we examine how the structure of coauthorship networks influences citation patterns. To investigate this relationship, we analyze the citation and collaboration behavior of Chilean astronomers from 2013 to 2015 using the Group-Oriented Relational Hyperevent Model, which allows us to study coauthorship and citation networks in a joint framework. Our findings suggest that when selecting which works to cite, authors favor recent research and maintain cognitive continuity across cited works. At the same time, we observe that coherent groups - closely connected coauthors - tend to be co-cited more frequently in subsequent publications, reinforcing the interdependence of collaboration and citation networks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining higher-order triadic interactions</title>
<link>https://arxiv.org/abs/2404.14997</link>
<guid>https://arxiv.org/abs/2404.14997</guid>
<content:encoded><![CDATA[
arXiv:2404.14997v2 Announce Type: replace-cross 
Abstract: Complex systems often involve higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have so far been mostly neglected. In this article, we propose a theoretical model that demonstrates that triadic interactions can modulate the mutual information between the dynamical state of two linked nodes. Leveraging this result, we propose the Triadic Interaction Mining (TRIM) algorithm to mine triadic interactions from node metadata, and we apply this framework to gene expression data, finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions that are often ignored, yet can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to the climate.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiReddit: Tracing Information and Attention Flows Between Online Platforms</title>
<link>https://arxiv.org/abs/2502.04942</link>
<guid>https://arxiv.org/abs/2502.04942</guid>
<content:encoded><![CDATA[
arXiv:2502.04942v2 Announce Type: replace-cross 
Abstract: The World Wide Web is a complex interconnected digital ecosystem, where information and attention flow between platforms and communities throughout the globe. These interactions co-construct how we understand the world, reflecting and shaping public discourse. Unfortunately, researchers often struggle to understand how information circulates and evolves across the web because platform-specific data is often siloed and restricted by linguistic barriers. To address this gap, we present a comprehensive, multilingual dataset capturing all Wikipedia mentions and links shared in posts and comments on Reddit 2020-2023, excluding those from private and NSFW subreddits. Each linked Wikipedia article is enriched with revision history, page view data, article ID, redirects, and Wikidata identifiers. Through a research agreement with Reddit, our dataset ensures user privacy while providing a query and ID mechanism that integrates with the Reddit and Wikipedia APIs. This enables extended analyses for researchers studying how information flows across platforms. For example, Reddit discussions use Wikipedia for deliberation and fact-checking which subsequently influences Wikipedia content, by driving traffic to articles or inspiring edits. By analyzing the relationship between information shared and discussed on these platforms, our dataset provides a foundation for examining the interplay between social media discourse and collaborative knowledge consumption and production.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
<link>https://arxiv.org/abs/2502.11299</link>
<guid>https://arxiv.org/abs/2502.11299</guid>
<content:encoded><![CDATA[
arXiv:2502.11299v3 Announce Type: replace-cross 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms. Whereas global platforms can have only a single instance, grassroots platforms can have multiple instances that emerge and operate independently of each other and of any global resource except the network, and can interoperate and coalesce into ever-larger instances once interconnected. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  We enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation; present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations; prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a crisp mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>