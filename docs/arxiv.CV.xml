<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI</title>
<link>https://arxiv.org/abs/2504.12197</link>
<guid>https://arxiv.org/abs/2504.12197</guid>
<content:encoded><![CDATA[
<div> Prototype learning, interpretability, concept mining, robustness, AI alignment  
<br><br>Summary:  
1. The article addresses the challenge of limited interpretability in deep AI models, emphasizing the need for decisions that are understandable and aligned with human expectations.  
2. Existing post-hoc methods like GradCAM offer heatmaps but limited conceptual insight, while prototype-based methods provide example-based explanations but often suffer from rigid region selection and lack semantic consistency.  
3. To overcome these issues, the authors propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without requiring extra supervision.  
4. PCMNet clusters these prototypes into concept groups and extracts concept activation vectors, thereby providing structured, concept-level explanations that enhance interpretability.  
5. The method also improves robustness to occlusion and challenging visual conditions, crucial for reliable AI systems.  
6. Experimental results across multiple image classification benchmarks demonstrate that PCMNet outperforms state-of-the-art techniques in interpretability, stability, and robustness.  
7. This work advances AI alignment by improving transparency, controllability, and trustworthiness in AI models.  
8. The authors have made their code publicly available on GitHub to support further research and application. <div>
arXiv:2504.12197v3 Announce Type: replace 
Abstract: As AI systems grow more capable, it becomes increasingly important that their decisions remain understandable and aligned with human expectations. A key challenge is the limited interpretability of deep models. Post-hoc methods like GradCAM offer heatmaps but provide limited conceptual insight, while prototype-based approaches offer example-based explanations but often rely on rigid region selection and lack semantic consistency.
  To address these limitations, we propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without additional supervision. By clustering these prototypes into concept groups and extracting concept activation vectors, PCMNet provides structured, concept-level explanations and enhances robustness to occlusion and challenging conditions, which are both critical for building reliable and aligned AI systems.
  Experiments across multiple image classification benchmarks show that PCMNet outperforms state-of-the-art methods in interpretability, stability, and robustness. This work contributes to AI alignment by enhancing transparency, controllability, and trustworthiness in AI systems. Our code is available at: https://github.com/alehdaghi/PCMNet.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2510.22946</link>
<guid>https://arxiv.org/abs/2510.22946</guid>
<content:encoded><![CDATA[
<div> Keywords: unified multimodal models, double fusion mechanism, multimodal self-attention, text-to-image generation, image editing<br><br>Summary: This paper addresses the challenge of training unified multimodal models, which typically require extensive computational resources when trained from scratch. The authors propose an efficient approach that strategically fuses publicly available pre-trained models designed specifically for generation or understanding tasks. The core innovation, called the double fusion mechanism, involves retaining the original model blocks while interleaving new multimodal self-attention blocks throughout the network. This design not only preserves the strengths of the individual base models but also enhances multimodal fusion by combining high-level semantic representations from the understanding encoder with low-level spatial information from the generation encoder. The method achieves competitive performance across several benchmarks using only around 35 billion training tokens. Notably, the approach attains a score of 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing tasks. The authors also release all associated code, model weights, and datasets to foster further research on unified multimodal modeling. <div>
arXiv:2510.22946v4 Announce Type: replace 
Abstract: Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment</title>
<link>https://arxiv.org/abs/2511.15831</link>
<guid>https://arxiv.org/abs/2511.15831</guid>
<content:encoded><![CDATA[
<div> Keywords: virtual try-on, multimodal large language model, semantic alignment, progressive training, data scarcity<br /><br />Summary:<br /><br />Image-based virtual try-on (VTON) aims to generate photorealistic images of people wearing designated garments, but creating a universal framework capable of managing diverse and complex scenarios remains challenging. Recent multi-task VTON approaches use textual instructions for guidance but face two main issues: the semantic gap between text and images and the scarcity of data in complicated cases. To solve these problems, the authors propose UniFit, a universal VTON system driven by a Multimodal Large Language Model (MLLM). The core innovation is the MLLM-Guided Semantic Alignment Module (MGSA), which combines multimodal inputs via an MLLM and learnable queries, employing a semantic alignment loss to capture cross-modal semantics and enhance coherence, thereby reducing the semantic gap. UniFit employs a two-stage progressive training strategy coupled with a self-synthesis pipeline, enabling it to effectively learn complex tasks from limited data. Experimental results demonstrate that UniFit supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, while achieving state-of-the-art performance. The authors have released their source code and pretrained models at the provided GitHub repository to facilitate further research and application. <div>
arXiv:2511.15831v1 Announce Type: new 
Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3</title>
<link>https://arxiv.org/abs/2511.15833</link>
<guid>https://arxiv.org/abs/2511.15833</guid>
<content:encoded><![CDATA[
<div> Segment Anything Model 3, Promptable Concept Segmentation, EfficientSAM3, Progressive Hierarchical Distillation, On-device segmentation<br /><br />Summary:<br /><br />The paper introduces EfficientSAM3, a family of efficient models designed to enable on-device visual understanding by distilling the capabilities of the Segment Anything Model 3 (SAM3) into lightweight student networks. The core approach, Progressive Hierarchical Distillation (PHD), transfers knowledge from SAM3 to students in three stages: first, Encoder Distillation aligns image features through prompt-in-the-loop training on the SA-1B dataset; second, Temporal Memory Distillation replaces SAM3’s dense memory with a compact Perceiver-based module trained on SA-V for efficient spatiotemporal feature compression and retrieval; third, End-to-End Fine-Tuning refines the entire model pipeline using official SAM3 PCS data to retain concept-level segmentation accuracy. This methodology enables creating multiple EfficientSAM3 student variants with RepViT, TinyViT, and EfficientViT backbones, striking a balance between performance and computational efficiency suitable for mobile or edge devices. The models are benchmarked on popular video object segmentation (VOS) datasets, demonstrating competitive performance while significantly reducing computational costs compared to the original unified SAM3 architecture. The work offers a practical solution to bring promptable concept segmentation and tracking to resource-constrained environments without substantial loss of fidelity to the original teacher model’s behavior. <div>
arXiv:2511.15833v1 Announce Type: new 
Abstract: The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion</title>
<link>https://arxiv.org/abs/2511.15874</link>
<guid>https://arxiv.org/abs/2511.15874</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, occlusion, dynamic sampling, multi-hypothesis inference, iterative refinement<br /><br />Summary:<br /><br />Accurate 6D object pose estimation is essential for applications such as robotics, augmented reality, and scene understanding. Traditional methods often rely on access to CAD models and a multi-stage pipeline involving detection, initial pose proposal, and refinement, but this approach struggles with occlusion, which causes errors early on that propagate and reduce accuracy. To tackle this, the authors introduce four key improvements to model-based 6D pose estimation techniques: (i) a dynamic non-uniform dense sampling strategy focusing computational resources on visible parts of the object, thereby reducing errors caused by occlusion; (ii) a multi-hypothesis inference mechanism that maintains several confidence-ranked pose candidates, preventing failures from relying on a single estimate; (iii) iterative refinement steps that progressively enhance pose accuracy; and (iv) occlusion-centered data augmentations during training to improve robustness and generalization. Additionally, they propose a new evaluation metric weighted by visibility to better assess performance under occlusion and reduce bias in existing benchmarks. Experimental results demonstrate that their approach significantly boosts accuracy by over 5% on the ICBIN dataset and more than 2% on BOP benchmarks, while also achieving approximately three times faster inference speed compared to previous methods. <div>
arXiv:2511.15874v1 Announce Type: new 
Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation</title>
<link>https://arxiv.org/abs/2511.15875</link>
<guid>https://arxiv.org/abs/2511.15875</guid>
<content:encoded><![CDATA[
<div> Keywords: historical maps, synthetic data, deep learning, cartographic style transfer, semantic segmentation<br /><br />Summary:<br /><br />1. This paper addresses the challenge of automating the analysis of historical maps using deep learning, which typically requires large annotated datasets that are scarce for specific and homogeneous historical map corpora.<br /><br />2. The authors propose generating synthetic historical map data by transferring the cartographic style of an original map corpus onto vector data, thus creating an effectively unlimited dataset that preserves both style and structure.<br /><br />3. Two methods are introduced to simulate the visual uncertainty and noise present in historical map scans: an automatic deep generative approach and a manual stochastic degradation technique, aimed at emulating data-dependent uncertainty.<br /><br />4. To validate the proposed data bootstrapping methods, the generated datasets were employed for domain-adaptive semantic segmentation using a Self-Constructing Graph Convolutional Network, demonstrating the applicability and effectiveness of synthetic data.<br /><br />5. Overall, the work provides a practical solution to overcome data scarcity for deep learning applied to historical maps by synthesizing realistic and diverse training samples, improving the performance of land-cover interpretation tasks within homogeneous historical map corpora. <div>
arXiv:2511.15875v1 Announce Type: new 
Abstract: The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes</title>
<link>https://arxiv.org/abs/2511.15884</link>
<guid>https://arxiv.org/abs/2511.15884</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, warehouse automation, RGB-D, category-level method, Box6D<br /><br />Summary:<br /><br />Accurate and efficient 6D pose estimation of novel objects in cluttered and occluded environments is essential for robotic manipulation tasks in warehouse automation, bin picking, logistics, and e-commerce fulfillment. Existing methods are divided into three groups: model-based approaches that rely on exact CAD models but struggle with environmental transfer; model-free approaches that use few reference images but fail under challenging conditions; and category-level approaches which balance flexibility and accuracy but lack environment and object priors, reducing industrial practicability. This paper introduces Box6D, a category-level 6D pose estimation technique specifically designed for storage boxes in warehouse contexts. Box6D uses a single RGB-D observation to infer box dimensions rapidly via binary search and estimates poses using a generic category CAD template instead of instance-specific models. It incorporates a depth-based plausibility filter and an early-stopping strategy to reject implausible pose hypotheses, significantly reducing computational load. Evaluations on real-world storage settings and public benchmarks demonstrate that Box6D achieves competitive or superior 6D pose accuracy compared to prior methods while reducing inference time by approximately 76%, making it suitable for practical industrial applications. <div>
arXiv:2511.15884v1 Announce Type: new 
Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification</title>
<link>https://arxiv.org/abs/2511.15923</link>
<guid>https://arxiv.org/abs/2511.15923</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, domain-specific video classification, rationale gap, self-generated rationales, fine-tuning  

<br /><br />Summary: Vision Language Models (VLMs) often face challenges in domain-specific video classification, especially when data is limited, due to a rationale gap that hinders effective semantic alignment between spatio-temporal content and abstract labels. To address this, the paper introduces a two-stage self-improvement method that does not require additional annotations. First, VLMs are prompted to generate detailed textual rationales for each video, allowing the model to internally articulate domain-specific reasoning. These self-generated rationales serve as intermediate supervision during a fine-tuning stage, aligning the model’s internal representations with domain nuances. Second, a conventional supervised fine-tuning is applied on task labels, leveraging the enhanced domain reasoning gained from the previous stage. Experimental results across multiple diverse datasets confirm that this approach substantially outperforms standard direct supervised fine-tuning. The paper validates self-generated rationale as an effective and annotation-efficient strategy to adapt VLMs for specialized video analysis tasks, bridging the semantic gap without extra labeled data and improving model performance in low-resource settings. <div>
arXiv:2511.15923v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Medical Visual Understanding From Multi-Granular Language Learning</title>
<link>https://arxiv.org/abs/2511.15943</link>
<guid>https://arxiv.org/abs/2511.15943</guid>
<content:encoded><![CDATA[
<div> Multi-Granular Language Learning, contrastive learning, multi-label alignment, cross-granularity, medical imaging<br /><br />Summary:<br /><br />This paper addresses the limitations of existing Contrastive Language-Image Pretraining (CLIP) methods, which primarily focus on single-label and single-granularity alignment, restricting their performance in complex fields like medical imaging. The authors propose Multi-Granular Language Learning (MGLL), a new contrastive learning framework designed to handle multi-label and cross-granularity alignment effectively. MGLL utilizes structured multi-label supervision, integrating textual descriptions from different annotation granularities, such as diagnostic descriptions and clinical explanations. A novel feature of MGLL is the introduction of soft-label supervision combined with point-wise constraints to improve the alignment between images and textual data. To maintain consistency across different granularities, MGLL employs a smooth Kullback-Leibler (KL) divergence, which also ensures computational efficiency. The framework is designed as a plug-and-play module, making it easily integrable with existing vision-language models. MGLL was pretrained on a large-scale multi-granular dataset constructed by the authors and evaluated across several downstream tasks, where it demonstrated superior performance compared to state-of-the-art methods. The authors have made the code publicly available to encourage further research and development in this area. <div>
arXiv:2511.15943v1 Announce Type: new 
Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Interpretable 2D Video Extraction from 3D Echocardiography</title>
<link>https://arxiv.org/abs/2511.15946</link>
<guid>https://arxiv.org/abs/2511.15946</guid>
<content:encoded><![CDATA[
<div> 3D echocardiography, automated 2D view selection, deep learning classifier, cardiac ultrasound, clinical validation<br /><br />Summary:<br /><br />1. Traditional cardiac ultrasound imaging relies heavily on 2D videos despite the heart's complex three-dimensional structure, limiting comprehensive assessment.<br /><br />2. 3D echocardiography provides higher-quality volumetric images that can improve diagnostic capability and streamline image acquisition, but clinicians are accustomed to interpreting standard 2D views.<br /><br />3. The authors propose an automated method that extracts standard 2D echocardiography views from 3D ultrasound volumes by combining a deep learning-based view classifier with heuristic rules based on anatomical landmarks and expert cardiologist input.<br /><br />4. This methodology was rigorously validated through blinded evaluation by three cardiologists on a dataset of 1,600 videos from two hospitals, achieving 96% accuracy in identifying standard views.<br /><br />5. Further validation demonstrated that the extracted 2D videos preserved crucial spatial and diagnostic features, enabling accurate real-world interpretation and effective use with AI models (EchoPrime, PanEcho) for detecting cardiac abnormalities and clinical measurement tools (EchoNet-Measurement).<br /><br />6. The authors publicly released both their code and a dataset of 29 3D echocardiography videos to promote further research and clinical translation. <div>
arXiv:2511.15946v1 Announce Type: new 
Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click</title>
<link>https://arxiv.org/abs/2511.15948</link>
<guid>https://arxiv.org/abs/2511.15948</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Scene Graph Generation, interactive framework, panoptic segmentation, human guidance, semantic reasoning<br /><br />Summary:<br /><br />This paper introduces Click2Graph, the first interactive framework designed for Panoptic Video Scene Graph Generation (PVSG) that integrates user interaction with spatial, temporal, and semantic reasoning. Unlike existing VSGG systems that operate as closed, feed-forward models without user input, Click2Graph allows precise user prompting, such as a click or bounding box, to guide the segmentation and tracking of subjects across video frames. The framework autonomously identifies interacting objects and predicts triplets to create a temporally consistent scene graph that captures dynamic relationships. Click2Graph features two innovative components: a Dynamic Interaction Discovery Module that generates object prompts conditioned on the selected subject, and a Semantic Classification Head that jointly reasons over entities and their predicates, enabling deeper semantic understanding. Experimental results on the OpenPVSG benchmark demonstrate that this approach effectively combines panoptic grounding, relational inference, and human prompting, establishing a new paradigm for controllable and interpretable video scene understanding. This work contributes a foundation for future research in user-guided scene graph generation in complex video environments by uniting visual prompting with comprehensive semantic and temporal analysis. <div>
arXiv:2511.15948v1 Announce Type: new 
Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts  triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer</title>
<link>https://arxiv.org/abs/2511.15967</link>
<guid>https://arxiv.org/abs/2511.15967</guid>
<content:encoded><![CDATA[
<div> CLIP, Open-vocabulary, Semantic Segmentation, Mutual Information, Fine-tuning  

<br /><br />Summary:  
This paper addresses the challenge of fine-tuning CLIP for open-vocabulary semantic segmentation without losing its strong vision-language alignment. The authors propose InfoCLIP, a novel approach grounded in information theory to stabilize modality alignment during fine-tuning. InfoCLIP introduces two mutual information-based objectives: the first compresses the pixel-text modality alignment from the pretrained CLIP model to reduce noise caused by coarse-grained local semantic features. The second objective maximizes mutual information between the pretrained CLIP's alignment knowledge and the fine-tuned model to facilitate the transfer of compact local semantic relations that better suit segmentation tasks. This method effectively mitigates overfitting on limited categories commonly seen in existing fine-tuning approaches. Extensive experiments on multiple benchmarks demonstrate that InfoCLIP enhances the generalization and adaptability of CLIP for open-vocabulary semantic segmentation. The results highlight InfoCLIP's superiority in asymmetric knowledge transfer and its ability to preserve the pretrained vision-language relationships while improving segmentation accuracy across unseen categories. <div>
arXiv:2511.15967v1 Announce Type: new 
Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation</title>
<link>https://arxiv.org/abs/2511.15968</link>
<guid>https://arxiv.org/abs/2511.15968</guid>
<content:encoded><![CDATA[
<div> Multi-task learning, breast ultrasound, tumor segmentation, consistency regularization, BI-RADS features<br /><br />Summary:  
This study addresses the challenge of destructive task interference in multi-task learning, where models trained to perform multiple tasks simultaneously often underperform compared to single-task baselines and exhibit limited generalization. To overcome this issue in the context of breast ultrasound-based tumor segmentation, the authors propose a novel consistency regularization approach. This method integrates differentiable BI-RADS-inspired morphological features to reduce interference between segmentation and classification tasks during training. The approach was developed and validated using the BrEaST dataset from Poland and tested for generalization on three external datasets from Spain and Egypt: UDIAT, BUSI, and BUS-UCLM. The results show statistically significant improvements (p < 0.001) in segmentation accuracy as measured by the Dice coefficient, with scores increasing from 0.59 to 0.81 on UDIAT, 0.56 to 0.66 on BUSI, and 0.49 to 0.69 on BUS-UCLM when comparing the proposed multi-task method to the baseline. Furthermore, the proposed approach achieves state-of-the-art segmentation performance on the rigorously validated UDIAT external dataset. These findings highlight the effectiveness of consistency regularization using morphological features in improving multi-task learning for medical image analysis. <div>
arXiv:2511.15968v1 Announce Type: new 
Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition</title>
<link>https://arxiv.org/abs/2511.15984</link>
<guid>https://arxiv.org/abs/2511.15984</guid>
<content:encoded><![CDATA[
<div> Keywords: visual semantic understanding, detection-guided generative framework, hierarchical category, attribute recognition, e-commerce datasets<br /><br />Summary:<br /><br />This paper addresses the challenge of achieving visual semantic understanding by proposing a unified framework that integrates object detection, category prediction, and attribute recognition. Unlike existing methods that depend on global similarity measures and often fail to differentiate fine-grained categories and diverse attributes, especially in large-scale e-commerce contexts, the authors introduce a novel detection-guided generative framework. The approach extracts refined region of interest (ROI)-level features for each detected object and leverages a BART-based generative model to sequentially produce semantic tokens. These tokens represent coarse-to-fine hierarchical categories and detailed property-value attribute pairs, enabling property-conditioned attribute recognition. The framework is designed to capture subtle distinctions and variations within categories more effectively than traditional similarity-based or multi-stage classification pipelines. Extensive experiments on proprietary large-scale e-commerce datasets, as well as open-source datasets, validate the superiority of this method. Results show significant improvements in fine-grained recognition accuracy and more coherent unified inference compared to existing solutions. This work contributes a powerful model that advances the state-of-the-art in combined object detection and semantic attribute prediction with broad applicability in e-commerce visual understanding. <div>
arXiv:2511.15984v1 Announce Type: new 
Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection</title>
<link>https://arxiv.org/abs/2511.15986</link>
<guid>https://arxiv.org/abs/2511.15986</guid>
<content:encoded><![CDATA[
<div> Medical imaging, fairness, multimodal large language models, in-context learning, demonstration selection<br /><br />Summary:<br /><br />1. Multimodal large language models (MLLMs) have demonstrated strong potential in medical image reasoning but face significant challenges related to fairness across diverse demographic groups.  
2. Existing debiasing methods often require large labeled datasets or extensive fine-tuning, which are impractical for foundation-scale models due to resource constraints.  
3. The authors propose leveraging In-Context Learning (ICL) as a lightweight and tuning-free alternative to improve fairness in MLLMs.  
4. Conventional demonstration selection (DS) strategies inadequately address fairness because they select exemplars that are demographically imbalanced, leading to biased outcomes.  
5. To counter this, the study introduces Fairness-Aware Demonstration Selection (FADS), a clustering-based sampling method that ensures demonstrations are both demographically balanced and semantically relevant.  
6. Experimental evaluation on various medical imaging benchmarks shows FADS consistently reduces disparities related to gender, race, and ethnicity while maintaining high accuracy in medical image reasoning tasks.  
7. The results highlight the potential of fairness-aware in-context learning as an efficient, scalable, and data-efficient approach to achieving equitable medical image reasoning in foundation-scale models. <div>
arXiv:2511.15986v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.16015</link>
<guid>https://arxiv.org/abs/2511.16015</guid>
<content:encoded><![CDATA[
<div> out-of-distribution detection, long-tailed datasets, graph convolutional networks, Gaussianization, vision benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting out-of-distribution (OOD) data in deep neural networks, especially within the context of long-tailed in-distribution (ID) datasets that cause high false positive rates and poor accuracy for tail classes.<br /><br />2. It proposes leveraging inter-sample relationships through a graph-based representation, constructed in the feature space of a pre-trained model, to enhance OOD detection performance.<br /><br />3. The method incorporates Gaussianization to adjust for distribution differences between the activation layers of the pre-trained model and the training data, aligning activations closer to a standard normal distribution.<br /><br />4. The initial graph structure is refined using graph convolutional networks (GCNs), resulting in a feature space that is more effective for OOD detection in long-tailed scenarios.<br /><br />5. Experimental results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks demonstrate significant improvements over state-of-the-art methods, notably reducing false positive rates and enhancing classification accuracy for tail classes. <div>
arXiv:2511.16015v1 Announce Type: new 
Abstract: Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial textures, human detection, sequence-level optimization, physical garments, cross-model transferability<br /><br />Summary:<br />1. The paper addresses the vulnerability of deep neural networks for human detection to adversarial attacks in real-world surveillance settings.<br />2. It introduces a sequence-level optimization framework to generate natural, printable adversarial textures for clothing items such as shirts, trousers, and hats.<br />3. The approach maps product images to UV space, converts them into a compact palette with control-point parameterization, and uses ICC locking to ensure printable colors.<br />4. A physically based human-garment simulation pipeline models motion, multi-angle camera views, cloth dynamics, and changing illumination.<br />5. The method uses an expectation-over-transformation objective with temporal weighting to minimize detection confidence across entire walking video sequences.<br />6. Experimental results demonstrate strong, stable concealment with robustness to viewpoint changes and effective transferability across different models.<br />7. Physical garments created through sublimation printing reliably suppress detection both indoors and outdoors, confirming the practical feasibility of the approach. <div>
arXiv:2511.16020v1 Announce Type: new 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16024</link>
<guid>https://arxiv.org/abs/2511.16024</guid>
<content:encoded><![CDATA[
<div> MoE, Real-world Image Super-Resolution, Mixture-of-Ranks, LoRA, Degradation Estimation<br /><br />Summary:  
This paper explores the application of sparsely-gated Mixture-of-Experts (MoE) architectures to real-world image super-resolution (Real-ISR), addressing the limitations of existing dense models. The authors propose a novel Mixture-of-Ranks (MoR) framework that treats each rank in Low-Rank Adaptation (LoRA) as an independent expert, enabling fine-grained knowledge recombination while preserving shared common-sense features. To dynamically guide expert activation depending on image degradation, a degradation estimation module is introduced, which utilizes CLIP embeddings combined with predefined positive-negative text pairs to assess relative degradation severity. The framework also incorporates zero-expert slots and a degradation-aware load-balancing loss, allowing adaptive allocation of computational resources by adjusting the number of active experts based on the complexity of degraded inputs. This approach enhances flexibility in capturing heterogeneous and complex real-world degradations and facilitates more efficient knowledge sharing under fixed computational budgets. Comprehensive experiments demonstrate the proposed MoR architecture achieves state-of-the-art performance in single-step Real-ISR tasks, validating its effectiveness and robustness compared to prior dense and fine-tuned diffusion-based models. <div>
arXiv:2511.16024v1 Announce Type: new 
Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning</title>
<link>https://arxiv.org/abs/2511.16026</link>
<guid>https://arxiv.org/abs/2511.16026</guid>
<content:encoded><![CDATA[
<div> Keywords: laser cutting, speckle sensing, material classification, deep learning, convolutional neural network  

<br /><br />Summary:  
This paper addresses the challenge of monitoring and controlling the laser cutting process by proposing a material classification technique based on speckle pattern analysis combined with deep learning. Laser cutting, widely used in various industries, produces dust and aerosols that pose environmental and health risks, making precise process control crucial. The proposed method leverages a convolutional neural network (CNN) trained on laser speckle pattern images to identify different material types in real-time, enhancing cutting safety and efficiency. Unlike previous methods that struggled with variations in laser color, this model achieves high classification accuracy even when the laser wavelength changes. Experimental results show the model reaches 98.30% accuracy on the training data and 96.88% on validation data. Additionally, when tested on a large, diverse dataset of 3000 new images across 30 material types, the model attains a strong F1-score of 0.9643. These results demonstrate the robustness and effectiveness of the technique, offering a reliable solution for material-aware laser cutting applications that can adapt to different operational conditions while maintaining high performance. <div>
arXiv:2511.16026v1 Announce Type: new 
Abstract: Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis</title>
<link>https://arxiv.org/abs/2511.16030</link>
<guid>https://arxiv.org/abs/2511.16030</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, sparse-view reconstruction, curriculum learning, student views, multi-signal metric<br /><br />Summary:<br /><br />1. The paper addresses the challenge of 3D scene reconstruction and rendering using 3D Gaussian Splatting (3DGS) in sparse-view settings, where limited viewpoint coverage causes supervision scarcity and overfitting.  
2. It proposes CuriGS, a curriculum-guided framework designed to improve sparse-view 3D reconstruction leveraging 3DGS by introducing the concept of student views—pseudo-views sampled near ground-truth (teacher) poses.  
3. Multiple groups of student views with varying levels of perturbation are generated for each teacher pose, and a curriculum schedule progressively unlocks higher perturbation levels during training to enhance learning stability.  
4. Each student view undergoes regularization based on depth correlation and co-regularization and is evaluated using a combined multi-signal metric integrating SSIM, LPIPS, and an image-quality measure to ensure training robustness and quality.  
5. The best-performing students at each perturbation level are periodically retained, and those meeting a quality threshold are promoted into the training set, resulting in stable augmentation and improved sparse training views, which leads to superior rendering fidelity and geometric consistency compared to state-of-the-art baselines on synthetic and real datasets. <div>
arXiv:2511.16030v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossmodal learning for Crop Canopy Trait Estimation</title>
<link>https://arxiv.org/abs/2511.16031</link>
<guid>https://arxiv.org/abs/2511.16031</guid>
<content:encoded><![CDATA[
<div> Keywords: plant phenotyping, UAV, satellite imagery, cross-modal learning, crop canopy trait estimation<br /><br />Summary:  
Recent advances in plant phenotyping have promoted the use of multi-sensor platforms for collecting crop canopy reflectance data, particularly employing UAVs due to their high resolution and effectiveness in crop monitoring and prediction. Satellite imagery, while useful for agricultural tasks, suffers from limited spatial resolution, which restricts its applicability in modern micro-plot management in farming systems. This study introduces a cross-modal learning approach designed to enhance low-resolution satellite images by infusing them with the detailed visual information typically available from UAV imagery. The research is based on a dataset of approximately co-registered satellite and UAV image pairs from replicated plots of 84 hybrid maize varieties collected across five different locations in the U.S. Corn Belt. The model learns fine-grained spatial and spectral correspondences between the two sensing modalities. Experimental results demonstrate that this method can generate UAV-like representations from satellite inputs that consistently outperform original satellite images in downstream tasks, including yield and nitrogen content prediction. This work highlights the potential of leveraging cross-modal correspondence learning to bridge the resolution gap between satellites and UAVs, thereby advancing agricultural monitoring accuracy and utility. <div>
arXiv:2511.16031v1 Announce Type: new 
Abstract: Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets</title>
<link>https://arxiv.org/abs/2511.16037</link>
<guid>https://arxiv.org/abs/2511.16037</guid>
<content:encoded><![CDATA[
<div> keywords: food recognition, domain-shift, long-tailed distribution, large language models, fine-grained classification<br /><br />Summary: Training models for food recognition faces significant challenges due to domain-shift problems where internet-crawled images differ visually from user-captured photos in real-life settings. Additionally, real-world food datasets often have a long-tailed distribution, meaning some categories have very few samples, and many dishes are visually similar, making fine-grained classification difficult. This paper proposes a novel framework that leverages large language models (LLMs) to tackle these issues. First, LLMs are utilized to analyze and interpret food images to generate descriptive food titles and ingredient lists. Second, the generated textual information and food images from multiple domains are projected into a shared embedding space, where their similarity is maximized. Finally, the model uses these aligned multimodal features to perform the recognition task. This approach unifies the benefits of multimodal representation learning to address domain adaptation, data imbalance, and subtle inter-class variations simultaneously. Experimental results demonstrate that this simple yet effective framework outperforms existing state-of-the-art methods designed specifically for long-tailed distributions, domain adaptation, and fine-grained food classification on two benchmark food datasets. This indicates the powerful role of LLMs and multimodal alignment in enhancing real-world food recognition systems. <div>
arXiv:2511.16037v1 Announce Type: new 
Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers</title>
<link>https://arxiv.org/abs/2511.16047</link>
<guid>https://arxiv.org/abs/2511.16047</guid>
<content:encoded><![CDATA[
<div> Visual autoregressive modeling, next-scale prediction, KV caching, multi-scale image generation, cache optimization  

<br /><br />Summary:  
This paper addresses the challenges of Key and Value (KV) caching in Visual Autoregressive (VAR) models using next-scale prediction for image generation. First, the authors identify that attending to tokens from local scales is crucial for maintaining high generation quality. Second, they observe that allocating limited KV memory to the coarsest scales, called condensed scales, helps stabilize multi-scale image generation. Third, they distinguish two types of layers in VAR transformers: cache-efficient layers, which show strong KV similarity across finer scales, and cache-demanding layers that exhibit weaker inter-scale similarity, leading to higher cache requirements. Based on these insights, the authors propose AMS-KV, a scale-adaptive KV caching policy prioritizing the storage of KVs from condensed and local scales to preserve relevant tokens. AMS-KV further improves cache utilization and computational efficiency by identifying cache-demanding layers through inter-scale similarity analysis. Experimental results show that AMS-KV reduces KV cache usage by up to 84.83% and decreases self-attention latency by 60.48% compared to baseline next-scale VAR models. Moreover, this method enables stable scaling to batch sizes of 256, doubling the batch size previously limited to 128 due to out-of-memory errors, and also improves throughput, demonstrating significant scalability and efficiency benefits for VAR-based image generation. <div>
arXiv:2511.16047v1 Announce Type: new 
Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.16049</link>
<guid>https://arxiv.org/abs/2511.16049</guid>
<content:encoded><![CDATA[
<div> 4D LiDAR, generative model, Hybrid-Cylindrical-Spherical representation, Spatio-Temporal Attention, controllable synthesis  

<br /><br />Summary: Synthesizing high-fidelity and controllable 4D LiDAR data is essential for scalable simulation environments in autonomous driving. The process is challenging due to the unique spherical geometry of sensors, temporal sparsity of point clouds, and dynamic scene complexity. The authors propose LiSTAR, a generative world model that operates on the sensor’s native geometry to improve data fidelity. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to reduce quantization artifacts commonly seen with Cartesian grids. To capture complex temporal dynamics from sparse data, it employs a Spatio-Temporal Attention with a Ray-Centric Transformer (START), which models feature evolution along sensor rays for improved temporal coherence. For controllable data synthesis, the method includes a novel 4D point cloud-aligned voxel layout for conditioning and a discrete Masked Generative START (MaskSTART) framework that creates a compact tokenized scene representation, enabling efficient, high-resolution, and layout-guided compositional generation. Extensive experiments demonstrate that LiSTAR achieves state-of-the-art results in 4D LiDAR reconstruction, prediction, and conditional generation, showing significant performance improvements such as a 76% reduction in generation MMD, a 32% increase in reconstruction IoU, and a 50% decrease in prediction L1 Median error. These advances establish a strong foundation for realistic and controllable simulations in autonomous systems. <div>
arXiv:2511.16049v1 Announce Type: new 
Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.16077</link>
<guid>https://arxiv.org/abs/2511.16077</guid>
<content:encoded><![CDATA[
<div> Keywords: video reasoning segmentation, reinforcement learning, frame sampler, reasoning chains, mask propagation<br /><br />Summary:<br /><br />The article introduces VideoSeg-R1, a novel framework that applies reinforcement learning to video reasoning segmentation, a task traditionally reliant on supervised fine-tuning with limited generalization capabilities. The framework features a decoupled architecture that tackles the problem by combining referring image segmentation with video mask propagation. VideoSeg-R1 operates in three distinct stages: first, a hierarchical text-guided frame sampler mimics human attention to select frames intelligently; second, a reasoning model generates spatial cues alongside explicit reasoning chains to facilitate interpretable and structured analysis; third, a segmentation-propagation module utilizes advanced tools SAM2 and XMem to extend segmentation masks across video frames. Additionally, the system incorporates a task difficulty-aware mechanism that dynamically adjusts the reasoning length, optimizing both efficiency and segmentation accuracy. Extensive experiments on various benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation settings. The proposed approach enhances generalization to out-of-distribution scenarios, offers explicit reasoning for better interpretability, and accelerates processing through adaptive control. The authors plan to release the code publicly to support further research and development in this field. <div>
arXiv:2511.16077v1 Announce Type: new 
Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectralTrain: A Universal Framework for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.16084</link>
<guid>https://arxiv.org/abs/2511.16084</guid>
<content:encoded><![CDATA[
<div> hyperspectral image classification, curriculum learning, PCA spectral downsampling, training efficiency, remote sensing<br /><br />Summary:<br /><br />1. This study addresses the computational challenges in hyperspectral image (HSI) classification by proposing SpectralTrain, a universal and architecture-agnostic training framework.<br /><br />2. SpectralTrain integrates curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling to gradually introduce spectral complexity while retaining crucial information.<br /><br />3. The approach improves learning efficiency by enabling models to capture spectral-spatial patterns with significantly reduced computational cost, achieving 2-7x faster training times.<br /><br />4. The framework is compatible with various architectures, optimizers, and loss functions, demonstrating flexibility across classical and state-of-the-art deep learning models.<br /><br />5. Extensive experiments on Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 datasets reveal strong generalization across different spatial scales, spectral properties, and application domains.<br /><br />6. Application to cloud classification highlights the framework’s potential for climate-related remote sensing tasks.<br /><br />7. SpectralTrain emphasizes training strategy optimization as a powerful complement to architectural design in improving HSI model performance.<br /><br />8. The authors provide open-source code to facilitate adoption and further research at https://github.com/mh-zhou/SpectralTrain. <div>
arXiv:2511.16084v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</title>
<link>https://arxiv.org/abs/2511.16091</link>
<guid>https://arxiv.org/abs/2511.16091</guid>
<content:encoded><![CDATA[
<div> radar SLAM, 3D Gaussian, dynamic object masking, large-scale mapping, outdoor reconstruction<br /><br />Summary:<br /><br />Rad-GS is a novel 4D radar-camera SLAM system tailored for kilometer-scale outdoor environments, which leverages 3D Gaussian functions as a differentiable spatial representation. It integrates raw radar point clouds enriched with Doppler information and geometrically enhanced point clouds to improve dynamic object masking in synchronized images, reducing rendering artifacts and increasing localization accuracy. The system also utilizes unsynchronized image frames for global refinement of the 3D Gaussian model, enhancing both texture consistency and novel view synthesis quality. To efficiently manage large-scale environments, Rad-GS employs a global octree data structure combined with a strategic approach to Gaussian primitive management, significantly suppressing noise while minimizing memory consumption. Extensive experimental evaluations and ablation studies show that Rad-GS performs on par with traditional 3D Gaussian methods based on camera or LiDAR data, establishing its robustness and reliability for outdoor mapping. Finally, real-world reconstructions at kilometer scales validate Rad-GS’s capability for detailed and scalable large-scale scene reconstruction using 4D mmWave radar, highlighting its potential as a practical solution for robust outdoor SLAM applications. <div>
arXiv:2511.16091v1 Announce Type: new 
Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs</title>
<link>https://arxiv.org/abs/2511.16107</link>
<guid>https://arxiv.org/abs/2511.16107</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, visual in-context learning, vision-language models, cross-task learning, prompt generation<br /><br />Summary: In this paper, the authors explore the capabilities of vision-language models (VLMs) to perform visual in-context learning (VICL) when the visual prompt and target images come from different tasks, a challenge known as cross-task VICL. They introduce T2T-VICL, a fully collaborative pipeline designed to investigate and enable cross-task VICL in unified VLMs. A key contribution is their mechanism to automatically generate and select text prompts that effectively describe the differences between two distinct low-level vision tasks, enabling better contextual understanding. Additionally, they create the first cross-task VICL dataset to benchmark and study these scenarios systematically. To enhance inference, the authors propose a novel framework that merges perceptual score-based reasoning with traditional evaluation metrics to achieve more reliable cross-task VICL performance. The approach demonstrates strong empirical results, achieving top-tier performance in nine cross-task scenarios and second-best results in ten others. This work expands the boundaries of VICL in VLMs by showing that these models can generalize and adapt across different visual tasks with appropriate prompting and evaluation strategies. <div>
arXiv:2511.16107v1 Announce Type: new 
Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustered Error Correction with Grouped 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.16112</link>
<guid>https://arxiv.org/abs/2511.16112</guid>
<content:encoded><![CDATA[
<div> 4D Gaussian Splatting, dynamic scene reconstruction, error correction, temporal consistency, perceptual rendering  

<br /><br />Summary:  
The paper addresses the challenges faced by existing 4D Gaussian Splatting (4DGS) methods in accurately reconstructing dynamic scenes, particularly issues with ambiguous pixel correspondences and insufficient densification in dynamic regions. The authors propose a novel method consisting of two main components: (1) Elliptical Error Clustering and Error Correcting Splat Addition, which identifies dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting, which enhances the consistency of mapping between splats and dynamic objects. The method classifies rendering errors into two types — missing-color and occlusion errors — and applies specific corrections such as backprojection or foreground splitting, guided by cross-view color consistency to ensure accuracy. Experimental evaluations on Neural 3D Video and Technicolor datasets reveal significant improvements in temporal consistency and perceptual rendering quality, achieving a 0.39 dB PSNR increase on the Technicolor Light Field dataset. Visualization results demonstrate better alignment of splats with dynamic objects and the effectiveness of the error correction techniques in detecting errors and initializing new splats properly. The authors also provide their implementation details and source code publicly via GitHub for reproducibility and further research. <div>
arXiv:2511.16112v1 Announce Type: new 
Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Complexity from Scale in Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2511.16117</link>
<guid>https://arxiv.org/abs/2511.16117</guid>
<content:encoded><![CDATA[
<div> Keywords: latent diffusion models, scale-independent latent space, hierarchical tokens, visual generation, coarse-to-fine generation<br /><br />Summary:<br /><br />Existing latent diffusion models typically link the scale of the data (image resolution or video frame rate) with content complexity by increasing latent tokens accordingly. However, this approach overlooks that the true latent capacity needed depends primarily on content complexity rather than scale, which merely sets an upper bound. Motivated by this insight, the paper introduces DCS-LDM, a novel visual generation framework that decouples information complexity from scale. DCS-LDM builds a hierarchical, scale-independent latent space using multi-level tokens that effectively model the complexity of samples. This approach enables decoding into arbitrary resolutions and frame rates without changing the underlying fixed latent representation. As a result, DCS-LDM offers a more flexible computation-quality tradeoff compared to traditional models. Additionally, the method separates structural and detailed visual information across different hierarchy levels, enabling a progressive coarse-to-fine generation paradigm. Experiments demonstrate that DCS-LDM achieves performance on par with state-of-the-art latent diffusion models. Importantly, it supports flexible generation across diverse scales and visual qualities, making it a versatile tool for visual synthesis tasks where varying output resolutions and frame rates are needed. <div>
arXiv:2511.16117v1 Announce Type: new 
Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTinker: Guided Flow Upsampling and Texture Mapping for High-Resolution Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2511.16124</link>
<guid>https://arxiv.org/abs/2511.16124</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Frame Interpolation, guided flow upsampling, texture mapping, motion estimation, high-resolution frames  

<br /><br />Summary:  
1. Estimating motion in high-resolution video frames is challenging due to large pixel movements and high computational costs.  
2. Conventional flow-based Video Frame Interpolation (VFI) methods typically predict bidirectional optical flows at low resolution and then upsample them (e.g., via bilinear interpolation) to the high-resolution scale, which often leads to blurred or mosaic-like artifacts around the flow edges.  
3. Low-resolution motion estimation fails to capture the fine pixel motions accurately at high resolutions, causing misaligned flows and resulting in ghosting and discontinuities in the interpolated frames.  
4. The proposed VTinker framework introduces two key components: Guided Flow Upsampling (GFU), which uses input frames as guidance to sharpen flow edges and reduce blurring during upsampling, and Texture Mapping, which creates an intermediate proxy frame to enable selecting and mapping clear texture blocks from input frames, thereby minimizing pixel-level artifacts.  
5. VTinker’s novel approach significantly improves the quality of interpolated video frames, achieving state-of-the-art performance according to extensive experimental results. The implementation codes are publicly available on GitHub. <div>
arXiv:2511.16124v1 Announce Type: new 
Abstract: Due to large pixel movement and high computational cost, estimating the motion of high-resolution frames is challenging. Thus, most flow-based Video Frame Interpolation (VFI) methods first predict bidirectional flows at low resolution and then use high-magnification upsampling (e.g., bilinear) to obtain the high-resolution ones. However, this kind of upsampling strategy may cause blur or mosaic at the flows' edges. Additionally, the motion of fine pixels at high resolution cannot be adequately captured in motion estimation at low resolution, which leads to the misalignment of task-oriented flows. With such inaccurate flows, input frames are warped and combined pixel-by-pixel, resulting in ghosting and discontinuities in the interpolated frame. In this study, we propose a novel VFI pipeline, VTinker, which consists of two core components: guided flow upsampling (GFU) and Texture Mapping. After motion estimation at low resolution, GFU introduces input frames as guidance to alleviate the blurring details in bilinear upsampling flows, which makes flows' edges clearer. Subsequently, to avoid pixel-level ghosting and discontinuities, Texture Mapping generates an initial interpolated frame, referred to as the intermediate proxy. The proxy serves as a cue for selecting clear texture blocks from the input frames, which are then mapped onto the proxy to facilitate producing the final interpolated frame via a reconstruction module. Extensive experiments demonstrate that VTinker achieves state-of-the-art performance in VFI. Codes are available at: https://github.com/Wucy0519/VTinker.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Noise Benefits AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.16136</link>
<guid>https://arxiv.org/abs/2511.16136</guid>
<content:encoded><![CDATA[
<div> generative models, AI-generated image detection, out-of-distribution generalization, positive-incentive noise, CLIP<br /><br />Summary: The paper addresses the challenge of detecting AI-generated images, especially under out-of-distribution scenarios where current detection methods struggle due to reliance on spurious shortcuts during training. It identifies that small perturbations in the feature space can reduce the dominance of these shortcuts and improve generalization. To leverage this insight, the authors propose PiN-CLIP, a novel approach combining a noise generator and a detection network trained jointly under a variational positive-incentive principle. This principle guides the creation of positive-incentive noise through cross-attention fusion of visual features and categorical semantic information, injecting this noise into the feature space to fine-tune the visual encoder. The tuning process suppresses shortcut-sensitive directions while enhancing stable forensic cues, allowing for more robust and generalized extraction of artifact representations indicative of synthetic images. The effectiveness of PiN-CLIP is demonstrated on a diverse open-world dataset containing synthetic images produced by 42 different generative models, showing a substantial improvement in detection accuracy with a 5.4% increase over previous state-of-the-art methods. This advancement highlights the method's potential to improve the reliability of AI-generated image detection in real-world applications. <div>
arXiv:2511.16136v1 Announce Type: new 
Abstract: The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video</title>
<link>https://arxiv.org/abs/2511.16137</link>
<guid>https://arxiv.org/abs/2511.16137</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality Enhancement, Compressed Video, Blind Methods, Degradation Representation, Hierarchical Termination<br /><br />Summary:<br /><br />This paper addresses the challenge of Quality Enhancement for Compressed Video (QECV) where existing methods mostly rely on known Quantization Parameters (QPs) using separate models for each QP, classified as non-blind methods. In real-world applications, QPs may be unknown, motivating the need for blind QECV approaches. Current blind methods generate degradation vectors through classification with cross-entropy loss, serving as channel attention for artifact removal; however, these vectors only capture global degradation and lack spatial detail, limiting their effectiveness across varying artifact patterns. To overcome this, the authors propose a pretrained Degradation Representation Learning (DRL) module that extracts high-dimensional, multiscale degradation features from video content to better guide artifact removal. Additionally, recognizing that both blind and non-blind methods use uniform architectures regardless of QP, they introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on compression level, aligning computational effort with the degree of degradation. Experiments demonstrate significant performance improvements, with PSNR gains of 110% (from 0.31 dB to 0.65 dB) over state-of-the-art blind methods at QP = 22 and a halving of average inference time at QP = 22 compared to QP = 42, indicating both enhanced quality and efficiency. <div>
arXiv:2511.16137v1 Announce Type: new 
Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time 3D Object Detection with Inference-Aligned Learning</title>
<link>https://arxiv.org/abs/2511.16140</link>
<guid>https://arxiv.org/abs/2511.16140</guid>
<content:encoded><![CDATA[
<div> Spatial-prioritized, Rank-aware, 3D object detection, Point clouds, Real-time  

<br /><br />Summary: Real-time 3D object detection from point clouds is critical for dynamic scene understanding in applications such as augmented reality, robotics, and navigation. This work presents a novel framework called SR3D (Spatial-prioritized and Rank-aware 3D object detection) designed specifically for indoor point cloud data. The main motivation is to address the training-inference gap caused by inconsistencies between spatial reliability and ranking awareness during training versus the ranking-based prediction selection used during inference. SR3D introduces two key components: a spatial-prioritized optimal transport assignment that emphasizes well-located and spatially reliable samples dynamically during training, and a rank-aware adaptive self-distillation scheme that incorporates ranking perception into the model via a self-distillation paradigm. These components are tailored to the spatial characteristics of point cloud data, helping the model to better align training procedures with inference-time behavior. Extensive experiments conducted on benchmark datasets ScanNet V2 and SUN RGB-D demonstrate that SR3D successfully bridges the training-inference gap, significantly improving detection accuracy without sacrificing real-time performance. Overall, this framework advances the state-of-the-art in 3D object detection for indoor point clouds by simultaneously enhancing spatial reliability and ranking awareness during training. <div>
arXiv:2511.16140v1 Announce Type: new 
Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection</title>
<link>https://arxiv.org/abs/2511.16143</link>
<guid>https://arxiv.org/abs/2511.16143</guid>
<content:encoded><![CDATA[
<div> Keywords: Water Body Change Detection, High Spatial Resolution, Deep Learning, Spatial Semantics, Attention Module<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting changes in water bodies from bi-temporal remote sensing images, highlighting the limitation of existing datasets with insufficient spatial resolution for accurate urban and rural analysis. To overcome this, the authors introduce a new dataset named HSRW-CD, featuring image pairs with spatial resolution higher than 3 meters, encompassing diverse types of water bodies. A novel Spatial Semantics and Continuity Perception (SSCP) attention module is proposed to improve deep learning models by better exploiting spatial semantic and structural features in change detection networks. The SSCP module consists of three components: Multi-Semantic spatial Attention (MSA), which enhances spatial semantic features and provides priors for channel attention; Structural Relation-aware Global Attention (SRGA), which captures spatial continuity of water bodies; and Channel-wise Self-Attention (CSA), which computes similarity across channels by leveraging the priors from MSA and SRGA. Designed as a plug-and-play module, SSCP can be integrated into existing WBCD models. Extensive experiments on the new HSRW-CD dataset and the Water-CD dataset validate that SSCP improves discrimination of water bodies and generalizes well. The dataset and code are publicly available at the provided GitHub repository. <div>
arXiv:2511.16143v1 Announce Type: new 
Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM</title>
<link>https://arxiv.org/abs/2511.16144</link>
<guid>https://arxiv.org/abs/2511.16144</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, SLAM, Language Embeddings, Real-time Mapping, Semantic Pruning  

<br /><br />Summary:  
This paper presents LEGO-SLAM, a novel framework integrating language understanding with 3D Gaussian Splatting (3DGS) for Simultaneous Localization and Mapping (SLAM). Current SLAM systems, despite generating photorealistic maps, lack open-vocabulary semantic understanding, limiting robotic interaction capabilities. The challenge lies in incorporating high-dimensional language features without incurring excessive memory usage and rendering overhead. LEGO-SLAM addresses this by employing a scene-adaptive encoder-decoder that compresses high-dimensional language embeddings into a 16-dimensional feature space, reducing memory requirements and improving rendering speed for real-time application. Unlike static models, LEGO-SLAM's encoder adjusts dynamically to new environments, enhancing adaptability. Additionally, the compact language features enable a pruning strategy that eliminates semantic redundancies, cutting the number of map Gaussians by over 60% without degrading visual quality. The framework also introduces a language-based loop detection method that leverages these compact features, removing the need for separate detection models. Experimental results validate that LEGO-SLAM maintains competitive mapping and tracking performance while providing open-vocabulary semantic capabilities at 15 frames per second, marking a significant advance for real-time, semantically-aware SLAM systems. <div>
arXiv:2511.16144v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2511.16150</link>
<guid>https://arxiv.org/abs/2511.16150</guid>
<content:encoded><![CDATA[
<div> Multimodal embeddings, Multimodal Large Language Models, Reasoning Guided Embeddings, contrastive training, multimodal retrieval<br /><br />Summary:<br /><br />This paper addresses the limitation in current methods for extracting multimodal embeddings from Multimodal Large Language Models (MLLMs), which typically treat embedding extraction as a direct encoding task without leveraging the models' generative reasoning capabilities. The authors propose a novel approach called Reasoning Guided Embeddings (RGE), which explicitly incorporates the generative reasoning process into embedding extraction. RGE works by conditioning the model to generate structured rationales according to given instructions before extracting embeddings, effectively preserving the reasoning steps within the representation. This approach enhances the quality of embeddings by integrating context-conditional inference signals, which standard direct encoding methods overlook. The method is trained using a contrastive learning framework that aligns reasoning outputs with improved embeddings. Experiments conducted on the MMEB benchmark demonstrate that RGE improves multimodal retrieval performance, achieving a 4.9% increase over non-reasoning baselines. Overall, the study confirms that embedding quality benefits significantly from the explicit inclusion of reasoning processes enabled by MLLMs, suggesting a new direction for enhancing multimodal representation learning by bridging generative reasoning and embedding extraction. <div>
arXiv:2511.16150v1 Announce Type: new 
Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.16156</link>
<guid>https://arxiv.org/abs/2511.16156</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, Structured Pruning, Knowledge Distillation, Image Generation, Resource Efficiency  

<br /><br />Summary:  
This paper addresses the high computational cost of Diffusion Transformers (DiTs) in image generation by introducing Pluggable Pruning with Contiguous Layer Distillation (PPCL), a novel pruning framework optimized for DiT architectures. First, the method identifies redundant layers via a combination of linear probing and first-order differential trend analysis of similarity metrics, enabling targeted pruning of model depth intervals. Second, PPCL implements a unique teacher-student alternating distillation scheme that simultaneously integrates both depth-wise and width-wise pruning during a single training phase, allowing flexible knowledge transfer across various pruning ratios without requiring retraining for each configuration. Experimental validation on multiple Multi-Modal Diffusion Transformer models demonstrates that PPCL can reduce model parameters by 50% with less than 3% degradation in key performance metrics, maintaining high-quality image generation. This significant compression makes DiTs more practical for deployment in environments with limited computational resources. Additionally, the framework is plug-and-play and versatile, supporting diverse pruning strategies efficiently. The authors have also made their code and pretrained checkpoints publicly available, facilitating further research and adoption within the community. <div>
arXiv:2511.16156v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2511.16160</link>
<guid>https://arxiv.org/abs/2511.16160</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial intelligence, Multimodal Large Language Models, Video2Layout, cognitive maps, spatial reasoning<br /><br />Summary:<br /><br />This paper addresses the challenge of enhancing spatial intelligence in Multimodal Large Language Models (MLLMs) by proposing a new framework called Video2Layout. Unlike existing methods that rely on discretized grid-based cognitive maps, Video2Layout reconstructs spatial layouts using continuous object boundary coordinates, allowing for more precise metric grounding of object sizes and inter-object distances. The approach is motivated by the limitations of raster representations in enabling fine-grained spatial reasoning. The method consists of two main stages: first, a supervised fine-tuning stage using a high-quality dataset generated from the AI2THOR simulator, which teaches the model to accurately map visual inputs to boundary coordinates; second, a reinforcement fine-tuning stage to improve the model’s generalization to real-world scenarios. To evaluate how the quantity of image inputs impacts spatial reasoning accuracy, the authors introduce QVS-Bench, a diagnostic benchmark specifically designed for this purpose. Experimental results on QVS-Bench and other common spatial reasoning benchmarks indicate that Video2Layout’s model, V2LO-7B, outperforms models trained on grid maps by an average margin of 4.92%, demonstrating the effectiveness of continuous coordinate representations for spatial understanding. The code for the framework is publicly accessible on GitHub. <div>
arXiv:2511.16160v1 Announce Type: new 
Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</title>
<link>https://arxiv.org/abs/2511.16161</link>
<guid>https://arxiv.org/abs/2511.16161</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud completion, symmetry priors, diffusion models, distribution learning, hierarchical upsampling  

<br /><br />Summary:  
Point cloud completion is a critical task in 3D vision involving reconstructing missing parts of 3D shapes while maintaining detailed local geometry and overall global structure. Existing methods that rely on direct regression of local symmetry transformations help preserve geometric details but have significant drawbacks. First, regression-based techniques often overfit by memorizing instance-specific transformations rather than learning generalizable geometric priors. Second, they are highly sensitive to noise in input data due to point-wise transformation regression, which diminishes robustness and generalization. To overcome these challenges, the paper introduces Simba, a novel framework that transforms point-wise regression into a distribution learning problem. Simba leverages symmetry priors integrated with diffusion models’ generative strengths, which prevents memorization of instant-specific features and better captures robust geometric structures. The authors also propose a hierarchical Mamba-based architecture designed to enable high-fidelity upsampling of point clouds, improving detail preservation. The approach is extensively evaluated on popular datasets including PCN, ShapeNet, and KITTI benchmarks, where it demonstrates state-of-the-art performance, validating its effectiveness and robustness in point cloud completion tasks. <div>
arXiv:2511.16161v1 Announce Type: new 
Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.16162</link>
<guid>https://arxiv.org/abs/2511.16162</guid>
<content:encoded><![CDATA[
<div> robustness, adversarial training, medical image segmentation, wavelet reconstruction, noise injection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of maintaining stable segmentation models under distribution shifts and perturbations in clinical medical imaging. 2. It critiques adversarial training (AT) as the mainstream method for robustness, highlighting its inherent clean-robustness trade-off and high costs that hinder its scalability and maintainability. 3. The authors propose a novel method called Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR), which injects small zero-mean noise at multiple layers during training to learn a frequency-bias prior, steering representations away from noise-sensitive directions. 4. LNG-SWR applies selective wavelet reconstruction on input/features to adapt frequencies by suppressing noise-sensitive bands, enhancing directional structures and shape cues, stabilizing boundary responses, and maintaining spectral consistency. 5. This approach is backbone-agnostic, adds minimal inference overhead, and can complement or replace adversarial training. 6. Experiments on CT and ultrasound datasets demonstrate consistent improvements in clean Dice/IoU scores and reduced performance drops under strong adversarial attacks using PGD and SSAH. 7. Combining LNG-SWR with adversarial training yields further additive robustness gains without compromising clean accuracy. 8. Overall, LNG-SWR offers a simple, effective, and engineering-friendly solution for robust medical image segmentation in both adversarial and standard training settings. <div>
arXiv:2511.16162v1 Announce Type: new 
Abstract: Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs</title>
<link>https://arxiv.org/abs/2511.16163</link>
<guid>https://arxiv.org/abs/2511.16163</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, verbose generation, adversarial perturbations, reinforcement learning, output token maximization<br /><br />Summary:<br />1. This paper addresses the efficiency challenges of Vision-Language Models (VLMs), focusing on their generation of lengthy and low-information-density outputs that increase energy consumption, latency, and token costs. <br />2. Existing methods to prolong VLM output rely on delaying the end-of-sequence (EOS) token occurrence but lack explicit optimization, resulting in poor control and stability. <br />3. The authors propose a verbose-text induction attack (VTIA), which injects imperceptible adversarial perturbations into benign images through a two-stage framework to maximize output token length. <br />4. The first stage performs adversarial prompt search using reinforcement learning to find prompt embeddings that induce the model’s language component to generate verbose outputs. <br />5. The second stage optimizes vision-aligned perturbations on input images to maximize similarity to the adversarial prompt’s visual embeddings, creating malicious images that trigger verbose generation. <br />6. Experiments on four popular VLMs demonstrate that VTIA substantially improves effectiveness, efficiency, and generalization in inducing verbose outputs, highlighting potential risks in deploying VLMs without robustness considerations. <div>
arXiv:2511.16163v1 Announce Type: new 
Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoVLA: Self-Evolving Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2511.16166</link>
<guid>https://arxiv.org/abs/2511.16166</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, long-horizon manipulation, stage hallucination, self-supervised learning, sim-to-real transfer<br /><br />Summary: EvoVLA is a novel self-supervised Vision-Language-Action (VLA) framework designed to tackle the challenge of long-horizon robotic manipulation tasks, which prior models struggle with due to stage hallucination where agents falsely report progress on multi-step tasks. EvoVLA employs three key components: Stage-Aligned Reward (SAR), which leverages triplet contrastive learning with Gemini-generated hard negatives to eliminate visual shortcuts; Pose-Based Object Exploration (POE), which enhances curiosity-driven exploration using relative object-gripper pose rather than raw pixel data; and Long-Horizon Memory, which stabilizes intrinsic shaping during extended task rollouts through selective context retention and gated fusion. Evaluations on the Discoverse-L benchmark involving three multi-stage tasks demonstrate a 10.2 percentage point improvement in average task success over the best baseline (OpenVLA-OFT), achieving 69.2% success. The model also exhibits 1.5 times greater sample efficiency and decreases stage hallucination from 38.5% to 14.8%. Real-world trials with physical robots show a 54.6% average success rate across four manipulation tasks, outperforming OpenVLA-OFT by 11 percentage points, indicating strong sim-to-real transfer and generalization capabilities. The project’s code and additional resources are publicly available online. <div>
arXiv:2511.16166v1 Announce Type: new 
Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective</title>
<link>https://arxiv.org/abs/2511.16170</link>
<guid>https://arxiv.org/abs/2511.16170</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary semantic segmentation, CLIP, multimodal alignment, distraction tokens, ReFocusing CLIP (RF-CLIP)  

<br /><br />Summary: This paper addresses the challenge of enhancing pixel-level multimodal alignment for open-vocabulary semantic segmentation (OVSS), which uses vision-language alignment to associate textual prompts with image pixels. The authors conduct an interpretability-driven investigation into CLIP's internal mechanisms, discovering a phenomenon analogous to human distraction where CLIP’s attention is diverted from relevant target regions to irrelevant tokens. These distraction tokens arise due to dimension-specific over-activation within CLIP's model. By analyzing and filtering out these distraction tokens, the authors demonstrate improved dense prediction performance. Building on this insight, they propose ReFocusing CLIP (RF-CLIP), a novel training-free method that mimics human distraction-refocusing behavior. RF-CLIP redirects CLIP’s attention from irrelevant distraction tokens back to target areas, thereby refining the granularity of multimodal pixel-level alignment. This approach significantly enhances the accuracy and quality of OVSS without incurring additional training costs. Evaluations on eight semantic segmentation benchmarks show that RF-CLIP achieves state-of-the-art performance while maintaining high inference efficiency. The work highlights the benefit of interpretability analyses in improving large pretrained vision-language models for dense prediction tasks. <div>
arXiv:2511.16170v1 Announce Type: new 
Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</title>
<link>https://arxiv.org/abs/2511.16175</link>
<guid>https://arxiv.org/abs/2511.16175</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Disentangled Visual Foresight, diffusion Transformer, meta queries, robot manipulation<br /><br />Summary:<br /><br />1. This paper introduces Mantis, a novel framework designed to improve Vision-Language-Action (VLA) models by addressing challenges related to predicting high-dimensional visual states and maintaining strong language comprehension.<br />2. Mantis features a Disentangled Visual Foresight (DVF) mechanism that decouples visual foresight prediction from the main backbone using a combination of meta queries and a diffusion Transformer (DiT) head.<br />3. By feeding the current visual state into the DiT via a residual connection, Mantis uses a next-state prediction objective that enables meta queries to automatically capture latent actions shaping the visual trajectory, thereby enhancing explicit action learning.<br />4. This disentanglement reduces the workload on the VLA backbone, preserving its ability to perform comprehension and reasoning through language supervision.<br />5. Empirical results demonstrate that Mantis, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, achieves a 96.7% success rate on the LIBERO benchmark after fine-tuning, outperforming strong baselines with faster convergence.<br />6. Real-world tests show that Mantis surpasses the leading open-source model π₀.₅ in instruction-following, generalization to unseen instructions, and reasoning capabilities.<br />7. The authors have released code and pretrained weights to facilitate further research and development in the VLA community. <div>
arXiv:2511.16175v1 Announce Type: new 
Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\pi_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.16184</link>
<guid>https://arxiv.org/abs/2511.16184</guid>
<content:encoded><![CDATA[
<div> Visible-Infrared Re-Identification, Unsupervised Domain Adaptation, Domain Discrepancies, Domain-Shared Learning, Gradual Alignment<br /><br />Summary:  
The paper addresses challenges in Visible-Infrared person Re-Identification (VI-ReID), focusing on the gap between public datasets and real-world applications, where most algorithms fail due to domain differences. It investigates Unsupervised Domain Adaptation VI-ReID (UDA-VI-ReID) to transfer knowledge from labeled source data to unlabeled target data without requiring new annotations, maintaining accuracy. Two core challenges are identified: inter-domain modality discrepancies (differences between source and target domains) and intra-domain modality discrepancies (differences between visible and infrared data within the same domain). To overcome these, the authors propose a novel two-stage model called Domain-Shared Learning and Gradual Alignment (DSLGA). In the first pre-training stage, the Domain-Shared Learning Strategy (DSLS) reduces ineffective pre-training by exploiting shared information between domains. In the second fine-tuning stage, the Gradual Alignment Strategy (GAS) progressively aligns cross-modality features via a cluster-to-holistic approach, addressing large intra-domain discrepancies. Additionally, a new testing protocol named CMDA-XD is introduced for evaluating UDA-VI-ReID models. Extensive experiments show the proposed method surpasses existing domain adaptation methods and even some supervised VI-ReID techniques, proving its effectiveness under various setups. <div>
arXiv:2511.16184v1 Announce Type: new 
Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2511.16186</link>
<guid>https://arxiv.org/abs/2511.16186</guid>
<content:encoded><![CDATA[
<div> Keywords: organ reconstruction, topology-preserving, template-based, deep learning, anatomical consistency<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reconstructing human organs composed of interconnected substructures whose geometries and spatial relationships are mutually constrained. Traditional deep-learning approaches tend to treat these parts independently, leading to anatomically implausible results.<br /><br />2. The authors introduce PrIntMesh, a novel framework that performs template-based, topology-preserving reconstruction of organs as unified systems rather than separate components.<br /><br />3. PrIntMesh begins with a connected organ template and jointly deforms its substructures to fit patient-specific anatomies. This joint deformation explicitly preserves internal boundaries and enforces smooth, artifact-free surfaces.<br /><br />4. The method is validated on multiple organs including the heart, hippocampus, and lungs, demonstrating high geometric accuracy, correct topological representation, and robustness even when trained with limited or noisy data.<br /><br />5. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency across substructures, and provides a data-efficient solution with strong clinical applicability. <div>
arXiv:2511.16186v1 Announce Type: new 
Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.16203</link>
<guid>https://arxiv.org/abs/2511.16203</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, adversarial robustness, multimodal attacks, cross-modal misalignment, embodied AI<br /><br />Summary: This paper presents VLA-Fool, a comprehensive investigation into the adversarial robustness of Vision-Language-Action (VLA) models, which integrate perception, reasoning, and action in embodied environments. The study addresses a gap in current research that largely neglects adversarial vulnerabilities under realistic multimodal and black-box conditions, focusing instead on single-modality perturbations. VLA-Fool systematically explores three categories of adversarial attacks: textual perturbations using gradient-based and prompt-based methods, visual perturbations through patch and noise distortions, and novel cross-modal misalignment attacks that disrupt the semantic alignment between visual perception and textual instruction. Additionally, the authors introduce a VLA-aware semantic space within linguistic prompts, establishing the first automated, semantically guided prompting framework tailored to enhance robustness evaluation. Experimental validation on the LIBERO benchmark with a fine-tuned OpenVLA model shows that even minimal multimodal perturbations cause substantial behavioral deviations, emphasizing the fragility of multimodal alignment in these embodied agents. The findings highlight critical vulnerabilities in current VLA systems and suggest that robust multimodal alignment is essential for reliable embodied AI performance under adversarial conditions. <div>
arXiv:2511.16203v1 Announce Type: new 
Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles</title>
<link>https://arxiv.org/abs/2511.16213</link>
<guid>https://arxiv.org/abs/2511.16213</guid>
<content:encoded><![CDATA[
<div> Clustering, Unsupervised Learning, Image Classification, Cluster Ensembling, Nearest Neighbor Selection<br /><br />Summary:<br /><br />This paper addresses the challenge of unsupervised image classification, also known as image clustering, which involves grouping unlabeled images into meaningful semantic categories. Unlike early methods that combined representation learning with clustering iteratively, the authors focus solely on clustering due to advancements in foundational models that provide strong frozen backbones. The proposed method, ICCE (Image Clustering through Cluster Ensembles), enhances a recent multi-head clustering framework by incorporating adaptive nearest neighbor selection and cluster ensembling strategies. Initially, ICCE trains multiple clustering heads on a frozen backbone to generate diverse clusterings of the image data. These varying cluster results are then unified via a cluster ensembling technique that resolves conflicts and achieves a consensus clustering. Finally, the consensus clustering output is used as pseudo-labels to train an image classifier. ICCE demonstrates state-of-the-art performance across ten benchmark datasets, reaching 99.3% accuracy on CIFAR10, 89% on CIFAR100, and notably exceeding 70% accuracy on the large-scale ImageNet dataset. This marks the first fully unsupervised image classification approach to surpass the 70% accuracy threshold on ImageNet, effectively narrowing the gap with fully supervised methods. <div>
arXiv:2511.16213v1 Announce Type: new 
Abstract: Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, "Image Clustering through Cluster Ensembles" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2511.16221</link>
<guid>https://arxiv.org/abs/2511.16221</guid>
<content:encoded><![CDATA[
<div> Deception Detection, Multimodal Large Language Models, Social Reasoning, Dataset, Memory Module<br /><br />Summary:<br /><br />1. The paper identifies a critical limitation of current state-of-the-art Multimodal Large Language Models (MLLMs): their inability to 'read the room' and detect deception in complex social interactions. 2. To address this, the authors introduce a new task called Multimodal Interactive Deception Assessment (MIDA) and provide a novel dataset that couples synchronized video and text with verified truthfulness labels for each statement, enabling rigorous evaluation. 3. A comprehensive benchmark evaluating 12 leading MLLMs reveals a significant performance gap, with even powerful models like GPT-4o struggling to reliably distinguish truthful from deceptive statements. 4. Analysis of failure modes shows these models struggle to ground language in multimodal social cues and lack mechanisms to model others’ knowledge, beliefs, or intentions, highlighting the need for new approaches. 5. As a solution, the authors propose a Social Chain-of-Thought (SoCoT) reasoning pipeline combined with a Dynamic Social Epistemic Memory (DSEM) module, which together improve performance on this difficult task and point toward more perceptive AI systems capable of human-like social reasoning. <div>
arXiv:2511.16221v1 Announce Type: new 
Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwiTrack: Tri-State Switch for Cross-Modal Object Tracking</title>
<link>https://arxiv.org/abs/2511.16227</link>
<guid>https://arxiv.org/abs/2511.16227</guid>
<content:encoded><![CDATA[
<div> Cross-modal tracking, RGB-NIR, state-switching framework, trajectory prediction, template reconstruction  

<br /><br />Summary:  
This paper addresses the challenge of cross-modal object tracking (CMOT), specifically tracking targets across alternating RGB and near-infrared (NIR) video frames in a scenario where only one modality is available per frame. Existing approaches that connect RGB and NIR inputs through a shared backbone are limited in capturing modality-specific features and are prone to object drift, particularly with unreliable inputs. To overcome these limitations, the authors propose SwiTrack, a novel state-switching framework that employs three specialized processing streams tailored to the modality state. RGB frames are directly processed by a visual encoder, while NIR frames are refined via a gated adapter integrated with the visual encoder to better calibrate shared latent features, improving robustness. For invalid or unreliable modality inputs, a consistency trajectory prediction module uses spatial and temporal cues to estimate target movement and prevent drift. Additionally, the framework incorporates dynamic template reconstruction, which iteratively updates the template features, and applies a similarity alignment loss to enhance cross-modal feature consistency. Experiments on recent benchmarks demonstrate that SwiTrack achieves state-of-the-art performance, improving precision and success rates by 7.2% and 4.3%, respectively, while running at real-time speeds of 65 frames per second. The code and models are publicly available online. <div>
arXiv:2511.16227v1 Announce Type: new 
Abstract: Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs</title>
<link>https://arxiv.org/abs/2511.16264</link>
<guid>https://arxiv.org/abs/2511.16264</guid>
<content:encoded><![CDATA[
<div> Keywords: full-body tracking, AR/VR, multi-layer perceptron, Memory-Block, multi-task learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving realistic and smooth full-body tracking for immersive AR/VR applications, where existing systems focus mainly on head and hand tracking via HMDs and controllers, resulting in incomplete 3D full-body reconstruction.<br /><br />2. The authors propose a novel neural network-based method using a multi-layer perceptron (MLP) enhanced with residual connections and a new component called Memory-Block to handle missing sensor data.<br /><br />3. Memory-Block represents missing sensor inputs with trainable code-vectors and integrates these with sparse signals from previous time instances, which improves temporal consistency in the full-body motion tracking.<br /><br />4. The approach is formulated as a multi-task learning problem, enabling the MLP backbone to learn robust feature representations that enhance prediction accuracy.<br /><br />5. Experimental results demonstrate that this method outperforms state-of-the-art baselines by significantly reducing prediction errors and achieves an efficient runtime of 72 frames per second on mobile HMDs, thereby optimizing the accuracy and computational tradeoff for real-time AR/VR applications. <div>
arXiv:2511.16264v1 Announce Type: new 
Abstract: Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TetraSDF: Precise Mesh Extraction with Multi-resolution Tetrahedral Grid</title>
<link>https://arxiv.org/abs/2511.16273</link>
<guid>https://arxiv.org/abs/2511.16273</guid>
<content:encoded><![CDATA[
arXiv:2511.16273v1 Announce Type: new 
Abstract: Extracting meshes that exactly match the zero-level set of neural signed distance functions (SDFs) remains challenging. Sampling-based methods introduce discretization error, while continuous piecewise affine (CPWA) analytic approaches apply only to plain ReLU MLPs. We present TetraSDF, a precise analytic meshing framework for SDFs represented by a ReLU MLP composed with a multi-resolution tetrahedral positional encoder. The encoder's barycentric interpolation preserves global CPWA structure, enabling us to track ReLU linear regions within an encoder-induced polyhedral complex. A fixed analytic input preconditioner derived from the encoder's metric further reduces directional bias and stabilizes training. Across multiple benchmarks, TetraSDF matches or surpasses existing grid-based encoders in SDF reconstruction accuracy, and its analytic extractor produces highly self-consistent meshes that remain faithful to the learned isosurfaces, all with practical runtime and memory efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</title>
<link>https://arxiv.org/abs/2511.16282</link>
<guid>https://arxiv.org/abs/2511.16282</guid>
<content:encoded><![CDATA[
arXiv:2511.16282v1 Announce Type: new 
Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability</title>
<link>https://arxiv.org/abs/2511.16294</link>
<guid>https://arxiv.org/abs/2511.16294</guid>
<content:encoded><![CDATA[
arXiv:2511.16294v1 Announce Type: new 
Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing 3D Gaussian Splattering for Mobile GPUs</title>
<link>https://arxiv.org/abs/2511.16298</link>
<guid>https://arxiv.org/abs/2511.16298</guid>
<content:encoded><![CDATA[
arXiv:2511.16298v1 Announce Type: new 
Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling</title>
<link>https://arxiv.org/abs/2511.16301</link>
<guid>https://arxiv.org/abs/2511.16301</guid>
<content:encoded><![CDATA[
arXiv:2511.16301v1 Announce Type: new 
Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders are Topic Models</title>
<link>https://arxiv.org/abs/2511.16309</link>
<guid>https://arxiv.org/abs/2511.16309</guid>
<content:encoded><![CDATA[
arXiv:2511.16309v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks</title>
<link>https://arxiv.org/abs/2511.16315</link>
<guid>https://arxiv.org/abs/2511.16315</guid>
<content:encoded><![CDATA[
arXiv:2511.16315v1 Announce Type: new 
Abstract: ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaTex: Seamless Texture Generation as Latent Color Diffusion</title>
<link>https://arxiv.org/abs/2511.16317</link>
<guid>https://arxiv.org/abs/2511.16317</guid>
<content:encoded><![CDATA[
arXiv:2511.16317v1 Announce Type: new 
Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WWE-UIE: A Wavelet &amp; White Balance Efficient Network for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2511.16321</link>
<guid>https://arxiv.org/abs/2511.16321</guid>
<content:encoded><![CDATA[
arXiv:2511.16321v1 Announce Type: new 
Abstract: Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2511.16322</link>
<guid>https://arxiv.org/abs/2511.16322</guid>
<content:encoded><![CDATA[
arXiv:2511.16322v1 Announce Type: new 
Abstract: Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks</title>
<link>https://arxiv.org/abs/2511.16341</link>
<guid>https://arxiv.org/abs/2511.16341</guid>
<content:encoded><![CDATA[
arXiv:2511.16341v1 Announce Type: new 
Abstract: Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach</title>
<link>https://arxiv.org/abs/2511.16343</link>
<guid>https://arxiv.org/abs/2511.16343</guid>
<content:encoded><![CDATA[
arXiv:2511.16343v1 Announce Type: new 
Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</title>
<link>https://arxiv.org/abs/2511.16349</link>
<guid>https://arxiv.org/abs/2511.16349</guid>
<content:encoded><![CDATA[
arXiv:2511.16349v1 Announce Type: new 
Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Order Matching Network for Alignment-Free Depth Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16361</link>
<guid>https://arxiv.org/abs/2511.16361</guid>
<content:encoded><![CDATA[
arXiv:2511.16361v1 Announce Type: new 
Abstract: Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration</title>
<link>https://arxiv.org/abs/2511.16364</link>
<guid>https://arxiv.org/abs/2511.16364</guid>
<content:encoded><![CDATA[
arXiv:2511.16364v1 Announce Type: new 
Abstract: Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement</title>
<link>https://arxiv.org/abs/2511.16378</link>
<guid>https://arxiv.org/abs/2511.16378</guid>
<content:encoded><![CDATA[
arXiv:2511.16378v1 Announce Type: new 
Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss</title>
<link>https://arxiv.org/abs/2511.16418</link>
<guid>https://arxiv.org/abs/2511.16418</guid>
<content:encoded><![CDATA[
arXiv:2511.16418v1 Announce Type: new 
Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation</title>
<link>https://arxiv.org/abs/2511.16428</link>
<guid>https://arxiv.org/abs/2511.16428</guid>
<content:encoded><![CDATA[
arXiv:2511.16428v1 Announce Type: new 
Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360{\deg} field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks for Surgical Scene Segmentation</title>
<link>https://arxiv.org/abs/2511.16430</link>
<guid>https://arxiv.org/abs/2511.16430</guid>
<content:encoded><![CDATA[
arXiv:2511.16430v1 Announce Type: new 
Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation</title>
<link>https://arxiv.org/abs/2511.16435</link>
<guid>https://arxiv.org/abs/2511.16435</guid>
<content:encoded><![CDATA[
arXiv:2511.16435v1 Announce Type: new 
Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreetView-Waste: A Multi-Task Dataset for Urban Waste Management</title>
<link>https://arxiv.org/abs/2511.16440</link>
<guid>https://arxiv.org/abs/2511.16440</guid>
<content:encoded><![CDATA[
arXiv:2511.16440v1 Announce Type: new 
Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</title>
<link>https://arxiv.org/abs/2511.16449</link>
<guid>https://arxiv.org/abs/2511.16449</guid>
<content:encoded><![CDATA[
arXiv:2511.16449v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs</title>
<link>https://arxiv.org/abs/2511.16454</link>
<guid>https://arxiv.org/abs/2511.16454</guid>
<content:encoded><![CDATA[
arXiv:2511.16454v1 Announce Type: new 
Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry</title>
<link>https://arxiv.org/abs/2511.16471</link>
<guid>https://arxiv.org/abs/2511.16471</guid>
<content:encoded><![CDATA[
arXiv:2511.16471v1 Announce Type: new 
Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow and Depth Assisted Video Prediction with Latent Transformer</title>
<link>https://arxiv.org/abs/2511.16484</link>
<guid>https://arxiv.org/abs/2511.16484</guid>
<content:encoded><![CDATA[
arXiv:2511.16484v1 Announce Type: new 
Abstract: Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</title>
<link>https://arxiv.org/abs/2511.16494</link>
<guid>https://arxiv.org/abs/2511.16494</guid>
<content:encoded><![CDATA[
arXiv:2511.16494v1 Announce Type: new 
Abstract: Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI</title>
<link>https://arxiv.org/abs/2511.16498</link>
<guid>https://arxiv.org/abs/2511.16498</guid>
<content:encoded><![CDATA[
arXiv:2511.16498v1 Announce Type: new 
Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras</title>
<link>https://arxiv.org/abs/2511.16521</link>
<guid>https://arxiv.org/abs/2511.16521</guid>
<content:encoded><![CDATA[
arXiv:2511.16521v1 Announce Type: new 
Abstract: Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization</title>
<link>https://arxiv.org/abs/2511.16524</link>
<guid>https://arxiv.org/abs/2511.16524</guid>
<content:encoded><![CDATA[
arXiv:2511.16524v1 Announce Type: new 
Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive vision-language learning with paraphrasing and negation</title>
<link>https://arxiv.org/abs/2511.16527</link>
<guid>https://arxiv.org/abs/2511.16527</guid>
<content:encoded><![CDATA[
arXiv:2511.16527v1 Announce Type: new 
Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration</title>
<link>https://arxiv.org/abs/2511.16532</link>
<guid>https://arxiv.org/abs/2511.16532</guid>
<content:encoded><![CDATA[
arXiv:2511.16532v1 Announce Type: new 
Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation</title>
<link>https://arxiv.org/abs/2511.16535</link>
<guid>https://arxiv.org/abs/2511.16535</guid>
<content:encoded><![CDATA[
arXiv:2511.16535v1 Announce Type: new 
Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</title>
<link>https://arxiv.org/abs/2511.16541</link>
<guid>https://arxiv.org/abs/2511.16541</guid>
<content:encoded><![CDATA[
arXiv:2511.16541v1 Announce Type: new 
Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering</title>
<link>https://arxiv.org/abs/2511.16542</link>
<guid>https://arxiv.org/abs/2511.16542</guid>
<content:encoded><![CDATA[
arXiv:2511.16542v1 Announce Type: new 
Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Supernet Training for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2511.16546</link>
<guid>https://arxiv.org/abs/2511.16546</guid>
<content:encoded><![CDATA[
arXiv:2511.16546v1 Announce Type: new 
Abstract: Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.
  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.
  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.
  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lite Any Stereo: Efficient Zero-Shot Stereo Matching</title>
<link>https://arxiv.org/abs/2511.16555</link>
<guid>https://arxiv.org/abs/2511.16555</guid>
<content:encoded><![CDATA[
arXiv:2511.16555v1 Announce Type: new 
Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening</title>
<link>https://arxiv.org/abs/2511.16566</link>
<guid>https://arxiv.org/abs/2511.16566</guid>
<content:encoded><![CDATA[
arXiv:2511.16566v1 Announce Type: new 
Abstract: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POMA-3D: The Point Map Way to 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2511.16567</link>
<guid>https://arxiv.org/abs/2511.16567</guid>
<content:encoded><![CDATA[
arXiv:2511.16567v1 Announce Type: new 
Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks</title>
<link>https://arxiv.org/abs/2511.16574</link>
<guid>https://arxiv.org/abs/2511.16574</guid>
<content:encoded><![CDATA[
arXiv:2511.16574v1 Announce Type: new 
Abstract: The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.
  For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.
  These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.16595</link>
<guid>https://arxiv.org/abs/2511.16595</guid>
<content:encoded><![CDATA[
arXiv:2511.16595v1 Announce Type: new 
Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap</title>
<link>https://arxiv.org/abs/2511.16617</link>
<guid>https://arxiv.org/abs/2511.16617</guid>
<content:encoded><![CDATA[
arXiv:2511.16617v1 Announce Type: new 
Abstract: The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
<link>https://arxiv.org/abs/2511.16618</link>
<guid>https://arxiv.org/abs/2511.16618</guid>
<content:encoded><![CDATA[
arXiv:2511.16618v1 Announce Type: new 
Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning</title>
<link>https://arxiv.org/abs/2511.16619</link>
<guid>https://arxiv.org/abs/2511.16619</guid>
<content:encoded><![CDATA[
arXiv:2511.16619v1 Announce Type: new 
Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Guided Upsampling for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.16623</link>
<guid>https://arxiv.org/abs/2511.16623</guid>
<content:encoded><![CDATA[
arXiv:2511.16623v1 Announce Type: new 
Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM 3D: 3Dfy Anything in Images</title>
<link>https://arxiv.org/abs/2511.16624</link>
<guid>https://arxiv.org/abs/2511.16624</guid>
<content:encoded><![CDATA[
arXiv:2511.16624v1 Announce Type: new 
Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</title>
<link>https://arxiv.org/abs/2511.16635</link>
<guid>https://arxiv.org/abs/2511.16635</guid>
<content:encoded><![CDATA[
arXiv:2511.16635v1 Announce Type: new 
Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming</title>
<link>https://arxiv.org/abs/2511.16642</link>
<guid>https://arxiv.org/abs/2511.16642</guid>
<content:encoded><![CDATA[
arXiv:2511.16642v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision</title>
<link>https://arxiv.org/abs/2511.16650</link>
<guid>https://arxiv.org/abs/2511.16650</guid>
<content:encoded><![CDATA[
arXiv:2511.16650v1 Announce Type: new 
Abstract: 3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.16653</link>
<guid>https://arxiv.org/abs/2511.16653</guid>
<content:encoded><![CDATA[
arXiv:2511.16653v1 Announce Type: new 
Abstract: Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Spatial Supersensing Without Spatial Supersensing</title>
<link>https://arxiv.org/abs/2511.16655</link>
<guid>https://arxiv.org/abs/2511.16655</guid>
<content:encoded><![CDATA[
arXiv:2511.16655v1 Announce Type: new 
Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PartUV: Part-Based UV Unwrapping of 3D Meshes</title>
<link>https://arxiv.org/abs/2511.16659</link>
<guid>https://arxiv.org/abs/2511.16659</guid>
<content:encoded><![CDATA[
arXiv:2511.16659v1 Announce Type: new 
Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</title>
<link>https://arxiv.org/abs/2511.16662</link>
<guid>https://arxiv.org/abs/2511.16662</guid>
<content:encoded><![CDATA[
arXiv:2511.16662v1 Announce Type: new 
Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</title>
<link>https://arxiv.org/abs/2511.16666</link>
<guid>https://arxiv.org/abs/2511.16666</guid>
<content:encoded><![CDATA[
arXiv:2511.16666v1 Announce Type: new 
Abstract: Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
<link>https://arxiv.org/abs/2511.16668</link>
<guid>https://arxiv.org/abs/2511.16668</guid>
<content:encoded><![CDATA[
arXiv:2511.16668v1 Announce Type: new 
Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
<link>https://arxiv.org/abs/2511.16669</link>
<guid>https://arxiv.org/abs/2511.16669</guid>
<content:encoded><![CDATA[
arXiv:2511.16669v1 Announce Type: new 
Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Think Fast and Slow for Visual Language Models</title>
<link>https://arxiv.org/abs/2511.16670</link>
<guid>https://arxiv.org/abs/2511.16670</guid>
<content:encoded><![CDATA[
arXiv:2511.16670v1 Announce Type: new 
Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
<link>https://arxiv.org/abs/2511.16671</link>
<guid>https://arxiv.org/abs/2511.16671</guid>
<content:encoded><![CDATA[
arXiv:2511.16671v1 Announce Type: new 
Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</title>
<link>https://arxiv.org/abs/2511.16672</link>
<guid>https://arxiv.org/abs/2511.16672</guid>
<content:encoded><![CDATA[
arXiv:2511.16672v1 Announce Type: new 
Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</title>
<link>https://arxiv.org/abs/2511.16673</link>
<guid>https://arxiv.org/abs/2511.16673</guid>
<content:encoded><![CDATA[
arXiv:2511.16673v1 Announce Type: new 
Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation for Pre-Trained Self-Supervised Vision Models</title>
<link>https://arxiv.org/abs/2511.16674</link>
<guid>https://arxiv.org/abs/2511.16674</guid>
<content:encoded><![CDATA[
arXiv:2511.16674v1 Announce Type: new 
Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI</title>
<link>https://arxiv.org/abs/2511.15717</link>
<guid>https://arxiv.org/abs/2511.15717</guid>
<content:encoded><![CDATA[
arXiv:2511.15717v1 Announce Type: cross 
Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniUltra: Interactive Parameter-Efficient SAM2 for Universal Ultrasound Segmentation</title>
<link>https://arxiv.org/abs/2511.15771</link>
<guid>https://arxiv.org/abs/2511.15771</guid>
<content:encoded><![CDATA[
arXiv:2511.15771v1 Announce Type: cross 
Abstract: The Segment Anything Model 2 (SAM2) demonstrates remarkable universal segmentation capabilities on natural images. However, its performance on ultrasound images is significantly degraded due to domain disparities. This limitation raises two critical challenges: how to efficiently adapt SAM2 to ultrasound imaging while maintaining parameter efficiency, and how to deploy the adapted model effectively in resource-constrained clinical environments. To address these issues, we propose UniUltra for universal ultrasound segmentation. Specifically, we first introduce a novel context-edge hybrid adapter (CH-Adapter) that enhances fine-grained perception across diverse ultrasound imaging modalities while achieving parameter-efficient fine-tuning. To further improve clinical applicability, we develop a deep-supervised knowledge distillation (DSKD) technique that transfers knowledge from the large image encoder of the fine-tuned SAM2 to a super lightweight encoder, substantially reducing computational requirements without compromising performance. Extensive experiments demonstrate that UniUltra outperforms state-of-the-arts with superior generalization capabilities. Notably, our framework achieves competitive performance using only 8.91% of SAM2's parameters during fine-tuning, and the final compressed model reduces the parameter count by 94.08% compared to the original SAM2, making it highly suitable for practical clinical deployment. The source code is available at https://github.com/xq141839/UniUltra.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos</title>
<link>https://arxiv.org/abs/2511.16183</link>
<guid>https://arxiv.org/abs/2511.16183</guid>
<content:encoded><![CDATA[
arXiv:2511.16183v1 Announce Type: cross 
Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Robot Dogs See the Unseeable</title>
<link>https://arxiv.org/abs/2511.16262</link>
<guid>https://arxiv.org/abs/2511.16262</guid>
<content:encoded><![CDATA[
arXiv:2511.16262v1 Announce Type: cross 
Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</title>
<link>https://arxiv.org/abs/2511.16268</link>
<guid>https://arxiv.org/abs/2511.16268</guid>
<content:encoded><![CDATA[
arXiv:2511.16268v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arctic-Extract Technical Report</title>
<link>https://arxiv.org/abs/2511.16470</link>
<guid>https://arxiv.org/abs/2511.16470</guid>
<content:encoded><![CDATA[
arXiv:2511.16470v1 Announce Type: cross 
Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
<link>https://arxiv.org/abs/2511.16518</link>
<guid>https://arxiv.org/abs/2511.16518</guid>
<content:encoded><![CDATA[
arXiv:2511.16518v1 Announce Type: cross 
Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Green Resilience of Cyber-Physical Systems: Doctoral Dissertation</title>
<link>https://arxiv.org/abs/2511.16593</link>
<guid>https://arxiv.org/abs/2511.16593</guid>
<content:encoded><![CDATA[
arXiv:2511.16593v1 Announce Type: cross 
Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space</title>
<link>https://arxiv.org/abs/2209.12746</link>
<guid>https://arxiv.org/abs/2209.12746</guid>
<content:encoded><![CDATA[
arXiv:2209.12746v3 Announce Type: replace 
Abstract: As research on image inversion advances, the process is generally divided into two stages. The first step is Image Embedding, involves using an encoder or optimization procedure to embed an image and obtain its corresponding latent code. The second stage, referred to as Result Refinement, further improves the inversion and editing outcomes. Although this refinement stage substantially enhances reconstruction fidelity, perception and editability remain largely unchanged and are highly dependent on the latent codes derived from the first stage. Therefore, a key challenge lies in obtaining latent codes that preserve reconstruction fidelity while simultaneously improving perception and editability. In this work, we first reveal that these two properties are closely related to the degree of alignment (or disalignment) between the inverted latent codes and the synthetic distribution. Based on this insight, we propose the \textbf{ Latent Space Alignment Inversion Paradigm (LSAP)}, which integrates both an evaluation metric and a unified inversion solution. Specifically, we introduce the \textbf{Normalized Style Space ($\mathcal{S^N}$ space)} and \textbf{Normalized Style Space Cosine Distance (NSCD)} to quantify the disalignment of inversion methods. Moreover, our paradigm can be optimized for both encoder-based and optimization-based embeddings, providing a consistent alignment framework. Extensive experiments across various domains demonstrate that NSCD effectively captures perceptual and editable characteristics, and that our alignment paradigm achieves state-of-the-art performance in both stages of inversion.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models</title>
<link>https://arxiv.org/abs/2401.17629</link>
<guid>https://arxiv.org/abs/2401.17629</guid>
<content:encoded><![CDATA[
arXiv:2401.17629v2 Announce Type: replace 
Abstract: Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Translation via Token Warping</title>
<link>https://arxiv.org/abs/2402.12099</link>
<guid>https://arxiv.org/abs/2402.12099</guid>
<content:encoded><![CDATA[
arXiv:2402.12099v3 Announce Type: replace 
Abstract: With the revolution of generative AI, video-related tasks have been widely studied. However, current state-of-the-art video models still lag behind image models in visual quality and user control over generated content. In this paper, we introduce TokenWarping, a novel framework for temporally coherent video translation. Existing diffusion-based video editing approaches rely solely on key and value patches in self-attention to ensure temporal consistency, often sacrificing the preservation of local and structural regions. Critically, these methods overlook the significance of the query patches in achieving accurate feature aggregation and temporal coherence. In contrast, TokenWarping leverages complementary token priors by constructing temporal correlations across different frames. Our method begins by extracting optical flows from source videos. During the denoising process of the diffusion model, these optical flows are used to warp the previous frame's query, key, and value patches, aligning them with the current frame's patches. By directly warping the query patches, we enhance feature aggregation in self-attention, while warping the key and value patches ensures temporal consistency across frames. This token warping imposes explicit constraints on the self-attention layer outputs, effectively ensuring temporally coherent translation. Our framework does not require any additional training or fine-tuning and can be seamlessly integrated with existing text-to-image editing methods. We conduct extensive experiments on various video translation tasks, demonstrating that TokenWarping surpasses state-of-the-art methods both qualitatively and quantitatively. Video demonstrations are available in supplementary materials.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Query Prompting for Multi-Domain Landmark Detection</title>
<link>https://arxiv.org/abs/2404.01194</link>
<guid>https://arxiv.org/abs/2404.01194</guid>
<content:encoded><![CDATA[
arXiv:2404.01194v2 Announce Type: replace 
Abstract: Medical landmark detection is crucial in various medical imaging modalities and procedures. Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks. In this work, we propose a universal model for multi-domain landmark detection by leveraging transformer architecture and developing a prompting component, named as Adaptive Query Prompting (AQP). Instead of embedding additional modules in the backbone network, we design a separate module to generate prompts that can be effectively extended to any other transformer network. In our proposed AQP, prompts are learnable parameters maintained in a memory space called prompt pool. The central idea is to keep the backbone frozen and then optimize prompts to instruct the model inference process. Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD. Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost. It has the potential to be extended to more landmark detection tasks. We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks. Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks</title>
<link>https://arxiv.org/abs/2406.04470</link>
<guid>https://arxiv.org/abs/2406.04470</guid>
<content:encoded><![CDATA[
arXiv:2406.04470v3 Announce Type: replace 
Abstract: This study assesses the ability of Large Vision-Language Models (LVLMs) to differentiate between AI-generated and human-generated images. It introduces a new automated benchmark construction method for this evaluation. The experiment compared common LVLMs with human participants using a mixed dataset of AI and human-created images. Results showed that LVLMs could distinguish between the image types to some extent but exhibited a rightward bias, and perform significantly worse compared to humans. To build on these findings, we developed an automated benchmark construction process using AI. This process involved topic retrieval, narrative script generation, error embedding, and image generation, creating a diverse set of text-image pairs with intentional errors. We validated our method through constructing two caparable benchmarks. This study highlights the strengths and weaknesses of LVLMs in real-world understanding and advances benchmark construction techniques, providing a scalable and automatic approach for AI model evaluation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IOR: Inversed Objects Replay for Incremental Object Detection</title>
<link>https://arxiv.org/abs/2406.04829</link>
<guid>https://arxiv.org/abs/2406.04829</guid>
<content:encoded><![CDATA[
arXiv:2406.04829v5 Announce Type: replace 
Abstract: Existing Incremental Object Detection (IOD) methods partially alleviate catastrophic forgetting when incrementally detecting new objects in real-world scenarios. However, many of these methods rely on the assumption that unlabeled old-class objects may co-occur with labeled new-class objects in the incremental data. When unlabeled old-class objects are absent, the performance of existing methods tends to degrade. The absence can be mitigated by generating old-class samples, but it incurs high costs. This paper argues that previous generation-based IOD suffers from redundancy, both in the use of generative models, which require additional training and storage, and in the overproduction of generated samples, many of which do not contribute significantly to performance improvements. To eliminate the redundancy, we propose Inversed Objects Replay (IOR). Specifically, we generate old-class samples by inversing the original detectors, thus eliminating the necessity of training and storing additional generative models. We propose augmented replay to reuse the objects in generated samples, reducing redundant generations. Moreover, we propose high-value knowledge distillation focusing on the positions of old-class objects overwhelmed by the background, which transfers the knowledge to the incremental detector. Extensive experiments conducted on MS COCO 2017 demonstrate that our method can efficiently improve detection performance in IOD scenarios with the absence of old-class objects. The code is available at https://github.com/JiaJia075/IOR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised learning of spatially varying regularization for diffeomorphic image registration</title>
<link>https://arxiv.org/abs/2412.17982</link>
<guid>https://arxiv.org/abs/2412.17982</guid>
<content:encoded><![CDATA[
arXiv:2412.17982v2 Announce Type: replace 
Abstract: Spatially varying regularization accommodates the deformation variations that may be necessary for different anatomical regions during deformable image registration. Historically, optimization-based registration models have harnessed spatially varying regularization to address anatomical subtleties. However, most modern deep learning-based models tend to gravitate towards spatially invariant regularization, wherein a homogenous regularization strength is applied across the entire image, potentially disregarding localized variations. In this paper, we propose a hierarchical probabilistic model that integrates a prior distribution on the deformation regularization strength, enabling the end-to-end learning of a spatially varying deformation regularizer directly from the data. The proposed method is straightforward to implement and easily integrates with various registration network architectures. Additionally, automatic tuning of hyperparameters is achieved through Bayesian optimization, allowing efficient identification of optimal hyperparameters for any given registration task. Comprehensive evaluations on publicly available datasets demonstrate that the proposed method significantly improves registration performance and enhances the interpretability of deep learning-based registration, all while maintaining smooth deformations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control</title>
<link>https://arxiv.org/abs/2501.02260</link>
<guid>https://arxiv.org/abs/2501.02260</guid>
<content:encoded><![CDATA[
arXiv:2501.02260v3 Announce Type: replace 
Abstract: We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Architectures for High Resolution Vision-Language Models</title>
<link>https://arxiv.org/abs/2501.02584</link>
<guid>https://arxiv.org/abs/2501.02584</guid>
<content:encoded><![CDATA[
arXiv:2501.02584v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Haze: Generative Nighttime Image Dehazing</title>
<link>https://arxiv.org/abs/2503.08073</link>
<guid>https://arxiv.org/abs/2503.08073</guid>
<content:encoded><![CDATA[
arXiv:2503.08073v2 Announce Type: replace 
Abstract: Nighttime image dehazing is particularly challenging when dense haze and intense glow severely degrade or entirely obscure background information. Existing methods often struggle due to insufficient background priors and limited generative capability, both of which are highly important under such conditions. In this paper, we introduce BeyondHaze, a generative nighttime dehazing method that not only reduces haze and glow effects but also reconstructs plausible background structures in regions where visual cues are heavily degraded. Our approach is built on two main ideas: obtaining strong background priors by adapting image diffusion models to nighttime dehazing, and enhancing generative ability in haze- and glow-obscured areas through guided training. Task-specific nighttime dehazing knowledge is distilled into an image diffusion model while preserving its capacity to generate clean images. The diffusion model is further trained on tailored image pairs to improve its ability to recover background details that are suppressed by haze effects. Since generative models may introduce hallucinated content, we design our framework to allow user control over the generative level, enabling a balance between visual realism and fidelity. Experiments on real-world nighttime images demonstrate that BeyondHaze substantially improves visibility and scene detail under dense haze.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation</title>
<link>https://arxiv.org/abs/2503.09878</link>
<guid>https://arxiv.org/abs/2503.09878</guid>
<content:encoded><![CDATA[
arXiv:2503.09878v3 Announce Type: replace 
Abstract: Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition</title>
<link>https://arxiv.org/abs/2503.14960</link>
<guid>https://arxiv.org/abs/2503.14960</guid>
<content:encoded><![CDATA[
arXiv:2503.14960v3 Announce Type: replace 
Abstract: Skeleton-based Human Action Recognition (HAR) is a vital technology in robotics and human-robot interaction. However, most existing methods concentrate primarily on full-body movements and often overlook subtle hand motions that are critical for distinguishing fine-grained actions. Recent work leverages a unified graph representation that combines body, hand, and foot keypoints to capture detailed body dynamics. Yet, these models often blur fine hand details due to the disparity between body and hand action characteristics and the loss of subtle features during the spatial-pooling. In this paper, we propose BHaRNet (Body-Hand action Recognition Network), a novel framework that augments a typical body-expert model with a hand-expert model. Our model jointly trains both streams with an ensemble loss that fosters cooperative specialization, functioning in a manner reminiscent of a Mixture-of-Experts (MoE). Moreover, cross-attention is employed via an expertized branch method and a pooling-attention module to enable feature-level interactions and selectively fuse complementary information. Inspired by MMNet, we also demonstrate the applicability of our approach to multi-modal tasks by leveraging RGB information, where body features guide RGB learning to capture richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive actions -- while maintaining fewer GFLOPs and parameters than the relevant unified methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Correspondence Learning for Relative Pose Estimation</title>
<link>https://arxiv.org/abs/2503.18671</link>
<guid>https://arxiv.org/abs/2503.18671</guid>
<content:encoded><![CDATA[
arXiv:2503.18671v2 Announce Type: replace 
Abstract: Relative pose estimation provides a promising way for achieving object-agnostic pose estimation. Despite the success of existing 3D correspondence-based methods, the reliance on explicit feature matching suffers from small overlaps in visible regions and unreliable feature estimation for invisible regions. Inspired by humans' ability to assemble two object parts that have small or no overlapping regions by considering object structure, we propose a novel Structure-Aware Correspondence Learning method for Relative Pose Estimation, which consists of two key modules. First, a structure-aware keypoint extraction module is designed to locate a set of kepoints that can represent the structure of objects with different shapes and appearance, under the guidance of a keypoint based image reconstruction loss. Second, a structure-aware correspondence estimation module is designed to model the intra-image and inter-image relationships between keypoints to extract structure-aware features for correspondence estimation. By jointly leveraging these two modules, the proposed method can naturally estimate 3D-3D correspondences for unseen objects without explicit feature matching for precise relative pose estimation. Experimental results on the CO3D, Objaverse and LineMOD datasets demonstrate that the proposed method significantly outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error on the CO3D dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Motion Unlearning</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
arXiv:2503.18674v2 Announce Type: replace 
Abstract: We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., "kicking" is "loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: https://www.pinlab.org/hmu.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation</title>
<link>https://arxiv.org/abs/2503.18944</link>
<guid>https://arxiv.org/abs/2503.18944</guid>
<content:encoded><![CDATA[
arXiv:2503.18944v2 Announce Type: replace 
Abstract: Vision foundation models (VFMs) trained on large-scale image datasets provide high-quality features that have significantly advanced 2D visual recognition. However, their potential in 3D scene segmentation remains largely untapped, despite the common availability of 2D images alongside 3D point cloud datasets. While significant research has been dedicated to 2D-3D fusion, recent state-of-the-art 3D methods predominantly focus on 3D data, leaving the integration of VFMs into 3D models underexplored. In this work, we challenge this trend by introducing DITR, a generally applicable approach that extracts 2D foundation model features, projects them to 3D, and finally injects them into a 3D point cloud segmentation model. DITR achieves state-of-the-art results on both indoor and outdoor 3D semantic segmentation benchmarks. To enable the use of VFMs even when images are unavailable during inference, we additionally propose to pretrain 3D models by distilling 2D foundation models. By initializing the 3D backbone with knowledge distilled from 2D VFMs, we create a strong basis for downstream 3D segmentation tasks, ultimately boosting performance across various datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape and Texture Recognition in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.23062</link>
<guid>https://arxiv.org/abs/2503.23062</guid>
<content:encoded><![CDATA[
arXiv:2503.23062v4 Announce Type: replace 
Abstract: Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shape and Textures dataset (LAS&amp;T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (VLM) recognize and represent shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape recognition capabilities of the LVLMs remain significantly below human performance. VLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler, more abstract 2D textures and shapes. These results are consistent across a wide range of leading LVLMs (GPT/Gemini/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of leading models to extract and represent low-level visual features. In contrast, humans and simple nets trained directly for these tasks achieve high accuracy. The LAS&amp;T dataset, featuring over 700,000 images for 2D/3D shape, texture, and material recognition and retrieval is freely available.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decade of You Only Look Once (YOLO) for Object Detection: A Review</title>
<link>https://arxiv.org/abs/2504.18586</link>
<guid>https://arxiv.org/abs/2504.18586</guid>
<content:encoded><![CDATA[
arXiv:2504.18586v3 Announce Type: replace 
Abstract: This review marks the tenth anniversary of You Only Look Once (YOLO), one of the most influential frameworks in real-time object detection. Over the past decade, YOLO has evolved from a streamlined detector into a diverse family of architectures characterized by efficient design, modular scalability, and cross-domain adaptability. The paper presents a technical overview of the main versions (from YOLOv1 to YOLOv13), highlights key architectural trends, and surveys the principal application areas in which YOLO has been adopted. It also addresses evaluation practices, ethical considerations, and potential future directions for the framework's continued development. The analysis aims to provide a comprehensive and critical perspective on YOLO's trajectory and ongoing transformation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.01406</link>
<guid>https://arxiv.org/abs/2505.01406</guid>
<content:encoded><![CDATA[
arXiv:2505.01406v2 Announce Type: replace 
Abstract: Video diffusion models can generate realistic and temporally consistent videos. This raises concerns about provenance, ownership, and integrity. Watermarking can help address these issues by embedding metadata directly into the content. To work well, a watermark needs enough capacity for meaningful metadata. It must also stay imperceptible and remain robust to common video manipulations. Existing methods struggle with limited capacity, extra inference cost, or reduced visual quality. We introduce VidStamp, a watermarking framework that embeds frame-level messages through the decoder of a latent video diffusion model. The decoder is fine-tuned in two stages. The first stage uses static image datasets to encourage spatial message separation. The second stage uses synthesized video sequences to restore temporal consistency. This approach enables high-capacity watermarks with minimal perceptual impact. VidStamp also supports dynamic watermarking through a control signal that selects message templates during inference. This adds flexibility and creates a second channel for communication. We evaluate VidStamp on Stable Video Diffusion (I2V), OpenSora, and Wan (T2V). The system embeds 48 bits per frame while preserving visual quality and staying robust to common distortions. Compared with VideoSeal, VideoShield, and RivaGAN, it achieves lower log P-values and stronger detectability. Its frame-wise watermarking design also enables precise temporal tamper localization, with an accuracy of 0.96, which exceeds the VideoShield baseline. Code: https://github.com/SPIN-UMass/VidStamp
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.03329</link>
<guid>https://arxiv.org/abs/2505.03329</guid>
<content:encoded><![CDATA[
arXiv:2505.03329v4 Announce Type: replace 
Abstract: Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97\%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at https://github.com/AMAP-ML/FluxText.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.04899</link>
<guid>https://arxiv.org/abs/2505.04899</guid>
<content:encoded><![CDATA[
arXiv:2505.04899v2 Announce Type: replace 
Abstract: Recent advances in representation learning often rely on holistic embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging, where downstream tasks depend on anatomically interpretable features. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while enabling fine-grained control for targeted clinical applications. Experiments on CT and MRI datasets demonstrate OWT's power: it not only achieves strong performance on standard tasks like image reconstruction and segmentation, but also unlocks novel, high-impact clinical capabilities including organ-specific tumor identification, organ-level retrieval and semantic-level generation, without requiring any additional training. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and a new perspective on how representations can be leveraged.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Reinforcement Learning for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2505.17534</link>
<guid>https://arxiv.org/abs/2505.17534</guid>
<content:encoded><![CDATA[
arXiv:2505.17534v3 Announce Type: replace 
Abstract: This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at https://github.com/mm-vl/ULM-R1.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation</title>
<link>https://arxiv.org/abs/2505.22444</link>
<guid>https://arxiv.org/abs/2505.22444</guid>
<content:encoded><![CDATA[
arXiv:2505.22444v2 Announce Type: replace 
Abstract: The emergence of large-scale pre-trained point cloud models has significantly advanced 3D scene understanding, but adapting these models to specific downstream tasks typically demands full fine-tuning, incurring high computational and storage costs. Parameter-efficient fine-tuning (PEFT) techniques, successful in natural language processing and 2D vision tasks, would underperform when naively applied to 3D point cloud models due to significant geometric and spatial distribution shifts. Existing PEFT methods commonly treat points as orderless tokens, neglecting important local spatial structures and global geometric contexts in 3D modeling. To bridge this gap, we introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT module specifically designed for 3D point cloud transformers. GEM explicitly integrates fine-grained local positional encodings with a lightweight latent attention mechanism to capture comprehensive global context, thereby effectively addressing the spatial and geometric distribution mismatch. Extensive experiments demonstrate that GEM achieves performance comparable to or sometimes even exceeding full fine-tuning, while only updating 1.6% of the model's parameters, fewer than other PEFT methods. With significantly reduced training time and memory requirements, our approach thus sets a new benchmark for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point cloud models. Code is available at https://github.com/LiyaoTang/GEM.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos</title>
<link>https://arxiv.org/abs/2506.05274</link>
<guid>https://arxiv.org/abs/2506.05274</guid>
<content:encoded><![CDATA[
arXiv:2506.05274v2 Announce Type: replace 
Abstract: Composed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving, and provides 180K triplets drawn from FineGym and FineDiving datasets. Previous CoVR benchmarks, focusing on temporal aspect, link each query to a single target segment taken from the same video, limiting practical usefulness. In TF-CoVR, we instead construct each  pair by prompting an LLM with the label differences between clips drawn from different videos; every pair is thus associated with multiple valid target videos (3.9 on average), reflecting real-world tasks such as sports-highlight generation. To model these temporal dynamics, we propose TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video encoder on fine-grained action classification to obtain temporally discriminative embeddings; (ii) align the composed query with candidate videos using contrastive learning. We conduct the first comprehensive study of image, video, and general multimodal embedding (GME) models on temporally fine-grained composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and after fine-tuning raises the state-of-the-art from 19.83 to 27.22.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation</title>
<link>https://arxiv.org/abs/2506.09109</link>
<guid>https://arxiv.org/abs/2506.09109</guid>
<content:encoded><![CDATA[
arXiv:2506.09109v2 Announce Type: replace 
Abstract: As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, an evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 22% F1 points. Additionally, we construct two datasets for culturally universal concepts, one comprising T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering</title>
<link>https://arxiv.org/abs/2506.09920</link>
<guid>https://arxiv.org/abs/2506.09920</guid>
<content:encoded><![CDATA[
arXiv:2506.09920v4 Announce Type: replace 
Abstract: Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer</title>
<link>https://arxiv.org/abs/2506.18904</link>
<guid>https://arxiv.org/abs/2506.18904</guid>
<content:encoded><![CDATA[
arXiv:2506.18904v3 Announce Type: replace 
Abstract: Illumination and texture editing are critical dimensions for world-to-world transfer, which is valuable for applications including sim2real and real2real visual data scaling up for embodied AI. Existing techniques generatively re-render the input video to realize the transfer, such as video relighting models and conditioned world generation models. Nevertheless, these models are predominantly limited to the domain of training data (e.g., portrait) or fall into the bottleneck of temporal consistency and computation efficiency, especially when the input video involves complex dynamics and long durations. In this paper, we propose TC-Light, a novel generative renderer to overcome these problems. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible re-rendering results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.19072</link>
<guid>https://arxiv.org/abs/2506.19072</guid>
<content:encoded><![CDATA[
arXiv:2506.19072v2 Announce Type: replace 
Abstract: Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII compared to popular open-source VLMs. The code is available at https://github.com/yimuwangcs/wise-hawaii.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Measurement: Efficient Estimation at Scale</title>
<link>https://arxiv.org/abs/2507.01372</link>
<guid>https://arxiv.org/abs/2507.01372</guid>
<content:encoded><![CDATA[
arXiv:2507.01372v2 Announce Type: replace 
Abstract: AI has the potential to transform scientific discovery by analyzing vast datasets with little human effort. However, current workflows often do not provide the accuracy or statistical guarantees that are needed. We introduce active measurement, a human-in-the-loop AI framework for scientific measurement. An AI model is used to predict measurements for individual units, which are then sampled for human labeling using importance sampling. With each new set of human labels, the AI model is improved and an unbiased Monte Carlo estimate of the total measurement is refined. Active measurement can provide precise estimates even with an imperfect AI model, and requires little human effort when the AI model is very accurate. We derive novel estimators, weighting schemes, and confidence intervals, and show that active measurement reduces estimation error compared to alternatives in several measurement tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset</title>
<link>https://arxiv.org/abs/2507.22152</link>
<guid>https://arxiv.org/abs/2507.22152</guid>
<content:encoded><![CDATA[
arXiv:2507.22152v2 Announce Type: replace 
Abstract: Background Brain tumours are the most common solid malignancies in children, encompassing diverse histological, molecular subtypes and imaging features and outcomes. Paediatric brain tumours (PBTs), including high- and low-grade gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation offers promising tools for tumour delineation, yet its performance across heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A retrospective single-centre cohort of 174 paediatric patients with HGG, LGG, medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual annotations were provided for four tumour subregions: whole tumour (WT), T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D nnU-Net model was trained and tested (121/53 split), with segmentation performance assessed using the Dice similarity coefficient (DSC) and compared against intra- and inter-rater variability. Results The model achieved robust performance for WT and T2H (mean DSC: 0.85), comparable to human annotator variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2 alone produced results nearly equivalent to the full protocol. Conclusions DL is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and CC segmentation, highlighting the need for further refinement. These findings support the potential for protocol simplification and automation to enhance volumetric assessment and streamline paediatric neuro-oncology workflows.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion</title>
<link>https://arxiv.org/abs/2508.04559</link>
<guid>https://arxiv.org/abs/2508.04559</guid>
<content:encoded><![CDATA[
arXiv:2508.04559v2 Announce Type: replace 
Abstract: Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios - for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce OMFA (One Model For All), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. OMFA is inspired by language modeling, where generation is guided by conditioning prompts. However, our framework differs fundamentally from LLMs in two key aspects. First, it employs a bidirectional modeling paradigm that symmetrically allows prompting either from the garment to generate try-on results or from the dressed person to recover the try-off garment. Second, it strictly adheres to Tweedie's formula, enabling faithful estimation of the underlying data distribution during the denoising process. Instead of imposing lower body constraints, OMFA is an entirely mask-free framework that requires only a single portrait and a target garment as input, and is designed to support flexible outfit combinations and cross-person garment transfer, making it better aligned with practical usage scenarios. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical solution for virtual garment synthesis.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring</title>
<link>https://arxiv.org/abs/2508.07369</link>
<guid>https://arxiv.org/abs/2508.07369</guid>
<content:encoded><![CDATA[
arXiv:2508.07369v2 Announce Type: replace 
Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models pretrained on data from a specific sensor often generalize poorly to data from other sensors. Existing methods to tackle such cross-sensor degradation include retraining model or zero-shot methods, but they are highly time-consuming or even need extra training data. To address these challenges, our method first performs modular decomposition on deep learning-based pansharpening models, revealing a general yet critical interface where high-dimensional fused features begin mapping to the channel space of the final image. % may need revisement A Feature Tailor is then integrated at this interface to address cross-sensor degradation at the feature level, and is trained efficiently with physics-aware unsupervised losses. Moreover, our method operates in a patch-wise manner, training on partial patches and performing parallel inference on all patches to boost efficiency. Our method offers two key advantages: (1) $\textit{Improved Generalization Ability}$: it significantly enhance performance in cross-sensor cases. (2) $\textit{Low Generalization Cost}$: it achieves sub-second training and inference, requiring only partial test inputs and no external data, whereas prior methods often take minutes or even hours. Experiments on the real-world data from multiple datasets demonstrate that our method achieves state-of-the-art quality and efficiency in tackling cross-sensor degradation. For example, training and inference of $512\times512\times8$ image within $\textit{0.2 seconds}$ and $4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest setting on a commonly used RTX 3090 GPU, which is over 100 times faster than zero-shot methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phased One-Step Adversarial Equilibrium for Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.21019</link>
<guid>https://arxiv.org/abs/2508.21019</guid>
<content:encoded><![CDATA[
arXiv:2508.21019v2 Announce Type: replace 
Abstract: Video diffusion generation suffers from critical sampling efficiency bottlenecks, particularly for large-scale models and long contexts. Existing video acceleration methods, adapted from image-based techniques, lack a single-step distillation ability for large-scale video models and task generalization for conditional downstream tasks. To bridge this gap, we propose the Video Phased Adversarial Equilibrium (V-PAE), a distillation framework that enables high-quality, single-step video generation from large-scale video models. Our approach employs a two-phase process. (i) Stability priming is a warm-up process to align the distributions of real and generated videos. It improves the stability of single-step adversarial distillation in the following process. (ii) Unified adversarial equilibrium is a flexible self-adversarial process that reuses generator parameters for the discriminator backbone. It achieves a co-evolutionary adversarial equilibrium in the Gaussian noise space. For the conditional tasks, we primarily preserve video-image subject consistency, which is caused by semantic degradation and conditional frame collapse during the distillation training in image-to-video (I2V) generation. Comprehensive experiments on VBench-I2V demonstrate that V-PAE outperforms existing acceleration methods by an average of 5.8% in the overall quality score, including semantic alignment, temporal coherence, and frame quality. In addition, our approach reduces the diffusion latency of the large-scale video model (e.g., Wan2.1-I2V-14B) by 100 times, while preserving competitive performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement</title>
<link>https://arxiv.org/abs/2509.09232</link>
<guid>https://arxiv.org/abs/2509.09232</guid>
<content:encoded><![CDATA[
arXiv:2509.09232v3 Announce Type: replace 
Abstract: In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at https://github.com/jiesihu/Medverse.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</title>
<link>https://arxiv.org/abs/2509.12090</link>
<guid>https://arxiv.org/abs/2509.12090</guid>
<content:encoded><![CDATA[
arXiv:2509.12090v2 Announce Type: replace 
Abstract: Reconstructing cardiac motion from CMR sequences is critical for diagnosis, prognosis, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their applicability during intervention when only sparse observations are available. We present TetHeart, the first end-to-end framework for unified 4D heart mesh recovery from both offline full-stack and intra-procedural sparse-slice observations. Our method leverages deformable tetrahedra to capture shape and motion in a coherent space shared across cardiac structures. Before a procedure, it initializes detailed, patient-specific heart meshes from high-quality full stacks, which can then be updated using whatever slices can be obtained in real-time, down to a single one during the procedure. TetHeart incorporates several key innovations: (i) an attentive slice-adaptive 2D-3D feature assembly mechanism that integrates information from arbitrary numbers of slices at any position; (ii) a distillation strategy to ensure accurate reconstruction under extreme sparsity; and (iii) a weakly supervised motion learning scheme requiring annotations only at keyframes, such as the end-diastolic and end-systolic phases. Trained and validated on three large public datasets and evaluated zero-shot on additional private interventional and public datasets without retraining, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localized Region Guidance for Class Activation Mapping in WSSS</title>
<link>https://arxiv.org/abs/2509.12496</link>
<guid>https://arxiv.org/abs/2509.12496</guid>
<content:encoded><![CDATA[
arXiv:2509.12496v2 Announce Type: replace 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations. Existing WSSS methods struggle with precise object boundary localization and focus only on the most discriminative regions. To address these challenges, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement using object proposals to guide CAM generation, ensuring complete object coverage; (2) Influence Function Integration that captures the relationship between training samples and model predictions; and (3) Multi-Scale Boundary Enhancement with progressive refinement strategies. IG-CAM achieves state-of-the-art performance on PASCAL VOC 2012 with 82.3% mIoU before post-processing, improving to 86.6% after CRF refinement, significantly outperforming previous WSSS methods. Extensive ablation studies validate each component's contribution, establishing IG-CAM as a new benchmark for weakly supervised semantic segmentation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning</title>
<link>https://arxiv.org/abs/2509.13161</link>
<guid>https://arxiv.org/abs/2509.13161</guid>
<content:encoded><![CDATA[
arXiv:2509.13161v2 Announce Type: replace 
Abstract: Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models. Code will be open-sourced at https://github.com/ziHoHe/SMV-CR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</title>
<link>https://arxiv.org/abs/2509.21223</link>
<guid>https://arxiv.org/abs/2509.21223</guid>
<content:encoded><![CDATA[
arXiv:2509.21223v2 Announce Type: replace 
Abstract: Pre-training has proven effective for learning transferable features in sign language understanding (SLU) tasks. Recently, skeleton-based methods have gained increasing attention because they can robustly handle variations in subjects and backgrounds without being affected by appearance or environmental factors. Current SLU methods continue to face three key limitations: 1) weak semantic grounding, as models often capture low-level motion patterns from skeletal data but struggle to relate them to linguistic meaning; 2) imbalance between local details and global context, with models either focusing too narrowly on fine-grained cues or overlooking them for broader context; and 3) inefficient cross-modal learning, as constructing semantically aligned representations across modalities remains difficult. To address these, we propose Sigma, a unified skeleton-based SLU framework featuring: 1) a sign-aware early fusion mechanism that facilitates deep interaction between visual and textual modalities, enriching visual features with linguistic context; 2) a hierarchical alignment learning strategy that jointly maximises agreements across different levels of paired features from different modalities, effectively capturing both fine-grained details and high-level semantic relationships; and 3) a unified pre-training framework that combines contrastive learning, text matching and language modelling to promote semantic consistency and generalisation. Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement</title>
<link>https://arxiv.org/abs/2509.23584</link>
<guid>https://arxiv.org/abs/2509.23584</guid>
<content:encoded><![CDATA[
arXiv:2509.23584v2 Announce Type: replace 
Abstract: Video Face Enhancement (VFE) aims to restore high-quality facial regions from degraded video sequences, enabling a wide range of practical applications. Despite substantial progress in the field, current methods that primarily rely on video super-resolution and generative frameworks continue to face three fundamental challenges: (1) computational inefficiency caused by iterative multi-step denoising in diffusion models; (2) faithfully modeling intricate facial textures while preserving temporal consistency; and (3) limited model generalization due to the lack of high-quality face video training data. To address these challenges, we propose VividFace, a novel and efficient one-step diffusion framework for VFE. Built upon the pretrained WANX video generation model, VividFace reformulates the traditional multi-step diffusion process as a single-step flow matching paradigm that directly maps degraded inputs to high-quality outputs with significantly reduced inference time. To enhance facial detail recovery, we introduce a Joint Latent-Pixel Face-Focused Training strategy that constructs spatiotemporally aligned facial masks to guide optimization toward critical facial regions in both latent and pixel spaces. Furthermore, we develop an MLLM-driven automated filtering pipeline that produces MLLM-Face90, a meticulously curated high-quality face video dataset, ensuring models learn from photorealistic facial textures. Extensive experiments demonstrate that VividFace achieves superior performance in perceptual quality, identity preservation, and temporal consistency across both synthetic and real-world benchmarks. We will publicly release our code, models, and dataset to support future research.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization</title>
<link>https://arxiv.org/abs/2510.16146</link>
<guid>https://arxiv.org/abs/2510.16146</guid>
<content:encoded><![CDATA[
arXiv:2510.16146v2 Announce Type: replace 
Abstract: The limited availability of annotated data in medical imaging makes semi-supervised learning increasingly appealing for its ability to learn from imperfect supervision. Recently, teacher-student frameworks have gained popularity for their training benefits and robust performance. However, jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios. To address this for medical image segmentation, we propose DuetMatch, a novel dual-branch semi-supervised framework with asynchronous optimization, where each branch optimizes either the encoder or decoder while keeping the other frozen. To improve consistency under noisy conditions, we introduce Decoupled Dropout Perturbation, enforcing regularization across branches. We also design Pair-wise CutMix Cross-Guidance to enhance model diversity by exchanging pseudo-labels through augmented input pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose Consistency Matching, refining labels using stable predictions from frozen teacher models. Extensive experiments on benchmark brain MRI segmentation datasets, including ISLES2022 and BraTS, show that DuetMatch consistently outperforms state-of-the-art methods, demonstrating its effectiveness and robustness across diverse semi-supervised segmentation scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
<link>https://arxiv.org/abs/2510.20470</link>
<guid>https://arxiv.org/abs/2510.20470</guid>
<content:encoded><![CDATA[
arXiv:2510.20470v2 Announce Type: replace 
Abstract: Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding, yet still struggle with inaccurate evidence localization. To address these limitations, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies context and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we 1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that include frame identification, evidence reasoning, and action decision, and 2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to progressively incentivize multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long video understanding tasks, validating its strong scalability and robustness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of Multi-scale Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2510.27237</link>
<guid>https://arxiv.org/abs/2510.27237</guid>
<content:encoded><![CDATA[
arXiv:2510.27237v2 Announce Type: replace 
Abstract: Whole slide image (WSI) analysis has emerged as an increasingly essential technique in computational pathology. Recent advances in the pathology foundation models (FMs) have demonstrated significant advantages in deriving meaningful patch-level or slide-level multi-scale features from WSIs. However, current pathology FMs have exhibited substantial heterogeneity caused by diverse private training datasets and different network architectures. This heterogeneity introduces performance variability when we utilize the features from different FMs in the downstream tasks. To fully explore the advantages of multiple FMs effectively, in this work, we propose a novel framework for the fusion of multi-scale heterogeneous pathology FMs, called FuseCPath, yielding a model with a superior ensemble performance. The main contributions of our framework can be summarized as follows: (i) To guarantee the representativeness of the training patches, we propose a multi-view clustering-based method to filter out the discriminative patches via multiple FMs' embeddings. (ii) To effectively fuse the patch-level FMs, we devise a cluster-level re-embedding strategy to online capture patch-level local features. (iii) To effectively fuse the slide-level FMs, we devise a collaborative distillation strategy to explore the connections between slide-level FMs. Extensive experiments demonstrate that the proposed FuseCPath achieves state-of-the-art performance across multiple tasks on diverse datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks</title>
<link>https://arxiv.org/abs/2511.00396</link>
<guid>https://arxiv.org/abs/2511.00396</guid>
<content:encoded><![CDATA[
arXiv:2511.00396v2 Announce Type: replace 
Abstract: We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO's key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV</title>
<link>https://arxiv.org/abs/2511.06741</link>
<guid>https://arxiv.org/abs/2511.06741</guid>
<content:encoded><![CDATA[
arXiv:2511.06741v3 Announce Type: replace 
Abstract: Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</title>
<link>https://arxiv.org/abs/2511.09540</link>
<guid>https://arxiv.org/abs/2511.09540</guid>
<content:encoded><![CDATA[
arXiv:2511.09540v3 Announce Type: replace 
Abstract: Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</title>
<link>https://arxiv.org/abs/2511.10241</link>
<guid>https://arxiv.org/abs/2511.10241</guid>
<content:encoded><![CDATA[
arXiv:2511.10241v2 Announce Type: replace 
Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Discriminative Feature Learning for Deep Multi-View Clustering</title>
<link>https://arxiv.org/abs/2103.15069</link>
<guid>https://arxiv.org/abs/2103.15069</guid>
<content:encoded><![CDATA[
arXiv:2103.15069v3 Announce Type: replace-cross 
Abstract: Multi-view clustering is an important research topic due to its capability to utilize complementary information from multiple views. However, there are few methods to consider the negative impact caused by certain views with unclear clustering structures, resulting in poor multi-view clustering performance. To address this drawback, we propose self-supervised discriminative feature learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders are applied to learn embedded features for each view independently. To leverage the multi-view complementary information, we concatenate all views' embedded features to form the global features, which can overcome the negative impact of some views' unclear clustering structures. In a self-supervised manner, pseudo-labels are obtained to build a unified target distribution to perform multi-view discriminative feature learning. During this process, global discriminative information can be mined to supervise all views to learn more discriminative features, which in turn are used to update the target distribution. Besides, this unified target distribution can make SDMVC learn consistent cluster assignments, which accomplishes the clustering consistency of multiple views while preserving their features' diversity. Experiments on various types of multi-view datasets show that SDMVC outperforms 14 competitors including classic and state-of-the-art methods. The code is available at https://github.com/SubmissionsIn/SDMVC.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing DEFORMISE: A deep learning framework for dementia diagnosis in the elderly using optimized MRI slice selection</title>
<link>https://arxiv.org/abs/2407.17324</link>
<guid>https://arxiv.org/abs/2407.17324</guid>
<content:encoded><![CDATA[
arXiv:2407.17324v3 Announce Type: replace-cross 
Abstract: Dementia, a debilitating neurological condition affecting millions worldwide, presents significant diagnostic challenges. In this work, we introduce DEFORMISE, a novel DEep learning Framework for dementia diagnOsis of eldeRly patients using 3D brain Magnetic resonance Imaging (MRI) scans with Optimized Slice sElection. Our approach features a unique technique for selectively processing MRI slices, focusing on the most relevant brain regions and excluding less informative sections. This methodology is complemented by a confidence-based classification committee composed of three novel deep learning models. Tested on the Open OASIS datasets, our method achieved an impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore, validation on the ADNI dataset confirmed the robustness and generalizability of our approach. The use of explainable AI (XAI) techniques and comprehensive ablation studies further substantiate the effectiveness of our techniques, providing insights into the decision-making process and the importance of our methodology. This research offers a significant advancement in dementia diagnosis, providing a highly accurate and efficient tool for clinical applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEARNER: Contrastive Pretraining for Learning Fine-Grained Patient Progression from Coarse Inter-Patient Labels</title>
<link>https://arxiv.org/abs/2411.01144</link>
<guid>https://arxiv.org/abs/2411.01144</guid>
<content:encoded><![CDATA[
arXiv:2411.01144v2 Announce Type: replace-cross 
Abstract: Predicting whether a treatment leads to meaningful improvement is a central challenge in personalized medicine, particularly when disease progression manifests as subtle visual changes over time. While data-driven deep learning (DL) offers a promising route to automate such predictions, acquiring large-scale longitudinal data for each individual patient remains impractical. To address this limitation, we explore whether inter-patient variability can serve as a proxy for learning intra-patient progression. We propose LEARNER, a contrastive pretraining framework that leverages coarsely labeled inter-patient data to learn fine-grained, patient-specific representations. Using lung ultrasound (LUS) and brain MRI datasets, we demonstrate that contrastive objectives trained on coarse inter-patient differences enable models to capture subtle intra-patient changes associated with treatment response. Across both modalities, our approach improves downstream classification accuracy and F1-score compared to standard MSE pretraining, highlighting the potential of inter-patient contrastive learning for individualized outcome prediction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</title>
<link>https://arxiv.org/abs/2502.02054</link>
<guid>https://arxiv.org/abs/2502.02054</guid>
<content:encoded><![CDATA[
arXiv:2502.02054v2 Announce Type: replace-cross 
Abstract: This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
arXiv:2503.16356v3 Announce Type: replace-cross 
Abstract: Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they often fail to generalize these updates to multi-hop reasoning tasks that rely on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we find that current layer-localized KE approaches (e.g., MEMIT, WISE), which edit only single or a few model layers, inadequately integrate updated knowledge into these reasoning pathways. To address this limitation, we present CaKE (Circuit-aware Knowledge Editing), a novel method that enhances the effective integration of updated knowledge in LLMs. By only leveraging a few curated data samples guided by our circuit-based analysis, CaKE stimulates the model to develop appropriate reasoning circuits for newly incorporated knowledge. Experiments show that CaKE enables more accurate and consistent use of edited knowledge across related reasoning tasks, achieving an average improvement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while requiring less memory than existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title>
<link>https://arxiv.org/abs/2504.02132</link>
<guid>https://arxiv.org/abs/2504.02132</guid>
<content:encoded><![CDATA[
arXiv:2504.02132v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) is instrumental for inhibiting hallucinations in large language models (LLMs) through the use of a factual knowledge base (KB). Although PDF documents are prominent sources of knowledge, text-based RAG pipelines are ineffective at capturing their rich multi-modal information. In contrast, visual document RAG (VD-RAG) uses screenshots of document pages as the KB, which has been shown to achieve state-of-the-art results. However, by introducing the image modality, VD-RAG introduces new attack vectors for adversaries to disrupt the system by injecting malicious documents into the KB. In this paper, we demonstrate the vulnerability of VD-RAG to poisoning attacks targeting both retrieval and generation. We define two attack objectives and demonstrate that both can be realized by injecting only a single adversarial image into the KB. Firstly, we introduce a targeted attack against one or a group of queries with the goal of spreading targeted disinformation. Secondly, we present a universal attack that, for any potential user query, influences the response to cause a denial-of-service in the VD-RAG system. We investigate the two attack objectives under both white-box and black-box assumptions, employing a multi-objective gradient-based optimization approach as well as prompting state-of-the-art generative models. Using two visual document datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (vision language models), we show VD-RAG is vulnerable to poisoning attacks in both the targeted and universal settings, yet demonstrating robustness to black-box attacks in the universal setting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Filter-Based Common Components Modeling for Cross-Subject EEG Decoding</title>
<link>https://arxiv.org/abs/2507.05268</link>
<guid>https://arxiv.org/abs/2507.05268</guid>
<content:encoded><![CDATA[
arXiv:2507.05268v2 Announce Type: replace-cross 
Abstract: Brain-computer interface (BCI) technology enables direct communication between the brain and external devices through electroencephalography (EEG) signals. However, existing decoding models often mix common and personalized components, leading to interference from individual variability that limits cross-subject decoding performance. To address this issue, this paper proposes a system filter that extends the concept of signal filtering to the system level. The method expands a system into its spectral representation, selectively removes unnecessary components, and reconstructs the system from the retained target components, thereby achieving explicit system-level decomposition and filtering. We further integrate the system filter into a Cross-Subject Decoding framework based on the System Filter (CSD-SF) and evaluate it on the four-class motor imagery (MI) task of the BCIC IV 2a dataset. Personalized models are transformed into relation spectrums, and statistical testing across subjects is used to remove personalized components. The remaining stable relations, representing common components across subjects, are then used to construct a common model for cross-subject decoding. Experimental results show an average improvement of 3.28% in decoding accuracy over baseline methods, demonstrating that the proposed system filter effectively isolates stable common components and enhances model robustness and generalizability in cross-subject EEG decoding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep-GLS: Report-Guided Generalized Label Smoothing for Robust Disease Detection</title>
<link>https://arxiv.org/abs/2508.02495</link>
<guid>https://arxiv.org/abs/2508.02495</guid>
<content:encoded><![CDATA[
arXiv:2508.02495v3 Announce Type: replace-cross 
Abstract: Unlike nature image classification where groundtruth label is explicit and of no doubt, physicians commonly interpret medical image conditioned on certainty like using phrase "probable" or "likely". Existing medical image datasets either simply overlooked the nuance and polarise into binary label. Here, we propose a novel framework that leverages a Large Language Model (LLM) to directly mine medical reports to utilise the uncertainty relevant expression for supervision signal. At first, we collect uncertainty keywords from medical reports. Then, we use Qwen-3 4B to identify the textual uncertainty and map them into an adaptive Generalized Label Smoothing (GLS) rate. This rate allows our model to treat uncertain labels not as errors, but as informative signals, effectively incorporating expert skepticism into the training process. We establish a new clinical expert uncertainty-aware benchmark to rigorously evaluate this problem. Experiments demonstrate that our approach significantly outperforms state-of-the-art methods in medical disease detection. The curated uncertainty words database, code, and benchmark will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.09201</link>
<guid>https://arxiv.org/abs/2508.09201</guid>
<content:encoded><![CDATA[
arXiv:2508.09201v3 Announce Type: replace-cross 
Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks, posing serious safety risks. To address this, existing detection methods either learn attack-specific parameters, which hinders generalization to unseen attacks, or rely on heuristically sound principles, which limit accuracy and efficiency. To overcome these limitations, we propose Learning to Detect (LoD), a general framework that accurately detects unknown jailbreak attacks by shifting the focus from attack-specific learning to task-specific learning. This framework includes a Multi-modal Safety Concept Activation Vector module for safety-oriented representation learning and a Safety Pattern Auto-Encoder module for unsupervised attack classification. Extensive experiments show that our method achieves consistently higher detection AUROC on diverse unknown attacks while improving efficiency. The code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection</title>
<link>https://arxiv.org/abs/2502.15488</link>
<guid>https://arxiv.org/abs/2502.15488</guid>
<content:encoded><![CDATA[
<div> Quantization, Multi-view 3D Detection, Position Embedding, Lookup Table, Autonomous Driving<br /><br />Summary:<br /><br />Camera-based multi-view 3D detection methods like PETR achieve strong benchmark performance but suffer from high computational cost and memory usage, limiting real-world deployment. Directly applying traditional quantization techniques to PETRs causes significant accuracy loss due to two main challenges: the large magnitude difference between image features and camera-ray positional embeddings (PE), and the complexity and approximation errors introduced when quantizing nonlinear operators that require hardware-unfriendly computations. This paper introduces FQ-PETR, a fully quantized framework for PETRs with three key innovations addressing these issues. First, the Quantization-Friendly LiDAR-ray Position Embedding (QFPE) replaces multi-point sampling with a LiDAR-prior-guided single-point sampling and anchor-based embedding, removing problematic nonlinearities such as inverse-sigmoid and aligning the PE scale with image features to maintain accuracy. Second, the Dual-Lookup Table (DULUT) algorithm approximates complex nonlinear functions through two cascaded linear lookup tables, providing high-fidelity results with minimal entries and no reliance on specialized hardware. Third, Quantization After Numerical Stabilization (QANS) applies quantization following softmax numerical stabilization to reduce attention distortion caused by large inputs. Experiments on PETRs and their variants demonstrate that FQ-PETR with 8-bit weights and activations (W8A8) achieves near-floating-point accuracy, with only about 1% degradation, while reducing latency by up to 75%, outperforming existing post-training and quantization-aware training baselines. <div>
arXiv:2502.15488v4 Announce Type: replace 
Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video</title>
<link>https://arxiv.org/abs/2511.14848</link>
<guid>https://arxiv.org/abs/2511.14848</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D Motion Transfer, Semantic Correspondence, Multiview Video, 4D Reconstruction<br /><br />Summary:  
The paper introduces Gaussian See, Gaussian Do, a novel technique for semantic 3D motion transfer using multiview video data, enabling rig-free and cross-category motion transfer between objects with meaningful semantic correspondences. The method builds on implicit motion transfer techniques by extracting motion embeddings from source videos through condition inversion, which are then applied to rendered frames of static target shapes. These generated videos serve as supervision inputs for dynamic 3D Gaussian Splatting reconstruction, enabling realistic motion synthesis. A key innovation is the anchor-based view-aware motion embedding mechanism that ensures consistency across different viewpoints and speeds up the model convergence process. The paper also proposes a robust 4D reconstruction pipeline designed to consolidate noisy supervision videos effectively to improve reconstruction quality. Additionally, the authors establish the first benchmark dataset for semantic 3D motion transfer to facilitate standardized evaluation. Experimental results demonstrate that their approach outperforms adapted baseline methods, achieving superior motion fidelity and structural consistency in the reconstructed dynamic 3D scenes. Code and dataset have been made publicly available at the project’s website to encourage further research in this domain. <div>
arXiv:2511.14848v1 Announce Type: new 
Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation</title>
<link>https://arxiv.org/abs/2511.14860</link>
<guid>https://arxiv.org/abs/2511.14860</guid>
<content:encoded><![CDATA[
<div> Dental caries segmentation, panoramic radiographs, convolutional neural networks, vision transformers, medical image analysis  

<br /><br />Summary:  
1. This study benchmarks twelve state-of-the-art neural network architectures, including convolutional neural networks (CNNs), vision transformers, and state-space mamba models, for automated dental caries segmentation in panoramic radiographs using the DC1000 dataset.  
2. The architectures evaluated include VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, all trained under identical conditions to ensure fair comparison.  
3. Results reveal that the CNN-based DoubleU-Net outperformed all other models, achieving the highest dice coefficient (0.7345), mean Intersection over Union (mIoU) of 0.5978, and precision of 0.8145.  
4. Transformer and Mamba-based architectures, despite their advantages in modeling global context, underperformed due to challenges such as limited annotated data and weaker spatial priors, which are critical for this specific medical imaging task.  
5. The findings highlight that aligning model architecture with domain-specific task requirements is more important than solely increasing model complexity for effective medical image segmentation.  
6. The study provides valuable insights for future research and practical deployment in automated dental caries diagnosis, and the implementation code is openly available on GitHub. <div>
arXiv:2511.14860v1 Announce Type: new 
Abstract: Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?</title>
<link>https://arxiv.org/abs/2511.14870</link>
<guid>https://arxiv.org/abs/2511.14870</guid>
<content:encoded><![CDATA[
<div> CAD, Boundary Representation, Signed Distance Function, Latent Diffusion, Marching Cubes<br /><br />Summary:<br /><br />1. The paper introduces a novel geometric representation for CAD Boundary Representation (B-Rep) models using volumetric distance functions, named B-Rep Distance Functions (BR-DF).<br /><br />2. BR-DF encodes the surface mesh geometry of CAD models as signed distance functions (SDF) and represents vertices, edges, faces, along with their topology as per-face unsigned distance functions (UDFs).<br /><br />3. An extended version of the Marching Cubes algorithm is employed to convert BR-DF directly into watertight, faceted CAD B-Rep models, ensuring the conversion process never fails.<br /><br />4. The approach exploits the volumetric nature of BR-DF by using a multi-branch latent diffusion model with a 3D U-Net backbone to simultaneously generate the SDF and per-face UDFs that define the BR-DF.<br /><br />5. Experimental results demonstrate that the proposed method achieves comparable performance to state-of-the-art CAD generation techniques while attaining an unprecedented 100% success rate in producing valid faceted B-Rep models. <div>
arXiv:2511.14870v1 Announce Type: new 
Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis</title>
<link>https://arxiv.org/abs/2511.14884</link>
<guid>https://arxiv.org/abs/2511.14884</guid>
<content:encoded><![CDATA[
<div> Keywords: GeoSceneGraph, 3D scene synthesis, equivariant graph neural networks, text conditioning, indoor scene graphs<br /><br />Summary:<br /><br />This paper introduces GeoSceneGraph, a novel method for synthesizing indoor 3D scenes directly from text prompts, addressing limitations in existing approaches. Traditional generative models either train from scratch, ignoring inherent graph structures and resulting in less coherent scenes, or rely on vision-language models (VLMs) which, while effective, are often too large for resource-limited devices like XR glasses or mobile phones. GeoSceneGraph leverages the graph structure and geometric symmetries of indoor scenes without requiring predefined relationship classes or ground-truth annotations, thereby capturing diverse object interactions naturally. The method is built on equivariant graph neural networks (EGNNs), which preserve scene symmetries and relational information, but existing EGNNs are typically constrained by low-dimensional inputs and lack support for complex modalities such as text. To overcome this, the authors propose a simple yet effective strategy to condition EGNNs on high-dimensional text features, enabling the integration of rich semantic information from text prompts. Ablation studies validate the effectiveness of their text conditioning approach. Overall, GeoSceneGraph achieves scene generation quality comparable to methods relying on annotated relationships while being more flexible and suitable for deployment on resource-constrained devices. <div>
arXiv:2511.14884v1 Announce Type: new 
Abstract: Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation</title>
<link>https://arxiv.org/abs/2511.14897</link>
<guid>https://arxiv.org/abs/2511.14897</guid>
<content:encoded><![CDATA[
<div> MRI synthesis, Ultra-Low Field, High-Field, Implicit Neural Representation, Tissue-type SNR<br /><br />Summary:  
1. The paper introduces an unsupervised bidirectional MRI synthesis method that converts High-Field (HF) magnitude images into Ultra-Low Field (ULF)-like images and vice versa.  
2. The approach is physics-inspired, simulating contrast changes between HF and ULF MRIs by estimating tissue-type Signal-to-Noise Ratio (SNR) values to model the HF-to-ULF transformation accurately.  
3. For the Super-Resolution task, an Implicit Neural Representation (INR) network is used to generate HF images from ULF data, predicting tissue-type segmentation and image intensity simultaneously without requiring observed HF images.  
4. The method was evaluated qualitatively using synthetic ULF-like images generated from standard 3T T₁-weighted images, and quantitatively validated with paired 3T and 64mT T₁-weighted images.  
5. Results show significant improvements in white matter–gray matter (WM-GM) contrast: a 52% increase in synthetic ULF-like images and 37% in 64mT images. Sensitivity analyses confirm the forward model’s robustness to variations in target contrast, noise levels, and initial seeding. <div>
arXiv:2511.14897v1 Announce Type: new 
Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization</title>
<link>https://arxiv.org/abs/2511.14899</link>
<guid>https://arxiv.org/abs/2511.14899</guid>
<content:encoded><![CDATA[
<div> multi-view image editing, diffusion model, cross-view consistency, Score Distillation Sampling, 3D prior  

<br /><br />Summary:  
This paper tackles the challenge of multi-view image editing from sparse input views that consist of images capturing a scene from different viewpoints. The objective is to edit the scene based on a textual instruction while ensuring consistency across all views. Existing approaches relying on per-scene neural fields or temporal attention mechanisms often result in artifacts and incoherent edits in this scenario. The authors introduce InstructMix2Mix (I-Mix2Mix), a novel framework that distills the editing abilities of a 2D diffusion model into a pretrained multi-view diffusion model. This approach leverages the multi-view model’s data-driven 3D prior to maintain cross-view consistency. A major innovation is replacing the traditional neural field consolidator used in Score Distillation Sampling (SDS) with a multi-view diffusion student model. This change requires several new adaptations: incremental updates of the student model across denoising timesteps, a tailored teacher noise scheduler designed to prevent training degeneration, and a specialized attention modification that improves cross-view coherence without increasing computational cost. Experimental results demonstrate that I-Mix2Mix significantly enhances multi-view consistency while preserving high-quality edits in each individual view, overcoming limitations of prior methods. <div>
arXiv:2511.14899v1 Announce Type: new 
Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2511.14900</link>
<guid>https://arxiv.org/abs/2511.14900</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, dermatological diagnosis, textbook-based reasoning, reinforcement learning, diagnostic accuracy  

<br /><br />Summary:  
The paper addresses the limitations of current vision-language models (VLMs) in dermatological diagnosis, focusing on data heterogeneity, lack of grounded diagnostic rationales, and poor scalability/generalization. To overcome these challenges, the authors propose SkinR1, a novel dermatological VLM that integrates deep, textbook-based reasoning with reinforcement learning (RL) for improved clinical reasoning and diagnosis. First, SkinR1 uses a textbook-based reasoning generator to create high-quality, hierarchy-aware, and differential-diagnosis-informed reasoning trajectories, providing expert-level supervision for training. Second, these trajectories are employed in supervised fine-tuning (SFT) to ground the model’s diagnostic reasoning ability effectively. Third, the model adopts a new RL training paradigm that incorporates the hierarchical disease structure, enabling the transfer of learned reasoning patterns from small, richly annotated datasets to larger, sparsely annotated ones. Extensive experiments across multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy compared to previous approaches. An ablation study confirms the crucial role of the reasoning foundation established during supervised fine-tuning. The work highlights the potential of combining hierarchical expert knowledge with reinforcement learning to enhance the trustworthiness and clinical utility of vision-language models in dermatology. <div>
arXiv:2511.14900v1 Announce Type: new 
Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding</title>
<link>https://arxiv.org/abs/2511.14901</link>
<guid>https://arxiv.org/abs/2511.14901</guid>
<content:encoded><![CDATA[
<div> Keywords: FarSLIP, remote sensing, multi-granularity dataset, fine-grained alignment, vision-language models<br /><br />Summary: This paper addresses limitations in existing CLIP-based models for remote sensing (RS) by enhancing fine-grained region-text alignment. It identifies two main problems: current RS image-text datasets rely mainly on global captions generated from object-level labels, thus underutilizing detailed supervision; and conventional region-text alignment methods effective in general domains degrade performance when directly applied to RS data. To overcome these challenges, the authors introduce MGRS-200k, the first multi-granularity RS image-text dataset with rich object-level textual labels aimed at improving region-category alignment. They analyze current fine-grained tuning methods for CLIP and find that explicit patch-level alignment harms semantic coherence and reduces model effectiveness. Building on this insight, the paper proposes FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework featuring a novel patch-to-patch distillation approach that aligns local and global visual features, enhancing discriminability without compromising coherence. Furthermore, FarSLIP replaces explicit patch alignment with CLS token-based region-category alignment to better leverage region-text supervision and improve spatial awareness. Experiments demonstrate that FarSLIP achieves state-of-the-art results on RS open-vocabulary semantic segmentation and excels in zero-shot classification and image-text retrieval tasks. The dataset, code, and models are publicly available for research advancement. <div>
arXiv:2511.14901v1 Announce Type: new 
Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnMIL: A generalizable multiple instance learning framework for computational pathology</title>
<link>https://arxiv.org/abs/2511.14907</link>
<guid>https://arxiv.org/abs/2511.14907</guid>
<content:encoded><![CDATA[
<div> Computational pathology, multiple-instance learning, foundation models, slide-level prediction, uncertainty estimation<br /><br />Summary:<br /><br />1. The study addresses the challenge of aggregating patch-level features extracted by pathology foundation models from whole-slide images (WSIs) into reliable and generalizable slide-level clinical predictions.  
2. The authors propose nnMIL, a simple yet flexible multiple-instance learning framework that incorporates random sampling at both patch and feature levels, facilitating large-batch optimization, task-specific sampling, and scalable training across diverse datasets and model types.  
3. nnMIL uses a lightweight aggregator with sliding-window inference, enabling ensemble predictions at the slide level and robust uncertainty estimation, which is crucial for clinical decision-making.  
4. Extensive validation was performed on 40,000 WSIs covering 35 clinical tasks and involving four different pathology foundation models, where nnMIL outperformed existing MIL methods in disease diagnosis, histologic subtyping, molecular biomarker detection, and pan-cancer prognosis prediction.  
5. The method also demonstrated strong generalization across models, reliable uncertainty quantification, and effective survival stratification in external cohorts, highlighting its potential for real-world clinical AI applications.  
6. Overall, nnMIL offers a practical, generalizable solution that bridges the gap from patch-level foundation model features to clinically meaningful slide-level inferences, advancing AI integration in computational pathology. <div>
arXiv:2511.14907v1 Announce Type: new 
Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-WIN: Building Chest Radiograph World Model via Predictive Sensing</title>
<link>https://arxiv.org/abs/2511.14918</link>
<guid>https://arxiv.org/abs/2511.14918</guid>
<content:encoded><![CDATA[
<div> Keywords: Chest X-ray, 3D anatomy, world model, contrastive alignment, domain adaptation<br /><br />Summary:<br /><br />The article introduces X-WIN, a novel world model designed to enhance Chest X-ray (CXR) analysis by integrating 3D anatomical knowledge derived from chest computed tomography (CT) scans. Recognizing the limitation of CXRs as 2D projection images that obscure 3D structures due to superposition, X-WIN learns to predict 2D projections in latent space, effectively internalizing 3D anatomical information. Critical to its training is the affinity-guided contrastive alignment loss, which leverages mutual similarities between different projections of the same volume to capture rich, correlated information. To ensure better generalization and adaptability, X-WIN incorporates real CXR images through masked image modeling and utilizes a domain classifier to align the statistical properties of real and simulated CXRs. Extensive experimental evaluations demonstrate that X-WIN surpasses existing foundation models in various downstream tasks, both in linear probing and few-shot fine-tuning settings. Additionally, the model exhibits the capability to generate 2D projections that can be employed for reconstructing 3D CT volumes, pointing to its potential in bridging the gap between 2D and 3D medical imaging modalities. Overall, X-WIN represents a significant step toward improved disease diagnosis and representation learning from CXRs by embedding volumetric knowledge within a learned world model. <div>
arXiv:2511.14918v1 Announce Type: new 
Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CPSL: Representing Volumetric Video via Content-Promoted Scene Layers</title>
<link>https://arxiv.org/abs/2511.14927</link>
<guid>https://arxiv.org/abs/2511.14927</guid>
<content:encoded><![CDATA[
<div> Volumetric video, Scene layers, Novel-view synthesis, Parallax, Real-time playback<br /><br />Summary: This paper introduces Content-Promoted Scene Layers (CPSL), a novel 2.5D video representation designed to deliver immersive volumetric video experiences with greater efficiency. Unlike traditional volumetric methods which rely on expensive 3D capture and rendering techniques, CPSL leverages per-frame depth and content saliency to decompose each frame into a small set of geometry-consistent 2D layers. These layers are equipped with soft alpha bands and an edge-depth cache, ensuring occlusion ordering and boundary continuity are preserved. By encoding the scene into lightweight 2D assets, CPSL enables parallax-corrected novel-view synthesis through depth-weighted warping and alpha compositing, circumventing costly 3D reconstruction. The approach also ensures temporal coherence via motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Evaluations across multiple benchmarks demonstrate that CPSL achieves superior perceptual quality and boundary fidelity compared to existing layer-based and neural-field baselines. Furthermore, this method significantly reduces storage and rendering costs by several folds. Overall, CPSL presents a practical and scalable solution to bridge conventional 2D video with immersive 2.5D media experiences, enhancing accessibility for on-demand and real-time interactive applications. <div>
arXiv:2511.14927v1 Announce Type: new 
Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities</title>
<link>https://arxiv.org/abs/2511.14945</link>
<guid>https://arxiv.org/abs/2511.14945</guid>
<content:encoded><![CDATA[
<div> Periodic workflows, multimodal sequences, unsupervised detection, anomaly detection, benchmark  

<br /><br />Summary: This paper addresses the challenge of modeling long-term periodic human activities, which are common in areas such as manufacturing, sports, and daily life but have low-contrast patterns and complex workflows compared to short-term periodic activities. To tackle this, the authors introduce the first comprehensive benchmark dataset consisting of 580 multimodal human activity sequences that capture long-term periodic workflows. The benchmark is designed to support three practical evaluation tasks: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. Additionally, the authors propose a lightweight, training-free baseline method capable of modeling diverse periodic workflow patterns without requiring annotated data. Experimental results demonstrate that the benchmark poses significant difficulties for existing unsupervised methods and zero-shot techniques leveraging large language models, highlighting the need for improved approaches in this domain. Notably, the proposed baseline outperforms competing methods by a large margin across all three evaluation tasks. Furthermore, in real-world deployment scenarios, the baseline exhibits advantages comparable to traditional supervised workflow detection systems, offering benefits such as eliminating the necessity for annotations and retraining. The project related to this work is publicly accessible at https://sites.google.com/view/periodicworkflow. <div>
arXiv:2511.14945v1 Announce Type: new 
Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems</title>
<link>https://arxiv.org/abs/2511.14948</link>
<guid>https://arxiv.org/abs/2511.14948</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view video synchronization, LED Clock, spatiotemporal alignment, RGB and IR cameras, pose estimation<br /><br />Summary:<br /><br />1. This paper addresses the challenge of accurately synchronizing multiple heterogeneous cameras, including professional and consumer devices, and both visible (RGB) and infrared (IR) sensors, particularly in uncontrolled, real-world settings.  
2. The authors introduce a novel, low-cost synchronization method based on a custom-built LED Clock that uses red and infrared LEDs to visually encode timing information within recorded video frames.  
3. Their method decodes the exposure window start and end times from the video data to achieve millisecond-level temporal alignment without requiring hardware synchronization capabilities.  
4. Benchmarking results show a low residual synchronization error of 1.34 ms RMSE, outperforming existing techniques based on light signals, audio, or timecodes.  
5. The synchronization improvements enhance downstream multi-view computer vision tasks such as pose estimation and 3D reconstruction.  
6. The system is validated in large-scale, complex scenarios involving over 25 heterogeneous cameras in both IR and RGB modalities, including surgical recording environments.  
7. This approach simplifies synchronization workflows and broadens access to advanced vision sensing in diverse, unconstrained industrial and clinical applications. <div>
arXiv:2511.14948v1 Announce Type: new 
Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial intelligence approaches for energy-efficient laser cutting machines</title>
<link>https://arxiv.org/abs/2511.14952</link>
<guid>https://arxiv.org/abs/2511.14952</guid>
<content:encoded><![CDATA[
<div> Keywords: laser cutting, energy reduction, deep learning, material classification, smoke level detection<br /><br />Summary:<br /><br />This research tackles the challenges of high energy consumption and environmental impact in laser cutting processes by proposing novel deep learning (DL) methods aimed at reducing energy use. The study identifies a common issue in CO2 laser suction pumps — their open-loop control systems — and introduces a closed-loop system that dynamically adjusts pump power according to the material being cut and the smoke generated. To enable this adaptivity, two material classification approaches are developed: one using lens-less speckle sensing combined with a customized Convolutional Neural Network (CNN), and another employing a USB camera with transfer learning based on the pre-trained VGG16 CNN model. In addition, a separate deep learning model is designed to detect smoke levels in real time, further optimizing the pump’s power output. This integrated system allows the exhaust suction pump to automatically switch off during downtime and adjust power dynamically during operation. Experimental results demonstrate significant energy savings, ranging from 20% to 50% reduction in the pump’s energy consumption. The approach offers a substantial contribution towards sustainable manufacturing practices by enhancing energy efficiency and reducing environmental impact in laser cutting operations. <div>
arXiv:2511.14952v1 Announce Type: new 
Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects</title>
<link>https://arxiv.org/abs/2511.14970</link>
<guid>https://arxiv.org/abs/2511.14970</guid>
<content:encoded><![CDATA[
<div> Transparent objects, depth estimation, semantic segmentation, edge-guided fusion, multi-modal training<br /><br />Summary:<br /><br />This paper addresses the challenge of perceiving transparent objects in computer vision, which is difficult due to transparency affecting both depth estimation and semantic segmentation accuracy. The authors propose Edge-Guided Spatial Attention (EGSA), a novel fusion mechanism that leverages boundary information to better integrate semantic and geometric features, reducing negative interactions between these tasks. EGSA demonstrates consistent improvements over the current state-of-the-art method, MODEST, particularly in transparent regions, while maintaining competitive segmentation performance on Syn-TODD and ClearPose benchmarks. Additionally, the paper presents a multi-modal progressive training strategy that starts by learning from edges derived from RGB images and transitions to edges from predicted depth images. This approach enables the model to utilize the rich texture information in RGB images initially and later focus on more relevant geometric features in depth maps, importantly eliminating the need for ground-truth depth data during training. These contributions emphasize the effectiveness of edge-guided fusion and multi-stage training in enhancing the perception of transparent objects, offering a robust framework that improves depth accuracy without sacrificing segmentation quality. <div>
arXiv:2511.14970v1 Announce Type: new 
Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.14981</link>
<guid>https://arxiv.org/abs/2511.14981</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, feature-based losses, teacher-student models, latent representations, image classification<br /><br />Summary:<br /><br />This paper proposes a novel knowledge distillation (KD) framework that focuses exclusively on feature-based losses to train the student model’s backbone, completely omitting traditional logit-based losses like cross entropy. Unlike existing methods that leverage both logits and intermediate layer features, this approach solely utilizes latent representations during distillation. The authors introduce a knowledge quality metric grounded in recent insights into the geometry of latent representations, which helps identify the most effective teacher layers for transferring knowledge. Extensive experiments were conducted on three diverse image classification datasets using four different student-teacher combinations, including convolutional neural networks and vision transformers. These experiments demonstrate that the proposed method achieves state-of-the-art results, significantly outperforming standard KD approaches with top-1 accuracy improvements of up to 15%. The approach not only simplifies the distillation process by eliminating the need for logit-based losses but also highlights the importance of selective feature guidance from the teacher model. To promote further research, the authors have made their code publicly available at the provided GitHub repository. This work offers a new perspective on optimizing knowledge transfer in model compression and efficiency enhancement. <div>
arXiv:2511.14981v1 Announce Type: new 
Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
<link>https://arxiv.org/abs/2511.14993</link>
<guid>https://arxiv.org/abs/2511.14993</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.14993v1 Keywords: Kandinsky 5.0, image synthesis, video synthesis, foundation models, training pipeline<br /><br />Summary:<br /><br />This report introduces Kandinsky 5.0, a cutting-edge family of foundation models designed for high-resolution image generation and up to 10-second video synthesis. The framework consists of three main model lines: Kandinsky 5.0 Image Lite, featuring 6-billion parameter image generation models; Kandinsky 5.0 Video Lite, lightweight 2-billion parameter models for rapid text-to-video and image-to-video synthesis; and Kandinsky 5.0 Video Pro, a 19-billion parameter model line achieving superior video generation quality. The authors provide an in-depth overview of the data curation lifecycle including collection, processing, filtering, and clustering that supports the multi-stage training pipeline. This extensive pipeline involves comprehensive pre-training complemented by quality-enhancing techniques like self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. Additionally, the report details novel architectural, training, and inference optimizations enabling Kandinsky 5.0 models to generate outputs at high speed while maintaining state-of-the-art performance, as validated through human evaluation. As an open-source, large-scale generative framework, Kandinsky 5.0 is designed for adaptability across various generative tasks. The release of both training checkpoints and code aims to foster accessibility and accelerate progress in high-quality generative model research. <div>
arXiv:2511.14993v1 Announce Type: new 
Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation</title>
<link>https://arxiv.org/abs/2511.14998</link>
<guid>https://arxiv.org/abs/2511.14998</guid>
<content:encoded><![CDATA[
<div> Financial documents, OCR evaluation, fact-level benchmark, financial facts, LLM-as-Judge<br /><br />Summary:<br /><br />1. The paper introduces FinCriticalED, a visual benchmark designed specifically for evaluating OCR and vision-language models on financial documents at the fact level, addressing challenges in visually dense and table-heavy layouts common in these documents.<br />2. Traditional OCR metrics such as ROUGE and edit distance focus only on surface-level text similarity, which can miss critical errors like sign inversions or shifted dates that materially alter financial interpretations; FinCriticalED shifts evaluation towards domain-critical factual correctness.<br />3. The dataset contains 500 image-HTML pairs annotated by financial experts, covering over 700 numerical and temporal facts, ensuring high-quality annotations verified for accuracy in signs, magnitudes, and time expressions.<br />4. The authors develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for complex financial documents, enabling more precise and domain-aware assessment.<br />5. Benchmarking results show that although the strongest proprietary models achieve the highest factual accuracy, significant errors remain in complex numerical and temporal contexts, indicating room for improvement; through rigorous quantitative evaluation and expert case studies, FinCriticalED provides a solid foundation for advancing visual factual precision in finance and other precision-critical fields. <div>
arXiv:2511.14998v1 Announce Type: new 
Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification</title>
<link>https://arxiv.org/abs/2511.15016</link>
<guid>https://arxiv.org/abs/2511.15016</guid>
<content:encoded><![CDATA[
<div> Keywords: Lifelong person Re-Identification, Visible-Infrared modalities, Knowledge disentanglement, Cross-modal alignment, Catastrophic forgetting  

<br /><br />Summary:  
This paper addresses Lifelong person Re-Identification (LReID), particularly the Visible-Infrared Lifelong person Re-IDentification (VI-LReID) task, which involves continuously training models on visible and infrared data collected sequentially to achieve consistent matching across day and night. Existing approaches rely on cross-modal knowledge distillation to prevent catastrophic forgetting but neglect the conflict between modality-specific knowledge acquisition and modality-common knowledge preservation, causing collaborative forgetting. To overcome these challenges, the authors propose CKDA (Cross-modality Knowledge Disentanglement and Alignment), a novel framework that explicitly separates and preserves modality-specific and common knowledge in a balanced fashion. CKDA introduces two prompting modules: Modality-Common Prompting (MCP) and Modality-Specific Prompting (MSP), designed to disentangle and purify discriminative features unique or shared across modalities, thereby reducing mutual interference. Additionally, the Cross-modal Knowledge Alignment (CKA) module aligns the newly learned knowledge with previously acquired knowledge using dual-modality prototypes, independently considering both inter- and intra-modality feature spaces to ensure balanced retention. Extensive experiments on four benchmark datasets demonstrate CKDA’s effectiveness and superiority over state-of-the-art methods in lifelong VI-ReID scenarios. The source code is publicly available, promoting reproducibility and further research. <div>
arXiv:2511.15016v1 Announce Type: new 
Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex-Valued 2D Gaussian Representation for Computer-Generated Holography</title>
<link>https://arxiv.org/abs/2511.15022</link>
<guid>https://arxiv.org/abs/2511.15022</guid>
<content:encoded><![CDATA[
<div> Keywords: hologram representation, Gaussian primitives, differentiable rasterizer, VRAM optimization, phase-only holograms<br /><br />Summary:<br /><br />This paper introduces a novel hologram representation that utilizes structured complex-valued 2D Gaussian primitives instead of traditional per-pixel storage, significantly reducing the parameter search space by up to a factor of 10. To facilitate end-to-end learning, the authors develop a differentiable rasterizer compatible with their representation, which is combined with a GPU-optimized kernel designed for simulating free-space light propagation. Experimental results demonstrate that this approach reduces VRAM consumption by up to 2.5 times and speeds up the optimization process by 50%, all while delivering higher-quality holographic reconstructions compared to existing methods. Furthermore, the authors propose a conversion technique that adapts their representation to various practical hologram formats, including both smooth and random phase-only holograms. This conversion effectively suppresses noise artifacts commonly found in previous approaches. Overall, by minimizing the parameter search space, the proposed representation offers a more scalable solution for hologram estimation, paving the way for advancements in next-generation computer-generated holography systems. <div>
arXiv:2511.15022v1 Announce Type: new 
Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans</title>
<link>https://arxiv.org/abs/2511.15029</link>
<guid>https://arxiv.org/abs/2511.15029</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, developmental alignment, mathematical cognition, ResNet-50, numerical representation<br /><br />Summary:<br />1. The study explores how computer vision (CV) models, specifically ResNet-50, exhibit developmental alignment with human mathematical cognition, meaning their learning progression mirrors that of children.<br />2. Previous research had found that CV models trained for image classification develop latent geometric and numerical representations similar to adults, and this study extends that work by focusing on developmental trajectories.<br />3. For geometry and topology concepts, developmental alignment was observed in certain areas such as Euclidean Geometry, Geometrical Figures, Metric Properties, and Topology but not in others including Chiral Figures, Geometric Transformations, and Symmetrical Figures.<br />4. Regarding numerical cognition, the model demonstrated developmental alignment by forming a human-like "mental number line," a key aspect of numerical understanding, as training experience increased.<br />5. The findings highlight the potential of CV models as tools to understand mathematical development in humans and suggest future research avenues including different model architectures and expanding benchmark datasets. <div>
arXiv:2511.15029v1 Announce Type: new 
Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space</title>
<link>https://arxiv.org/abs/2511.15046</link>
<guid>https://arxiv.org/abs/2511.15046</guid>
<content:encoded><![CDATA[
<div> human-object interaction, HOI detection, generation, unified token space, semi-supervised learning  

<br /><br />Summary:  
The paper addresses human-object interaction (HOI) by unifying detection and generation tasks, which have been conventionally treated separately. This separation has limited the comprehensive understanding of interactions. To overcome this, the authors propose UniHOI, a framework that models HOI detection and generation together using a unified token space, facilitating effective knowledge sharing and better generalization across tasks. UniHOI introduces a symmetric interaction-aware attention module that enhances the joint modeling process. Additionally, a unified semi-supervised learning paradigm is designed to support bidirectional mapping between images and interaction semantics, even when annotation data is limited. Extensive experiments validate that UniHOI achieves state-of-the-art performance in both HOI detection and generation domains. It notably improves accuracy by 4.9% on long-tailed HOI detection benchmarks, demonstrating better handling of rare interaction categories. Furthermore, UniHOI substantially boosts interaction metrics by 42.0% in open-vocabulary generation tasks, indicating superior generalization to unseen or novel interactions. Overall, the approach advances integrated HOI understanding by effectively bridging visual and semantic interaction representations under limited supervision. <div>
arXiv:2511.15046v1 Announce Type: new 
Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method</title>
<link>https://arxiv.org/abs/2511.15052</link>
<guid>https://arxiv.org/abs/2511.15052</guid>
<content:encoded><![CDATA[
<div> Hyperspectral imaging, multispectral imaging, image fusion, spectral variability, low-rank decomposition<br /><br />Summary:<br /><br />This paper addresses the problem of fusing hyperspectral images (HSI) with multispectral images (MSI) to enhance the spatial resolution of HSI, focusing on challenges caused by inter-image variability—spectral variability and spatially localized changes arising from differing acquisition conditions. Unlike existing approaches that directly transform the images and potentially worsen the fusion problem's ill-posedness, the authors propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. This model first interprets spectral variability as changes in the spectral degradation operator. To recover spatial details lost due to localized changes, the target HSI is decomposed into low-rank and residual components, with the residual capturing fine details. Dimensionality reduction is performed on both components to exploit spectral correlations effectively. An implicit regularizer is introduced to incorporate spatial prior information, enhancing reconstruction quality. The optimization problem is solved via a Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where an external denoiser addresses the implicit regularizer subproblem. The authors provide a thorough convergence analysis and demonstrate through extensive experiments that DLRRF outperforms existing methods in handling inter-image variability during HSI and MSI fusion. <div>
arXiv:2511.15052v1 Announce Type: new 
Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues</title>
<link>https://arxiv.org/abs/2511.15054</link>
<guid>https://arxiv.org/abs/2511.15054</guid>
<content:encoded><![CDATA[
<div> Keywords: nuclei segmentation, knowledge distillation, student-teacher architecture, cross-tissue, semi-supervised learning<br /><br />Summary:  
Accurate nuclei segmentation in whole slide images (WSIs) of microscopy remains challenging due to variability in staining, imaging conditions, and tissue morphology across samples. To address these issues, the paper proposes CellGenNet, a knowledge distillation framework designed for robust cross-tissue cell segmentation under limited supervision. CellGenNet employs a student-teacher architecture where a high-capacity teacher network is first trained on sparse annotations and subsequently produces soft pseudo-labels for unlabeled image regions. The student network is then optimized using a joint objective combining ground-truth labels, teacher-generated probabilistic targets, and a hybrid loss function that integrates binary cross-entropy with Tversky loss. This hybrid loss allows asymmetric penalties to better tackle class imbalance and preserve minority nuclear structures effectively. Additionally, the framework incorporates consistency regularization and layerwise dropout to stabilize feature representations and encourage dependable feature transfer between teacher and student models. Extensive experiments conducted on diverse cancer tissue WSIs demonstrate that CellGenNet surpasses both fully supervised and semi-supervised baseline methods in segmentation accuracy and generalization. Overall, the approach facilitates scalable and reproducible histopathology analysis by efficiently leveraging limited labeled data alongside unlabeled regions through knowledge distillation. <div>
arXiv:2511.15054v1 Announce Type: new 
Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2511.15057</link>
<guid>https://arxiv.org/abs/2511.15057</guid>
<content:encoded><![CDATA[
<div> Keywords: universal ultrasound segmentation, semi-supervised learning, prompt-guided decoding, uncertainty-driven calibration, multi-organ dataset  

<br /><br />Summary:  
1. The paper addresses the limitation of existing ultrasound image segmentation methods, which are typically specialized for particular anatomical structures or tasks, hindering their widespread clinical application.  
2. It introduces the task of universal semi-supervised ultrasound image segmentation, aiming to develop a framework capable of segmenting multiple organs and handling diverse segmentation tasks simultaneously.  
3. The proposed method, ProPL, integrates a shared vision encoder with prompt-guided dual decoders that utilize a prompting-upon-decoding mechanism to flexibly adapt to different segmentation tasks.  
4. ProPL incorporates an uncertainty-driven pseudo-label calibration (UPLC) module to enhance self-training performance by refining pseudo-labels generated from unlabeled data.  
5. A novel, comprehensive ultrasound dataset covering 5 organs and 8 segmentation tasks is introduced to support research and benchmarking in universal ultrasound segmentation.  
6. Extensive experiments validate that ProPL consistently outperforms state-of-the-art methods across multiple metrics and tasks, setting a new benchmark in the domain of universal ultrasound image segmentation. <div>
arXiv:2511.15057v1 Announce Type: new 
Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Multimodal Large Language Models on Vertically Written Japanese Text</title>
<link>https://arxiv.org/abs/2511.15059</link>
<guid>https://arxiv.org/abs/2511.15059</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, Japanese OCR, Vertical writing, Document understanding, Synthetic dataset<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) have recently advanced and are increasingly applied to tasks involving visual document understanding across multiple languages, including Japanese. <br /><br />2. A key challenge in processing Japanese documents is the vertical writing style, common in many Japanese documents, which requires specialized model support. <br /><br />3. Existing research on handling vertically written Japanese text by MLLMs is limited, motivating the need for targeted evaluation and improvement. <br /><br />4. The study generates a synthetic OCR dataset containing both horizontally and vertically written Japanese text by rendering text into images, which is utilized for fine-tuning and evaluating MLLMs. Additionally, a real-world evaluation dataset with vertically written Japanese documents is created. <br /><br />5. Evaluation results show that current MLLMs underperform on vertically written text compared to horizontal text. However, fine-tuning models with the synthesized Japanese OCR dataset significantly improves their ability to read vertical writing, addressing a previously unmet capability. The datasets and code have been made publicly available to support ongoing research. <div>
arXiv:2511.15059v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks</title>
<link>https://arxiv.org/abs/2511.15065</link>
<guid>https://arxiv.org/abs/2511.15065</guid>
<content:encoded><![CDATA[
<div> Video Models, Spatial Reasoning, VR-Bench, Maze-Solving, Test-Time Scaling<br /><br />Summary:<br /><br />This work introduces VR-Bench, a comprehensive benchmark aimed at evaluating video models' reasoning capabilities specifically in spatial reasoning tasks. The benchmark consists of 7,920 procedurally generated videos across five distinct maze types with various visual styles, designed to require spatial planning and multi-step reasoning. Unlike discrete text corpora, videos provide continuous spatial and temporal information that is ideal for spatial reasoning, motivating the exploration of reasoning through video generation. The authors demonstrate that supervised fine-tuning (SFT) effectively enhances the reasoning abilities of video models. Experiments show that video models outperform leading vision-language models (VLMs) in spatial perception and generalize well across a range of scenarios, tasks, and complexity levels. Additionally, a notable test-time scaling effect is discovered, where applying diverse sampling during inference boosts the reliability of reasoning performance by 10–20%. Overall, the results highlight the unique potential and scalability of the reasoning-via-video paradigm, suggesting it as a promising direction for advancing spatial reasoning tasks using video-based models. <div>
arXiv:2511.15065v1 Announce Type: new 
Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching</title>
<link>https://arxiv.org/abs/2511.15066</link>
<guid>https://arxiv.org/abs/2511.15066</guid>
<content:encoded><![CDATA[
<div> Keywords: Bokeh rendering, depth-free, flow matching, cross-attention, controllable synthesis<br /><br />Summary:<br /><br />This paper introduces BokehFlow, a novel framework for rendering controllable bokeh effects in images without relying on depth information. Traditional and neural methods for bokeh rendering usually depend on accurate depth maps, while generative models often face limitations in controllability and computational efficiency. BokehFlow addresses these challenges by leveraging flow matching techniques to generate photorealistic shallow depth-of-field effects directly from all-in-focus images. A key innovation is the use of a cross-attention mechanism that allows users to exert semantic control over focus regions and blur intensity through text prompts, enhancing user interaction and customization. To facilitate model training and objective evaluation, the authors collect and synthesize four diverse datasets specifically tailored for bokeh effect learning. Extensive experiments reveal that BokehFlow not only produces visually compelling and realistic bokeh but also delivers more precise control compared to existing depth-dependent and generative approaches. Furthermore, the method surpasses others in efficiency, making it practical for real-world applications. Overall, BokehFlow represents a significant advance by removing the dependency on depth inputs while maintaining high-quality, user-controllable bokeh rendering. <div>
arXiv:2511.15066v1 Announce Type: new 
Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation</title>
<link>https://arxiv.org/abs/2511.15077</link>
<guid>https://arxiv.org/abs/2511.15077</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D single object tracking, LiDAR point clouds, high temporal variation, MambaTrack3D, computational efficiency

<br /><br />Summary: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds due to temporal redundancy and complex computation. Existing memory-based trackers commonly suffer from quadratic computational complexity and insufficient use of geometric priors, limiting their efficiency and performance. To address these issues, the paper proposes MambaTrack3D, a novel HTV-oriented tracking framework based on the Mamba state space model. A key innovation is the Mamba-based Inter-frame Propagation (MIP) module, which replaces traditional single-frame feature extraction with an inter-frame propagation strategy, achieving near-linear computational complexity while explicitly modeling spatial relationships across historical frames. Additionally, the Grouped Feature Enhancement Module (GFEM) separates foreground and background semantics at the channel level, reducing temporal redundancy in the memory bank to boost tracking accuracy and efficiency. Extensive experiments conducted on challenging HTV benchmarks such as KITTI-HTV and nuScenes-HTV show that MambaTrack3D consistently outperforms both HTV-specific and conventional trackers, achieving up to 6.5% improvement in success and 9.5% in precision compared to HVTrack under moderate temporal gaps. Furthermore, the method remains competitive on the standard KITTI dataset, demonstrating strong generalization ability across diverse tracking scenarios. Overall, MambaTrack3D delivers a superior accuracy-efficiency trade-off with robust performance in both specialized HTV and conventional environments. <div>
arXiv:2511.15077v1 Announce Type: new 
Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2511.15085</link>
<guid>https://arxiv.org/abs/2511.15085</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Emotion Recognition, inter-modal emotion conflicts, TiCAL, hyperbolic space, consistency estimation<br /><br />Summary:<br /><br />1. This article addresses the challenge of inter-modal emotion conflicts in Multimodal Emotion Recognition (MER), where different modalities (visual, auditory, textual) within the same sample may exhibit conflicting emotional cues.  
2. The authors propose a novel framework called Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise process of human emotion perception.  
3. TiCAL dynamically evaluates the consistency of training samples by using pseudo unimodal emotion labels combined with a typicality estimation, allowing the model to handle inconsistent emotional signals across modalities better.  
4. To enhance emotion representation, features are embedded into a hyperbolic space, which helps capture subtle and fine-grained distinctions between emotion categories more effectively than traditional embedding spaces.  
5. Experimental results on benchmark datasets like CMU-MOSEI and MER2023 demonstrate that TiCAL significantly improves recognition accuracy, particularly for samples with high modality inconsistency, achieving around a 2.6% performance boost over the previous state-of-the-art method DMD. <div>
arXiv:2511.15085v1 Announce Type: new 
Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis</title>
<link>https://arxiv.org/abs/2511.15092</link>
<guid>https://arxiv.org/abs/2511.15092</guid>
<content:encoded><![CDATA[
<div> Keywords: pose-guided human image generation, diffusion model, multi-view priors, appearance prior module, joint conditional injection<br /><br />Summary:<br /><br />1. Pose-guided human image generation faces significant challenges due to incomplete textures derived from single reference views and a lack of explicit cross-view interaction, leading to inconsistencies in generated images.  
2. The paper introduces the Jointly Conditioned Diffusion Model (JCDM), a novel diffusion-based framework designed to leverage multi-view priors for improved image synthesis.  
3. The Appearance Prior Module (APM) within JCDM creates a holistic identity-preserving prior by intelligently inferring missing texture and appearance details from incomplete input references, addressing the main limitation of single view inputs.  
4. The Joint Conditional Injection (JCI) mechanism enables effective fusion of cues from multiple views and injects a shared conditioning signal into the backbone denoising process, which helps align important visual attributes such as identity, color, and texture across different poses.  
5. JCDM is flexible, supporting variable numbers of reference views, and integrates seamlessly with standard diffusion backbones by introducing minimal and targeted architectural modifications. Experimental results demonstrate that JCDM achieves state-of-the-art fidelity and cross-view consistency, significantly advancing the quality and coherence of pose-guided human image generation. <div>
arXiv:2511.15092v1 Announce Type: new 
Abstract: Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.15098</link>
<guid>https://arxiv.org/abs/2511.15098</guid>
<content:encoded><![CDATA[
<div> Discrete diffusion-based multimodal large language models, visual token redundancy, pruning strategies, inference efficiency, multimodal understanding tasks  

<br /><br />Summary:  
This paper investigates efficiency challenges in discrete diffusion-based multimodal large language models (dMLLMs), which provide advantages like parallel decoding and bidirectional context modeling compared to autoregressive MLLMs but suffer from high computational costs due to full-sequence attention at each denoising step. The authors focus on visual token redundancy, finding it occurs predominantly in from-scratch dMLLMs tackling long-answer tasks. They analyze how pruning visual tokens to reduce redundancy impacts model performance and efficiency, showing that non-negligible information loss results from pruning, but only from-scratch dMLLMs can recover this information progressively during later denoising steps. The study further compares acceleration methods, revealing that layer-skipping benefits AR-to-diffusion dMLLMs, while progressive or late-step pruning is a more effective approach for from-scratch dMLLMs. Overall, the work sheds light on modality-specific redundancies and pruning effects, providing novel insights for optimizing inference speed without severely compromising accuracy, thus advancing the practical deployment of dMLLMs in various multimodal understanding applications. <div>
arXiv:2511.15098v1 Announce Type: new 
Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.15102</link>
<guid>https://arxiv.org/abs/2511.15102</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, Gaussian Blending, alpha blending, rendering artifacts

<br /><br />Summary: The paper addresses limitations in 3D Gaussian Splatting (3DGS), a recent technique that greatly improved novel view synthesis. Existing 3DGS methods produce noticeable visual artifacts when rendering views at sampling rates different from those used in training, specifically erosion-induced blurring when zooming in and dilation-induced staircase artifacts when zooming out. The authors identify the root cause as the conventional alpha blending approach that treats alpha and transmittance as scalar values over pixels. To overcome this, they introduce Gaussian Blending, a novel method that models alpha and transmittance as spatially varying distributions instead of scalars. This approach allows transmittance to be updated by considering the spatial distribution of alpha values within a pixel area, enabling contributions from nearby background splats to better preserve image details. Gaussian Blending integrates seamlessly with existing 3DGS and other novel view synthesis frameworks, requires no additional memory, and preserves real-time rendering speeds. Extensive experiments demonstrate that this method effectively captures fine details at both seen and unseen sampling rates, consistently outperforming state-of-the-art novel view synthesis models. This advancement addresses previous visual discrepancies and improves generalization across varying sampling resolutions. <div>
arXiv:2511.15102v1 Announce Type: new 
Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring</title>
<link>https://arxiv.org/abs/2511.15117</link>
<guid>https://arxiv.org/abs/2511.15117</guid>
<content:encoded><![CDATA[
<div> Keywords: elder care, event-triggered system, GMM background modeling, SVM machine learning, social media communication

<br /><br />Summary:  
This study focuses on monitoring both the physical and mental states of elderly individuals through an event-triggered system designed to detect specific events such as watch dog alerts, danger notices, and photo link communications. The system uses Gaussian Mixture Model (GMM) background modeling to identify motion behaviors of visitors and elders during watch dog and danger notice events. Experiments were conducted in home scenarios involving five families to detect and record these three event types based on real-life activities. Captured images from these events were further analyzed using Support Vector Machine (SVM) machine learning techniques to enhance event recognition. To address the elders' lack of technical experience, the system incorporates an intuitive operation method aligned with normal daily activities. This design facilitates communication between elders and their relatives through social media platforms, providing a seamless and natural interaction process. The approach aims to improve elder safety and emotional connections by combining automated event detection with user-friendly communication tools adapted to the needs of elderly users. <div>
arXiv:2511.15117v1 Announce Type: new 
Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation</title>
<link>https://arxiv.org/abs/2511.15118</link>
<guid>https://arxiv.org/abs/2511.15118</guid>
<content:encoded><![CDATA[
<div> Few-shot segmentation, Segment Anything Model, Unbiased Semantic Decoding, Contrastive Language-Image Pre-training, Visual-text prompt generator

<br /><br />Summary:  
This work addresses the challenges in few-shot segmentation by integrating the Segment Anything Model (SAM) with an Unbiased Semantic Decoding (USD) strategy. 1) Although SAM exhibits strong generalization and object-specific extraction capabilities, its decoding process depends heavily on accurate prompts, which previously focused mainly on the support set, leading to biased decoding on unknown classes. 2) The proposed USD strategy overcomes this limitation by extracting target information simultaneously from both the support and query images, facilitating more consistent predictions using semantic guidance from the Contrastive Language-Image Pre-training (CLIP) model. 3) Two feature enhancement strategies are designed to improve SAM’s semantic discrimination: a global supplement at the image level provides generalized category information from the support image, while a local guidance at the pixel level offers precise target location information from the query image. 4) To generate target-focused prompt embeddings, a learnable visual-text target prompt generator is introduced, which interacts between text embeddings and CLIP visual features. 5) Importantly, this framework enhances target attention without requiring re-training of the foundational vision models, exploiting semantic features to guide prompts enriched with target information, thus improving generalization and accuracy in few-shot segmentation tasks. <div>
arXiv:2511.15118v1 Announce Type: new 
Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images</title>
<link>https://arxiv.org/abs/2511.15132</link>
<guid>https://arxiv.org/abs/2511.15132</guid>
<content:encoded><![CDATA[
<div> Active learning, medical imaging, acquisition strategies, performance adaptation, annotation efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of reducing annotation costs in medical imaging by improving active learning methods. Traditional single acquisition strategies often show inconsistent performance throughout the active learning cycle. To overcome this, the authors introduce WaveFuse-AL, a novel framework that adaptively fuses multiple well-known acquisition strategies: BALD, BADGE, Entropy, and CoreSet. WaveFuse-AL employs cyclical (sinusoidal) temporal priors combined with performance-driven adaptation, allowing the dynamic adjustment of the importance of each strategy over time. This adaptive fusion approach balances exploration and exploitation to select the most informative samples more effectively. The framework is validated on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results reveal that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines. The improvements are statistically significant in ten out of twelve metric evaluations, demonstrating greater annotation efficiency under limited budget conditions. Overall, WaveFuse-AL offers a more robust and effective active learning framework adaptable to various stages of learning and diverse medical imaging tasks. <div>
arXiv:2511.15132v1 Announce Type: new 
Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging</title>
<link>https://arxiv.org/abs/2511.15151</link>
<guid>https://arxiv.org/abs/2511.15151</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Curriculum Learning, Spatiotemporal Encoding, Approximate Rank Pooling, Neuroimaging, Brain Disease Classification<br /><br />Summary: The paper introduces Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an innovative end-to-end framework designed to enhance high-dimensional neuroimaging analyses for clinical diagnosis. The central component of this approach is data-driven spatiotemporal encoding (DaSE), which uses Approximate Rank Pooling (ARP) to transform complex three-dimensional brain volumetric data into compact and informative two-dimensional dynamic representations. This dimensionality reduction facilitates efficient processing while preserving critical spatial and temporal information. DCL-SE incorporates a dynamic curriculum learning strategy governed by a Dynamic Group Mechanism (DGM) that progressively trains the model's decoder. This curriculum guides the model from capturing broad, global anatomical structures to identifying detailed and fine pathological features, improving feature extraction quality. The framework is extensively evaluated on six publicly available neuroimaging datasets, spanning tasks such as Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction. Across these diverse applications, DCL-SE consistently outperforms existing state-of-the-art methods in terms of accuracy, robustness, and interpretability of the results. The study highlights the value of tailored, compact architectures that are optimized for specific tasks, providing an effective alternative to large-scale general-purpose pretrained networks in neuroimaging contexts. <div>
arXiv:2511.15151v1 Announce Type: new 
Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection</title>
<link>https://arxiv.org/abs/2511.15153</link>
<guid>https://arxiv.org/abs/2511.15153</guid>
<content:encoded><![CDATA[
<div> Keywords: HD maps, 3D point cloud updating, urban change detection, SceneEdited dataset, autonomous navigation<br /><br />Summary:  
1. The paper addresses the issue of maintaining accurate, up-to-date High-Definition (HD) maps, which are essential for urban planning, infrastructure monitoring, and autonomous navigation.  
2. It highlights the challenge that current change detection methods face when it comes to updating 3D maps, especially those relying on 2D image-based change detection, creating a gap between detection and map updating.  
3. To bridge this gap, the authors introduce SceneEdited, the first large-scale dataset specifically designed for city-wide HD map maintenance through 3D point cloud updates.  
4. SceneEdited encompasses over 800 scenes covering 73 km of driving and about 3 km² of urban area, featuring more than 23,000 synthesized object changes including missing roadside infrastructure, buildings, overpasses, and utility poles, generated both manually and automatically with over 2000 outdated scene versions.  
5. Each scene provides calibrated RGB images, LiDAR scans, and detailed change masks, making it suitable for training and evaluating change detection and map updating methods.  
6. The authors also provide baseline methods based on image-based structure-from-motion pipelines and a comprehensive toolkit designed to support scalability, trackability, and portability for expanding the dataset and unifying annotations.  
7. Both the dataset and toolkit are publicly released on GitHub, establishing SceneEdited as a standardized benchmark for 3D HD map updating research. <div>
arXiv:2511.15153v1 Announce Type: new 
Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2511.15159</link>
<guid>https://arxiv.org/abs/2511.15159</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical training, feedback generation, Instrument-Action-Target triplets, video recognition, GPT-4o<br /><br />Summary:<br /><br />1. The paper addresses the critical need for high-quality intraoperative feedback in surgical training to improve trainee skills and long-term acquisition. 2. It proposes a structure-aware pipeline that learns a surgical action ontology derived from real trainer-to-trainee feedback transcripts covering 33 surgeries, used to inform feedback generation. 3. The authors contribute by mining Instrument-Action-Target (IAT) triplets from feedback text and normalizing these into clustered categories, fine-tuning a video-to-IAT recognition model that incorporates surgical context and temporal instrument motions. 4. The study demonstrates how conditioning GPT-4o on IAT triplets enhances the generation of clinically relevant, trainer-style feedback. 5. Experimental results show that context and temporal tracking improve video-to-IAT recognition AUC scores (Instrument from 0.67 to 0.74, Action from 0.60 to 0.63, Tissue from 0.74 to 0.79). 6. For feedback text generation, GPT-4o conditioned on IAT triplets improved average fidelity scores from 2.17 to 2.44 (+12.4%) with admissible feedback doubling from 21% to 42%. 7. Traditional text metrics also improved, with word error rates decreasing by 15-31% and ROUGE scores rising by 9-64%. 8. Grounding feedback generation in explicit IAT structures not only improves fidelity but also produces clinician-verifiable rationales, supporting transparency and auditability in surgical training feedback. <div>
arXiv:2511.15159v1 Announce Type: new 
Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance</title>
<link>https://arxiv.org/abs/2511.15164</link>
<guid>https://arxiv.org/abs/2511.15164</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal continual learning, catastrophic forgetting, gradient approximation, parameter space geometry, replay buffer<br /><br />Summary:<br /><br />This paper addresses the challenge of catastrophic forgetting in multimodal continual instruction tuning, which occurs when models lose performance on previously learned tasks while adapting to new ones. The authors present a novel perspective that treats catastrophic forgetting as a consequence of missing gradients from old tasks during the learning of new tasks. To counter this, they propose approximating these missing gradients by exploiting the geometric relationship between current model parameters and parameters optimized for previous tasks, specifically using the directional vector connecting them as a surrogate gradient. Furthermore, their method integrates this approximated gradient with real gradients obtained from a limited replay buffer containing samples from past tasks. A Bernoulli sampling strategy is introduced to dynamically balance the trade-off between maintaining model stability (retaining old knowledge) and plasticity (learning new knowledge). Extensive experiments on multimodal continual instruction tuning benchmarks validate that their approach achieves state-of-the-art performance, mitigating forgetting effectively without increasing the model size, thus preserving a compact and efficient architecture. This work provides a new direction for continual learning by framing missing gradient recovery through the lens of parameter space geometry, enabling robust continual adaptation in large multimodal language models. <div>
arXiv:2511.15164v1 Announce Type: new 
Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation</title>
<link>https://arxiv.org/abs/2511.15167</link>
<guid>https://arxiv.org/abs/2511.15167</guid>
<content:encoded><![CDATA[
<div> Depth estimation, self-supervised learning, contrastive learning, adverse weather, robustness<br /><br />Summary:<br /><br />1. This paper addresses the significant challenge of self-supervised depth estimation performance degradation in adverse weather conditions like rain and fog, where visibility reduction critically impairs depth prediction accuracy. <br />2. The authors propose a novel framework named SEC-Depth, which introduces a self-evolution contrastive learning strategy aimed at robust depth estimation under challenging environmental conditions. <br />3. SEC-Depth constructs temporally evolving latency models by leveraging intermediate parameters generated during the training process to capture optimization states across different training stages through a dynamic update strategy. <br />4. The core innovation is the self-evolution contrastive loss (SECL), which uses outputs from historical latency models as negative samples, enabling adaptive adjustment of learning objectives and implicitly sensing weather degradation severity without the need for manual intervention. <br />5. Experiments demonstrate that SEC-Depth integrates well with a variety of existing baseline models and substantially improves the robustness of depth estimation in zero-shot evaluation settings, confirming its effectiveness for autonomous driving and robotics applications in adverse weather. <div>
arXiv:2511.15167v1 Announce Type: new 
Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction</title>
<link>https://arxiv.org/abs/2511.15179</link>
<guid>https://arxiv.org/abs/2511.15179</guid>
<content:encoded><![CDATA[
<div> Human Motion Prediction, Multimodality, Metric, Clustering, Validity<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating Human Motion Prediction (HMP) methods that produce multiple possible future motions from a single past motion sequence. Existing evaluation metrics often fail to properly assess probabilistic predictions, as they do not sufficiently consider the distribution of predicted motions across diverse motion modes or their kinematic validity. To overcome these shortcomings, the authors introduce a novel metric called Multimodality-aware Metric using Clustering-based Modes (MMCM). MMCM assesses two key criteria: (a) coverage, by clustering motion space into distinct modes to check if predicted motions spread among multiple plausible futures, and (b) validity, by verifying that predicted motions align with kinematically valid future motions observed in a real dataset. The clustering approach ensures sensible grouping of motion modes, which allows MMCM to explicitly evaluate the diversity and realism of motion predictions. Experimental results demonstrate that MMCM provides a more accurate and insightful evaluation of multimodal human motion predictions compared to previous methods. The proposed metric thereby facilitates better development and assessment of stochastic HMP models. The code for MMCM is publicly available, enabling broader use and validation in the research community. <div>
arXiv:2511.15179v1 Announce Type: new 
Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</title>
<link>https://arxiv.org/abs/2511.15186</link>
<guid>https://arxiv.org/abs/2511.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: lesion segmentation, chest X-rays, instruction-guided segmentation, MIMIC-ILS dataset, vision-language model<br /><br />Summary:<br /><br />1. The paper addresses limitations in current chest X-ray (CXR) lesion segmentation models caused by a small set of target labels and dependence on complex expert-level text inputs, which hinder practical applications.<br /><br />2. The authors propose a new approach called instruction-guided lesion segmentation (ILS), which allows segmenting diverse lesion types using simple, user-friendly instructions.<br /><br />3. To support this paradigm, they develop MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, created through an automated multimodal pipeline that extracts annotations from chest X-ray images and corresponding medical reports.<br /><br />4. MIMIC-ILS comprises 1.1 million instruction-answer pairs from 192,000 images and 91,000 unique segmentation masks, covering seven major lesion categories.<br /><br />5. To validate the dataset's utility, the authors introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS that can accurately segment different lesion types and generate textual explanations based on user instructions.<br /><br />6. Experimental results demonstrate high performance in segmentation accuracy and textual output quality, underscoring the effectiveness of the proposed pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level lesion grounding in CXR images. <div>
arXiv:2511.15186v1 Announce Type: new 
Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</title>
<link>https://arxiv.org/abs/2511.15188</link>
<guid>https://arxiv.org/abs/2511.15188</guid>
<content:encoded><![CDATA[
<div> Brain age estimation, Vision Transformer, Residual CNN, MRI, Neurodegeneration<br /><br />Summary:<br /><br />1. This paper introduces BrainRotViT, a hybrid model combining Vision Transformer (ViT) global context encoding and residual CNN local refinement for brain age estimation from structural MRI.  
2. A ViT encoder is first trained on age and sex classification at the slice level, then frozen to generate embeddings from sagittal slices, which are input to a residual CNN regressor incorporating subject sex to predict continuous brain age.  
3. The model achieves strong performance with a mean absolute error (MAE) of 3.34 years, Pearson r=0.98, Spearman ρ=0.97, and R²=0.95 on validation using 11 MRI datasets from over 130 acquisition sites, outperforming baseline and state-of-the-art methods.  
4. It generalizes well to four independent cohorts, with MAEs between 3.77 and 5.04 years.  
5. Analysis of brain age gap reveals links to Alzheimer's disease, cognitive impairment, and autism spectrum disorder.  
6. Attention maps highlight brain regions associated with aging, including the cerebellar vermis, precentral/postcentral gyri, temporal lobes, and medial superior frontal gyrus.  
7. The study presents an interpretable, efficient, and generalizable approach filling the gap between CNN and transformer methods, advancing research in aging and neurodegeneration. <div>
arXiv:2511.15188v1 Announce Type: new 
Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $\rho=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition</title>
<link>https://arxiv.org/abs/2511.15197</link>
<guid>https://arxiv.org/abs/2511.15197</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot generation, object composition, style disentanglement, masked-attention, dataset curation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of inserting real-world objects into stylized domains, a task where existing reference-based object composition methods fail. 2. It introduces "Insert In Style," the first zero-shot generative framework that combines practical usability with high-fidelity results without needing per-subject finetuning or text prompts. 3. The key innovations include a multi-stage training protocol that disentangles identity, style, and composition representations, and a specialized masked-attention architecture that enforces this disentanglement during the generation process, preventing concept interference common in unified-attention models. 4. The framework is trained on a newly created 100k-sample dataset, obtained via a large-scale generation pipeline coupled with a rigorous two-stage filtering process to ensure both semantic identity and style coherence. 5. A new public benchmark for stylized composition is introduced, and the method demonstrates state-of-the-art performance, significantly outperforming prior techniques on identity and style metrics, supported by user studies. <div>
arXiv:2511.15197v1 Announce Type: new 
Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval</title>
<link>https://arxiv.org/abs/2511.15201</link>
<guid>https://arxiv.org/abs/2511.15201</guid>
<content:encoded><![CDATA[
<div> keywords: cross-modal retrieval, causal bias, food image, recipe representation, ingredient confounder  

<br /><br />Summary:  
1. The paper tackles the problem of learning effective representations for recipes and food images in cross-modal retrieval tasks.  
2. It points out that existing methods treating recipes purely as textual descriptions of dish appearance introduce bias, as the visual representation of a dish may not fully correspond to all recipe details due to factors like cooking methods and presentation.  
3. The authors introduce a causal theory perspective, identifying ingredients as confounders that create bias in similarity judgments between images and recipes.  
4. They propose a causal intervention approach with backdoor adjustment to remove this bias, reformulating the food-to-recipe retrieval model to include an additional corrective term.  
5. Empirical results on the Recipe1M dataset demonstrate that their approach achieves near-oracle performance (MedR=1) at various test sizes (1K, 10K, and 50K), surpassing previous state-of-the-art methods.  
6. Additionally, they develop a plug-and-play neural module, a multi-label ingredient classifier, to facilitate debiasing in retrieval models.  
7. Overall, their method improves the robustness and accuracy of cross-modal recipe-image retrieval by explicitly accounting for the causal relationship and bias inherent in the task. <div>
arXiv:2511.15201v1 Announce Type: new 
Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Based Benchmarking Metrics for Multimodal Synthetic Images</title>
<link>https://arxiv.org/abs/2511.15204</link>
<guid>https://arxiv.org/abs/2511.15204</guid>
<content:encoded><![CDATA[
<div> Physics-Constrained Multimodal Data Evaluation, large language models, vision-language models, semantic accuracy, physics-guided reasoning<br /><br />Summary:<br /><br />This paper addresses the limitations of current evaluation metrics such as BLEU, CIDEr, VQA score, SigLIP-2, and CLIPScore, which often fail to capture semantic or structural accuracy, especially in domain-specific or context-dependent applications. The authors propose a novel Physics-Constrained Multimodal Data Evaluation (PCMDE) metric designed to overcome these deficiencies by integrating large language models (LLMs), knowledge-based mapping, and vision-language models (VLMs). The PCMDE architecture consists of three key stages: first, feature extraction through object detection and VLMs to gather spatial and semantic multimodal information; second, a Confidence-Weighted Component Fusion mechanism that adaptively validates each component using confidence scores, ensuring robust fusion of extracted features; and third, physics-guided reasoning leveraging LLMs to enforce structural and relational constraints such as alignment, positional consistency, and coherence. This combination aims to enhance the accuracy and reliability of multimodal data evaluation by incorporating domain knowledge and reasoning capabilities beyond superficial matching. Ultimately, PCMDE represents a significant step toward semantically and structurally aware evaluation metrics that are better suited to nuanced, domain-specific scenarios, enabling more meaningful assessments of generated data in multimodal contexts. <div>
arXiv:2511.15204v1 Announce Type: new 
Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning</title>
<link>https://arxiv.org/abs/2511.15242</link>
<guid>https://arxiv.org/abs/2511.15242</guid>
<content:encoded><![CDATA[
<div> SkinGPT-R1, dermatology, chain of thought, DermCoT, DermBench<br /><br />Summary:<br /><br />1. The paper introduces SkinGPT-R1, a vision language model specialized for dermatology that explicitly performs diagnostic chain of thought reasoning step-by-step, making the reasoning process verifiable.<br />2. To facilitate skin-specific reasoning, the authors create DermCoT, a dataset combining 10,000 filtered training cases from DermEval with 3,000 certified cases scored by dermatologists, ensuring high-quality standardized dermatologic narratives.<br />3. DermEval is defined as a physician-aligned six-dimensional evaluator, and DermBench is introduced as the corresponding benchmark designed to assess the quality of dermatologic chain of thought reasoning.<br />4. On DermBench, SkinGPT-R1 achieves a leading average score of 4.031 out of 5 across the six clinician-defined dimensions, outperforming 14 other vision and medical vision language models, and improving average performance over the previous Vision-R1 model by approximately 41%.<br />5. SkinGPT-R1 also demonstrates stable accuracy improvements on three separate dermatology classification benchmarks, maintaining strong competitiveness with other advanced vision language models.<br />6. Ablation studies confirm that chain of thought supervision based on DermCoT significantly enhances model performance, and incorporating dermatology-aware visual distillation yields consistent additional benefits in both narrative explanation quality and classification accuracy. <div>
arXiv:2511.15242v1 Announce Type: new 
Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplitFlux: Learning to Decouple Content and Style from a Single Image</title>
<link>https://arxiv.org/abs/2511.15258</link>
<guid>https://arxiv.org/abs/2511.15258</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, content-style disentanglement, SplitFlux, LoRA fine-tuning, SDXL

<br /><br />Summary:  
This paper addresses the challenge of disentangling image content and style for customized image generation, noting limitations in current SDXL-based methods and the Flux model. Through systematic analysis of Flux, two main observations are made: Single Dream Blocks are crucial for generation, with early blocks focusing on content control and later blocks governing style. Building on these insights, the authors propose SplitFlux, which fine-tunes Single Dream Blocks using LoRA to effectively separate content and style, allowing content to be re-embedded into new contexts. SplitFlux introduces two key components: Rank-Constrained Adaptation compresses update rank and amplifies magnitudes within certain blocks to maintain content identity and prevent content leakage into style blocks; Visual-Gated LoRA splits content LoRA into two branches guided by image saliency— a high-rank branch retains main subject details, and a low-rank branch encodes residual information, reducing overfitting and supporting smooth re-embedding. Extensive experimental results demonstrate SplitFlux’s superiority over state-of-the-art methods, showing better content preservation and stylization quality across various scenarios. The approach offers a robust solution for high-quality, disentangled image generation by effectively separating and controlling content and style components. <div>
arXiv:2511.15258v1 Announce Type: new 
Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Query Networks for Object Detection with Automotive Radar</title>
<link>https://arxiv.org/abs/2511.15271</link>
<guid>https://arxiv.org/abs/2511.15271</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D radar, object detection, Graph Query Networks, relational reasoning, NuScenes dataset<br /><br />Summary: This paper addresses the challenge of object detection using 3D radar for automotive perception, focusing on the difficulties caused by radar's sparse and irregular reflections due to long wavelengths. It introduces Graph Query Networks (GQN), an innovative attention-based framework that represents radar-detected objects as graphs to better extract relational and contextual features. A key innovation is the concept of graph queries, which dynamically attend to the bird's-eye view (BEV) space to build tailored object-specific graphs. The framework includes two novel modules: EdgeFocus, which enhances relational reasoning between graph nodes, and DeepContext Pooling, which aggregates contextual information effectively. Experimental results on the NuScenes dataset demonstrate that GQN significantly boosts relative mean Average Precision (mAP) by up to 53%, achieving an 8.2% improvement over the strongest previous radar-based detection methods. Additionally, the approach reduces the peak graph construction overhead by 80%, making it computationally efficient while maintaining moderate FLOPs. Overall, this work advances 3D radar object detection by combining graph-based attention mechanisms and efficient contextual modeling, leading to more accurate and faster radar perception in autonomous driving scenarios. <div>
arXiv:2511.15271v1 Announce Type: new 
Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Centric Relational Reasoning for 3D Scene Graph Prediction</title>
<link>https://arxiv.org/abs/2511.15288</link>
<guid>https://arxiv.org/abs/2511.15288</guid>
<content:encoded><![CDATA[
arXiv:2511.15288v1 Announce Type: new 
Abstract: 3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Generative Synthetic Data for X-ray Prohibited Item Detection</title>
<link>https://arxiv.org/abs/2511.15299</link>
<guid>https://arxiv.org/abs/2511.15299</guid>
<content:encoded><![CDATA[
arXiv:2511.15299v1 Announce Type: new 
Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language</title>
<link>https://arxiv.org/abs/2511.15308</link>
<guid>https://arxiv.org/abs/2511.15308</guid>
<content:encoded><![CDATA[
arXiv:2511.15308v1 Announce Type: new 
Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models</title>
<link>https://arxiv.org/abs/2511.15311</link>
<guid>https://arxiv.org/abs/2511.15311</guid>
<content:encoded><![CDATA[
arXiv:2511.15311v1 Announce Type: new 
Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data</title>
<link>https://arxiv.org/abs/2511.15312</link>
<guid>https://arxiv.org/abs/2511.15312</guid>
<content:encoded><![CDATA[
arXiv:2511.15312v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs</title>
<link>https://arxiv.org/abs/2511.15316</link>
<guid>https://arxiv.org/abs/2511.15316</guid>
<content:encoded><![CDATA[
arXiv:2511.15316v1 Announce Type: new 
Abstract: Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive thresholding pattern for fingerprint forgery detection</title>
<link>https://arxiv.org/abs/2511.15322</link>
<guid>https://arxiv.org/abs/2511.15322</guid>
<content:encoded><![CDATA[
arXiv:2511.15322v1 Announce Type: new 
Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.15343</link>
<guid>https://arxiv.org/abs/2511.15343</guid>
<content:encoded><![CDATA[
arXiv:2511.15343v1 Announce Type: new 
Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers</title>
<link>https://arxiv.org/abs/2511.15369</link>
<guid>https://arxiv.org/abs/2511.15369</guid>
<content:encoded><![CDATA[
arXiv:2511.15369v1 Announce Type: new 
Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training</title>
<link>https://arxiv.org/abs/2511.15379</link>
<guid>https://arxiv.org/abs/2511.15379</guid>
<content:encoded><![CDATA[
arXiv:2511.15379v1 Announce Type: new 
Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2511.15390</link>
<guid>https://arxiv.org/abs/2511.15390</guid>
<content:encoded><![CDATA[
arXiv:2511.15390v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation</title>
<link>https://arxiv.org/abs/2511.15396</link>
<guid>https://arxiv.org/abs/2511.15396</guid>
<content:encoded><![CDATA[
arXiv:2511.15396v1 Announce Type: new 
Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling False Positives in Image Segmentation via Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.15406</link>
<guid>https://arxiv.org/abs/2511.15406</guid>
<content:encoded><![CDATA[
arXiv:2511.15406v1 Announce Type: new 
Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models</title>
<link>https://arxiv.org/abs/2511.15411</link>
<guid>https://arxiv.org/abs/2511.15411</guid>
<content:encoded><![CDATA[
arXiv:2511.15411v1 Announce Type: new 
Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes</title>
<link>https://arxiv.org/abs/2511.15429</link>
<guid>https://arxiv.org/abs/2511.15429</guid>
<content:encoded><![CDATA[
arXiv:2511.15429v1 Announce Type: new 
Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection</title>
<link>https://arxiv.org/abs/2511.15433</link>
<guid>https://arxiv.org/abs/2511.15433</guid>
<content:encoded><![CDATA[
arXiv:2511.15433v1 Announce Type: new 
Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.15435</link>
<guid>https://arxiv.org/abs/2511.15435</guid>
<content:encoded><![CDATA[
arXiv:2511.15435v1 Announce Type: new 
Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing</title>
<link>https://arxiv.org/abs/2511.15440</link>
<guid>https://arxiv.org/abs/2511.15440</guid>
<content:encoded><![CDATA[
arXiv:2511.15440v1 Announce Type: new 
Abstract: Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras</title>
<link>https://arxiv.org/abs/2511.15459</link>
<guid>https://arxiv.org/abs/2511.15459</guid>
<content:encoded><![CDATA[
arXiv:2511.15459v1 Announce Type: new 
Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v1 Announce Type: new 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners</title>
<link>https://arxiv.org/abs/2511.15468</link>
<guid>https://arxiv.org/abs/2511.15468</guid>
<content:encoded><![CDATA[
arXiv:2511.15468v1 Announce Type: new 
Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection</title>
<link>https://arxiv.org/abs/2511.15476</link>
<guid>https://arxiv.org/abs/2511.15476</guid>
<content:encoded><![CDATA[
arXiv:2511.15476v1 Announce Type: new 
Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI</title>
<link>https://arxiv.org/abs/2511.15481</link>
<guid>https://arxiv.org/abs/2511.15481</guid>
<content:encoded><![CDATA[
arXiv:2511.15481v1 Announce Type: new 
Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels</title>
<link>https://arxiv.org/abs/2511.15496</link>
<guid>https://arxiv.org/abs/2511.15496</guid>
<content:encoded><![CDATA[
arXiv:2511.15496v1 Announce Type: new 
Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Expand Images for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2511.15499</link>
<guid>https://arxiv.org/abs/2511.15499</guid>
<content:encoded><![CDATA[
arXiv:2511.15499v1 Announce Type: new 
Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Text Guided Few-Shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.15515</link>
<guid>https://arxiv.org/abs/2511.15515</guid>
<content:encoded><![CDATA[
arXiv:2511.15515v1 Announce Type: new 
Abstract: Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture</title>
<link>https://arxiv.org/abs/2511.15535</link>
<guid>https://arxiv.org/abs/2511.15535</guid>
<content:encoded><![CDATA[
arXiv:2511.15535v1 Announce Type: new 
Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scriboora: Rethinking Human Pose Forecasting</title>
<link>https://arxiv.org/abs/2511.15565</link>
<guid>https://arxiv.org/abs/2511.15565</guid>
<content:encoded><![CDATA[
arXiv:2511.15565v1 Announce Type: new 
Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer-Use Agents as Judges for Generative User Interface</title>
<link>https://arxiv.org/abs/2511.15567</link>
<guid>https://arxiv.org/abs/2511.15567</guid>
<content:encoded><![CDATA[
arXiv:2511.15567v1 Announce Type: new 
Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector</title>
<link>https://arxiv.org/abs/2511.15571</link>
<guid>https://arxiv.org/abs/2511.15571</guid>
<content:encoded><![CDATA[
arXiv:2511.15571v1 Announce Type: new 
Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers</title>
<link>https://arxiv.org/abs/2511.15572</link>
<guid>https://arxiv.org/abs/2511.15572</guid>
<content:encoded><![CDATA[
arXiv:2511.15572v1 Announce Type: new 
Abstract: Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning</title>
<link>https://arxiv.org/abs/2511.15578</link>
<guid>https://arxiv.org/abs/2511.15578</guid>
<content:encoded><![CDATA[
arXiv:2511.15578v1 Announce Type: new 
Abstract: With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking</title>
<link>https://arxiv.org/abs/2511.15580</link>
<guid>https://arxiv.org/abs/2511.15580</guid>
<content:encoded><![CDATA[
arXiv:2511.15580v1 Announce Type: new 
Abstract: 3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition</title>
<link>https://arxiv.org/abs/2511.15597</link>
<guid>https://arxiv.org/abs/2511.15597</guid>
<content:encoded><![CDATA[
arXiv:2511.15597v1 Announce Type: new 
Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery</title>
<link>https://arxiv.org/abs/2511.15600</link>
<guid>https://arxiv.org/abs/2511.15600</guid>
<content:encoded><![CDATA[
arXiv:2511.15600v1 Announce Type: new 
Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.15603</link>
<guid>https://arxiv.org/abs/2511.15603</guid>
<content:encoded><![CDATA[
arXiv:2511.15603v1 Announce Type: new 
Abstract: Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Think and When to Look: Uncertainty-Guided Lookback</title>
<link>https://arxiv.org/abs/2511.15613</link>
<guid>https://arxiv.org/abs/2511.15613</guid>
<content:encoded><![CDATA[
arXiv:2511.15613v1 Announce Type: new 
Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</title>
<link>https://arxiv.org/abs/2511.15618</link>
<guid>https://arxiv.org/abs/2511.15618</guid>
<content:encoded><![CDATA[
arXiv:2511.15618v1 Announce Type: new 
Abstract: Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</title>
<link>https://arxiv.org/abs/2511.15622</link>
<guid>https://arxiv.org/abs/2511.15622</guid>
<content:encoded><![CDATA[
arXiv:2511.15622v1 Announce Type: new 
Abstract: Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\href{https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2511.15633</link>
<guid>https://arxiv.org/abs/2511.15633</guid>
<content:encoded><![CDATA[
arXiv:2511.15633v1 Announce Type: new 
Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography</title>
<link>https://arxiv.org/abs/2511.15640</link>
<guid>https://arxiv.org/abs/2511.15640</guid>
<content:encoded><![CDATA[
arXiv:2511.15640v1 Announce Type: new 
Abstract: Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling</title>
<link>https://arxiv.org/abs/2511.15645</link>
<guid>https://arxiv.org/abs/2511.15645</guid>
<content:encoded><![CDATA[
arXiv:2511.15645v1 Announce Type: new 
Abstract: Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases</title>
<link>https://arxiv.org/abs/2511.15656</link>
<guid>https://arxiv.org/abs/2511.15656</guid>
<content:encoded><![CDATA[
arXiv:2511.15656v1 Announce Type: new 
Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI</title>
<link>https://arxiv.org/abs/2511.15658</link>
<guid>https://arxiv.org/abs/2511.15658</guid>
<content:encoded><![CDATA[
arXiv:2511.15658v1 Announce Type: new 
Abstract: Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.
  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisPlay: Self-Evolving Vision-Language Models from Images</title>
<link>https://arxiv.org/abs/2511.15661</link>
<guid>https://arxiv.org/abs/2511.15661</guid>
<content:encoded><![CDATA[
arXiv:2511.15661v1 Announce Type: new 
Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features</title>
<link>https://arxiv.org/abs/2511.15675</link>
<guid>https://arxiv.org/abs/2511.15675</guid>
<content:encoded><![CDATA[
arXiv:2511.15675v1 Announce Type: new 
Abstract: Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</title>
<link>https://arxiv.org/abs/2511.15690</link>
<guid>https://arxiv.org/abs/2511.15690</guid>
<content:encoded><![CDATA[
arXiv:2511.15690v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Image Classification using Spectral-Spatial Mixer Network</title>
<link>https://arxiv.org/abs/2511.15692</link>
<guid>https://arxiv.org/abs/2511.15692</guid>
<content:encoded><![CDATA[
arXiv:2511.15692v1 Announce Type: new 
Abstract: This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First Frame Is the Place to Go for Video Content Customization</title>
<link>https://arxiv.org/abs/2511.15700</link>
<guid>https://arxiv.org/abs/2511.15700</guid>
<content:encoded><![CDATA[
arXiv:2511.15700v1 Announce Type: new 
Abstract: What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Visually, Reason Textually: Vision-Language Synergy in ARC</title>
<link>https://arxiv.org/abs/2511.15703</link>
<guid>https://arxiv.org/abs/2511.15703</guid>
<content:encoded><![CDATA[
arXiv:2511.15703v1 Announce Type: new 
Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</title>
<link>https://arxiv.org/abs/2511.15705</link>
<guid>https://arxiv.org/abs/2511.15705</guid>
<content:encoded><![CDATA[
arXiv:2511.15705v1 Announce Type: new 
Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoMa v2: Harder Better Faster Denser Feature Matching</title>
<link>https://arxiv.org/abs/2511.15706</link>
<guid>https://arxiv.org/abs/2511.15706</guid>
<content:encoded><![CDATA[
arXiv:2511.15706v1 Announce Type: new 
Abstract: Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of Graph Based Vision Transformers Architectures for Accurate Temperature Prediction in Fiber Specklegram Sensors</title>
<link>https://arxiv.org/abs/2511.14792</link>
<guid>https://arxiv.org/abs/2511.14792</guid>
<content:encoded><![CDATA[
arXiv:2511.14792v1 Announce Type: cross 
Abstract: Fiber Specklegram Sensors (FSS) are highly effective for environmental monitoring, particularly for detecting temperature variations. However, the nonlinear nature of specklegram data presents significant challenges for accurate temperature prediction. This study investigates the use of transformer-based architectures, including Vision Transformers (ViTs), Swin Transformers, and emerging models such as Learnable Importance Non-Symmetric Attention Vision Transformers (LINA-ViT) and Multi-Adaptive Proximity Vision Graph Attention Transformers (MAP-ViGAT), to predict temperature from specklegram data over a range of 0 to 120 Celsius. The results show that ViTs achieved a Mean Absolute Error (MAE) of 1.15, outperforming traditional models such as CNNs. GAT-ViT and MAP-ViGAT variants also demonstrated competitive accuracy, highlighting the importance of adaptive attention mechanisms and graph-based structures in capturing complex modal interactions and phase shifts in specklegram data. Additionally, this study incorporates Explainable AI (XAI) techniques, including attention maps and saliency maps, to provide insights into the decision-making processes of the transformer models, improving interpretability and transparency. These findings establish transformer architectures as strong benchmarks for optical fiber-based temperature sensing and offer promising directions for industrial monitoring and structural health assessment applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence</title>
<link>https://arxiv.org/abs/2511.14823</link>
<guid>https://arxiv.org/abs/2511.14823</guid>
<content:encoded><![CDATA[
arXiv:2511.14823v1 Announce Type: cross 
Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard</title>
<link>https://arxiv.org/abs/2511.14876</link>
<guid>https://arxiv.org/abs/2511.14876</guid>
<content:encoded><![CDATA[
arXiv:2511.14876v1 Announce Type: cross 
Abstract: To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference</title>
<link>https://arxiv.org/abs/2511.14961</link>
<guid>https://arxiv.org/abs/2511.14961</guid>
<content:encoded><![CDATA[
arXiv:2511.14961v1 Announce Type: cross 
Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Denoising Using Transformed L1 (TL1) Regularization via ADMM</title>
<link>https://arxiv.org/abs/2511.15060</link>
<guid>https://arxiv.org/abs/2511.15060</guid>
<content:encoded><![CDATA[
arXiv:2511.15060v1 Announce Type: cross 
Abstract: Total variation (TV) regularization is a classical tool for image denoising, but its convex $\ell_1$ formulation often leads to staircase artifacts and loss of contrast. To address these issues, we introduce the Transformed $\ell_1$ (TL1) regularizer applied to image gradients. In particular, we develop a TL1-regularized denoising model and solve it using the Alternating Direction Method of Multipliers (ADMM), featuring a closed-form TL1 proximal operator and an FFT-based image update under periodic boundary conditions. Experimental results demonstrate that our approach achieves superior denoising performance, effectively suppressing noise while preserving edges and enhancing image contrast.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer</title>
<link>https://arxiv.org/abs/2511.15067</link>
<guid>https://arxiv.org/abs/2511.15067</guid>
<content:encoded><![CDATA[
arXiv:2511.15067v1 Announce Type: cross 
Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer</title>
<link>https://arxiv.org/abs/2511.15090</link>
<guid>https://arxiv.org/abs/2511.15090</guid>
<content:encoded><![CDATA[
arXiv:2511.15090v1 Announce Type: cross 
Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits</title>
<link>https://arxiv.org/abs/2511.15173</link>
<guid>https://arxiv.org/abs/2511.15173</guid>
<content:encoded><![CDATA[
arXiv:2511.15173v1 Announce Type: cross 
Abstract: The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
arXiv:2511.15244v1 Announce Type: cross 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15256</link>
<guid>https://arxiv.org/abs/2511.15256</guid>
<content:encoded><![CDATA[
arXiv:2511.15256v1 Announce Type: cross 
Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception</title>
<link>https://arxiv.org/abs/2511.15279</link>
<guid>https://arxiv.org/abs/2511.15279</guid>
<content:encoded><![CDATA[
arXiv:2511.15279v1 Announce Type: cross 
Abstract: In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</title>
<link>https://arxiv.org/abs/2511.15351</link>
<guid>https://arxiv.org/abs/2511.15351</guid>
<content:encoded><![CDATA[
arXiv:2511.15351v1 Announce Type: cross 
Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[
arXiv:2511.15407v1 Announce Type: cross 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel CustNetGC Boosted Model with Spectral Features for Parkinson's Disease Prediction</title>
<link>https://arxiv.org/abs/2511.15485</link>
<guid>https://arxiv.org/abs/2511.15485</guid>
<content:encoded><![CDATA[
arXiv:2511.15485v1 Announce Type: cross 
Abstract: Parkinson's disease is a neurodegenerative disorder that can be very tricky to diagnose and treat. Such early symptoms can include tremors, wheezy breathing, and changes in voice quality as critical indicators of neural damage. Notably, there has been growing interest in utilizing changes in vocal attributes as markers for the detection of PD early on. Based on this understanding, the present paper was designed to focus on the acoustic feature analysis based on voice recordings of patients diagnosed with PD and healthy controls (HC). In this paper, we introduce a novel classification and visualization model known as CustNetGC, combining a Convolutional Neural Network (CNN) with Custom Network Grad-CAM and CatBoost to enhance the efficiency of PD diagnosis. We use a publicly available dataset from Figshare, including voice recordings of 81 participants: 40 patients with PD and 41 healthy controls. From these recordings, we extracted the key spectral features: L-mHP and Spectral Slopes. The L-mHP feature combines three spectrogram representations: Log-Mel spectrogram, harmonic spectrogram, and percussive spectrogram, which are derived using Harmonic-Percussive Source Separation (HPSS). Grad-CAM was used to highlight the important regions in the data, thus making the PD predictions interpretable and effective. Our proposed CustNetGC model achieved an accuracy of 99.06% and precision of 95.83%, with the area under the ROC curve (AUC) recorded at 0.90 for the PD class and 0.89 for the HC class. Additionally, the combination of CatBoost, a gradient boosting algorithm, enhanced the robustness and the prediction performance by properly classifying PD and non-PD samples. Therefore, the results provide the potential improvement in the CustNetGC system in enhancing diagnostic accuracy and the interpretability of the Parkinson's Disease prediction model.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NTK-Guided Implicit Neural Teaching</title>
<link>https://arxiv.org/abs/2511.15487</link>
<guid>https://arxiv.org/abs/2511.15487</guid>
<content:encoded><![CDATA[
arXiv:2511.15487v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Evaluation of Russian-language Architectures</title>
<link>https://arxiv.org/abs/2511.15552</link>
<guid>https://arxiv.org/abs/2511.15552</guid>
<content:encoded><![CDATA[
arXiv:2511.15552v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MHR: Momentum Human Rig</title>
<link>https://arxiv.org/abs/2511.15586</link>
<guid>https://arxiv.org/abs/2511.15586</guid>
<content:encoded><![CDATA[
arXiv:2511.15586v1 Announce Type: cross 
Abstract: We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.15605</link>
<guid>https://arxiv.org/abs/2511.15605</guid>
<content:encoded><![CDATA[
arXiv:2511.15605v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</title>
<link>https://arxiv.org/abs/2511.15704</link>
<guid>https://arxiv.org/abs/2511.15704</guid>
<content:encoded><![CDATA[
arXiv:2511.15704v1 Announce Type: cross 
Abstract: Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-source-free Domain Adaptation via Uncertainty-aware Adaptive Distillation</title>
<link>https://arxiv.org/abs/2402.06213</link>
<guid>https://arxiv.org/abs/2402.06213</guid>
<content:encoded><![CDATA[
arXiv:2402.06213v2 Announce Type: replace 
Abstract: Source-free domain adaptation (SFDA) alleviates the domain discrepancy among data obtained from domains without accessing the data for the awareness of data privacy. However, existing conventional SFDA methods face inherent limitations in medical contexts, where medical data are typically collected from multiple institutions using various equipment. To address this problem, we propose a simple yet effective method, named Uncertainty-aware Adaptive Distillation (UAD) for the multi-source-free unsupervised domain adaptation (MSFDA) setting. UAD aims to perform well-calibrated knowledge distillation from (i) model level to deliver coordinated and reliable base model initialisation and (ii) instance level via model adaptation guided by high-quality pseudo-labels, thereby obtaining a high-performance target domain model. To verify its general applicability, we evaluate UAD on two image-based diagnosis benchmarks among two multi-centre datasets, where our method shows a significant performance gain compared with existing works. The code is available at https://github.com/YXSong000/UAD.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization</title>
<link>https://arxiv.org/abs/2404.03179</link>
<guid>https://arxiv.org/abs/2404.03179</guid>
<content:encoded><![CDATA[
arXiv:2404.03179v3 Announce Type: replace 
Abstract: Video event localization tasks include temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods tend to over-specialize on individual tasks, neglecting the equal importance of these different events for a complete understanding of video content. In this work, we aim to develop a unified framework to solve TAL, SED and AVEL tasks together to facilitate holistic video understanding. However, it is challenging since different tasks emphasize distinct event characteristics and there are substantial disparities in existing task-specific datasets (size/domain/duration). It leads to unsatisfactory results when applying a naive multi-task strategy. To tackle the problem, we introduce UniAV, a Unified Audio-Visual perception network to effectively learn and share mutually beneficial knowledge across tasks and modalities. Concretely, we propose a unified audio-visual encoder to derive generic representations from multiple temporal scales for videos from all tasks. Meanwhile, task-specific experts are designed to capture the unique knowledge specific to each task. Besides, instead of using separate prediction heads, we develop a novel unified language-aware classifier by utilizing semantic-aligned task prompts, enabling our model to flexibly localize various instances across tasks with an impressive open-set ability to localize novel categories. Extensive experiments demonstrate that UniAV, with its unified architecture, significantly outperforms both single-task models and the naive multi-task baseline across all three tasks. It achieves superior or on-par performances compared to the state-of-the-art task-specific methods on ActivityNet 1.3, DESED and UnAV-100 benchmarks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition</title>
<link>https://arxiv.org/abs/2404.10210</link>
<guid>https://arxiv.org/abs/2404.10210</guid>
<content:encoded><![CDATA[
arXiv:2404.10210v5 Announce Type: replace 
Abstract: In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved remarkable performance in skeleton-based action recognition. The reliance on high-energy-consuming continuous floating-point operations inherent in GCN-based methods poses significant challenges for deployment in energy-constrained, battery-powered edge devices. To address these limitations, MK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation, is proposed to leverage the energy efficiency of Spiking Neural Networks (SNNs) for skeleton-based action recognition for the first time. By integrating the energy-saving properties of SNNs with the graph representation capabilities of GCNs, MK-SGN achieves significant reductions in energy consumption while maintaining competitive recognition accuracy. Firstly, we formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse multimodal skeleton data represented as spike-form features. Secondly, we propose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the Spiking Temporal Convolution (STC) module, to capture spatial relationships and temporal dynamics of spike-form features. Finally, we propose an integrated knowledge distillation strategy to transfer information from the multimodal GCN to the SGN, incorporating both intermediate-layer distillation and soft-label distillation to enhance the performance of the SGN. MK-SGN exhibits substantial advantages, surpassing state-of-the-art GCN frameworks in energy efficiency and outperforming state-of-the-art SNN frameworks in recognition accuracy. The proposed method achieves a remarkable reduction in energy consumption, exceeding 98\% compared to conventional GCN-based approaches. This research establishes a robust baseline for developing high-performance, energy-efficient SNN-based models for skeleton-based action recognition
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unobtrusive Monitoring of Simulated Physical Weakness Using Fine-Grained Behavioral Features and Personalized Modeling</title>
<link>https://arxiv.org/abs/2406.10045</link>
<guid>https://arxiv.org/abs/2406.10045</guid>
<content:encoded><![CDATA[
arXiv:2406.10045v2 Announce Type: replace 
Abstract: Aging and chronic conditions affect older adults' daily lives, making early detection of developing health issues crucial. Weakness, common in many conditions, alters physical movements and daily activities subtly. However, detecting such changes can be challenging due to their subtle and gradual nature. To address this, we employ a non-intrusive camera sensor to monitor individuals' daily sitting and relaxing activities for signs of weakness. We simulate weakness in healthy subjects by having them perform physical exercise and observing the behavioral changes in their daily activities before and after workouts. The proposed system captures fine-grained features related to body motion, inactivity, and environmental context in real-time while prioritizing privacy. A Bayesian Network is used to model the relationships between features, activities, and health conditions. We aim to identify specific features and activities that indicate such changes and determine the most suitable time scale for observing the change. Results show 0.97 accuracy in distinguishing simulated weakness at the daily level. Fine-grained behavioral features, including non-dominant upper body motion speed and scale, and inactivity distribution, along with a 300-second window, are found most effective. However, individual-specific models are recommended as no universal set of optimal features and activities was identified across all participants.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self Pre-training with Topology- and Spatiality-aware Masked Autoencoders for 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2406.10519</link>
<guid>https://arxiv.org/abs/2406.10519</guid>
<content:encoded><![CDATA[
arXiv:2406.10519v3 Announce Type: replace 
Abstract: Masked Autoencoders (MAEs) have been shown to be effective in pre-training Vision Transformers (ViTs) for natural and medical image analysis problems. By reconstructing missing pixel/voxel information in visible patches, a ViT encoder can aggregate contextual information for downstream tasks. But, existing MAE pre-training methods, which were specifically developed with the ViT architecture, lack the ability to capture geometric shape and spatial information, which is critical for medical image segmentation tasks. In this paper, we propose a novel extension of known MAEs for self pre-training (i.e., models pre-trained on the same target dataset) for 3D medical image segmentation. (1) We propose a new topological loss to preserve geometric shape information by computing topological signatures of both the input and reconstructed volumes, learning geometric shape information. (2) We introduce a pre-text task that predicts the positions of the centers and eight corners of 3D crops, enabling the MAE to aggregate spatial information. (3) We extend the MAE pre-training strategy to a hybrid state-of-the-art (SOTA) medical image segmentation architecture and co-pretrain it alongside the ViT. (4) We develop a fine-tuned model for downstream segmentation tasks by complementing the pre-trained ViT encoder with our pre-trained SOTA model. Extensive experiments on five public 3D segmentation datasets show the effectiveness of our new approach.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2411.19067</link>
<guid>https://arxiv.org/abs/2411.19067</guid>
<content:encoded><![CDATA[
arXiv:2411.19067v2 Announce Type: replace 
Abstract: Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2412.04939</link>
<guid>https://arxiv.org/abs/2412.04939</guid>
<content:encoded><![CDATA[
arXiv:2412.04939v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have garnered significant attention recently and demonstrate outstanding capabilities in various tasks such as OCR, VQA, captioning, $\textit{etc}$. However, hallucination remains a persistent issue. While numerous methods have been proposed to mitigate hallucinations, achieving notable improvements, these methods primarily focus on mitigating hallucinations about $\textbf{object/noun-related}$ concepts. Verb concepts, crucial for understanding human actions, have been largely overlooked. In this paper, to the best of our knowledge, we are the $\textbf{first}$ to investigate the $\textbf{verb hallucination}$ phenomenon of MLLMs from various perspectives. Our findings reveal that most state-of-the-art MLLMs suffer from severe verb hallucination. To assess the effectiveness of existing mitigation methods for object concept hallucination on verb hallucination, we evaluated these methods and found that they do not effectively address verb hallucination. To address this issue, we propose a novel rich verb knowledge-based tuning method to mitigate verb hallucination. The experiment results demonstrate that our method significantly reduces hallucinations related to verbs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Evaluation and Guidelines for Segment Anything Model in Surgical Video Analysis</title>
<link>https://arxiv.org/abs/2501.00525</link>
<guid>https://arxiv.org/abs/2501.00525</guid>
<content:encoded><![CDATA[
arXiv:2501.00525v2 Announce Type: replace 
Abstract: Surgical video segmentation is critical for AI to interpret spatial-temporal dynamics in surgery, yet model performance is constrained by limited annotated data. The SAM2 model, pretrained on natural videos, offers potential for zero-shot surgical segmentation, but its applicability in complex surgical environments, with challenges like tissue deformation and instrument variability, remains unexplored. We present the first comprehensive evaluation of the zero-shot capability of SAM2 in 9 surgical datasets (17 surgery types), covering laparoscopic, endoscopic, and robotic procedures. We analyze various prompting (points, boxes, mask) and {finetuning (dense, sparse) strategies}, robustness to surgical challenges, and generalization across procedures and anatomies. Key findings reveal that while SAM2 demonstrates notable zero-shot adaptability in structured scenarios (e.g., instrument segmentation, {multi-organ segmentation}, and scene segmentation), its performance varies under dynamic surgical conditions, highlighting gaps in handling temporal coherence and domain-specific artifacts. These results highlight future pathways to adaptive data-efficient solutions for the surgical data science field.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Document Image Dewarping via Hybrid Deep Learning and Cubic Polynomial Geometry Restoration</title>
<link>https://arxiv.org/abs/2501.03145</link>
<guid>https://arxiv.org/abs/2501.03145</guid>
<content:encoded><![CDATA[
arXiv:2501.03145v3 Announce Type: replace 
Abstract: Camera-captured document images often suffer from geometric distortions caused by paper deformation, perspective distortion, and lens aberrations, significantly reducing OCR accuracy. This study develops an efficient automated method for document image dewarping that balances accuracy with computational efficiency. We propose a hybrid approach combining deep learning for document detection with classical computer vision for geometry restoration. YOLOv8 performs initial document segmentation and mask generation. Subsequently, classical CV techniques construct a topological 2D grid through cubic polynomial interpolation of document boundaries, followed by image remapping to correct nonlinear distortions. A new annotated dataset and open-source framework are provided to facilitate reproducibility and further research. Experimental evaluation against state-of-the-art methods (RectiNet, DocGeoNet, DocTr++) and mobile applications (DocScan, CamScanner, TapScanner) demonstrates superior performance. Our method achieves the lowest median Character Error Rate (CER=0.0235), Levenshtein Distance (LD=27.8), and highest Jaro--Winkler similarity (JW=0.902), approaching the quality of scanned originals. The approach requires significantly fewer computational resources and memory compared to pure deep learning solutions while delivering better OCR readability and geometry restoration quality. The proposed hybrid methodology effectively restores document geometry with computational efficiency superior to existing deep learning approaches, making it suitable for resource-constrained applications while maintaining high-quality document digitization. Project page: https://github.com/HorizonParadox/DRCCBI
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-invariant brain morphometry: application to sulcal depth</title>
<link>https://arxiv.org/abs/2501.05436</link>
<guid>https://arxiv.org/abs/2501.05436</guid>
<content:encoded><![CDATA[
arXiv:2501.05436v2 Announce Type: replace 
Abstract: The geometry of the human cortex is complex and highly variable, with interactions between brain size, cortical folding, and age well-documented in the literature. However, few studies have explored how global brain size influences morphometry features of the cortical surface derived from anatomical MRI. In this work, we focus on sulcal depth, an imaging phenotype that has gained attention in both basic research and clinical applications. We make key contributions to the field by: 1) providing the first quantitative analysis of the influence of brain size on sulcal depth measurements; 2) introducing a novel, scale-invariant method for sulcal depth estimation based on an original formalization of the problem; 3) presenting a validation framework and sharing our code and benchmark data with the community; and 4) demonstrating the biological relevance of our new sulcal depth measure using a large sample of 1,987 subjects spanning the developmental period from 26 weeks post-conception to adulthood.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireCastNet: Earth-as-a-Graph for Seasonal Fire Prediction</title>
<link>https://arxiv.org/abs/2502.01550</link>
<guid>https://arxiv.org/abs/2502.01550</guid>
<content:encoded><![CDATA[
arXiv:2502.01550v2 Announce Type: replace 
Abstract: With climate change intensifying fire weather conditions globally, accurate seasonal wildfire forecasting has become critical for disaster preparedness and ecosystem management. We introduce FireCastNet, a novel deep learning architecture that combines 3D convolutional encoding with GraphCast-based Graph Neural Networks (GNNs) to model complex spatio-temporal dependencies for global wildfire prediction. Our approach leverages the SeasFire dataset, a comprehensive multivariate Earth system datacube containing climate, vegetation, and human-related variables, to forecast burned area patterns up to six months in advance. FireCastNet treats the Earth as an interconnected graph, enabling it to capture both local fire dynamics and long-range teleconnections that influence wildfire behavior across different spatial and temporal scales. Through comprehensive benchmarking against state-of-the-art models including GRU, Conv-GRU, Conv-LSTM, U-TAE, and TeleViT, we demonstrate that FireCastNet achieves superior performance in global burned area forecasting, with particularly strong results in fire-prone regions such as Africa, South America, and Southeast Asia. Our analysis reveals that longer input time-series significantly improve prediction robustness, while spatial context integration enhances model performance across extended forecasting horizons. Additionally, we implement local area modeling techniques that provide enhanced spatial resolution and accuracy for region-specific predictions. These findings highlight the importance of modeling Earth system interactions for long-term wildfire prediction.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations</title>
<link>https://arxiv.org/abs/2502.16697</link>
<guid>https://arxiv.org/abs/2502.16697</guid>
<content:encoded><![CDATA[
arXiv:2502.16697v2 Announce Type: replace 
Abstract: Interpretability is crucial to enhance trust in machine learning models for medical diagnostics. However, most state-of-the-art image classifiers based on neural networks are not interpretable. As a result, clinicians often resort to known biomarkers for diagnosis, although biomarker-based classification typically performs worse than large neural networks. This work proposes a method that surpasses the performance of established machine learning models while simultaneously improving prediction interpretability for diabetic retinopathy staging from optical coherence tomography angiography (OCTA) images. Our method is based on a novel biology-informed heterogeneous graph representation that models retinal vessel segments, intercapillary areas, and the foveal avascular zone (FAZ) in a human-interpretable way. This graph representation allows us to frame diabetic retinopathy staging as a graph-level classification task, which we solve using an efficient graph neural network. We benchmark our method against well-established baselines, including classical biomarker-based classifiers, convolutional neural networks (CNNs), and vision transformers. Our model outperforms all baselines on two datasets. Crucially, we use our biology-informed graph to provide explanations of unprecedented detail. Our approach surpasses existing methods in precisely localizing and identifying critical vessels or intercapillary areas. In addition, we give informative and human-interpretable attributions to critical characteristics. Our work contributes to the development of clinical decision-support tools in ophthalmology.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?</title>
<link>https://arxiv.org/abs/2503.02687</link>
<guid>https://arxiv.org/abs/2503.02687</guid>
<content:encoded><![CDATA[
arXiv:2503.02687v3 Announce Type: replace 
Abstract: Due to the significant effort required for data collection and annotation in 3D perception tasks, mixed sample data augmentation (MSDA) has been widely studied to generate diverse training samples by mixing existing data. Recently, many MSDA techniques have been developed for point clouds, but they mainly target LiDAR data, leaving their application to radar point clouds largely unexplored. In this paper, we examine the feasibility of applying existing MSDA methods to radar point clouds and identify several challenges in adapting these techniques. These obstacles stem from the radar's irregular angular distribution, deviations from a single-sensor polar layout in multi-radar setups, and point sparsity. To address these issues, we propose Class-Aware PillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar level in 3D point clouds, guided by class labels. Unlike methods that rely a single mix ratio to the entire sample, CAPMix assigns an independent ratio to each pillar, boosting sample diversity. To account for the density of different classes, we use class-specific distributions: for dense objects (e.g., large vehicles), we skew ratios to favor points from another sample, while for sparse objects (e.g., pedestrians), we sample more points from the original. This class-aware mixing retains critical details and enriches each sample with new information, ultimately generating more diverse training data. Experimental results demonstrate that our method not only significantly boosts performance but also outperforms existing MSDA approaches across two datasets (Bosch Street and K-Radar). We believe that this straightforward yet effective approach will spark further investigation into MSDA techniques for radar data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Latent Space to Rule All Degradations: Unifying Restoration Knowledge for Image Fusion</title>
<link>https://arxiv.org/abs/2503.07033</link>
<guid>https://arxiv.org/abs/2503.07033</guid>
<content:encoded><![CDATA[
arXiv:2503.07033v3 Announce Type: replace 
Abstract: All-in-One Degradation-Aware Fusion Models (ADFMs) as one of multi-modal image fusion models, which aims to address complex scenes by mitigating degradations from source images and generating high-quality fused images. Mainstream ADFMs rely on end-to-end learning and heavily synthesized datasets to achieve degradation awareness and fusion. This rough learning strategy and non-real world scenario dataset dependence often limit their upper-bound performance, leading to low-quality results. To address these limitations, we present LURE, a Learning-driven Unified REpresentation model for infrared and visible image fusion, which is degradation-aware. LURE learns a Unified Latent Feature Space (ULFS) to avoid the dependency on complex data formats inherent in previous end-to-end learning pipelines. It further improves image fusion quality by leveraging the intrinsic relationships between multi-modalities. A novel loss function is also proposed to drive the learning of unified latent representations more stable.More importantly, LURE seamlessly incorporates existing high-quality real-world image restoration datasets. To further enhance the model's representation capability, we design a simple yet effective structure, termed internal residual block, to facilitate the learning of latent features. Experiments show our method outperforms state-of-the-art (SOTA) methods across general fusion, degradation-aware fusion, and downstream tasks. The code is available in the supplementary materials.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
arXiv:2503.07265v3 Announce Type: replace 
Abstract: Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text-to-image generation. To address this challenge, we propose \textbf{WISE}, the first benchmark specifically designed for \textbf{W}orld Knowledge-\textbf{I}nformed \textbf{S}emantic \textbf{E}valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 subdomains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce \textbf{WiScore}, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at \href{https://github.com/PKU-YuanGroup/WISE}{PKU-YuanGroup/WISE}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integration of nested cross-validation, automated hyperparameter optimization, high-performance computing to reduce and quantify the variance of test performance estimation of deep learning models</title>
<link>https://arxiv.org/abs/2503.08589</link>
<guid>https://arxiv.org/abs/2503.08589</guid>
<content:encoded><![CDATA[
arXiv:2503.08589v2 Announce Type: replace 
Abstract: Background and Objectives: The variability and biases in the real-world performance benchmarking of deep learning models for medical imaging compromise their trustworthiness for real-world deployment. The common approach of holding out a single fixed test set fails to quantify the variance in the estimation of test performance metrics. This study introduces NACHOS (Nested and Automated Cross-validation and Hyperparameter Optimization using Supercomputing) to reduce and quantify the variance of test performance metrics of deep learning models. Methods: NACHOS integrates Nested Cross-Validation (NCV) and Automated Hyperparameter Optimization (AHPO) within a parallelized high-performance computing (HPC) framework. NACHOS was demonstrated on a chest X-ray repository and an Optical Coherence Tomography (OCT) dataset under multiple data partitioning schemes. Beyond performance estimation, DACHOS (Deployment with Automated Cross-validation and Hyperparameter Optimization using Supercomputing) is introduced to leverage AHPO and cross-validation to build the final model on the full dataset, improving expected deployment performance. Results: The findings underscore the importance of NCV in quantifying and reducing estimation variance, AHPO in optimizing hyperparameters consistently across test folds, and HPC in ensuring computational feasibility. Conclusions: By integrating these methodologies, NACHOS and DACHOS provide a scalable, reproducible, and trustworthy framework for DL model evaluation and deployment in medical imaging. To maximize public availability, the full open-source codebase is provided at https://github.com/thepanlab/NACHOS
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space</title>
<link>https://arxiv.org/abs/2503.09215</link>
<guid>https://arxiv.org/abs/2503.09215</guid>
<content:encoded><![CDATA[
arXiv:2503.09215v4 Announce Type: replace 
Abstract: Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In this paper, we propose a driving World Model named EOT-WM, unifying Ego-Other vehicle Trajectories in videos for driving simulation. Specifically, it remains a challenge to match multiple trajectories in the BEV space with each vehicle in the video to control the video generation. We first project ego-other vehicle trajectories in the BEV space into the image coordinate for vehicle-trajectory match via pixel positions. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30% in FID and 55% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvFusion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems</title>
<link>https://arxiv.org/abs/2504.01689</link>
<guid>https://arxiv.org/abs/2504.01689</guid>
<content:encoded><![CDATA[
arXiv:2504.01689v2 Announce Type: replace 
Abstract: Diffusion Models have demonstrated remarkable capabilities in handling inverse problems, offering high-quality posterior-sampling-based solutions. Despite significant advances, a fundamental trade-off persists regarding the way the conditioned synthesis is employed: Zero-shot approaches can accommodate any linear degradation but rely on approximations that reduce accuracy. In contrast, training-based methods model the posterior correctly, but cannot adapt to the degradation at test-time. Here we introduce InvFusion, the first training-based degradation-aware posterior sampler. InvFusion combines the best of both worlds -- the strong performance of supervised approaches and the flexibility of zero-shot methods. This is achieved through a novel architectural design that seamlessly integrates the degradation operator directly into the diffusion denoiser. We compare InvFusion against existing general-purpose posterior samplers, both degradation-aware zero-shot techniques and blind training-based methods. Experiments on the FFHQ and ImageNet datasets demonstrate state-of-the-art performance. Beyond posterior sampling, we further demonstrate the applicability of our architecture, operating as a general Minimum Mean Square Error predictor, and as a Neural Posterior Principal Component estimator.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Stream Filtering via Probability Flux Estimation</title>
<link>https://arxiv.org/abs/2504.07503</link>
<guid>https://arxiv.org/abs/2504.07503</guid>
<content:encoded><![CDATA[
arXiv:2504.07503v2 Announce Type: replace 
Abstract: Event cameras asynchronously capture brightness changes with microsecond latency, offering exceptional temporal precision but suffering from severe noise and signal inconsistencies. Unlike conventional signals, events carry state information through polarities and process information through inter-event time intervals. However, existing event filters often ignore the latter, producing outputs that are sparser than the raw input and limiting the reconstruction of continuous irradiance dynamics. We propose the Event Density Flow Filter (EDFilter), a framework that models event generation as threshold-crossing probability fluxes arising from the stochastic diffusion of irradiance trajectories. EDFilter performs nonparametric, kernel-based estimation of probability flux and reconstructs the continuous event density flow using an O(1) recursive solver, enabling real-time processing. The Rotary Event Dataset (RED), featuring microsecond-resolution ground-truth irradiance flow under controlled illumination is also presented for event quality evaluation. Experiments demonstrate that EDFilter achieves high-fidelity, physically interpretable event denoising and motion reconstruction.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials</title>
<link>https://arxiv.org/abs/2504.12679</link>
<guid>https://arxiv.org/abs/2504.12679</guid>
<content:encoded><![CDATA[
arXiv:2504.12679v3 Announce Type: replace 
Abstract: Building Graphical User Interface (GUI) agents is a promising research direction, which simulates human interaction with computers or mobile phones to perform diverse GUI tasks. However, a major challenge in developing generalized GUI agents is the lack of sufficient trajectory data across various operating systems and applications, mainly due to the high cost of manual annotations. In this paper, we propose the TongUI framework that builds generalized GUI agents by learning from rich multimodal web tutorials. Concretely, we crawl and process online GUI tutorials (such as videos and articles) into GUI agent trajectory data, through which we produce the GUI-Net dataset containing 143K trajectory data across five operating systems and more than 200 applications. We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net, which show remarkable performance improvements on commonly used grounding and navigation benchmarks, outperforming baseline agents about 10\% on multiple benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring the significance of our TongUI framework. We will fully open-source the code, the GUI-Net dataset, and the trained models soon.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UINO-FSS: Unifying Representation Learning and Few-shot Segmentation via Hierarchical Distillation and Mamba-HyperCorrelation</title>
<link>https://arxiv.org/abs/2504.15669</link>
<guid>https://arxiv.org/abs/2504.15669</guid>
<content:encoded><![CDATA[
arXiv:2504.15669v3 Announce Type: replace 
Abstract: Few-shot semantic segmentation has attracted growing interest for its ability to generalize to novel object categories using only a few annotated samples. To address data scarcity, recent methods incorporate multiple foundation models to improve feature transferability and segmentation performance. However, they often rely on dual-branch architectures that combine pre-trained encoders to leverage complementary strengths, a design that limits flexibility and efficiency. This raises a fundamental question: can we build a unified model that integrates knowledge from different foundation architectures? Achieving this is, however, challenging due to the misalignment between class-agnostic segmentation capabilities and fine-grained discriminative representations. To this end, we present UINO-FSS, a novel framework built on the key observation that early-stage DINOv2 features exhibit distribution consistency with SAM's output embeddings. This consistency enables the integration of both models' knowledge into a single-encoder architecture via coarse-to-fine multimodal distillation. In particular, our segmenter consists of three core components: a bottleneck adapter for embedding alignment, a meta-visual prompt generator that leverages dense similarity volumes and semantic embeddings, and a mask decoder. Using hierarchical cross-model distillation, we effectively transfer SAM's knowledge into the segmenter, further enhanced by Mamba-based 4D correlation mining on support-query pairs. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ show that UINO-FSS achieves new state-of-the-art results under the 1-shot setting, with mIoU of 80.6 (+3.8%) on PASCAL-5$^i$ and 64.5 (+4.1%) on COCO-20$^i$, demonstrating the effectiveness of our unified approach.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToDRE: Effective Visual Token Pruning via Token Diversity and Task Relevance</title>
<link>https://arxiv.org/abs/2505.18757</link>
<guid>https://arxiv.org/abs/2505.18757</guid>
<content:encoded><![CDATA[
arXiv:2505.18757v2 Announce Type: replace 
Abstract: Visual token pruning aims to compress and prune redundant visual tokens which play a critical role in efficient inference with large vision-language models (LVLMs). However, most existing work estimates visual redundancy using a single metric, such as cross-modal attention or visual token similarity. We show that visual token diversity and task-specific token relevance are two crucial yet orthogonal factors that complement each other in conveying useful information and should therefore be treated separately for more effective visual token pruning. Building upon this insight, we design TODRE, a two-stage and training-free framework that incorporates Token Diversity and task RElevance for effective token compression and efficient LVLM inference. Instead of pruning redundant tokens, we introduce a greedy max-sum diversification algorithm that selects and retains a subset of diverse and representative visual tokens after the vision encoder. On top of that, ToDRE leverages an "information migration" mechanism to eliminate task-irrelevant visual tokens within certain decoder layers of large language model(LLM) to further improve token pruning and LVLM inference. Extensive experiments show that ToDRE prunes 90% of visual tokens after the vision encoder as well as all visual tokens in certain LLM decoder layers, leading to a 2.6x speed-up in total inference time while maintaining 95.0% model performance plus excellent model compatibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Spectral Prior</title>
<link>https://arxiv.org/abs/2505.19873</link>
<guid>https://arxiv.org/abs/2505.19873</guid>
<content:encoded><![CDATA[
arXiv:2505.19873v2 Announce Type: replace 
Abstract: We introduce the Deep Spectral Prior (DSP), a new framework for unsupervised image reconstruction that operates entirely in the complex frequency domain. Unlike the Deep Image Prior (DIP), which optimises pixel-level errors and is highly sensitive to overfitting, DSP performs joint learning of amplitude and phase to capture the full spectral structure of images. We derive a rigorous theoretical characterisation of DSP's optimisation dynamics, proving that it follows frequency-dependent descent trajectories that separate informative low-frequency modes from stochastic high-frequency noise. This spectral mode separation explains DSP's self-regularising behaviour and, for the first time, formally establishes the elimination of DIP's major limitation-its reliance on manual early stopping. Moreover, DSP induces an implicit projection onto a frequency-consistent manifold, ensuring convergence to stable, physically plausible reconstructions without explicit priors or supervision. Extensive experiments on denoising, inpainting, and deblurring demonstrate that DSP consistently surpasses DIP and other unsupervised baselines, achieving superior fidelity, robustness, and theoretical interpretability within a unified, unsupervised data-free framework.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction</title>
<link>https://arxiv.org/abs/2505.21117</link>
<guid>https://arxiv.org/abs/2505.21117</guid>
<content:encoded><![CDATA[
arXiv:2505.21117v3 Announce Type: replace 
Abstract: The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 57% and 87% for RMSE Rotation and Translation, respectively.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Mapping for Evolving Scenes</title>
<link>https://arxiv.org/abs/2506.06909</link>
<guid>https://arxiv.org/abs/2506.06909</guid>
<content:encoded><![CDATA[
arXiv:2506.06909v2 Announce Type: replace 
Abstract: Mapping systems with novel view synthesis (NVS) capabilities, most notably 3D Gaussian Splatting (3DGS), are widely used in computer vision and across various applications, including augmented reality, robotics, and autonomous driving. However, many current approaches are limited to static scenes. While recent works have begun addressing short-term dynamics (motion within the camera's view), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene-adaptation mechanism that continuously updates 3DGS to reflect the latest changes. Since maintaining consistency remains challenging due to stale observations that disrupt the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We thoroughly evaluate Gaussian Mapping for Evolving Scenes (\ours) on both synthetic and real-world datasets, achieving a 29.7\% improvement in PSNR and a 3 times improvement in L1 depth error over the most competitive baseline.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery</title>
<link>https://arxiv.org/abs/2506.10689</link>
<guid>https://arxiv.org/abs/2506.10689</guid>
<content:encoded><![CDATA[
arXiv:2506.10689v2 Announce Type: replace 
Abstract: Accurate automatic screening of minors in unconstrained images requires models robust to distribution shift and resilient to the under-representation of children in public datasets. To address these issues, we propose a multi-task architecture with dedicated under/over-age discrimination tasks based on a frozen FaRL vision-language backbone joined with a compact two-layer MLP that shares features across one age-regression head and four binary underage heads (12, 15, 18, and 21 years). This design focuses on the legally critical age range while keeping the backbone frozen. Class imbalance is mitigated through an $\alpha$-reweighted focal loss and age-balanced mini-batch sampling, while an age gap removes ambiguous samples near thresholds.
  Evaluation is conducted on our new Overall Underage Benchmark (303k cleaned training images, 110k test images), defining both the "ASORES-39k" restricted overall test, which removes the noisiest domains, and the age estimation wild-shifts test "ASWIFT-20k" of 20k-images, stressing extreme poses ($>$45{\deg}), expressions, and low image quality to emulate real-world shifts.
  Trained on the cleaned overall set with resampling and age gap, our multiage model "F" reduces the mean absolute error on ASORES-39k from 4.175 y (age-only baseline) to 4.068 y and improves under-18 detection from F2 score of 0.801 to 0.857 at 1% false-adult rate. Under the ASWIFT-20k, the same configuration nearly sustains 0.99 recall while F2 rises from 0.742 to 0.833, demonstrating robustness to domain shift.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Representation Learning with Observational Grouping for CXR Classification</title>
<link>https://arxiv.org/abs/2506.20582</link>
<guid>https://arxiv.org/abs/2506.20582</guid>
<content:encoded><![CDATA[
arXiv:2506.20582v2 Announce Type: replace 
Abstract: Identifiable causal representation learning seeks to uncover the true causal relationships underlying a data generation process. In medical imaging, this presents opportunities to improve the generalisability and robustness of task-specific latent features. This work introduces the concept of grouping observations to learn identifiable representations for disease classification in chest X-rays via an end-to-end framework. Our experiments demonstrate that these causal representations improve generalisability and robustness across multiple classification tasks when grouping is used to enforce invariance w.r.t race, sex, and imaging views.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object Re-Identification</title>
<link>https://arxiv.org/abs/2507.04638</link>
<guid>https://arxiv.org/abs/2507.04638</guid>
<content:encoded><![CDATA[
arXiv:2507.04638v3 Announce Type: replace 
Abstract: Multi-modal object Re-IDentification (ReID) has gained considerable attention with the goal of retrieving specific targets across cameras using heterogeneous visual data sources. At present, multi-modal object ReID faces two core challenges: (1) learning robust features under fine-grained local noise caused by occlusion, frame loss, and other disruptions; and (2) effectively integrating heterogeneous modalities to enhance multi-modal representation. To address the above challenges, we propose a robust approach named Uncertainty-Guided Graph model for multi-modal object ReID (UGG-ReID). UGG-ReID is designed to mitigate noise interference and facilitate effective multi-modal fusion by estimating both local and sample-level aleatoric uncertainty and explicitly modeling their dependencies. Specifically, we first propose the Gaussian patch-graph representation model that leverages uncertainty to quantify fine-grained local cues and capture their structural relationships. This process boosts the expressiveness of modal-specific information, ensuring that the generated embeddings are both more informative and robust. Subsequently, we design an uncertainty-guided mixture of experts strategy that dynamically routes samples to experts exhibiting low uncertainty. This strategy effectively suppresses noise-induced instability, leading to enhanced robustness. Meanwhile, we design an uncertainty-guided routing to strengthen the multi-modal interaction, improving the performance. UGG-ReID is comprehensively evaluated on five representative multi-modal object ReID datasets, encompassing diverse spectral modalities. Experimental results show that the proposed method achieves excellent performance on all datasets and is significantly better than current methods in terms of noise immunity. Our code is available at https://github.com/wanxixi11/UGG-ReID.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation</title>
<link>https://arxiv.org/abs/2507.06618</link>
<guid>https://arxiv.org/abs/2507.06618</guid>
<content:encoded><![CDATA[
arXiv:2507.06618v2 Announce Type: replace 
Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point cloud segmentation, designing efficient 3D-to-2D mapping that dynamically adapts to the spatial geometry from view variations. Existing projection-based methods leverage view-independent projection in complex scenes, relying on straight lines to generate direct rays or upward curves to reduce occlusions. However, their view independence provides projection rays that are limited to pre-defined parameters by human settings, restricting point awareness and failing to capture sufficient projection diversity across different view planes. Although multiple projections per view plane are commonly used to enhance spatial variety, the projected redundancy leads to excessive computational overhead and inefficiency in image processing. To address these limitations, we design a framework of VDP to generate data-driven projections from 3D point distributions, producing highly informative single-image inputs by predicting rays inspired by the adaptive behavior of fireworks. In addition, we construct color regularization to optimize the framework, which emphasizes essential features within semantic pixels and suppresses the non-semantic features within black pixels, thereby maximizing 2D space utilization in a projected image. As a result, our approach, PointVDP, develops lightweight projections in marginal computation costs. Experiments on S3DIS and ScanNet benchmarks show that our approach achieves competitive results, offering a resource-efficient solution for semantic understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images</title>
<link>https://arxiv.org/abs/2507.14670</link>
<guid>https://arxiv.org/abs/2507.14670</guid>
<content:encoded><![CDATA[
arXiv:2507.14670v2 Announce Type: replace 
Abstract: Accurately predicting gene expression from histopathology images offers a scalable and non-invasive approach to molecular profiling, with significant implications for precision medicine and computational pathology. However, existing methods often underutilize the cross-modal representation alignment between histopathology images and gene expression profiles across multiple representational levels, thereby limiting their prediction performance. To address this, we propose Gene-DML, a unified framework that structures latent space through Dual-pathway Multi-Level discrimination to enhance correspondence between morphological and transcriptional modalities. The multi-scale instance-level discrimination pathway aligns hierarchical histopathology representations extracted at local, neighbor, and global levels with gene expression profiles, capturing scale-aware morphological-transcriptional relationships. In parallel, the cross-level instance-group discrimination pathway enforces structural consistency between individual (image/gene) instances and modality-crossed (gene/image, respectively) groups, strengthening the alignment across modalities. By jointly modeling fine-grained and structural-level discrimination, Gene-DML is able to learn robust cross-modal representations, enhancing both predictive accuracy and generalization across diverse biological contexts. Extensive experiments on public spatial transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art performance in gene expression prediction. The code and processed datasets are available at https://github.com/YXSong000/Gene-DML.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes</title>
<link>https://arxiv.org/abs/2507.23473</link>
<guid>https://arxiv.org/abs/2507.23473</guid>
<content:encoded><![CDATA[
arXiv:2507.23473v2 Announce Type: replace 
Abstract: The widespread application of Unmanned Aerial Vehicles (UAVs) has raised serious public safety and privacy concerns, making UAV perception crucial for anti-UAV tasks. However, existing UAV tracking datasets predominantly feature conspicuous objects and lack diversity in scene complexity and attribute representation, limiting their applicability to real-world scenarios. To overcome these limitations, we present the CST Anti-UAV, a new thermal infrared dataset specifically designed for Single Object Tracking (SOT) in Complex Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k high-quality bounding box annotations, highlighting two key properties: a significant number of tiny-sized UAV targets and the diverse and complex scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to incorporate complete manual frame-level attribute annotations, enabling precise evaluations under varied challenges. To conduct an in-depth performance analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed dataset. Experimental results demonstrate that tracking tiny UAVs in complex environments remains a challenge, as the state-of-the-art method achieves only 35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410 dataset. These findings underscore the limitations of existing benchmarks and the need for further advancements in UAV tracking research. The CST Anti-UAV benchmark is about to be publicly released, which not only fosters the development of more robust SOT methods but also drives innovation in anti-UAV systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration</title>
<link>https://arxiv.org/abs/2508.04797</link>
<guid>https://arxiv.org/abs/2508.04797</guid>
<content:encoded><![CDATA[
arXiv:2508.04797v3 Announce Type: replace 
Abstract: Advancements in image sensing have elevated the importance of Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as extreme downsampling or transformation from the spatial to the frequency domain, encounter significant drawbacks: downsampling induces irreversible information loss in UHD images, while our frequency analysis reveals that pure frequency-domain approaches are ineffective for spatially confined image artifacts, primarily due to the loss of degradation locality. To overcome these limitations, we present RetinexDual, a novel Retinex theory-based framework designed for generalized UHD IR tasks. RetinexDual leverages two complementary sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination Adaptor (FIA). SAMBA, responsible for correcting the reflectance component, utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba, which effectively reduces artifacts and restores intricate details. On the other hand, FIA ensures precise correction of color and illumination distortions by operating in the frequency domain and leveraging the global context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows that it outperforms recent methods qualitatively and quantitatively. Ablation studies demonstrate the importance of employing distinct designs for each branch in RetinexDual, as well as the effectiveness of its various components.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViewBridge:Revisiting Cross-View Localization from Image Matching</title>
<link>https://arxiv.org/abs/2508.10716</link>
<guid>https://arxiv.org/abs/2508.10716</guid>
<content:encoded><![CDATA[
arXiv:2508.10716v2 Announce Type: replace 
Abstract: Cross-view localization aims to estimate the 3-DoF pose of a ground-view image by aligning it with aerial or satellite imagery. Existing methods typically address this task through direct regression or feature alignment in a shared bird's-eye view (BEV) space. Although effective for coarse alignment, these methods fail to establish fine-grained and geometrically reliable correspondences under large viewpoint variations, thereby limiting both the accuracy and interpretability of localization results. Consequently, we revisit cross-view localization from the perspective of image matching and propose a unified framework that enhances both matching and localization. Specifically, we introduce a Surface Model that constrains BEV feature projection to physically valid regions for geometric consistency, and a SimRefiner that adaptively refines similarity distributions to enhance match reliability. To further support research in this area, we present CVFM, the first benchmark with 32,509 cross-view image pairs annotated with pixel-level correspondences. Extensive experiments demonstrate that our approach achieves geometry-consistent and fine-grained correspondences across extreme viewpoints and further improves the accuracy and stability of cross-view localization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.12711</link>
<guid>https://arxiv.org/abs/2508.12711</guid>
<content:encoded><![CDATA[
arXiv:2508.12711v3 Announce Type: replace 
Abstract: The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrary-Scale 3D Gaussian Super-Resolution</title>
<link>https://arxiv.org/abs/2508.16467</link>
<guid>https://arxiv.org/abs/2508.16467</guid>
<content:encoded><![CDATA[
arXiv:2508.16467v2 Announce Type: replace 
Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2509.03951</link>
<guid>https://arxiv.org/abs/2509.03951</guid>
<content:encoded><![CDATA[
arXiv:2509.03951v3 Announce Type: replace 
Abstract: The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title>
<link>https://arxiv.org/abs/2509.07928</link>
<guid>https://arxiv.org/abs/2509.07928</guid>
<content:encoded><![CDATA[
arXiv:2509.07928v2 Announce Type: replace 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNIV: Unified Foundation Model for Infrared and Visible Modalities</title>
<link>https://arxiv.org/abs/2509.15642</link>
<guid>https://arxiv.org/abs/2509.15642</guid>
<content:encoded><![CDATA[
arXiv:2509.15642v2 Announce Type: replace 
Abstract: Joint RGB-infrared perception is essential for achieving robustness under diverse weather and illumination conditions. Although foundation models excel within single modalities, they suffer from substantial cross-modal degradation, an issue we attribute to a pattern shortcut, i.e., a modal bias that prioritizes superficial sensor patterns over underlying semantics. To address this problem, we introduce UNIV, a Unified foundation model for Infrared and Visible modalities. At the core of UNIV lies Patch Cross-modal Contrastive Learning (PCCL), a self-supervised contrastive learning strategy that constructs a unified cross-modal feature space. PCCL employs a frozen pre-trained model to sample pseudo patch pairs based on semantic similarity, and aligns infrared-visible representations by attracting semantically related pairs while repelling unrelated ones. This process simultaneously enhances cross-modal alignment and inter-class semantic separability, guiding the model to focus on semantic structure rather than falling into pattern shortcuts. To further enable cross-modal learning, we introduce MVIP, the most comprehensive visible-infrared benchmark to date, containing 98,992 precisely aligned image pairs across diverse scenes. Extensive experiments demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU for semantic segmentation and +0.7 mAP for detection), while maintaining competitive accuracy on RGB tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clothing agnostic Pre-inpainting Virtual Try-ON</title>
<link>https://arxiv.org/abs/2509.17654</link>
<guid>https://arxiv.org/abs/2509.17654</guid>
<content:encoded><![CDATA[
arXiv:2509.17654v3 Announce Type: replace 
Abstract: With the development of deep learning technology, virtual try-on technology has devel-oped important application value in the fields of e-commerce, fashion, and entertainment. The recently proposed Leffa technology has addressed the texture distortion problem of diffusion-based models, but there are limitations in that the bottom detection inaccuracy and the existing clothing silhouette persist in the synthesis results. To solve this problem, this study proposes CaP-VTON (Clothing Agnostic Pre-Inpainting Virtual Try-On). CaP-VTON integrates DressCode-based multi-category masking and Stable Diffu-sion-based skin inflation preprocessing; in particular, a generated skin module was in-troduced to solve skin restoration problems that occur when long-sleeved images are con-verted to short-sleeved or sleeveless ones, introducing a preprocessing structure that im-proves the naturalness and consistency of full-body clothing synthesis, and allowing the implementation of high-quality restoration considering human posture and color. As a result, CaP-VTON achieved 92.5%, which is 15.4% better than Leffa, in short-sleeved syn-thesis accuracy, and consistently reproduced the style and shape of the reference clothing in visual evaluation. These structures maintain model-agnostic properties and are appli-cable to various diffusion-based virtual inspection systems; they can also contribute to applications that require high-precision virtual wearing, such as e-commerce, custom styling, and avatar creation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs</title>
<link>https://arxiv.org/abs/2509.18015</link>
<guid>https://arxiv.org/abs/2509.18015</guid>
<content:encoded><![CDATA[
arXiv:2509.18015v2 Announce Type: replace 
Abstract: Recent work has shown promising performance of frontier large language models (LLMs) and their multimodal counterparts in medical quizzes and diagnostic tasks, highlighting their potential for broad clinical utility given their accessible, general-purpose nature. However, beyond diagnosis, a fundamental aspect of medical image interpretation is the ability to localize pathological findings. Evaluating localization not only has clinical and educational relevance but also provides insight into a model's spatial understanding of anatomy and disease. Here, we systematically assess two general-purpose MLLMs (GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to localize pathologies on chest radiographs, using a prompting pipeline that overlays a spatial grid and elicits coordinate-based predictions. Averaged across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%), all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark (80.1%). Despite modest performance, error analysis revealed that GPT-5's predictions were largely in anatomically plausible regions, just not always precisely localized. GPT-4 performed well on pathologies with fixed anatomical locations, but struggled with spatially variable findings and exhibited anatomically implausible predictions more frequently. MedGemma demonstrated the lowest performance on all pathologies, but showed improvements when provided examples through few shot prompting. Our findings highlight both the promise and limitations of current MLLMs in medical imaging and underscore the importance of integrating them with task-specific tools for reliable use.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</title>
<link>https://arxiv.org/abs/2509.24473</link>
<guid>https://arxiv.org/abs/2509.24473</guid>
<content:encoded><![CDATA[
arXiv:2509.24473v3 Announce Type: replace 
Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\% to 41.8\% (+5.2\%), and the mean MindCube accuracy rose from 31.4\% to 38.1\% (+6.7\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?</title>
<link>https://arxiv.org/abs/2509.24709</link>
<guid>https://arxiv.org/abs/2509.24709</guid>
<content:encoded><![CDATA[
arXiv:2509.24709v3 Announce Type: replace 
Abstract: The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available at https://github.com/SIGMME/IWR-Bench.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Odometry with Transformers</title>
<link>https://arxiv.org/abs/2510.03348</link>
<guid>https://arxiv.org/abs/2510.03348</guid>
<content:encoded><![CDATA[
arXiv:2510.03348v2 Announce Type: replace 
Abstract: Despite the rapid development of large 3D models, classical optimization-based approaches dominate the field of visual odometry (VO). Thus, current approaches to VO heavily rely on camera parameters and many handcrafted components, most of which involve complex bundle adjustment and feature-matching processes. Although disregarded in the literature, we find it problematic in terms of both (1) speed, that performs bundle adjustment requires a significant amount of time, and (2) scalability, as hand-crafted components struggle to learn from large-scale training data. In this work, we introduce a simple yet efficient architecture, Visual Odometry Transformer (VoT), that formulates monocular visual odometry as a direct relative pose regression problem. Our approach streamlines the monocular visual odometry pipeline in an end-to-end manner, effectively eliminating the need for handcrafted components such as bundle adjustment, feature matching, or camera calibration. We show that VoT is up to 4 times faster than traditional approaches, yet with competitive or better performance. Compared to recent 3D foundation models, VoT runs 10 times faster with strong scaling behavior in terms of both model sizes and training data. Moreover, VoT generalizes well in both low-data regimes and previously unseen scenarios, reducing the gap between optimization-based and end-to-end approaches.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI</title>
<link>https://arxiv.org/abs/2510.04705</link>
<guid>https://arxiv.org/abs/2510.04705</guid>
<content:encoded><![CDATA[
arXiv:2510.04705v2 Announce Type: replace 
Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis assessment, yet labeled data is often scarce and unevenly distributed across imaging modalities and vendor systems. We propose a label-efficient segmentation approach that promotes cross-modality generalization under real-world conditions, where GED4 hepatobiliary-phase annotations are limited, non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial misalignment and missing phases are common. Our method integrates a foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training with cross pseudo supervision to leverage unlabeled volumes, and a standardized preprocessing pipeline. Without requiring spatial registration, the model learns to generalize across MRI phases and vendors, demonstrating robust segmentation performance in both labeled and unlabeled domains. Our results exhibit the effectiveness of our proposed label-efficient baseline for liver segmentation in multi-phase, multi-vendor MRI and highlight the potential of combining foundation model adaptation with co-training for real-world clinical imaging tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Denoising Framework for Real-World Ultra-Low-Dose Lung CT Images Based on an Image Purification Strategy</title>
<link>https://arxiv.org/abs/2510.07492</link>
<guid>https://arxiv.org/abs/2510.07492</guid>
<content:encoded><![CDATA[
arXiv:2510.07492v3 Announce Type: replace 
Abstract: Computed Tomography (CT) is a vital diagnostic tool in clinical practice, yet the health risks associated with ionizing radiation cannot be overlooked. Low-dose CT (LDCT) helps mitigate radiation exposure but simultaneously leads to reduced image quality. Consequently, researchers have sought to reconstruct clear images from LDCT scans using artificial intelligence-based image enhancement techniques. However, these studies typically rely on synthetic LDCT images for algorithm training, which introduces significant domain-shift issues and limits the practical effectiveness of these algorithms in real-world scenarios. To address this challenge, we constructed a real-world paired lung dataset, referred to as Patient-uLDCT (ultra-low-dose CT), by performing multiple scans on volunteers. The radiation dose for the low-dose images in this dataset is only 2% of the normal dose, substantially lower than the conventional 25% low-dose and 10% ultra-low-dose levels. Furthermore, to resolve the anatomical misalignment between normal-dose and uLDCT images caused by respiratory motion during acquisition, we propose a novel purification strategy to construct corresponding aligned image pairs. Finally, we introduce a Frequency-domain Flow Matching model (FFM) that achieves excellent image reconstruction performance. Code is available at https://github.com/MonkeyDadLufy/flow-matching.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
<link>https://arxiv.org/abs/2510.13515</link>
<guid>https://arxiv.org/abs/2510.13515</guid>
<content:encoded><![CDATA[
arXiv:2510.13515v2 Announce Type: replace 
Abstract: Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v3 Announce Type: replace 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent</title>
<link>https://arxiv.org/abs/2510.21704</link>
<guid>https://arxiv.org/abs/2510.21704</guid>
<content:encoded><![CDATA[
arXiv:2510.21704v2 Announce Type: replace 
Abstract: When a vision model performs image recognition, which visual attributes drive its predictions? Detecting unintended reliance on specific visual features is critical for ensuring model robustness, preventing overfitting, and avoiding spurious correlations. We introduce an automated framework for detecting such dependencies in trained vision models. At the core of our method is a self-reflective agent that systematically generates and tests hypotheses about visual attributes that a model may rely on. This process is iterative: the agent refines its hypotheses based on experimental outcomes and uses a self-evaluation protocol to assess whether its findings accurately explain model behavior. When inconsistencies arise, the agent self-reflects over its findings and triggers a new cycle of experimentation. We evaluate our approach on a novel benchmark of 130 models designed to exhibit diverse visual attribute dependencies across 18 categories. Our results show that the agent's performance consistently improves with self-reflection, with a significant performance increase over non-reflective baselines. We further demonstrate that the agent identifies real-world visual attribute dependencies in state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object detector.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment</title>
<link>https://arxiv.org/abs/2510.22827</link>
<guid>https://arxiv.org/abs/2510.22827</guid>
<content:encoded><![CDATA[
arXiv:2510.22827v2 Announce Type: replace 
Abstract: Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes. Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability). We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges. It scores alignment with an explanation-oriented rubric mapped to [-1, 1]; constrains judgments to a closed label set; requires evidence grounded in the visible content; and mandates abstention when cues are insufficient. Unlike CLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions; unlike mitigation that alters generators, it targets evaluation fairness. We evaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to religion, culture, and disability; and assess profession correctness and alignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions. We also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes. Across datasets, judge models outperform contrastive and face-centric baselines on demographic prediction and improve mean alignment while maintaining high profession accuracy, enabling more reliable, reproducible fairness audits.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflict Adaptation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24804</link>
<guid>https://arxiv.org/abs/2510.24804</guid>
<content:encoded><![CDATA[
arXiv:2510.24804v2 Announce Type: replace 
Abstract: A signature of human cognitive control is conflict adaptation: improved performance on a high-conflict trial following another high-conflict trial. This phenomenon offers an account for how cognitive control, a scarce resource, is recruited. Using a sequential Stroop task, we find that 12 of 13 vision-language models (VLMs) tested exhibit behavior consistent with conflict adaptation, with the lone exception likely reflecting a ceiling effect. To understand the representational basis of this behavior, we use sparse autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B. Partially overlapping supernodes emerge for text and color in both early and late layers, and their relative sizes mirror the automaticity asymmetry between reading and color naming in humans. We further isolate a conflict-modulated supernode in layers 24-25 whose ablation significantly increases Stroop errors while minimally affecting congruent trials.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompAgent: An Agentic Framework for Visual Compliance Verification</title>
<link>https://arxiv.org/abs/2511.00171</link>
<guid>https://arxiv.org/abs/2511.00171</guid>
<content:encoded><![CDATA[
arXiv:2511.00171v2 Announce Type: replace 
Abstract: Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent Multimodal Large Language Models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools-such as object detectors, face analyzers, NSFW detectors, and captioning models-and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A compliance verification agent then integrates image, tool outputs, and policy context to perform multimodal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and robust tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spot The Ball: A Benchmark for Visual Social Inference</title>
<link>https://arxiv.org/abs/2511.00261</link>
<guid>https://arxiv.org/abs/2511.00261</guid>
<content:encoded><![CDATA[
arXiv:2511.00261v2 Announce Type: replace 
Abstract: Humans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people's gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics--such as guessing near the image center or nearby players--while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</title>
<link>https://arxiv.org/abs/2511.01767</link>
<guid>https://arxiv.org/abs/2511.01767</guid>
<content:encoded><![CDATA[
arXiv:2511.01767v2 Announce Type: replace 
Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2511.05923</link>
<guid>https://arxiv.org/abs/2511.05923</guid>
<content:encoded><![CDATA[
arXiv:2511.05923v3 Announce Type: replace 
Abstract: Despite the remarkable advancements of Large Vision-Language Models (LVLMs), the mechanistic interpretability remains underexplored. Existing analyses are insufficiently comprehensive and lack examination covering visual and textual tokens, model components, and the full range of layers. This limitation restricts actionable insights to improve the faithfulness of model output and the development of downstream tasks, such as hallucination mitigation. To address this limitation, we introduce Fine-grained Cross-modal Causal Tracing (FCCT) framework, which systematically quantifies the causal effects on visual object perception. FCCT conducts fine-grained analysis covering the full range of visual and textual tokens, three core model components including multi-head self-attention (MHSA), feed-forward networks (FFNs), and hidden states, across all decoder layers. Our analysis is the first to demonstrate that MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information, while FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations. Building on these insights, we propose Intermediate Representation Injection (IRI), a training-free inference-time technique that reinforces visual object information flow by precisely intervening on cross-modal representations at specific components and layers, thereby enhancing perception and mitigating hallucination. Consistent improvements across five widely used benchmarks and LVLMs demonstrate IRI achieves state-of-the-art performance, while preserving inference speed and other foundational performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning</title>
<link>https://arxiv.org/abs/2511.06958</link>
<guid>https://arxiv.org/abs/2511.06958</guid>
<content:encoded><![CDATA[
arXiv:2511.06958v2 Announce Type: replace 
Abstract: Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackStudio: An Integrated Toolkit for Markerless Tracking</title>
<link>https://arxiv.org/abs/2511.07624</link>
<guid>https://arxiv.org/abs/2511.07624</guid>
<content:encoded><![CDATA[
arXiv:2511.07624v2 Announce Type: replace 
Abstract: Markerless motion tracking has advanced rapidly in the past 10 years and currently offers powerful opportunities for behavioural, clinical, and biomechanical research. While several specialised toolkits provide high performance for specific tasks, using existing tools still requires substantial technical expertise. There remains a gap in accessible, integrated solutions that deliver sufficient tracking for non-experts across diverse settings.
  TrackStudio was developed to address this gap by combining established open-source tools into a single, modular, GUI-based pipeline that works out of the box. It provides automatic 2D and 3D tracking, calibration, preprocessing, feature extraction, and visualisation without requiring any programming skills. We supply a user guide with practical advice for video acquisition, synchronisation, and setup, alongside documentation of common pitfalls and how to avoid them.
  To validate the toolkit, we tested its performance across three environments using either low-cost webcams or high-resolution cameras, including challenging conditions for body position, lightning, and space and obstructions. Across 76 participants, average inter-frame correlations exceeded 0.98 and average triangulation errors remained low (<13.6mm for hand tracking), demonstrating stable and consistent tracking. We further show that the same pipeline can be extended beyond hand tracking to other body and face regions. TrackStudio provides a practical, accessible route into markerless tracking for researchers or laypeople who need reliable performance without specialist expertise.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling</title>
<link>https://arxiv.org/abs/2511.07710</link>
<guid>https://arxiv.org/abs/2511.07710</guid>
<content:encoded><![CDATA[
arXiv:2511.07710v2 Announce Type: replace 
Abstract: Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Deepfake Detection: Leveraging Dual-Mechanism Optimization</title>
<link>https://arxiv.org/abs/2511.10150</link>
<guid>https://arxiv.org/abs/2511.10150</guid>
<content:encoded><![CDATA[
arXiv:2511.10150v3 Announce Type: replace 
Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions</title>
<link>https://arxiv.org/abs/2308.08365</link>
<guid>https://arxiv.org/abs/2308.08365</guid>
<content:encoded><![CDATA[
arXiv:2308.08365v2 Announce Type: replace-cross 
Abstract: Microscopy images are crucial for life science research, allowing detailed inspection and characterization of cellular and tissue-level structures and functions. However, microscopy data are unavoidably affected by image degradations, such as noise, blur, or others. Many such degradations also contribute to a loss of image contrast, which becomes especially pronounced in deeper regions of thick samples. Today, best performing methods to increase the quality of images are based on Deep Learning approaches, which typically require ground truth (GT) data during training. Our inability to counteract blurring and contrast loss when imaging deep into samples prevents the acquisition of such clean GT data. The fact that the forward process of blurring and contrast loss deep into tissue can be modeled, allowed us to propose a new method that can circumvent the problem of unobtainable GT data. To this end, we first synthetically degraded the quality of microscopy images even further by using an approximate forward model for deep tissue image degradations. Then we trained a neural network that learned the inverse of this degradation function from our generated pairs of raw and degraded images. We demonstrated that networks trained in this way can be used out-of-distribution (OOD) to improve the quality of less severely degraded images, e.g. the raw data imaged in a microscope. Since the absolute level of degradation in such microscopy images can be stronger than the additional degradation introduced by our forward model, we also explored the effect of iterative predictions. Here, we observed that in each iteration the measured image contrast kept improving while detailed structures in the images got increasingly removed. Therefore, dependent on the desired downstream analysis, a balance between contrast improvement and retention of image details has to be found.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Out-of-Distribution Objects through Class-Conditioned Inpainting</title>
<link>https://arxiv.org/abs/2402.03292</link>
<guid>https://arxiv.org/abs/2402.03292</guid>
<content:encoded><![CDATA[
arXiv:2402.03292v4 Announce Type: replace-cross 
Abstract: Recent object detectors have achieved impressive accuracy in identifying objects seen during training. However, real-world deployment often introduces novel and unexpected objects, referred to as out-of-distribution (OOD) objects, posing significant challenges to model trustworthiness. Modern object detectors are typically overconfident, making it unreliable to use their predictions alone for OOD detection. To address this, we propose leveraging an auxiliary model as a complementary solution. Specifically, we utilize an off-the-shelf text-to-image generative model, such as Stable Diffusion, which is trained with objective functions distinct from those of discriminative object detectors. We hypothesize that this fundamental difference enables the detection of OOD objects by measuring inconsistencies between the models. Concretely, for a given detected object bounding box and its predicted in-distribution class label, we perform class-conditioned inpainting on the image with the object removed. If the object is OOD, the inpainted image is likely to deviate significantly from the original, making the reconstruction error a robust indicator of OOD status. Extensive experiments demonstrate that our approach consistently surpasses existing zero-shot and non-zero-shot OOD detection methods, establishing a robust framework for enhancing object detection systems in dynamic environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2409.13930</link>
<guid>https://arxiv.org/abs/2409.13930</guid>
<content:encoded><![CDATA[
arXiv:2409.13930v2 Announce Type: replace-cross 
Abstract: Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, leading to distorted CT images with artifacts. This results in an ill-posed problem known as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, our experiments are conducted on two different LACT datasets, i.e., ChromSTEM and C4KC-KiTS. Through extensive experiments, we show that by leveraging learned Mean-Reverting SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, RN-SDEs can restore high-quality images from severe degradation and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Style Content Decomposition-based Data Augmentation for Domain Generalizable Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2502.20619</link>
<guid>https://arxiv.org/abs/2502.20619</guid>
<content:encoded><![CDATA[
arXiv:2502.20619v3 Announce Type: replace-cross 
Abstract: Due to domain shifts across diverse medical imaging modalities, learned segmentation models often suffer significant performance degradation during deployment. We posit that these domain shifts can generally be categorized into two main components: 1) "style" shifts, referring to global disparities in image properties such as illumination, contrast, and color; and 2) "content" shifts, which involve local discrepancies in anatomical structures. To address the domain shifts in medical image segmentation, we first factorize an image into style codes and content maps, explicitly modeling the "style" and "content" components. Building on this, we introduce a Style-Content decomposition-based data augmentation algorithm (StyCona), which performs augmentation on both the global style and local content of source-domain images, enabling the training of a well-generalized model for domain generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to segmentation model architectures. Experiments on cardiac magnetic resonance imaging and fundus photography segmentation tasks, with single and multiple target domains respectively, demonstrate the effectiveness of StyCona and its superiority over state-of-the-art domain generalization methods. The code is available at https://github.com/Senyh/StyCona.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</title>
<link>https://arxiv.org/abs/2503.10488</link>
<guid>https://arxiv.org/abs/2503.10488</guid>
<content:encoded><![CDATA[
arXiv:2503.10488v3 Announce Type: replace-cross 
Abstract: Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce a novel framework for streaming gesture generation that extends Rolling Diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. Our framework is universally compatible with existing diffusion-based gesture generation model, transforming them into streaming methods capable of continuous generation without requiring post-processing. We evaluate our framework on ZEGGS and BEAT, strong benchmarks for real-world applicability. Applied to state-of-the-art baselines on both datasets, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time co-speech gesture synthesis. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that employs a ladder-based noise scheduling strategy to simultaneously denoise multiple frames. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 4x speedup with high visual fidelity and temporal coherence in our experiments. Comprehensive user studies further validate our framework ability to generate realistic, diverse gestures closely synchronized with the audio input.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beacon2Science: Enhancing STEREO/HI beacon data with machine learning for efficient CME tracking</title>
<link>https://arxiv.org/abs/2503.15288</link>
<guid>https://arxiv.org/abs/2503.15288</guid>
<content:encoded><![CDATA[
arXiv:2503.15288v2 Announce Type: replace-cross 
Abstract: Observing and forecasting coronal mass ejections (CME) in real-time is crucial due to the strong geomagnetic storms they can generate that can have a potentially damaging effect, for example, on satellites and electrical devices. With its near-real-time availability, STEREO/HI beacon data is the perfect candidate for early forecasting of CMEs. However, previous work concluded that CME arrival prediction based on beacon data could not achieve the same accuracy as with high-resolution science data due to data gaps and lower quality. We present our novel machine-learning pipeline entitled ``Beacon2Science'', bridging the gap between beacon and science data to improve CME tracking. Through this pipeline, we first enhance the quality (signal-to-noise ratio and spatial resolution) of beacon data. We then increase the time resolution of enhanced beacon images through learned interpolation to match science data's 40-minute resolution. We maximize information coherence between consecutive frames with adapted model architecture and loss functions through the different steps. The improved beacon images are comparable to science data, showing better CME visibility than the original beacon data. Furthermore, we compare CMEs tracked in beacon, enhanced beacon, and science images. The tracks extracted from enhanced beacon data are closer to those from science images, with a mean average error of $\sim 0.5 ^\circ$ of elongation compared to $1^\circ$ with original beacon data. The work presented in this paper paves the way for its application to forthcoming missions such as Vigil and PUNCH.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the (Un)Faithfulness of Concept-Based Explanations</title>
<link>https://arxiv.org/abs/2504.10833</link>
<guid>https://arxiv.org/abs/2504.10833</guid>
<content:encoded><![CDATA[
arXiv:2504.10833v3 Announce Type: replace-cross 
Abstract: Deep vision models perform input-output computations that are hard to interpret. Concept-based explanation methods (CBEMs) increase interpretability by re-expressing parts of the model with human-understandable semantic units, or concepts. Checking if the derived explanations are faithful -- that is, they represent the model's internal computation -- requires a surrogate that combines concepts to compute the output. Simplifications made for interpretability inevitably reduce faithfulness, resulting in a tradeoff between the two. State-of-the-art unsupervised CBEMs (U-CBEMs) have reported increasingly interpretable concepts, while also being more faithful to the model. However, we observe that the reported improvement in faithfulness artificially results from either (1) using overly complex surrogates, which introduces an unmeasured cost to the explanation's interpretability, or (2) relying on deletion-based approaches that, as we demonstrate, do not properly measure faithfulness. We propose Surrogate Faithfulness (SURF), which (1) replaces prior complex surrogates with a simple, linear surrogate that measures faithfulness without changing the explanation's interpretability and (2) introduces well-motivated metrics that assess loss across all output classes, not just the predicted class. We validate SURF with a measure-over-measure study by proposing a simple sanity check -- explanations with random concepts should be less faithful -- which prior surrogates fail. SURF enables the first reliable faithfulness benchmark of U-CBEMs, revealing that many visually compelling U-CBEMs are not faithful. Code to be released.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capture Stage Matting: Challenges, Approaches, and Solutions for Offline and Real-Time Processing</title>
<link>https://arxiv.org/abs/2507.07623</link>
<guid>https://arxiv.org/abs/2507.07623</guid>
<content:encoded><![CDATA[
arXiv:2507.07623v2 Announce Type: replace-cross 
Abstract: Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is matting, i.e., separating captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of these characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need for extensive annotations, both offline and real-time. For an objective evaluation, we introduce a validation methodology using a state-of-the-art diffusion model to demonstrate the benefits of our approach.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Collaboration and Explainability for 2D/3D Registration Quality Assurance</title>
<link>https://arxiv.org/abs/2507.17597</link>
<guid>https://arxiv.org/abs/2507.17597</guid>
<content:encoded><![CDATA[
arXiv:2507.17597v2 Announce Type: replace-cross 
Abstract: Purpose: As surgery increasingly integrates advanced imaging, algorithms, and robotics to automate complex tasks, human judgment of system correctness remains a vital safeguard for patient safety. A critical example is 2D/3D registration, where small registration misalignments can lead to surgical errors. Current visualization strategies alone are insufficient to reliably enable humans to detect these misalignments, highlighting the need for enhanced decision-support tools.
  Methods: We propose the first artificial intelligence (AI) model tailored to 2D/3D registration quality assessment, augmented with explainable AI (XAI) mechanisms to clarify the model's predictions. Using both objective measures (e.g., accuracy, sensitivity, precision, specificity) and subjective evaluations (e.g., workload, trust, and understanding), we systematically compare decision-making across four conditions: AI-only, Human-only, Human+AI, and Human+XAI.
  Results: The AI-only condition achieved the highest accuracy, whereas collaborative paradigms (Human+AI and Human+XAI) improved sensitivity, precision, and specificity compared to standalone approaches. Participants experienced significantly lower workload in collaborative conditions relative to the Human-only condition. Moreover, participants reported a greater understanding of AI predictions in the Human+XAI condition than in Human+AI, although no significant differences were observed between the two collaborative paradigms in perceived trust or workload.
  Conclusion: Human-AI collaboration can enhance 2D/3D registration quality assurance, with explainability mechanisms improving user understanding. Future work should refine XAI designs to optimize decision-making performance and efficiency. Extending both the algorithmic design and human-XAI collaboration elements holds promise for more robust quality assurance of 2D/3D registration.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Radiographic Knee Alignment in Total Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment</title>
<link>https://arxiv.org/abs/2508.10941</link>
<guid>https://arxiv.org/abs/2508.10941</guid>
<content:encoded><![CDATA[
arXiv:2508.10941v2 Announce Type: replace-cross 
Abstract: Knee osteoarthritis (OA) is one of the most widespread and burdensome health problems [1-4]. Total knee replacement (TKR) may be offered as treatment for end-stage knee OA. Nevertheless, TKR is an invasive procedure involving prosthesis implantation at the knee joint, and around 10% of patients are dissatisfied following TKR [5,6]. Dissatisfaction is often assessed through patient-reported outcome measures (PROMs) [7], which are usually completed by patients and assessed by health professionals to evaluate the condition of TKR patients. In clinical practice, predicting poor TKR outcomes in advance could help optimise patient selection and improve management strategies. Radiographic knee alignment is an important biomarker for predicting TKR outcomes and long-term joint health. Abnormalities such as femoral or tibial deformities can directly influence surgical planning, implant selection, and postoperative recovery [8,9]. Traditional alignment measurement is manual, time-consuming, and requires long-leg radiographs, which are not always undertaken in clinical practice. Instead, standard anteroposterior (AP) knee radiographs are often the main imaging modality. Automated methods for alignment assessment in standard knee radiographs are potentially clinically valuable for improving efficiency in the knee OA treatment pathway.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention</title>
<link>https://arxiv.org/abs/2509.24006</link>
<guid>https://arxiv.org/abs/2509.24006</guid>
<content:encoded><![CDATA[
arXiv:2509.24006v2 Announce Type: replace-cross 
Abstract: In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B. The code is available at https://github.com/thu-ml/SLA.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</title>
<link>https://arxiv.org/abs/2510.25801</link>
<guid>https://arxiv.org/abs/2510.25801</guid>
<content:encoded><![CDATA[
arXiv:2510.25801v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation</title>
<link>https://arxiv.org/abs/2511.07573</link>
<guid>https://arxiv.org/abs/2511.07573</guid>
<content:encoded><![CDATA[
arXiv:2511.07573v2 Announce Type: replace-cross 
Abstract: The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an "outfit token" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a "target item token" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</title>
<link>https://arxiv.org/abs/2511.09611</link>
<guid>https://arxiv.org/abs/2511.09611</guid>
<content:encoded><![CDATA[
<div> Keywords: thinking-aware generation, ParaBench, MMaDA-Parallel, cross-modal consistency, Parallel Reinforcement Learning  

<br /><br />Summary: The paper addresses the issue of performance degradation in existing sequential, autoregressive approaches to thinking-aware generation, specifically due to error propagation. To analyze this problem, the authors introduce ParaBench, a benchmark for evaluating both text and image outputs. Their findings indicate a strong correlation between performance degradation and misalignment between generated reasoning and final images. To tackle this challenge, they propose MMaDA-Parallel, a parallel multimodal diffusion framework that facilitates continuous interaction between text and images throughout the denoising process. This model uses supervised finetuning followed by a novel optimization strategy called Parallel Reinforcement Learning (ParaRL), which applies semantic rewards to ensure cross-modal consistency. Experimental results demonstrate that MMaDA-Parallel significantly enhances alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel. The study establishes a more robust framework for thinking-aware image synthesis and contributes to the field by providing open-sourced code for further research and development. <div>
arXiv:2511.09611v3 Announce Type: replace 
Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction</title>
<link>https://arxiv.org/abs/2511.08945</link>
<guid>https://arxiv.org/abs/2511.08945</guid>
<content:encoded><![CDATA[
<div> Keywords: Fractal Generative Models, Hausdorff Dimension, output diversity, image quality, rejection sampling  

<br /><br />Summary: The focus of this paper is to enhance the diversity of outputs generated by Fractal Generative Models (FGMs) while maintaining high visual quality. FGMs are recognized for generating high-quality images, but their self-similarity poses a challenge in output diversity. The authors introduce a method leveraging the Hausdorff Dimension (HD) from fractal geometry to quantify structural complexity, which is pivotal for improving output diversity. A learnable HD estimation technique is proposed to predict HD from image embeddings, alleviating computational cost concerns. However, they identify that merely incorporating HD into a hybrid loss does not sufficiently enhance diversity due to potential degradation of image quality and limited improvements in generation diversity. To address these issues, they implement an HD-based loss with a monotonic momentum-driven scheduling strategy during training, optimizing hyperparameters for superior diversity without compromising visual quality. During inference, HD-guided rejection sampling is utilized to select geometrically richer outputs. Extensive testing on the ImageNet dataset indicates that the proposed FGM-HD framework achieves a 39% improvement in output diversity compared to traditional FGMs while maintaining similar image quality. This work is a pioneering effort in integrating HD into FGM development. <div>
arXiv:2511.08945v2 Announce Type: replace 
Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning</title>
<link>https://arxiv.org/abs/2511.10040</link>
<guid>https://arxiv.org/abs/2511.10040</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D variational autoencoder, unsigned distance fields, local-to-global architecture, reconstruction accuracy, generative quality  

<br /><br />Summary: This paper addresses the challenge of generating high-fidelity 3D content by leveraging a novel framework based on unsigned distance fields (UDFs). Unlike traditional methods that rely on signed distance fields (SDFs), which encounter issues with non-manifold geometries and require costly preprocessing, UDFs provide a more efficient means of representing complex shapes. The proposed local-to-global (LoG) architecture processes UDFs by dividing them into uniform subvolumes known as UBlocks, enabling the combination of local detail capture via 3D convolutions and global consistency through sparse transformers. Additionally, a Pad-Average strategy is implemented to ensure smooth transitions at the boundaries of these subvolumes during reconstruction. This modular approach allows for scaling to ultra-high resolutions of up to $2048^3$, a significant achievement for 3D VAEs. Experimental results indicate that the framework outperforms existing methods in terms of reconstruction accuracy and generative quality, achieving superior surface smoothness and geometric flexibility. These advancements offer promising prospects for the generation of intricate 3D content in various applications. <div>
arXiv:2511.10040v2 Announce Type: replace 
Abstract: Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to $2048^3$-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising</title>
<link>https://arxiv.org/abs/2511.10500</link>
<guid>https://arxiv.org/abs/2511.10500</guid>
<content:encoded><![CDATA[
<div> Total Variation, Learnable Total Variation, Lambda Mapping Network, adaptive smoothing, image reconstruction<br /><br />Summary:<br /><br />1. The article addresses the limitations of traditional Total Variation (TV) methods for image denoising, specifically their reliance on a fixed lambda parameter which affects efficiency and usability.<br />2. It introduces a novel Learnable Total Variation (LTV) framework that integrates an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet). LambdaNet predicts a per-pixel regularization map, allowing spatially adaptive smoothing that varies across the image.<br />3. This combined pipeline is trained end-to-end, enabling joint optimization of the reconstruction process and the regularization strength, resulting in smoothing that is strong in homogeneous regions and relaxed near anatomical boundaries to preserve edges.<br />4. Experimental validation on the DeepLesion dataset, using a realistic noise model inspired by the LoDoPaB-CT methodology, demonstrates that LTV outperforms classical TV and FBP+U-Net approaches, with average improvements of +2.9 dB in PSNR and +6% in SSIM.<br />5. The proposed LTV framework offers an interpretable alternative to traditional black-box convolutional neural networks and lays the groundwork for extending to 3D reconstruction and incorporating data-consistency constraints in imaging tasks. <div>
arXiv:2511.10500v2 Announce Type: replace 
Abstract: Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</title>
<link>https://arxiv.org/abs/2511.10555</link>
<guid>https://arxiv.org/abs/2511.10555</guid>
<content:encoded><![CDATA[
<div> Keywords: visual stylization, style code, image generation, diffusion model, style embeddings<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating novel and consistent visual styles in artistic creation, a task that previous methods have struggled with due to reliance on complex inputs like lengthy textual prompts, reference images, or fine-tuning.<br /><br />2. It introduces a new task called code-to-style image generation, which uses a single numerical code to condition image generation, aiming to produce images with novel and consistent visual styles.<br /><br />3. The authors propose CoTyle, the first open-source method tackling this task, bridging a gap previously dominated by proprietary industrial solutions like Midjourney.<br /><br />4. CoTyle involves training a discrete style codebook from image collections to extract style embeddings, which serve as conditions for a text-to-image diffusion model (T2I-DM) capable of generating stylistic images.<br /><br />5. An autoregressive style generator is trained over the discrete embeddings to model their distribution and enable the synthesis of novel style codes. During inference, a numerical style code is mapped to a unique style embedding, guiding the T2I-DM to output images matching that style.<br /><br />This method offers simplicity and diverse stylistic outputs, effectively demonstrating that a visual style can be captured and controlled by a single numerical code. Extensive experiments verify its ability to reproducibly generate a wide range of styles from minimal input. <div>
arXiv:2511.10555v3 Announce Type: replace 
Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title>
<link>https://arxiv.org/abs/2511.10094</link>
<guid>https://arxiv.org/abs/2511.10094</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, physical plausibility, Matryoshka Transcoders, evaluation benchmark, feature learning

<br /><br />Summary: The paper addresses the issue of physical plausibility errors in generative models, which can produce realistic outputs but often fail to adhere to physical constraints. These errors are critical in application but are often missed by existing evaluation methods, highlighting a gap in automated identification and interpretation of these issues. To tackle this, the authors introduce Matryoshka Transcoders, a framework that combines transcoder architectures with the Matryoshka representation learning paradigm. This innovative approach facilitates hierarchical sparse feature learning at multiple granularity levels. By leveraging a physical plausibility classifier's intermediate representations and large multimodal models for interpretation, the framework identifies diverse failure modes without the need for manual feature engineering. This leads to improved feature relevance and accuracy compared to traditional methods. Additionally, the discovered visual patterns are utilized to create a benchmark for evaluating physical plausibility in generative models. The authors analyze eight state-of-the-art generative models, providing insights into their shortcomings regarding physical constraints and paving the way for future enhancements in model performance. <div>
arXiv:2511.10094v2 Announce Type: replace-cross 
Abstract: Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Transcoders, a novel framework for the automatic discovery and interpretation of physical plausibility features in generative models. Our approach extends the Matryoshka representation learning paradigm to transcoder architectures, enabling hierarchical sparse feature learning at multiple granularity levels. By training on intermediate representations from a physical plausibility classifier and leveraging large multimodal models for interpretation, our method identifies diverse physics-related failure modes without manual feature engineering, achieving superior feature relevance and feature accuracy compared to existing approaches. We utilize the discovered visual patterns to establish a benchmark for evaluating physical plausibility in generative models. Our analysis of eight state-of-the-art generative models provides valuable insights into how these models fail to follow physical constraints, paving the way for further model improvements.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation</title>
<link>https://arxiv.org/abs/2511.13744</link>
<guid>https://arxiv.org/abs/2511.13744</guid>
<content:encoded><![CDATA[
<div> autonomous driving, closed-loop simulation, BEV perception, nuCarla dataset, CARLA simulator<br /><br />Summary:<br /><br />1. The paper addresses the challenge of end-to-end (E2E) autonomous driving, which requires closed-loop simulation for jointly training and evaluating perception, planning, and control components in interactive environments. <br /><br />2. Existing real-world datasets mostly support open-loop learning with limited value for closed-loop testing due to their non-interactive data collection methods, limiting the development of meaningful intermediate representations like bird's-eye-view (BEV) features.<br /><br />3. To overcome these limitations, the authors introduce nuCarla, a large-scale BEV perception dataset created within the CARLA simulator, designed to be nuScenes-style and fully compatible with the nuScenes format.<br /><br />4. The nuCarla dataset matches the scale of nuScenes but offers more balanced class distributions and is directly usable for closed-loop simulation deployment, facilitating more reliable E2E autonomous driving research.<br /><br />5. Additionally, nuCarla provides high-performance BEV backbone models achieving state-of-the-art detection results, with both data and models released as open benchmarks aimed at accelerating the development and safety-awareness of closed-loop E2E autonomous driving systems. <div>
arXiv:2511.13744v1 Announce Type: new 
Abstract: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition</title>
<link>https://arxiv.org/abs/2511.13775</link>
<guid>https://arxiv.org/abs/2511.13775</guid>
<content:encoded><![CDATA[
<div> Keywords: Open Set Recognition, overconfidence, uncertainty estimation, feature space, unknown detection  

<br /><br />Summary: Open Set Recognition (OSR) aims to classify known classes accurately while rejecting unknown samples. A challenge in OSR arises from unknown samples that resemble known classes, leading to inter-class overlap in the feature space, which often results in models being overly confident and misclassifying these unknowns. This overconfidence blurs the decision boundary between known and unknown classes, complicating OSR. To address this issue, the authors propose a framework that reduces overconfidence due to inter-class overlap. The proposed framework includes two main components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to create diverse predictions and assess uncertainty, and an unknown detection module designed with separate learning-based classifiers, functioning in a two-stage process. This module utilizes the estimated uncertainty to better differentiate between known and unknown classes, thereby enhancing OSR performance. Experimental results demonstrate that the proposed framework significantly outperforms existing OSR methods across three public datasets, underscoring its efficacy in managing inter-class overlap and improving classification decisions. <div>
arXiv:2511.13775v1 Announce Type: new 
Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection</title>
<link>https://arxiv.org/abs/2511.13784</link>
<guid>https://arxiv.org/abs/2511.13784</guid>
<content:encoded><![CDATA[
<div> Keywords: Few-shot Video Object Detection, temporal consistency, novel object generalization, feature propagation, AP improvements

<br /><br />Summary:  
Few-shot Video Object Detection (FSVOD) is a task aimed at detecting novel objects in videos using limited labeled examples, addressing the limitations of conventional detection methods that require extensive training data. Key challenges include maintaining temporal consistency across frames with occlusions and variations in appearance, and achieving generalization for novel objects without complex region proposals, which can be resource-intensive and specific to tasks. The authors propose a novel object-aware temporal modeling approach that includes a filtering mechanism to selectively propagate high-confidence object features across video frames. This technique allows for efficient feature progression, minimizing noise accumulation and enhancing detection accuracy in few-shot scenarios. By employing few-shot training for detection and classification heads with focused feature propagation, robust temporal consistency is achieved without the need for explicit object tube proposals. The results show notable performance gains, with Average Precision (AP) improvements of 3.7% for FSVOD-500, 5.3% for FSYTV-40, 4.3% for VidOR, and 4.5 for VidVRD in a 5-shot setting. Further evaluations reveal enhancements in 1-shot, 3-shot, and 10-shot configurations, with the authors making their code publicly available. <div>
arXiv:2511.13784v1 Announce Type: new 
Abstract: Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching</title>
<link>https://arxiv.org/abs/2511.13794</link>
<guid>https://arxiv.org/abs/2511.13794</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal image fusion, flow matching, pseudo-label selection, Fusion Refiner, continual learning<br /><br />Summary:<br />1. The paper addresses limitations in current multi-modal image fusion methods, which often require task-specific models leading to high training costs and reduced scalability.<br />2. It proposes formulating image fusion as a direct probabilistic transport problem from source modalities to the fused image distribution, using flow matching to enhance sampling efficiency and structural consistency.<br />3. To overcome the shortage of high-quality fused images for supervision, the method collects fusion results from various state-of-the-art models as priors and applies a task-aware selection function to choose the most reliable pseudo-labels for each task.<br />4. A Fusion Refiner module is introduced that adopts a divide-and-conquer strategy to systematically identify, decompose, and improve degraded components within selected pseudo-labels.<br />5. For multi-task scenarios, elastic weight consolidation and experience replay are integrated to preserve cross-task performance and boost continual learning by maintaining parameter stability and memory retention.<br />6. The proposed approach achieves competitive performance on diverse fusion tasks, significantly improves sampling efficiency, and maintains a lightweight model design.<br />7. The code for this approach is made publicly available at the provided GitHub repository. <div>
arXiv:2511.13794v1 Announce Type: new 
Abstract: Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion</title>
<link>https://arxiv.org/abs/2511.13795</link>
<guid>https://arxiv.org/abs/2511.13795</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time crash detection, trajectory-free, diffusion model, road segment map, ControlNet<br /><br />Summary: The paper presents a novel two-stage framework for real-time crash detection that bypasses the need for vehicle trajectory acquisition and tracking by directly utilizing road segment maps containing individual-level traffic dynamics. The first stage involves a diffusion-based model called Mapfusion, which progressively corrupts a normal road segment map with Gaussian noise and then denoises it. This denoising process leverages sequential embeddings that capture temporal dynamics within segment map sequences. To improve generation control and incorporate contextual background information, the generation model integrates ControlNet. In the second stage, crash detection is carried out by comparing the monitored live road segment map with the generated future segment maps from the diffusion model. Mapfusion is trained solely on non-crash vehicle motion data, enabling it to realistically simulate road segment evolution based on learned motion patterns. It also demonstrates robustness to varying sampling intervals. Experimental evaluation on real-world crash scenarios shows that this two-stage trajectory-free approach effectively identifies crashes with high accuracy, addressing the common limitations of traditional trajectory-dependent crash detection methods and enhancing proactive traffic safety and efficiency management. <div>
arXiv:2511.13795v1 Announce Type: new 
Abstract: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model</title>
<link>https://arxiv.org/abs/2511.13800</link>
<guid>https://arxiv.org/abs/2511.13800</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, foundation models, seismic data, Hilbert encoding, training strategy

<br /><br />Summary: This article discusses the significant advances in using transformer-based foundation models within scientific fields such as drug discovery and materials research, particularly focusing on the challenges posed by seismic data. Seismic data has unique characteristics which necessitate specialized processing techniques for effective model pretraining. The authors identify that existing vision transformers (ViTs) fail to adequately capture the essential high- and low-frequency seismic information due to their sequential tokenization approach. To address this issue, they propose a novel training strategy named Adaptive Two-Grid foundation model training strategy (ADATG) with Hilbert encoding specifically for seismogram data. This approach utilizes spectrum decomposition to effectively separate high- and low-frequency elements, employing hierarchical Hilbert encoding for improved data representation. Additionally, the model's training strategy progresses from focusing on coarse-level information to fine-level features, aligning with the frequency principle observed in ViTs. Extensive experiments validate the effectiveness of the proposed methods, highlighting the significance of tailored data encoding and training strategies in enhancing visual seismic foundation models during pretraining. The research ultimately contributes to better processing techniques for visualizing seismic data in AI applications. <div>
arXiv:2511.13800v1 Announce Type: new 
Abstract: Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video</title>
<link>https://arxiv.org/abs/2511.13802</link>
<guid>https://arxiv.org/abs/2511.13802</guid>
<content:encoded><![CDATA[
<div> Keywords: dementia, facial micro dynamics, video analysis, YT DemTalk, screening  

<br /><br />Summary: This paper focuses on passive dementia screening through analysis of short camera-facing talking head videos. The authors develop a facial temporal micro dynamics approach that enables the detection of early neuro cognitive changes without the need for scripted language or clinician intervention. Unlike existing methods that rely on speech and scripted formats, this study explores temporal facial kinematics, including blink dynamics, mouth/jaw motions, gaze variability, and head adjustments, to identify potential dementia signs. The research stabilizes facial signals to create interpretable microdynamic time series, summarizing them into compact statistics for screening. The authors introduce a new dataset called YT DemTalk, comprising 300 clips featuring individuals with self-reported dementia and controls for model testing and benchmarking. Through ablation studies, gaze lability and mouth/jaw dynamics are highlighted as key indicators, achieving notable performance metrics with lightweight classifiers: AUROC of 0.953, Average Precision of 0.961, F1-score of 0.851, and an accuracy of 0.857. This work paves the way for scalable, unscripted video analysis for dementia screening across various settings. <div>
arXiv:2511.13802v1 Announce Type: new 
Abstract: We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2511.13853</link>
<guid>https://arxiv.org/abs/2511.13853</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, video generation models, Generative Visual Reasoning Benchmark, cognitive dimensions, multi-step planning

<br /><br />Summary: This article discusses the limitations of Chain-of-Thought (CoT) prompting in large language models (LLMs), which is restricted to discrete text and cannot replicate the continuous dynamics of the physical world. It highlights the emergence of video generation models utilizing Chain-of-Frames (CoF) reasoning to represent thought as visual sequences grounded in physics. However, existing benchmarks primarily focus on visual fidelity and fail to evaluate CoF reasoning, resulting in a gap in understanding models' cognitive abilities in key areas such as planning and logic. To address this, the authors introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework based on cognitive science that breaks down CoF reasoning into six cognitive dimensions and 24 subtasks. Gen-ViRe employs a combination of multi-source data curation, minimal prompting, and VLM-assisted evaluation to provide a quantitative assessment of video models' reasoning capabilities. Experimental results on state-of-the-art systems demonstrate a notable disparity between visual quality and reasoning depth. The benchmark establishes a vital foundation for diagnosing and improving the performance of models as genuine world simulators. <div>
arXiv:2511.13853v1 Announce Type: new 
Abstract: While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RSPose: Ranking Based Losses for Human Pose Estimation</title>
<link>https://arxiv.org/abs/2511.13857</link>
<guid>https://arxiv.org/abs/2511.13857</guid>
<content:encoded><![CDATA[
<div> Keywords: human pose estimation, heatmap losses, ranking-based losses, mAP alignment, RSPose<br /><br />Summary:  
The paper addresses key limitations in heatmap-based human pose estimation methods, specifically identifying three problems: (P1) the commonly used Mean Squared Error (MSE) loss treats all pixel deviations equally and fails to focus on accurate joint peak localization; (P2) heatmaps suffer from spatial and class-wise imbalance; and (P3) there exists a mismatch between the evaluation metric (mean Average Precision, mAP) and the loss functions used during training. To overcome these issues, the authors propose novel ranking-based loss functions that theoretically and empirically outperform traditional heatmap losses such as MSE and KL-Divergence. These new losses enhance the correlation between confidence scores and joint localization quality, which improves instance selection during Non-Maximum Suppression (NMS) and leads to better mAP performance. The proposed method, termed RSPose, is validated on both one-dimensional and two-dimensional heatmaps across three popular datasets: COCO, CrowdPose, and MPII. The work is notable for being the first to introduce loss functions explicitly aligned with the mAP evaluation metric in human pose estimation. RSPose achieves state-of-the-art results on the COCO validation set, reaching 79.9 mAP with the ViTPose-H vision transformer model. Additionally, the method improves SimCC Resnet-50 by 1.5 AP on COCO-val, reaching 73.6 AP, demonstrating broad applicability and effectiveness. <div>
arXiv:2511.13857v1 Announce Type: new 
Abstract: While heatmap-based human pose estimation methods have shown strong performance, they suffer from three main problems: (P1) "Commonly used Mean Squared Error (MSE)" Loss may not always improve joint localization because it penalizes all pixel deviations equally, without focusing explicitly on sharpening and correctly localizing the peak corresponding to the joint; (P2) heatmaps are spatially and class-wise imbalanced; and, (P3) there is a discrepancy between the evaluation metric (i.e., mAP) and the loss functions.
  We propose ranking-based losses to address these issues.
  Both theoretically and empirically, we show that our proposed losses are superior to commonly used heatmap losses (MSE, KL-Divergence). Our losses considerably increase the correlation between confidence scores and localization qualities, which is desirable because higher correlation leads to more accurate instance selection during Non-Maximum Suppression (NMS) and better Average Precision (mAP) performance. We refer to the models trained with our losses as RSPose.
  We show the effectiveness of RSPose across two different modes: one-dimensional and two-dimensional heatmaps, on three different datasets (COCO, CrowdPose, MPII).
  To the best of our knowledge, we are the first to propose losses that align with the evaluation metric (mAP) for human pose estimation.
  RSPose outperforms the previous state of the art on the COCO-val set and achieves an mAP score of 79.9 with ViTPose-H, a vision transformer model for human pose estimation.
  We also improve SimCC Resnet-50, a coordinate classification-based pose estimation method, by 1.5 AP on the COCO-val set, achieving 73.6 AP.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmenting Collision Sound Sources in Egocentric Videos</title>
<link>https://arxiv.org/abs/2511.13863</link>
<guid>https://arxiv.org/abs/2511.13863</guid>
<content:encoded><![CDATA[
<div> Keywords: Collision Sound Source Segmentation, audio-conditioned segmentation, egocentric video, foundation models, weakly-supervised learning<br /><br />Summary:<br /><br />1. The paper introduces a novel task called Collision Sound Source Segmentation (CS3), aiming to identify and segment objects responsible for collision sounds within visual input, specifically video frames, by conditioning on the audio signal.<br /><br />2. CS3 is challenging because collision sounds result from interactions between two objects, making the acoustic signature dependent on both, unlike isolated sound events.<br /><br />3. The study focuses on egocentric video data, which typically contains clear sounds but features cluttered scenes, small objects, and brief interaction episodes, complicating segmentation.<br /><br />4. To tackle these challenges, the authors propose a weakly-supervised audio-conditioned segmentation method leveraging foundation models CLIP and SAM2, combined with egocentric cues such as detecting objects held in the hands that are likely to cause collision sounds.<br /><br />5. The proposed approach is evaluated on two newly introduced benchmarks for CS3, EPIC-CS3 and Ego4D-CS3, outperforming competitive baselines by factors of 3× and 4.7× in mean Intersection over Union (mIoU), demonstrating the effectiveness of the method. <div>
arXiv:2511.13863v1 Announce Type: new 
Abstract: Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.
  To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRLoc: Geometric Representation Regression for Visual Localization</title>
<link>https://arxiv.org/abs/2511.13864</link>
<guid>https://arxiv.org/abs/2511.13864</guid>
<content:encoded><![CDATA[
<div> Absolute Pose Regression, Geometric Representation Regression, 6-DoF pose, disentangled geometric representations, inverse rendering<br /><br />Summary:<br /><br />1. The paper critiques traditional Absolute Pose Regression (APR) methods for visual localization, pointing out that such models often behave as black boxes that regress 6-DoF camera poses directly from query images, which can cause overfitting to training views rather than learning true 3D scene geometry.<br /><br />2. To address this, the authors introduce a new paradigm called Geometric Representation Regression (GRR), inspired by the inverse of novel view synthesis. Instead of directly predicting the pose, GRR regresses underlying geometric representations from a single image.<br /><br />3. Specifically, the model predicts two disentangled components in the world coordinate system: (a) a bundle of ray directions used for estimating camera rotation, and (b) a corresponding pointmap used for estimating camera translation.<br /><br />4. These components are combined through a differentiable deterministic solver to recover the final 6-DoF camera pose, enabling a clear separation between visual-to-geometry mapping and pose calculation.<br /><br />5. The disentangling of rotation and translation predictions introduces a strong geometric prior, improving robustness and generalization. Experiments show that the approach achieves state-of-the-art results on standard benchmarks such as 7-Scenes and Cambridge Landmarks datasets, validating the efficacy of modeling absolute pose estimation as an inverse rendering problem. <div>
arXiv:2511.13864v1 Announce Type: new 
Abstract: Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction</title>
<link>https://arxiv.org/abs/2511.13869</link>
<guid>https://arxiv.org/abs/2511.13869</guid>
<content:encoded><![CDATA[
<div> Bladder cancer, recurrence prediction, multi-sequence MRI, hierarchical gated attention, H-CNN-ViT<br /><br />Summary:<br /><br />Bladder cancer is a highly prevalent malignancy with recurrence rates reaching up to 78%, making precise postoperative monitoring vital. Multi-sequence contrast-enhanced MRI is a standard tool for detecting recurrence, but interpretation remains difficult due to postoperative changes like scarring and tissue remodeling. To address this, the authors present a novel, curated multi-sequence and multi-modal MRI dataset tailored specifically for bladder cancer recurrence prediction, filling a critical gap in available resources. They propose a new model, H-CNN-ViT, which employs hierarchical gated attention within a multi-branch architecture combining convolutional neural networks (CNN) for local feature extraction and Vision Transformers (ViT) for global feature representation. This design allows selective weighting of features based on context, ensuring balanced and targeted fusion of imaging modalities. Each MRI sequence is processed independently to preserve unique channel-specific information before integration. Evaluation on the introduced dataset demonstrates that H-CNN-ViT attains a superior Area Under the Curve (AUC) of 78.6%, outperforming existing state-of-the-art methods. The authors also provide public access to their model and code, supporting future research and development in AI-assisted bladder cancer recurrence monitoring. <div>
arXiv:2511.13869v1 Announce Type: new 
Abstract: Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT}.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QwenCLIP: Boosting Medical Vision-Language Pretraining via LLM Embeddings and Prompt tuning</title>
<link>https://arxiv.org/abs/2511.13876</link>
<guid>https://arxiv.org/abs/2511.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, QwenCLIP, LLM, medical semantics, radiology benchmarks  

<br /><br />Summary: The article introduces QwenCLIP, a novel vision-language framework designed to enhance the capabilities of the existing Contrastive Language-Image Pretraining (CLIP) in the medical domain. While CLIP has shown strong generalization for vision-language tasks, its text encoder is limited to 77 tokens, limiting its effectiveness in processing long radiology reports. Existing adaptations using domain-specific encoders like PubMedBERT or ClinicalBERT face similar constraints, particularly in input length (typically 512 tokens) and semantic understanding. QwenCLIP addresses these shortcomings by replacing CLIP's text encoder with a large language model (LLM)-based embedding module, such as Qwen3-Embedding. This replacement is combined with learnable prompts that aim to enhance cross-modal alignment. By utilizing the extended context window and richer semantic representations offered by LLMs, QwenCLIP is capable of capturing comprehensive medical semantics from lengthy clinical texts. This ultimately leads to significant improvements in medical image-text alignment and enhances performance on various radiology benchmarks. The code for QwenCLIP is publicly available, facilitating further research and development in this area. <div>
arXiv:2511.13876v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong generalization for vision-language tasks in computer vision and medical domains, yet its text encoder accepts only up to 77 tokens, which limits its ability to represent long and information-rich radiology reports. Recent adaptations using domain-specific encoders, such as PubMedBERT or ClinicalBERT, mitigate this issue by leveraging medical corpora, but remain constrained by their limited input length (typically 512 tokens) and relatively shallow semantic understanding. To address these limitations, we propose QwenCLIP, a vision-language framework that replaces CLIP's text encoder with a large language model (LLM)-based embedding module (e.g., Qwen3-Embedding) and introduces learnable prompts to enhance cross-modal alignment. By leveraging the extended context window and richer representations of LLMs, QwenCLIP captures comprehensive medical semantics from long-form clinical text, substantially improving medical image-text alignment and downstream performance on radiology benchmarks. Our code is publicly available at https://github.com/Wxy-24/QwenCLIP.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection</title>
<link>https://arxiv.org/abs/2511.13877</link>
<guid>https://arxiv.org/abs/2511.13877</guid>
<content:encoded><![CDATA[
<div> Keywords: lumbar spine degeneration, EfficientNet, VGG19, Pseudo-Newton Boosting, Sparsity-Induced Feature Reduction

<br /><br />Summary: This paper introduces a novel hybrid model architecture for classifying lumbar spine degeneration using DICOM images, combining EfficientNet and VGG19 with custom components. Unlike traditional transfer learning, the model integrates a Pseudo-Newton Boosting layer that adjusts feature weights to focus on detailed anatomical features often overlooked. Additionally, it employs a Sparsity-Induced Feature Reduction Layer that eliminates redundancy in learned features, resulting in compact yet robust representations pertinent to lumbar spine pathology. This multi-tiered framework addresses limitations prevalent in conventional transfer learning methods, especially in high-dimensional medical imaging contexts. The proposed architecture demonstrates a significant performance enhancement, achieving precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and accuracy of 88.1%, surpassing the baseline, EfficientNet. The paper will detail the architectural design, preprocessing pipeline, and experimental findings, ultimately contributing to the advancement of automated diagnostic tools for medical images. <div>
arXiv:2511.13877v1 Announce Type: new 
Abstract: This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLMs Guided Interpretable Decision Making for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.13881</link>
<guid>https://arxiv.org/abs/2511.13881</guid>
<content:encoded><![CDATA[
<div> Vision-language models, autonomous driving, visual question answering, multi-modal fusion, scene understanding<br /><br />Summary:<br /><br />This paper addresses the limitations of current vision-language models (VLMs) when applied directly to decision-making in autonomous driving (AD), particularly within visual question answering (VQA) frameworks that rely on handcrafted prompts and often yield inconsistent results. The authors evaluate state-of-the-art open-source VLMs using ego-view visual inputs on high-level driving tasks and find these models struggle to provide reliable, context-aware decisions. To overcome this, they propose repurposing VLMs not as direct decision-makers but as semantic enhancers that generate rich, structured, and linguistically detailed scene descriptions to complement existing vision-based benchmarks. Building upon these enriched semantic representations, the study introduces a multi-modal interactive architecture that effectively fuses visual and linguistic information, leading to improved decision accuracy and interpretable textual explanations for driving actions. Additionally, a post-hoc refinement module is designed to leverage VLMs for enhancing the reliability of predictions after initial decision outputs. Extensive experiments on two autonomous driving benchmarks confirm that this approach achieves state-of-the-art performance, demonstrating a promising strategy for integrating VLMs into AD systems that require both robustness and interpretability. <div>
arXiv:2511.13881v1 Announce Type: new 
Abstract: Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Data Scaling Law for Medical Segmentation</title>
<link>https://arxiv.org/abs/2511.13883</link>
<guid>https://arxiv.org/abs/2511.13883</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, medical imaging, data scaling, augmentation strategies, segmentation performance  

<br /><br />Summary: This study investigates the power law scaling of population loss in trained deep neural networks, particularly in the context of medical anatomical segmentation, an area that is notably underexplored. It analyzes the scaling relationship with dataset size across 15 semantic tasks and 4 imaging modalities, revealing that larger datasets consistently enhance segmentation performance. The research leverages topological isomorphism in images that share anatomical structures to evaluate the effectiveness of deformation-guided augmentation strategies, including random elastic deformation and registration-guided deformation. Additionally, a novel scalable image augmentation method is proposed, which generates diffeomorphic mappings based on image registration to create realistic deformations. Experimental results indicate that both registered and generated deformation-based augmentations significantly improve data utilization efficiency. The newly introduced generated deformation approach achieves outstanding performance and faster convergence, exceeding standard power law scaling trends without needing extra data. Overall, this work enhances understanding of segmentation scalability and the impact of topological variations in medical imaging, contributing to the development of more efficient models with lower annotation and computational costs. <div>
arXiv:2511.13883v1 Announce Type: new 
Abstract: The population loss of trained deep neural networks often exhibits power law scaling with the size of the training dataset, guiding significant performance advancements in deep learning applications. In this study, we focus on the scaling relationship with data size in the context of medical anatomical segmentation, a domain that remains underexplored. We analyze scaling laws for anatomical segmentation across 15 semantic tasks and 4 imaging modalities, demonstrating that larger datasets significantly improve segmentation performance, following similar scaling trends. Motivated by the topological isomorphism in images sharing anatomical structures, we evaluate the impact of deformation-guided augmentation strategies on data scaling laws, specifically random elastic deformation and registration-guided deformation. We also propose a novel, scalable image augmentation approach that generates diffeomorphic mappings from geodesic subspace based on image registration to introduce realistic deformation. Our experimental results demonstrate that both registered and generated deformation-based augmentation considerably enhance data utilization efficiency. The proposed generated deformation method notably achieves superior performance and accelerated convergence, surpassing standard power law scaling trends without requiring additional data. Overall, this work provides insights into the understanding of segmentation scalability and topological variation impact in medical imaging, thereby leading to more efficient model development with reduced annotation and computational costs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Hema: Unified Model for Digital Hematopathology</title>
<link>https://arxiv.org/abs/2511.13889</link>
<guid>https://arxiv.org/abs/2511.13889</guid>
<content:encoded><![CDATA[
<div> Keywords: digital hematopathology, multi-task model, Uni-Hema, Hema-Former, image analysis

<br /><br />Summary: Digital hematopathology necessitates a detailed cell-level analysis across various disease categories, including malignancies like leukemia, infectious diseases such as malaria, and non-malignant disorders including sickle cell disease. Current models, whether focused on single-tasks or integrating vision-language components, face limitations in providing cohesive multi-task and multi-modal reasoning. To address these issues, the authors introduce Uni-Hema, a unified multi-task model specifically designed for digital hematopathology. This model incorporates tasks such as detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema utilizes 46 publicly available datasets, amounting to over 700,000 images and 21,000 question-answer pairs. It is built upon Hema-Former, a multimodal module that effectively connects visual and textual representations. Comprehensive experiments reveal that Uni-Hema performs comparably or even better than traditional single-task models trained on individual datasets while offering interpretable insights and morphology-relevant data at the single-cell level. This innovative framework sets a new benchmark for multi-task and multi-modal applications in the field of digital hematopathology, with plans to publicly release its code. <div>
arXiv:2511.13889v1 Announce Type: new 
Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</title>
<link>https://arxiv.org/abs/2511.13891</link>
<guid>https://arxiv.org/abs/2511.13891</guid>
<content:encoded><![CDATA[
<div> Keywords: Ephemeral Gullies, Weak Supervision, Vision Language Models, Remote Sensing, Semi-supervised Detection  

<br /><br />Summary:  
1) The paper addresses the challenge of detecting ephemeral gullies in agricultural fields, which are difficult to identify due to their short temporal existence and the scarcity of accurately labeled data.  
2) It introduces the first weakly supervised pipeline for ephemeral gully detection that leverages remote sensing imagery and Vision Language Models (VLMs) to reduce the need for extensive manual labeling.  
3) The method exploits the knowledge embedded in pretrained VLMs and employs a teacher-student model: the teacher learns from noisy labels generated by the VLMs, while the student is trained under weak supervision using these teacher-generated labels alongside a noise-aware loss function.  
4) The authors also provide the first large-scale dataset for semi-supervised detection of ephemeral gullies, containing over 18,000 high-resolution remote sensing images collected over 13 years, with a subset labeled by soil and plant scientists and a large portion unlabeled.  
5) Experimental results confirm that this approach outperforms both VLMs alone and the initial label model when training the student under weak supervision. Additionally, the paper makes all code and datasets publicly available to facilitate further research in this area. <div>
arXiv:2511.13891v1 Announce Type: new 
Abstract: Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors</title>
<link>https://arxiv.org/abs/2511.13897</link>
<guid>https://arxiv.org/abs/2511.13897</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal realism, generative video models, motion vectors, evaluation metrics, MV-RGB fusion

<br /><br />Summary: 
The central issue in generative video models is their inability to capture temporal realism, primarily due to evaluation metrics that focus on spatial appearance without sensitivity to motion dynamics. This study presents a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted from compressed video streams, such as H.264 and HEVC. These codec-generated MVs offer resolution-consistent descriptors of motion dynamics, enabling the quantification of realism through Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between real and generated video MV statistics. Experiments using the GenVidBench dataset unveil systematic discrepancies in motion representation, with different metrics favoring generators like Pika, VC2, and Text2Video-Zero, while CogVideo shows the most significant deviations. Visual outputs of MV fields highlight issues such as center bias and grid artifacts that traditional frame-level metrics fail to address. Additionally, the study explores MV-RGB fusion techniques—such as channel concatenation and motion-aware modules—that enhance downstream classification tasks, achieving high accuracy rates with models like ResNet and I3D. The findings suggest that incorporating motion vectors into generative models can significantly improve temporal reasoning and diagnostic capabilities for motion defects. The study’s implementation is publicly available for further research. <div>
arXiv:2511.13897v1 Announce Type: new 
Abstract: Temporal realism remains a central weakness of current generative video models, as most evaluation metrics prioritize spatial appearance and offer limited sensitivity to motion. We introduce a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted directly from compressed video streams. Codec-generated MVs from standards such as H.264 and HEVC provide lightweight, resolution-consistent descriptors of motion dynamics. We quantify realism by computing Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between MV statistics of real and generated videos. Experiments on the GenVidBench dataset containing videos from eight state-of-the-art generators reveal systematic discrepancies from real motion: entropy-based divergences rank Pika and SVD as closest to real videos, MV-sum statistics favor VC2 and Text2Video-Zero, and CogVideo shows the largest deviations across both measures. Visualizations of MV fields and class-conditional motion heatmaps further reveal center bias, sparse and piecewise constant flows, and grid-like artifacts that frame-level metrics do not capture. Beyond evaluation, we investigate MV-RGB fusion through channel concatenation, cross-attention, joint embedding, and a motion-aware fusion module. Incorporating MVs improves downstream classification across ResNet, I3D, and TSN backbones, with ResNet-18 and ResNet-34 reaching up to 97.4% accuracy and I3D achieving 99.0% accuracy on real-versus-generated discrimination. These findings demonstrate that compressed-domain MVs provide an effective temporal signal for diagnosing motion defects in generative videos and for strengthening temporal reasoning in discriminative models. The implementation is available at: https://github.com/KurbanIntelligenceLab/Motion-Vector-Learning
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing</title>
<link>https://arxiv.org/abs/2511.13904</link>
<guid>https://arxiv.org/abs/2511.13904</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Transportation Systems, Multi-Camera Vehicle Tracking, real-time performance, scalability, SAE-MCVT

<br /><br />Summary: In modern Intelligent Transportation Systems (ITS), cameras are crucial for providing information to various stakeholders. A significant challenge is Multi-Camera Vehicle Tracking (MCVT), which is essential for applications like anomaly detection and traffic density estimation. However, existing MCVT methods mainly focus on accuracy while neglecting real-time performance and scalability, which are critical for real-world applications, especially in city-scale environments where the number of cameras increases. To tackle these challenges, the authors propose SAE-MCVT, the first scalable real-time MCVT framework. This system consists of multiple edge devices that interact with a central workstation. On the edge, live RTSP video streams are processed through several modules for object detection, tracking, geo-mapping, and feature extraction, with only essential metadata transmitted to the central workstation. The central side focuses on calculating cross-camera associations using spatial-temporal constraints learned through a self-supervised model. Experimental results on the RoundaboutHD dataset indicate that SAE-MCVT operates in real-time with 2K 15 FPS video streams and achieves an IDF1 score of 61.2, marking a significant advancement for city-scale MCVT deployment. <div>
arXiv:2511.13904v1 Announce Type: new 
Abstract: In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles</title>
<link>https://arxiv.org/abs/2511.13909</link>
<guid>https://arxiv.org/abs/2511.13909</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, road safety, autonomous vehicles, traffic signs, zero-shot evaluation  

<br /><br />Summary:  
1. The study focuses on the importance of adhering to road safety norms not only for humans but also for AI systems managing autonomous vehicles.  
2. It evaluates the understanding of road safety concepts by multi-modal large language models (LLMs) using schematic and illustrative representations.  
3. A pilot dataset was curated from school textbooks, consisting of images showing traffic signs and road safety norms, to test the models’ capabilities in a zero-shot setting.  
4. Preliminary results reveal that these multi-modal LLMs struggle significantly with safety reasoning, indicating a gap between how humans learn these concepts and how models interpret them.  
5. The paper provides detailed analysis of the performance gaps, highlighting areas where future research could improve AI comprehension of road safety to enhance autonomous vehicle behavior and safety compliance. <div>
arXiv:2511.13909v1 Announce Type: new 
Abstract: Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding</title>
<link>https://arxiv.org/abs/2511.13924</link>
<guid>https://arxiv.org/abs/2511.13924</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, reinforcement learning, Visual Grounding, Curriculum-based Relative Policy Optimization, localization performance

<br /><br />Summary: The paper addresses the effectiveness of Chain-of-Thought (CoT) prompting in enhancing reasoning processes in NLP and computer vision tasks. However, it reveals that fine-tuning CoT reasoning with reinforcement learning (RL) can lead to performance degradation in Visual Grounding tasks, particularly when CoT outputs become lengthy or complex. The study also highlights that an increase in dataset size does not always correlate with better performance due to the varying complexities of data. To tackle these challenges, the authors propose a novel strategy called Curriculum-based Relative Policy Optimization (CuRPO), which uses CoT output length and generalized Intersection over Union (gIoU) rewards as indicators of complexity. This method structures training data from simpler to more challenging examples. Comprehensive experiments conducted on datasets like RefCOCO, RefCOCO+, RefCOCOg, and LISA demonstrate that CuRPO consistently outperforms existing methods, achieving improvements of up to +12.52 mAP on RefCOCO. Additionally, CuRPO shows remarkable efficiency and robustness, achieving strong localization results even in few-shot learning scenarios, which is particularly advantageous for ambiguous and complex text descriptions. The code for CuRPO is available on GitHub. <div>
arXiv:2511.13924v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets</title>
<link>https://arxiv.org/abs/2511.13944</link>
<guid>https://arxiv.org/abs/2511.13944</guid>
<content:encoded><![CDATA[
<div> Keywords: cluster-based, frame selection, information leakage, video-derived frames, dataset partitions  

<br /><br />Summary: The article presents a new approach to frame selection designed to address the issue of information leakage within datasets derived from videos. The proposed method employs a cluster-based strategy to group visually similar frames prior to the division of the dataset into training, validation, and test sets. This clustering process ensures that the resulting partitions are more representative of the data as a whole, leading to a more balanced distribution of frames across these sets. By doing so, it not only enhances the reliability of the dataset partitions but also mitigates potential biases that can arise from conventional splitting techniques. The authors argue that this approach can significantly improve the performance of machine learning models trained on video-derived datasets by providing them with a more accurate representation of the underlying data. Overall, the proposed cluster-based frame selection strategy emerges as a promising solution to enhance data integrity and effectiveness in video analysis tasks. <div>
arXiv:2511.13944v1 Announce Type: new 
Abstract: We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers</title>
<link>https://arxiv.org/abs/2511.13945</link>
<guid>https://arxiv.org/abs/2511.13945</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, Procedurally-Generated Data, Pretraining, Data Efficiency, ImageNet-1k<br /><br />Summary:<br /><br />This paper investigates how to improve vision transformers (ViTs) by integrating inductive biases derived from non-visual, procedurally-generated data created using formal algorithms like grammars. By pretraining ViTs on such data, which deliberately lacks natural or semantic image content, the approach encourages the models to develop abstract computational priors rather than relying solely on standard visual patch embeddings. This "warm-up" phase effectively bypasses the initial patch embedding step, priming the network to internalize more generic structural biases useful across different modalities. The authors demonstrate that following this procedural-data warm-up with conventional image-based training on datasets like ImageNet-1k enhances multiple performance metrics. Specifically, allocating only 1% of the training budget to procedural data results in a remarkable accuracy improvement of over 1.7% on ImageNet-1k. Moreover, this small fraction of procedural pretraining equates in impact to using an additional 28% of ImageNet-1k data, highlighting significant gains in data efficiency and faster convergence. These findings point to the potential for developing novel, domain-agnostic pretraining strategies that leverage abstract computational structures to boost transformer performance in vision tasks. <div>
arXiv:2511.13945v1 Announce Type: new 
Abstract: Transformers show remarkable versatility across domains, suggesting the existence of inductive biases beneficial across modalities. In this work, we explore a new way to instil such generic biases in vision transformers (ViTs) by pretraining on procedurally-generated data devoid of visual or semantic content. We generate this data with simple algorithms such as formal grammars, so the results bear no relationship to either natural or synthetic images. We use this procedurally-generated data to pretrain ViTs in a warm-up phase that bypasses their visual patch embedding mechanisms, thus encouraging the models to internalise abstract computational priors. When followed by standard image-based training, this warm-up significantly improves data efficiency, convergence speed, and downstream performance. On ImageNet-1k for example, allocating just 1% of the training budget to procedural data improves final accuracy by over 1.7%. In terms of its effect on performance, 1% procedurally generated data is thus equivalent to 28% of the ImageNet-1k data. These findings suggest a promising path toward new data-efficient and domain-agnostic pretraining strategies.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single Tensor Cell Segmentation using Scalar Field Representations</title>
<link>https://arxiv.org/abs/2511.13947</link>
<guid>https://arxiv.org/abs/2511.13947</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, scalar fields, Poisson equation, watershed method, U-Net  

<br /><br />Summary: The paper explores a novel approach for cell image segmentation using scalar fields. The objective is to create a continuous scalar field within image domains that enables effective segmentation of cell instances. The segmentation process leverages the watershed method, based on a scalar field parameterized by a trained network. The authors focus on two types of scalar fields: solutions to the Poisson partial differential equation and a diffusion model representing the steady-state of the heat equation. Notably, the approach requires minimizing just the field residuals, eliminating the need for regularization, which enhances robustness against outliers and supports the maintenance of sharp cell boundaries. The implementation process is simplified due to the use of a single tensor for training a U-Net, resulting in decreased training and inference times as well as reduced energy consumption and memory usage, making it suitable for edge computing environments. Competitive results are presented on public datasets, demonstrating that this geometrically insightful method achieves high-quality cell segmentation outcomes, underscoring the efficacy and efficiency of the proposed technique. <div>
arXiv:2511.13947v1 Announce Type: new 
Abstract: We investigate image segmentation of cells under the lens of scalar fields. Our goal is to learn a continuous scalar field on image domains such that its segmentation produces robust instances for cells present in images. This field is a function parameterized by the trained network, and its segmentation is realized by the watershed method. The fields we experiment with are solutions to the Poisson partial differential equation and a diffusion mimicking the steady-state solution of the heat equation. These solutions are obtained by minimizing just the field residuals, no regularization is needed, providing a robust regression capable of diminishing the adverse impacts of outliers in the training data and allowing for sharp cell boundaries. A single tensor is all that is needed to train a \unet\ thus simplifying implementation, lowering training and inference times, hence reducing energy consumption, and requiring a small memory footprint, all attractive features in edge computing. We present competitive results on public datasets from the literature and show that our novel, simple yet geometrically insightful approach can achieve excellent cell segmentation results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation</title>
<link>https://arxiv.org/abs/2511.13948</link>
<guid>https://arxiv.org/abs/2511.13948</guid>
<content:encoded><![CDATA[
<div> Keywords: Echocardiography, Large Language Model, Video-level reasoning, Measurement-feasibility, Clinical interpretation<br /><br />Summary:<br /><br />1. Purpose: The study addresses the challenge of performing video-level reasoning and guideline-based measurement analysis in echocardiographic interpretation, which current deep learning models for cardiac ultrasound lack. The authors introduce EchoAgent, a novel framework designed to automate and structure this complex task with interpretable outputs.<br /><br />2. Methods: EchoAgent integrates specialized computer vision modules controlled by a Large Language Model (LLM) to carry out temporal localization, spatial measurement, and clinical interpretation on echocardiographic videos. A key innovation is the measurement-feasibility prediction model that autonomously determines if anatomical structures in each video frame are reliably measurable, guiding the selection of appropriate tools.<br /><br />3. Benchmarking: The researchers compiled a benchmark dataset comprising diverse, clinically validated video-query pairs to rigorously evaluate EchoAgent’s performance, emphasizing real-world relevance and clinical accuracy.<br /><br />4. Results: EchoAgent delivers accurate and interpretable results despite the complexity of spatiotemporal video analysis. Its outputs are firmly grounded in visual evidence and adhere strictly to clinical guidelines, enhancing transparency and traceability.<br /><br />5. Conclusion: The framework demonstrates the feasibility of agentic, guideline-aligned reasoning in echocardiographic video analysis, offering fully automated, trustworthy AI support in cardiac ultrasound. EchoAgent represents a significant advancement towards interpretable and clinically reliable ultrasound diagnostics. <div>
arXiv:2511.13948v1 Announce Type: new 
Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Skill-Attributes for Transferable Assessment in Video</title>
<link>https://arxiv.org/abs/2511.13993</link>
<guid>https://arxiv.org/abs/2511.13993</guid>
<content:encoded><![CDATA[
<div> Keywords: skill assessment, video representations, CrossTrainer, multimodal model, transferable attributes

<br /><br />Summary: The article discusses the challenges of skill assessment from video, focusing on evaluating and providing feedback on a person's physical performance across various sports. Current models are limited to specific sports and require expert-level supervision, which is costly and scarce. To address this, the authors introduce the CrossTrainer approach, which identifies transferable skill attributes, such as balance and control, that are applicable across different sports. The method employs a multimodal language model to generate specific, actionable feedback for athletes, such as recommendations for improving hand positioning, and also assesses proficiency levels, ranging from beginner to expert. The effectiveness of CrossTrainer is validated through multiple datasets, demonstrating significant performance improvements—up to 60% relative to existing state-of-the-art models—both in cross-sport (transfer) and intra-sport (in-domain) testing scenarios. By distilling shared behaviors that indicate human skill, this new video representation method offers enhanced generalization capabilities compared to previous techniques, ultimately enriching the potential applications of multimodal large language models in skill assessment contexts. <div>
arXiv:2511.13993v1 Announce Type: new 
Abstract: Skill assessment from video entails rating the quality of a person's physical performance and explaining what could be done better. Today's models specialize for an individual sport, and suffer from the high cost and scarcity of expert-level supervision across the long tail of sports. Towards closing that gap, we explore transferable video representations for skill assessment. Our CrossTrainer approach discovers skill-attributes, such as balance, control, and hand positioning -- whose meaning transcends the boundaries of any given sport, then trains a multimodal language model to generate actionable feedback for a novel video, e.g., "lift hands more to generate more power" as well as its proficiency level, e.g., early expert. We validate the new model on multiple datasets for both cross-sport (transfer) and intra-sport (in-domain) settings, where it achieves gains up to 60% relative to the state of the art. By abstracting out the shared behaviors indicative of human skill, the proposed video representation generalizes substantially better than an array of existing techniques, enriching today's multimodal large language models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution</title>
<link>https://arxiv.org/abs/2511.14014</link>
<guid>https://arxiv.org/abs/2511.14014</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-contrast MRI, super-resolution, convolutional dictionary, feature decoupling, image reconstruction  

<br /><br />Summary: The paper presents a novel approach for multi-contrast magnetic resonance imaging (MRI) super-resolution, aiming to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging information from HR reference images with varied contrasts. The authors highlight the significance of enhancing anatomical details for improved diagnosis and clinical decisions while addressing challenges posed by contrast disparities in the various modalities. To overcome these issues, they propose a dual-prompt expert network using a convolutional dictionary feature decoupling (CD-DPE) strategy. This involves an iterative convolutional dictionary feature decoupling module (CD-FDM) to isolate features into cross-contrast and intra-contrast components, reducing redundancy. To ensure effective feature integration, a dual-prompt feature fusion expert module (DP-FFEM) is introduced, utilizing a frequency prompt for relevant feature selection and an adaptive routing prompt for optimal feature fusion methods. Experimental results on public multi-contrast MRI datasets demonstrate that CD-DPE surpasses state-of-the-art methods in detail reconstruction. Additional tests on unseen datasets reveal its strong generalization capabilities, indicating its potential for clinical applications in MRI imaging. <div>
arXiv:2511.14014v1 Announce Type: new 
Abstract: Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RISE: Single Static Radar-based Indoor Scene Understanding</title>
<link>https://arxiv.org/abs/2511.14019</link>
<guid>https://arxiv.org/abs/2511.14019</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor scene understanding, millimeter-wave radar, layout reconstruction, object detection, privacy-preserving

<br /><br />Summary: Robust indoor scene understanding faces challenges due to occlusions and privacy risks associated with traditional optical sensors like RGB and LiDAR. Millimeter-wave (mmWave) radar offers a solution, providing privacy and the ability to penetrate obstacles, although it struggles with low spatial resolution. This paper introduces RISE, the first benchmark and system dedicated to indoor scene understanding using single-static radar. RISE focuses on layout reconstruction and object detection, leveraging the insight that multipath reflections can provide valuable geometric cues. A novel Bi-Angular Multipath Enhancement technique is proposed to model Angle-of-Arrival and Angle-of-Departure, which helps recover secondary reflections and reveals hidden structures. Additionally, a simulation-to-reality Hierarchical Diffusion framework is presented to convert fragmented radar data into coherent layouts and object detection. The benchmark comprises a large-scale dataset of 50,000 frames from 100 real indoor trajectories. Experiments indicate that RISE significantly improves performance, reducing Chamfer Distance by 60% (to 16 cm) in layout reconstruction and achieving 58% IoU in mmWave-based object detection, marking RISE as a foundational tool for geometry-aware and privacy-preserving indoor scene understanding. <div>
arXiv:2511.14019v1 Announce Type: new 
Abstract: Robust and privacy-preserving indoor scene understanding remains a fundamental open problem. While optical sensors such as RGB and LiDAR offer high spatial fidelity, they suffer from severe occlusions and introduce privacy risks in indoor environments. In contrast, millimeter-wave (mmWave) radar preserves privacy and penetrates obstacles, but its inherently low spatial resolution makes reliable geometric reasoning difficult.
  We introduce RISE, the first benchmark and system for single-static-radar indoor scene understanding, jointly targeting layout reconstruction and object detection. RISE is built upon the key insight that multipath reflections, traditionally treated as noise, encode rich geometric cues. To exploit this, we propose a Bi-Angular Multipath Enhancement that explicitly models Angle-of-Arrival and Angle-of-Departure to recover secondary (ghost) reflections and reveal invisible structures. On top of these enhanced observations, a simulation-to-reality Hierarchical Diffusion framework transforms fragmented radar responses into complete layout reconstruction and object detection.
  Our benchmark contains 50,000 frames collected across 100 real indoor trajectories, forming the first large-scale dataset dedicated to radar-based indoor scene understanding. Extensive experiments show that RISE reduces the Chamfer Distance by 60% (down to 16 cm) compared to the state of the art in layout reconstruction, and delivers the first mmWave-based object detection, achieving 58% IoU. These results establish RISE as a new foundation for geometry-aware and privacy-preserving indoor scene understanding using a single static radar.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI Plane Orientation Detection using a Context-Aware 2.5D Model</title>
<link>https://arxiv.org/abs/2511.14021</link>
<guid>https://arxiv.org/abs/2511.14021</guid>
<content:encoded><![CDATA[
<div> Keywords: anatomical planes, MRI, classifier, metadata, brain tumor detection  

<br /><br />Summary: This study addresses the challenge of identifying anatomical planes (axial, coronal, and sagittal) in 2D MRI slices, a task that automated systems find difficult. The absence of plane orientation metadata can complicate analysis and decrease the accuracy of diagnostic classifiers, especially when integrating diverse datasets. To resolve this issue, the authors develop a classifier that accurately generates plane orientation metadata. They implement a 2.5D context-aware model that uses multi-slice information to enhance feature learning and mitigate confusion from isolated slices. The model is trained on both 3D slice sequences and 2D images, achieving significant accuracy improvements—from 98.74% with the 2D reference model to 99.49% with the 2.5D approach, marking a 60% reduction in errors. The utility of the generated metadata is validated in a brain tumor detection task, where a gated strategy utilizing metadata-enhanced predictions increases accuracy from 97.0% to 98.0%, resulting in a 33.3% reduction in misdiagnoses. The developed plane orientation model is incorporated into an interactive web application, which is made available as open-source software. <div>
arXiv:2511.14021v1 Announce Type: new 
Abstract: Humans can easily identify anatomical planes (axial, coronal, and sagittal) on a 2D MRI slice, but automated systems struggle with this task. Missing plane orientation metadata can complicate analysis, increase domain shift when merging heterogeneous datasets, and reduce accuracy of diagnostic classifiers. This study develops a classifier that accurately generates plane orientation metadata. We adopt a 2.5D context-aware model that leverages multi-slice information to avoid ambiguity from isolated slices and enable robust feature learning. We train the 2.5D model on both 3D slice sequences and static 2D images. While our 2D reference model achieves 98.74% accuracy, our 2.5D method raises this to 99.49%, reducing errors by 60%, highlighting the importance of 2.5D context. We validate the utility of our generated metadata in a brain tumor detection task. A gated strategy selectively uses metadata-enhanced predictions based on uncertainty scores, boosting accuracy from 97.0% with an image-only model to 98.0%, reducing misdiagnoses by 33.3%. We integrate our plane orientation model into an interactive web application and provide it open-source.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINGUAL: Language-INtegrated GUidance in Active Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.14028</link>
<guid>https://arxiv.org/abs/2511.14028</guid>
<content:encoded><![CDATA[
<div> Keywords: active learning, segmentation, natural language guidance, in-context learning, active domain adaptation  

<br /><br />Summary:  
1. The paper addresses challenges in active learning (AL) for medical image segmentation, where experts need to annotate regions of interest (ROIs) that are often blurry and ambiguous, making the annotation process labor-intensive and cognitively demanding.  
2. Conventional AL faces a trade-off between ROI size and annotation effort: larger ROIs reduce cognitive load but increase annotation cost, while smaller ROIs require finer precision and more expert attention.  
3. To mitigate these issues, the authors propose LINGUAL, a novel framework that accepts natural language instructions from experts and converts them into executable programs via in-context learning.  
4. LINGUAL automates the execution of a sequence of sub-tasks derived from the instructions without any further human intervention, thus bypassing the need for precise boundary delineation.  
5. Experiments demonstrate that LINGUAL effectively supports active domain adaptation (ADA), achieving performance comparable or superior to traditional AL methods, while reducing estimated annotation time by approximately 80%. <div>
arXiv:2511.14028v1 Announce Type: new 
Abstract: Although active learning (AL) in segmentation tasks enables experts to annotate selected regions of interest (ROIs) instead of entire images, it remains highly challenging, labor-intensive, and cognitively demanding due to the blurry and ambiguous boundaries commonly observed in medical images. Also, in conventional AL, annotation effort is a function of the ROI- larger regions make the task cognitively easier but incur higher annotation costs, whereas smaller regions demand finer precision and more attention from the expert. In this context, language guidance provides an effective alternative, requiring minimal expert effort while bypassing the cognitively demanding task of precise boundary delineation in segmentation. Towards this goal, we introduce LINGUAL: a framework that receives natural language instructions from an expert, translates them into executable programs through in-context learning, and automatically performs the corresponding sequence of sub-tasks without any human intervention. We demonstrate the effectiveness of LINGUAL in active domain adaptation (ADA) achieving comparable or superior performance to AL baselines while reducing estimated annotation time by approximately 80%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-free Detection of AI-generated images via Cropping Robustness</title>
<link>https://arxiv.org/abs/2511.14030</link>
<guid>https://arxiv.org/abs/2511.14030</guid>
<content:encoded><![CDATA[
<div> AI-generated image detection, self-supervised models, Haar wavelet decomposition, RandomResizedCrop, training-free method  

<br /><br />Summary:  
This paper addresses the problem of detecting AI-generated images amid the rapid progress of vision-generative models. Instead of relying on dataset-specific training, the authors propose a training-free detection method called WaRPAD, which leverages self-supervised models pre-trained with augmentations such as RandomResizedCrop. These models inherently produce consistent embeddings across different image resolutions. WaRPAD capitalizes on the sensitivity of neighborhood pixel differences to resizing by quantifying changes in image embeddings along high-frequency components extracted using Haar wavelet decomposition. To mimic robustness against cropping, the method rescales images to multiples of the model's input size, splits them into patches, calculates detection scores per patch, and averages these to form a final detection score. The approach is validated on datasets featuring diverse resolutions and various domains, testing images generated by 23 different generative models. Results demonstrate that WaRPAD achieves competitive detection performance and exhibits strong resilience to common test-time image corruptions. The method’s reliance on invariance to RandomResizedCrop, a commonly used augmentation in self-supervised learning, allows it to be effectively applied across different self-supervised models without the need for additional training or prior knowledge of specific datasets. <div>
arXiv:2511.14030v1 Announce Type: new 
Abstract: AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization</title>
<link>https://arxiv.org/abs/2511.14031</link>
<guid>https://arxiv.org/abs/2511.14031</guid>
<content:encoded><![CDATA[
<div> Keywords: fashion image generation, garment details, diffusion-based framework, fine-grained controllability, region-adaptive attention

<br /><br />Summary:  
Garment-centric fashion image generation aims to create realistic and controllable images of human models wearing specific garments, which is increasingly relevant in e-commerce. The main challenges include accurately preserving garment details and achieving fine-grained control over model appearance. Existing methods often involve garment deformation, leading to texture distortions and limited control over attributes. To tackle these issues, the paper introduces FashionMAC, a diffusion-based framework that eliminates the need for garment deformation by directly outpainting garments segmented from dressed individuals, ensuring the preservation of intricate details. Additionally, a novel region-adaptive decoupled attention (RADA) mechanism is proposed, combined with a chained mask injection strategy. This approach enhances fine-grained appearance controllability by adaptively predicting generated regions for specific text attributes and focusing the attributes on these regions. Extensive experiments demonstrate the superior performance of FashionMAC compared to state-of-the-art methods, showcasing its effectiveness in generating high-quality fashion showcase images while maintaining visual fidelity and detailed controllability. <div>
arXiv:2511.14031v1 Announce Type: new 
Abstract: Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping</title>
<link>https://arxiv.org/abs/2511.14033</link>
<guid>https://arxiv.org/abs/2511.14033</guid>
<content:encoded><![CDATA[
<div> Keywords: flood prediction, latent diffusion models, super-resolution, real-time, generalizability  

<br /><br />Summary: The paper addresses the crucial need for effective flood prediction to aid emergency planning and minimize losses. Traditional hydrodynamic models are accurate but computationally demanding, making them unsuitable for real-time, large-scale use. Recent advancements using convolutional neural networks for flood map super-resolution provide speed and accuracy but lack generalizability to new areas. To overcome these limitations, the authors propose a novel approach utilizing latent diffusion models, enabling super-resolution of coarse-grid flood maps. This method aims to replicate the precision of fine-grid maps while drastically reducing inference time. Experimental findings indicate that latent diffusion models significantly lower the computational time for generating high-quality flood maps without sacrificing accuracy, thus facilitating real-time flood risk management. Additionally, these models demonstrate enhanced generalizability across diverse geographical locations, with transfer learning further expediting their application in unfamiliar regions. The approach also integrates physics-informed inputs to mitigate the typical black-box nature of machine learning models, boosting interpretability. Overall, this innovative technique presents a promising solution for timely and efficient flood prediction and management. Code for the implementation is available on GitHub. <div>
arXiv:2511.14033v1 Announce Type: new 
Abstract: Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at https://github.com/neosunhan/flood-diff.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saliency-Guided Deep Learning for Bridge Defect Detection in Drone Imagery</title>
<link>https://arxiv.org/abs/2511.14040</link>
<guid>https://arxiv.org/abs/2511.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, concrete bridge, drone imagery, YOLOX, saliency

<br /><br />Summary: 
Anomaly object detection and classification are critical challenges within computer vision and pattern recognition. This paper presents a novel method aimed at the automatic detection, localization, and classification of defects in concrete bridge structures utilizing drone imagery. The proposed framework consists of two main stages: the first stage involves applying saliency techniques to generate defect region proposals, leveraging the local discontinuities in surface patterns characteristic of defects compared to their surroundings. The second stage implements a YOLOX-based deep learning detector, which processes saliency-enhanced images enhanced through bounding-box level brightness augmentation specifically applied to the identified salient defect areas. Extensive experimental results on standard datasets validate the effectiveness of this framework, demonstrating promising performance in terms of accuracy and computational efficiency. These strengths suggest a significant potential for integrating this method into a self-powered inspection system, thereby facilitating ongoing monitoring and maintenance of concrete bridge structures. The approach not only enhances defect identification but also contributes to the overall safety and longevity of infrastructure through timely detection and classification of anomalies. <div>
arXiv:2511.14040v1 Announce Type: new 
Abstract: Anomaly object detection and classification are one of the main challenging tasks in computer vision and pattern recognition. In this paper, we propose a new method to automatically detect, localize and classify defects in concrete bridge structures using drone imagery. This framework is constituted of two main stages. The first stage uses saliency for defect region proposals where defects often exhibit local discontinuities in the normal surface patterns with regard to their surrounding. The second stage employs a YOLOX-based deep learning detector that operates on saliency-enhanced images obtained by applying bounding-box level brightness augmentation to salient defect regions. Experimental results on standard datasets confirm the performance of our framework and its suitability in terms of accuracy and computational efficiency, which give a huge potential to be implemented in a self-powered inspection system.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Context Matters: Improving Conditioning for Autoregressive Models</title>
<link>https://arxiv.org/abs/2511.14063</link>
<guid>https://arxiv.org/abs/2511.14063</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive models, image generation, semantic alignment, instruction editing, SCAR  

<br /><br />Summary: Recently, autoregressive (AR) models have demonstrated significant potential in image generation, showcasing improved scalability and ease of integration with multi-modal systems compared to diffusion methods. However, extending AR models for general image editing presents challenges, particularly due to weak conditioning, which results in poor adherence to instructions and visual artifacts. To tackle these issues, the paper introduces SCAR, a Semantic-Context-driven method for AR models. SCAR features two vital components: Compressed Semantic Prefilling, which encodes high-level semantics into an efficient prefix, and Semantic Alignment Guidance, which optimizes the last visual hidden states to align with target semantics during decoding. This method differs from traditional decoding-stage injection techniques by enhancing the flexibility of vector-quantized prefilling while addressing semantic limitations and cost concerns. SCAR effectively generalizes across both next-token and next-set AR paradigms with minimal changes to the architecture. The methodology achieves superior visual fidelity and semantic alignment on benchmarks for instruction editing and controllable generation, outperforming previous AR-based methods while maintaining a high degree of controllability. The authors also mention that all code will be released, facilitating further research and application. <div>
arXiv:2511.14063v1 Announce Type: new 
Abstract: Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs</title>
<link>https://arxiv.org/abs/2511.14072</link>
<guid>https://arxiv.org/abs/2511.14072</guid>
<content:encoded><![CDATA[
<div> Large Vision-Language Models, token compression, object-centric representations, segmentation decoder, centroid-guided sorting<br /><br />Summary: Large Vision-Language Models (LVLMs) face significant computational and memory challenges due to the quadratic increase in visual tokens as image resolution grows. Existing token compression methods lack effective high-level semantic understanding, which results in suboptimal merges, redundant information, or loss of contextual details. To overcome these issues, the paper proposes CORE (Compact Object-centric REpresentations), a novel token compression approach that uses an efficient segmentation decoder to produce object masks. These masks provide semantic guidance for merging visual tokens into compact, object-centric representations. Additionally, the method introduces a centroid-guided sorting mechanism to restore coherent spatial ordering of merged tokens, preserving important positional information. Exhaustive experiments demonstrate that CORE sets new state-of-the-art performance on six authoritative benchmarks for fixed-rate compression. It also achieves significant efficiency improvements in adaptive-rate compression scenarios. Remarkably, even when retaining only 2.2% of all visual tokens, CORE retains 97.4% of the baseline performance. This work highlights the superiority of leveraging object-centric representations for both efficient and effective processing within LVLMs. <div>
arXiv:2511.14072v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) usually suffer from prohibitive computational and memory costs due to the quadratic growth of visual tokens with image resolution. Existing token compression methods, while varied, often lack a high-level semantic understanding, leading to suboptimal merges, information redundancy, or context loss. To address these limitations, we introduce CORE (Compact Object-centric REpresentations), a new paradigm for visual token compression. CORE leverages an efficient segmentation decoder to generate object masks, which serve as a high-level semantic prior to guide the merging of visual tokens into a compact set of object-centric representations. Furthermore, a novel centroid-guided sorting mechanism restores a coherent spatial order to the merged tokens, preserving vital positional information. Extensive experiments show that CORE not only establishes a new state-of-the-art on six authoritative benchmarks for fixed-rate compression, but also achieves dramatic efficiency gains in adaptive-rate settings. Even under extreme compression, after aggressively retaining with only 2.2% of all visual tokens, CORE still maintains 97.4% of baseline performance. Our work demonstrates the superiority of object-centric representations for efficient and effective LVLM processing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification</title>
<link>https://arxiv.org/abs/2511.14082</link>
<guid>https://arxiv.org/abs/2511.14082</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Medical image analysis, Zero-Training, Task-specific classifier, Rare diseases  

<br /><br />Summary:  
Deep learning models have revolutionized medical image analysis; however, their reliance on large-scale, well-annotated datasets poses a significant challenge, particularly in the medical domain where data collection is often difficult and expensive. This paper presents a new approach known as Zero-Training Task-Specific Model Synthesis (ZS-TMS) to address this issue. Rather than adapting existing models, the authors propose a framework called Semantic-Guided Parameter Synthesizer (SGPS), which synthesizes task-specific classifier parameters directly from minimal multi-modal inputs, such as a single example image and a clinical text description. The generative engine encodes this information to produce the weights for an efficient classifier, enabling immediate deployment without the need for further training or fine-tuning. The authors conducted extensive evaluations using challenging datasets, including the ISIC 2018 skin lesion dataset and a custom rare disease dataset, demonstrating that SGPS outperforms advanced few-shot and zero-shot learning methods, particularly in low data scenarios (1-shot and 5-shot classification). This work significantly contributes to the rapid development of AI diagnostic tools, especially for rare diseases where data availability is critically limited. <div>
arXiv:2511.14082v1 Announce Type: new 
Abstract: Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated glenoid bone loss measurement and segmentation in CT scans for pre-operative planning in shoulder instability</title>
<link>https://arxiv.org/abs/2511.14083</link>
<guid>https://arxiv.org/abs/2511.14083</guid>
<content:encoded><![CDATA[
<div> Keywords: glenoid bone loss, deep learning, automated measurement, shoulder instability, CT scans  

<br /><br />Summary:  
Reliable measurement of glenoid bone loss is crucial for surgical planning in shoulder instability. Current methods often lack efficiency and are prone to variability among different readers. This study presents a fully automated deep learning pipeline designed to measure glenoid bone loss from three-dimensional computed tomography (CT) scans, using a linear-based, en-face view, best-circle method. The research involved retrospective analysis of CT images from 91 patients, with an average age of 40 years, and included manual labels for segmentation, landmarks, and bone loss metrics. The algorithm consists of three stages: segmentation via a U-Net, anatomical landmark detection through a second network predicting glenoid rim points, and geometric fitting using principal component analysis (PCA) and circle fitting. The automated measurements demonstrated a strong correlation with expert consensus readings, surpassing surgeon-to-surgeon consistency. The accuracy in classifying patients into varying levels of bone loss was notable, with specific recall rates achieved for low and high severity categories. This innovative approach offers a time-efficient and reliable solution for preoperative assessment and screening of significant glenoid bone loss. Code and dataset are accessible at https://github.com/Edenliu1/Auto-Glenoid-Measurement-DL-Pipeline. <div>
arXiv:2511.14083v1 Announce Type: new 
Abstract: Reliable measurement of glenoid bone loss is essential for operative planning in shoulder instability, but current manual and semi-automated methods are time-consuming and often subject to interreader variability. We developed and validated a fully automated deep learning pipeline for measuring glenoid bone loss on three-dimensional computed tomography (CT) scans using a linear-based, en-face view, best-circle method. Shoulder CT images of 91 patients (average age, 40 years; range, 14-89 years; 65 men) were retrospectively collected along with manual labels including glenoid segmentation, landmarks, and bone loss measurements. The multi-stage algorithm has three main stages: (1) segmentation, where we developed a U-Net to automatically segment the glenoid and humerus; (2) anatomical landmark detection, where a second network predicts glenoid rim points; and (3) geometric fitting, where we applied principal component analysis (PCA), projection, and circle fitting to compute the percentage of bone loss. The automated measurements showed strong agreement with consensus readings and exceeded surgeon-to-surgeon consistency (intraclass correlation coefficient (ICC) 0.84 vs 0.78), including in low- and high-bone-loss subgroups (ICC 0.71 vs 0.63 and 0.83 vs 0.21, respectively; P < 0.001). For classifying patients into low, medium, and high bone-loss categories, the pipeline achieved a recall of 0.714 for low and 0.857 for high severity, with no low cases misclassified as high or vice versa. These results suggest that our method is a time-efficient and clinically reliable tool for preoperative planning in shoulder instability and for screening patients with substantial glenoid bone loss. Code and dataset are available at https://github.com/Edenliu1/Auto-Glenoid-Measurement-DL-Pipeline.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Scene Editing for 3D Grounding in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14086</link>
<guid>https://arxiv.org/abs/2511.14086</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D-LLMs, grounding, DEER-3D, scene editing, spatial manipulation  

<br /><br />Summary: Despite advancements in 3D language models (3D-LLMs), there are challenges in connecting language to visual and spatial elements in 3D environments. These challenges arise largely due to the focus of training data on language reasoning rather than spatial understanding, leading to unresolved grounding biases. The proposed solution is to utilize 3D scene editing to create precise visual counterfactuals, allowing for fine-grained spatial manipulations without necessitating expensive scene reconstruction or extensive 3D data collection. The framework introduced, DEER-3D, emphasizes a structured workflow of "Decompose, Diagnostic Evaluation, Edit, and Re-train" to precisely tackle identified grounding failures. This involves diagnosing specific predicate-level errors, followed by implementing minimal, targeted adjustments to the 3D scenes, such as recoloring or repositioning, to generate counterfactuals for model fine-tuning. Evaluation across multiple benchmarks for 3D grounding and scene understanding showcases consistent improvements in grounding accuracy, demonstrating the framework's effectiveness. DEER-3D effectively integrates linguistic reasoning with spatial grounding, addressing the limitations of current 3D-LLMs through targeted, error-driven edits. <div>
arXiv:2511.14086v1 Announce Type: new 
Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention</title>
<link>https://arxiv.org/abs/2511.14087</link>
<guid>https://arxiv.org/abs/2511.14087</guid>
<content:encoded><![CDATA[
<div> Grouped Coordinate Attention, ResNet-50, Medical Image Segmentation, Global Dependency, Computational Efficiency<br /><br />Summary:<br /><br />1. Medical image segmentation is crucial for computer-aided diagnosis, preoperative planning, and disease monitoring, relying heavily on accurate and efficient models.<br />2. Traditional U-Net convolutional neural networks excel due to their encoder-decoder architecture and skip connections but face challenges capturing long-range dependencies.<br />3. Transformer-based approaches improve global context modeling but demand substantial computational resources and large datasets.<br />4. The paper introduces GCA-ResUNet, which integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks to jointly capture global dependencies across channels and spatial locations.<br />5. GCA enhances feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared to self-attention mechanisms.<br />6. Experimental results on the Synapse dataset show a Dice score of 86.11%, and on the ACDC dataset, 92.64%, outperforming several state-of-the-art methods.<br />7. GCA-ResUNet maintains fast inference times and computational efficiency, making it practical for real-world medical image segmentation.<br />8. The study demonstrates that GCA can effectively augment convolutional architectures with global modeling capabilities, achieving high accuracy with resource efficiency. <div>
arXiv:2511.14087v1 Announce Type: new 
Abstract: Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.14093</link>
<guid>https://arxiv.org/abs/2511.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-view Geo-localization, transformer, Swin-Transformer, Mixture-of-Experts, anchor-free detection<br /><br />Summary:<br />1. The paper addresses the challenge of cross-view object Geo-localization, specifically matching objects from drone images to large-scale satellite imagery despite differences in viewpoint, scale, and background complexity. <br />2. The authors propose SMGeo, an end-to-end transformer-based model that supports click prompting, enabling real-time interactive Geo-localization.<br />3. SMGeo uses a fully transformer-based architecture, incorporating a Swin-Transformer for joint feature encoding of drone and satellite images.<br />4. To enhance inter-modal and intra-view feature dependencies, SMGeo introduces a grid-level sparse Mixture-of-Experts (GMoE) in the cross-view encoder that selectively activates specialized experts based on grid content, scale, and source.<br />5. The model employs an anchor-free transformer detection head for coordinate regression, directly predicting object locations via heatmap supervision, thus avoiding scale bias and complexity from anchor boxes.<br />6. Experimental results show SMGeo significantly outperforms prior methods such as DetGeo on metrics like accuracy at IoU=0.25 and mean IoU (e.g., 87.51% vs. 61.97% accuracy on the test set).<br />7. Ablation studies confirm that shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts each contribute complementary improvements to the overall performance. <div>
arXiv:2511.14093v1 Announce Type: new 
Abstract: Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images. Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage "retrieval-matching" pipelines are prone to cumulative errors. To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization. This model supports click prompting and can output object Geo-localization in real time when prompted to allow for interactive use. The model employs a fully transformer-based architecture, utilizing a Swin-Transformer for joint feature encoding of both drone and satellite imagery and an anchor-free transformer detection head for coordinate regression. In order to better capture both inter-modal and intra-view dependencies, we introduce a grid-level sparse Mixture-of-Experts (GMoE) into the cross-view encoder, allowing it to adaptively activate specialized experts according to the content, scale and source of each grid. We also employ an anchor-free detection head for coordinate regression, directly predicting object locations via heat-map supervision in the reference images. This approach avoids scale bias and matching complexity introduced by predefined anchor boxes. On the drone-to-satellite task, SMGeo achieves leading performance in accuracy at IoU=0.25 and mIoU metrics (e.g., 87.51%, 62.50%, and 61.45% in the test set, respectively), significantly outperforming representative methods such as DetGeo (61.97%, 57.66%, and 54.05%, respectively). Ablation studies demonstrate complementary gains from shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BCE3S: Binary Cross-Entropy Based Tripartite Synergistic Learning for Long-tailed Recognition</title>
<link>https://arxiv.org/abs/2511.14097</link>
<guid>https://arxiv.org/abs/2511.14097</guid>
<content:encoded><![CDATA[
<div> Keywords: long-tailed recognition, binary cross-entropy, feature properties, classifier separability, contrastive learning

<br /><br />Summary: This paper addresses the challenges in long-tailed recognition (LTR) tasks, emphasizing the need for enhanced intra-class compactness and inter-class separability across both head and tail classes. Existing methods using cross-entropy (CE) loss often fail to create desirable feature properties and exacerbate class imbalance due to the coupling of classifier vectors in the CE's Softmax. To overcome these limitations, the authors propose a new approach called BCE-based tripartite synergistic learning (BCE3S) that utilizes binary cross-entropy (BCE) in three key components: (1) Joint learning that optimizes both classifier and sample features while decoupling the metrics between features and imbalanced classifier vectors enhances compactness and separability more effectively than CE-based learning; (2) Contrastive learning that further improves intra-class compactness of features; (3) Uniform learning that balances classifier separability and improves feature properties by synergistically interacting with joint learning. Extensive experiments demonstrate that LTR models trained using BCE3S achieve superior compactness and separability among features, as well as balanced classifier separability, resulting in state-of-the-art performance on various long-tailed datasets, including CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and iNaturalist2018. <div>
arXiv:2511.14097v1 Announce Type: new 
Abstract: For long-tailed recognition (LTR) tasks, high intra-class compactness and inter-class separability in both head and tail classes, as well as balanced separability among all the classifier vectors, are preferred. The existing LTR methods based on cross-entropy (CE) loss not only struggle to learn features with desirable properties but also couple imbalanced classifier vectors in the denominator of its Softmax, amplifying the imbalance effects in LTR. In this paper, for the LTR, we propose a binary cross-entropy (BCE)-based tripartite synergistic learning, termed BCE3S, which consists of three components: (1) BCE-based joint learning optimizes both the classifier and sample features, which achieves better compactness and separability among features than the CE-based joint learning, by decoupling the metrics between feature and the imbalanced classifier vectors in multiple Sigmoid; (2) BCE-based contrastive learning further improves the intra-class compactness of features; (3) BCE-based uniform learning balances the separability among classifier vectors and interactively enhances the feature properties by combining with the joint learning. The extensive experiments show that the LTR model trained by BCE3S not only achieves higher compactness and separability among sample features, but also balances the classifier's separability, achieving SOTA performance on various long-tailed datasets such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and iNaturalist2018.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2511.14099</link>
<guid>https://arxiv.org/abs/2511.14099</guid>
<content:encoded><![CDATA[
<div> All-in-One Image Restoration, Frequency-Aware Planning, Multimodal Large Language Model, LoRA-based Mixture-of-Experts, Diffusion-based Executor<br /><br />Summary:  
The paper introduces FAPE-IR, a novel framework for All-in-One Image Restoration (AIO-IR) designed to handle multiple types of image degradations within complex scenarios using a unified model. Unlike traditional methods that depend on task-specific architectures or latent routing strategies, FAPE-IR leverages a frozen Multimodal Large Language Model (MLLM) as a planner, which analyzes degraded images and generates frequency-aware restoration plans. These plans dynamically guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module embedded within a diffusion-based executor, enabling the selective activation of high- or low-frequency expert networks based on input frequency features. The framework also incorporates adversarial training techniques and a novel frequency regularization loss to enhance image quality and reduce artifacts effectively. By combining semantic planning from the MLLM with frequency-based restoration, FAPE-IR provides an interpretable and adaptable solution capable of addressing a range of degradation types in a single model. Extensive experimental evaluation demonstrates that FAPE-IR achieves state-of-the-art results across seven diverse image restoration tasks and shows strong zero-shot generalization capabilities when confronted with mixed and unseen degradations in real-world scenarios. <div>
arXiv:2511.14099v1 Announce Type: new 
Abstract: All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations</title>
<link>https://arxiv.org/abs/2511.14100</link>
<guid>https://arxiv.org/abs/2511.14100</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning video editing, implicit queries, multi-hop reasoning, RIVER model, video content manipulation<br /><br />Summary: Text-driven video editing traditionally requires explicit descriptions of editing targets, including precise spatial and temporal information. This becomes challenging when users make implicit queries referencing semantic attributes or object relationships rather than explicit details. To address this, the authors introduce a new task called reasoning video editing, which demands models to interpret implicit queries via multi-hop reasoning to correctly identify editing targets before modification. They propose RIVER (Reasoning-based Implicit Video Editor), a novel model that separates reasoning from generation by creating digital twin representations of video content, preserving spatial relationships, temporal trajectories, and semantic attributes. A large language model processes these representations along with the implicit query to perform multi-hop reasoning and produce structured instructions, which guide a diffusion-based editor for pixel-level video modification. RIVER is trained using reinforcement learning with rewards designed to evaluate both reasoning accuracy and generation quality. The authors also introduce RVEBenchmark, a dataset of 100 videos with 519 implicit queries categorized by reasoning complexity, specifically created for assessing reasoning video editing. RIVER achieves the best results on RVEBenchmark and surpasses six baseline methods on existing benchmarks VegGIE and FiVE, demonstrating its state-of-the-art performance in implicit query-based video editing tasks. <div>
arXiv:2511.14100v1 Announce Type: new 
Abstract: Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment</title>
<link>https://arxiv.org/abs/2511.14107</link>
<guid>https://arxiv.org/abs/2511.14107</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular depth estimation, self-supervised, real-time, lightweight, encoder-decoder

<br /><br />Summary: Depth information is essential for autonomous driving and robot navigation, and self-supervised monocular depth estimation offers a flexible approach. However, traditional models often require substantial computational resources, which poses challenges for practical applications. To overcome this limitation, the authors present RTS-Mono, a lightweight and efficient real-time self-supervised monocular depth estimation method. RTS-Mono utilizes a sophisticated encoder-decoder architecture, where the encoder is based on Lite-Encoder, while the decoder employs a multi-scale sparse fusion framework to enhance performance and speed while minimizing redundancy. In evaluations using the KITTI dataset, RTS-Mono demonstrates state-of-the-art performance at both high and low resolutions with a notably low parameter count of 3 million. Specifically, it improves performance metrics like Absolute Relative Error (Abs Rel) and Squared Relative Error (Sq Rel) by 5.6% and 9.8% at low resolution, and enhances Sq Rel and Root Mean Square Error (RMSE) by 6.1% and 1.9% at high resolution. In real-world deployments, the model achieves exceptional accuracy and can process data in real-time at 49 frames per second on the Nvidia Jetson Orin. The source code is publicly accessible on GitHub. <div>
arXiv:2511.14107v1 Announce Type: new 
Abstract: Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</title>
<link>https://arxiv.org/abs/2511.14109</link>
<guid>https://arxiv.org/abs/2511.14109</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Place Recognition, Optimal Transport, Asymmetric Aggregation, Geometric Constraints, Matching Accuracy  

<br /><br />Summary:  
Visual Place Recognition (VPR) focuses on matching query images against a database by utilizing visual cues. Traditional state-of-the-art methods primarily aggregate features from deep learning models to create global descriptors. Optimal transport-based methods have attempted to reformulate the matching issue as a transport problem; however, the Sinkhorn algorithm's symmetric treatment of source and target distributions can limit performance when there are significant discrepancies between image features and cluster centers. To address this, we present a novel method called $A^2$GC-VPR, which implements asymmetric aggregation with geometric constraints for locally aggregated descriptors. Our approach incorporates row-column normalization averaging along with separate marginal calibration to facilitate asymmetric matching tailored to the differing distributions in VPR tasks. Additionally, we embed geometric constraints utilizing learnable coordinate embeddings, which compute compatibility scores that are combined with feature similarities, thereby ensuring that spatially similar features are clustered together. We validated the effectiveness of our method through extensive experiments on benchmark datasets, including MSLS, NordLand, and Pittsburgh, which demonstrated improved matching accuracy and robustness compared to existing approaches. <div>
arXiv:2511.14109v1 Announce Type: new 
Abstract: Visual Place Recognition (VPR) aims to match query images against a database using visual cues. State-of-the-art methods aggregate features from deep backbones to form global descriptors. Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions. We propose an asymmetric aggregation VPR method with geometric constraints for locally aggregated descriptors, called $A^2$GC-VPR. Our method employs row-column normalization averaging with separate marginal calibration, enabling asymmetric matching that adapts to distributional discrepancies in visual place recognition. Geometric constraints are incorporated through learnable coordinate embeddings, computing compatibility scores fused with feature similarities, thereby promoting spatially proximal features to the same cluster and enhancing spatial awareness. Experimental results on MSLS, NordLand, and Pittsburgh datasets demonstrate superior performance, validating the effectiveness of our approach in improving matching accuracy and robustness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer</title>
<link>https://arxiv.org/abs/2511.14111</link>
<guid>https://arxiv.org/abs/2511.14111</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, Cascaded-Chunk Feed Forward Network, Energy Efficiency, ImageNet-1K, Accuracy-Per-FLOP (APF)  

<br /><br />Summary:  
This paper introduces Cascaded-ViT (CViT), a novel lightweight and compute-efficient vision transformer architecture designed specifically for resource-constrained devices like mobile phones and drones. The core innovation of CViT is the Cascaded-Chunk Feed Forward Network (CCFFN), which improves parameter and FLOP efficiency by splitting input features, enabling reduced computational overhead without compromising accuracy. Experiments on the ImageNet-1K dataset demonstrate that the CViT-XL model achieves a competitive 75.5% Top-1 accuracy, while reducing FLOPs by 15% and energy consumption by 3.3% compared to the EfficientViT-M5 baseline. The CViT family of models consistently exhibits lower energy consumption across various sizes, highlighting their suitability for deployment on battery-limited platforms. Additionally, the authors propose a new evaluation metric called Accuracy-Per-FLOP (APF), which provides a measure of computing efficiency relative to accuracy. CViT models perform at the top in terms of APF, with CViT-L achieving 2.2% higher accuracy than EfficientViT-M2 while maintaining comparable APF scores. Overall, CViT advances the state-of-the-art in efficient vision transformers by balancing accuracy, computation, and energy consumption, supporting practical applications where resource constraints are a critical concern. <div>
arXiv:2511.14111v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coffee: Controllable Diffusion Fine-tuning</title>
<link>https://arxiv.org/abs/2511.14113</link>
<guid>https://arxiv.org/abs/2511.14113</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion, fine-tuning, undesired concepts, prompt regularization, bias mitigation<br /><br />Summary:<br /><br />1. Text-to-image diffusion models offer flexible and diverse content generation via text prompts, making them ideal for customization through fine-tuning on limited user data. 2. A major challenge in fine-tuning is preventing models from learning undesired concepts present in the fine-tuning dataset and avoiding entanglement of these concepts with user prompts. 3. This issue is critical for tasks such as bias mitigation, preventing malicious model adaptation, disentangling attributes, and enabling more generalizable fine-tuning in diffusion models. 4. The paper proposes "Coffee," a novel method that uses language-based specification of undesired concepts to regularize the model adaptation process by ensuring prompt embeddings do not align with these unwanted concepts. 5. Coffee requires no additional training and allows easy modification of undesired concepts by simply changing textual descriptions. Experimental evaluation shows that Coffee effectively prevents text-to-image models from adopting specified undesired concepts during fine-tuning and outperforms existing baselines. Code release is planned upon acceptance. <div>
arXiv:2511.14113v1 Announce Type: new 
Abstract: Text-to-image diffusion models can generate diverse content with flexible prompts, which makes them well-suited for customization through fine-tuning with a small amount of user-provided data. However, controllable fine-tuning that prevents models from learning undesired concepts present in the fine-tuning data, and from entangling those concepts with user prompts, remains an open challenge. It is crucial for downstream tasks like bias mitigation, preventing malicious adaptation, attribute disentanglement, and generalizable fine-tuning of diffusion policy. We propose Coffee that allows using language to specify undesired concepts to regularize the adaptation process. The crux of our method lies in keeping the embeddings of the user prompt from aligning with undesired concepts. Crucially, Coffee requires no additional training and enables flexible modification of undesired concepts by modifying textual descriptions. We evaluate Coffee by fine-tuning on images associated with user prompts paired with undesired concepts. Experimental results demonstrate that Coffee can prevent text-to-image models from learning specified undesired concepts during fine-tuning and outperforms existing methods. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.14120</link>
<guid>https://arxiv.org/abs/2511.14120</guid>
<content:encoded><![CDATA[
<div> Pedestrian behavior, Multi-view video, Vision-language models, Cognitive phase segmentation, Traffic safety analytics  

<br /><br />Summary:  
This paper addresses the critical issue of pedestrian-vehicle incidents, which account for over 20% of global traffic fatalities, by proposing a novel AI-driven framework called Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR). The framework processes multi-view video streams through four distinct stages: event-triggered acquisition of video from multiple angles, segmentation of pedestrian behavior into cognitive phases, phase-specific multi-view reasoning, and hierarchical synthesis leading to detailed diagnostic reports. MP-PVIR incorporates behavioral theory by automatically dividing incidents into cognitive phases, facilitating synchronized analysis across multiple views within each phase. Two specially designed vision-language models support this process: TG-VLM for behavioral phase segmentation, achieving a mean Intersection over Union (mIoU) of 0.4881, and PhaVR-VLM for phase-aware multi-view reasoning, which attains a captioning score of 33.063 and question-answering accuracy up to 64.70%. A large language model is employed to generate comprehensive reports synthesizing scene understanding, behavioral interpretation, causal reasoning, and tailored prevention strategies. Evaluation on the Woven Traffic Safety dataset demonstrates MP-PVIR’s effectiveness in transforming complex multi-view video data into actionable insights, significantly advancing AI capabilities for traffic safety analytics within vehicle-infrastructure cooperative systems. <div>
arXiv:2511.14120v1 Announce Type: new 
Abstract: Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Via Convolutional Nearest Neighbors</title>
<link>https://arxiv.org/abs/2511.14137</link>
<guid>https://arxiv.org/abs/2511.14137</guid>
<content:encoded><![CDATA[
arXiv:2511.14137v1 Announce Type: new 
Abstract: The shift from Convolutional Neural Networks to Transformers has reshaped computer vision, yet these two architectural families are typically viewed as fundamentally distinct. We argue that convolution and self-attention, despite their apparent differences, can be unified within a single k-nearest neighbor aggregation framework. The critical insight is that both operations are special cases of neighbor selection and aggregation; convolution selects neighbors by spatial proximity, while attention selects by feature similarity, revealing they exist on a continuous spectrum. We introduce Convolutional Nearest Neighbors (ConvNN), a unified framework that formalizes this connection. Crucially, ConvNN serves as a drop-in replacement for convolutional and attention layers, enabling systematic exploration of the intermediate spectrum between these two extremes. We validate the framework's coherence on CIFAR-10 and CIFAR-100 classification tasks across two complementary architectures: (1) Hybrid branching in VGG improves accuracy on both CIFAR datasets by combining spatial-proximity and feature-similarity selection; and (2) ConvNN in ViT outperforms standard attention and other attention variants on both datasets. Extensive ablations on $k$ values and architectural variants reveal that interpolating along this spectrum provides regularization benefits by balancing local and global receptive fields. Our work provides a unifying framework that dissolves the apparent distinction between convolution and attention, with implications for designing more principled and interpretable vision architectures.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</title>
<link>https://arxiv.org/abs/2511.14143</link>
<guid>https://arxiv.org/abs/2511.14143</guid>
<content:encoded><![CDATA[
arXiv:2511.14143v1 Announce Type: new 
Abstract: Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion</title>
<link>https://arxiv.org/abs/2511.14149</link>
<guid>https://arxiv.org/abs/2511.14149</guid>
<content:encoded><![CDATA[
arXiv:2511.14149v1 Announce Type: new 
Abstract: Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\&amp;T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2{\deg} while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wave-Former: Through-Occlusion 3D Reconstruction via Wireless Shape Completion</title>
<link>https://arxiv.org/abs/2511.14152</link>
<guid>https://arxiv.org/abs/2511.14152</guid>
<content:encoded><![CDATA[
arXiv:2511.14152v1 Announce Type: new 
Abstract: We present Wave-Former, a novel method capable of high-accuracy 3D shape reconstruction for completely occluded, diverse, everyday objects. This capability can open new applications spanning robotics, augmented reality, and logistics. Our approach leverages millimeter-wave (mmWave) wireless signals, which can penetrate common occlusions and reflect off hidden objects. In contrast to past mmWave reconstruction methods, which suffer from limited coverage and high noise, Wave-Former introduces a physics-aware shape completion model capable of inferring full 3D geometry. At the heart of Wave-Former's design is a novel three-stage pipeline which bridges raw wireless signals with recent advancements in vision-based shape completion by incorporating physical properties of mmWave signals. The pipeline proposes candidate geometric surfaces, employs a transformer-based shape completion model designed specifically for mmWave signals, and finally performs entropy-guided surface selection. This enables Wave-Former to be trained using entirely synthetic point-clouds, while demonstrating impressive generalization to real-world data.In head-to-head comparisons with state-of-the-art baselines, Wave-Former raises recall from 54% to 72% while maintaining a high precision of 85%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Representation and Synergy Invariances: A Povable Framework for Generalized Multimodal Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2511.14157</link>
<guid>https://arxiv.org/abs/2511.14157</guid>
<content:encoded><![CDATA[
arXiv:2511.14157v1 Announce Type: new 
Abstract: Multimodal Face Anti-Spoofing (FAS) methods, which integrate multiple visual modalities, often suffer even more severe performance degradation than unimodal FAS when deployed in unseen domains. This is mainly due to two overlooked risks that affect cross-domain multimodal generalization. The first is the modal representation invariant risk, i.e., whether representations remain generalizable under domain shift. We theoretically show that the inherent class asymmetry in FAS (diverse spoofs vs. compact reals) enlarges the upper bound of generalization error, and this effect is further amplified in multimodal settings. The second is the modal synergy invariant risk, where models overfit to domain-specific inter-modal correlations. Such spurious synergy cannot generalize to unseen attacks in target domains, leading to performance drops. To solve these issues, we propose a provable framework, namely Multimodal Representation and Synergy Invariance Learning (RiSe). For representation risk, RiSe introduces Asymmetric Invariant Risk Minimization (AsyIRM), which learns an invariant spherical decision boundary in radial space to fit asymmetric distributions, while preserving domain cues in angular space. For synergy risk, RiSe employs Multimodal Synergy Disentanglement (MMSD), a self-supervised task enhancing intrinsic, generalizable modal features via cross-sample mixing and disentanglement. Theoretical analysis and experiments verify RiSe, which achieves state-of-the-art cross-domain performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs</title>
<link>https://arxiv.org/abs/2511.14159</link>
<guid>https://arxiv.org/abs/2511.14159</guid>
<content:encoded><![CDATA[
arXiv:2511.14159v1 Announce Type: new 
Abstract: Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.14169</link>
<guid>https://arxiv.org/abs/2511.14169</guid>
<content:encoded><![CDATA[
arXiv:2511.14169v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition</title>
<link>https://arxiv.org/abs/2511.14179</link>
<guid>https://arxiv.org/abs/2511.14179</guid>
<content:encoded><![CDATA[
arXiv:2511.14179v1 Announce Type: new 
Abstract: Existing self-supervised contrastive learning methods for skeleton-based action recognition often process all skeleton regions uniformly, and adopt a first-in-first-out (FIFO) queue to store negative samples, which leads to motion information loss and non-optimal negative sample selection. To address these challenges, this paper proposes Dominance-Game Contrastive Learning network for skeleton-based action Recognition (DoGCLR), a self-supervised framework based on game theory. DoGCLR models the construction of positive and negative samples as a dynamic Dominance Game, where both sample types interact to reach an equilibrium that balances semantic preservation and discriminative strength. Specifically, a spatio-temporal dual weight localization mechanism identifies key motion regions and guides region-wise augmentations to enhance motion diversity while maintaining semantics. In parallel, an entropy-driven dominance strategy manages the memory bank by retaining high entropy (hard) negatives and replacing low-entropy (weak) ones, ensuring consistent exposure to informative contrastive signals. Extensive experiments are conducted on NTU RGB+D and PKU-MMD datasets. On NTU RGB+D 60 X-Sub/X-View, DoGCLR achieves 81.1%/89.4% accuracy, and on NTU RGB+D 120 X-Sub/X-Set, DoGCLR achieves 71.2%/75.5% accuracy, surpassing state-of-the-art methods by 0.1%, 2.7%, 1.1%, and 2.3%, respectively. On PKU-MMD Part I/Part II, DoGCLR performs comparably to the state-of-the-art methods and achieves a 1.9% higher accuracy on Part II, highlighting its strong robustness on more challenging scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniSER: A Foundation Model for Unified Soft Effects Removal</title>
<link>https://arxiv.org/abs/2511.14183</link>
<guid>https://arxiv.org/abs/2511.14183</guid>
<content:encoded><![CDATA[
arXiv:2511.14183v1 Announce Type: new 
Abstract: Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2511.14184</link>
<guid>https://arxiv.org/abs/2511.14184</guid>
<content:encoded><![CDATA[
arXiv:2511.14184v1 Announce Type: new 
Abstract: Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation</title>
<link>https://arxiv.org/abs/2511.14185</link>
<guid>https://arxiv.org/abs/2511.14185</guid>
<content:encoded><![CDATA[
arXiv:2511.14185v1 Announce Type: new 
Abstract: Most existing autonomous-driving datasets (e.g., KITTI, nuScenes, and the Waymo Perception Dataset), collected by human-driving mode or unidentified driving mode, can only serve as early training for the perception and prediction of autonomous vehicles (AVs). To evaluate the real behavioral safety of AVs controlled in the black box, we present the first end-to-end benchmark dataset collected entirely by autonomous-driving mode in the real world. This dataset contains over 100 hours of naturalistic data from multiple production autonomous-driving vehicle models in the market. We segment the original data into 32,727 key frames, each consisting of four synchronized camera images and high-precision GNSS/IMU data (0.8 cm localization accuracy). For each key frame, 20 Hz vehicle trajectories spanning the past 6 s and future 5 s are provided, along with detailed 2D annotations of surrounding vehicles, pedestrians, traffic lights, and traffic signs. These key frames have rich scenario-level attributes, including driver intent, area type (covering highways, urban roads, and residential areas), lighting (day, night, or dusk), weather (clear or rain), road surface (paved or unpaved), traffic and vulnerable road users (VRU) density, traffic lights, and traffic signs (warning, prohibition, and indication). To evaluate the safety of AVs, we employ an end-to-end motion planning model that predicts vehicle trajectories with an Average Displacement Error (ADE) of 1.4 m on autonomous-driving frames. The dataset continues to expand by over 10 hours of new data weekly, thereby providing a sustainable foundation for research on AV driving behavior analysis and safety evaluation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation</title>
<link>https://arxiv.org/abs/2511.14186</link>
<guid>https://arxiv.org/abs/2511.14186</guid>
<content:encoded><![CDATA[
arXiv:2511.14186v1 Announce Type: new 
Abstract: Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semantic Learning for Multi-Class Aorta Segmentation</title>
<link>https://arxiv.org/abs/2511.14187</link>
<guid>https://arxiv.org/abs/2511.14187</guid>
<content:encoded><![CDATA[
arXiv:2511.14187v1 Announce Type: new 
Abstract: The aorta, the body's largest artery, is prone to pathologies such as dissection, aneurysm, and atherosclerosis, which often require timely intervention. Minimally invasive repairs involving branch vessels necessitate detailed 3D anatomical analysis. Existing methods often overlook hierarchical anatomical relationships while struggling with severe class imbalance inherent in vascular structures. We address these challenges with a curriculum learning strategy that leverages a novel fractal softmax for hierarchical semantic learning. Inspired by human cognition, our approach progressively learns anatomical constraints by decomposing complex structures from simple to complex components. The curriculum learning framework naturally addresses class imbalance by first establishing robust feature representations for dominant classes before tackling rare but anatomically critical structures, significantly accelerating model convergence in multi-class scenarios. Our two-stage inference strategy achieves up to fivefold acceleration, enhancing clinical practicality. On the validation set at epoch 50, our hierarchical semantic loss improves the Dice score of nnU-Net ResEnc M by 11.65%. The proposed model demonstrates a 5.6% higher Dice score than baselines on the test set. Experimental results show significant improvements in segmentation accuracy and efficiency, making the framework suitable for real-time clinical applications. The implementation code for this challenge entry is publicly available at: https://github.com/PengchengShi1220/AortaSeg24. The code for fractal softmax will be available at https://github.com/PengchengShi1220/fractal-softmax.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Data Curation for Object Detection via Marginal Contributions to Dataset-level Average Precision</title>
<link>https://arxiv.org/abs/2511.14197</link>
<guid>https://arxiv.org/abs/2511.14197</guid>
<content:encoded><![CDATA[
arXiv:2511.14197v1 Announce Type: new 
Abstract: High-quality data has become a primary driver of progress under scale laws, with curated datasets often outperforming much larger unfiltered ones at lower cost. Online data curation extends this idea by dynamically selecting training samples based on the model's evolving state. While effective in classification and multimodal learning, existing online sampling strategies rarely extend to object detection because of its structural complexity and domain gaps. We introduce DetGain, an online data curation method specifically for object detection that estimates the marginal perturbation of each image to dataset-level Average Precision (AP) based on its prediction quality. By modeling global score distributions, DetGain efficiently estimates the global AP change and computes teacher-student contribution gaps to select informative samples at each iteration. The method is architecture-agnostic and minimally intrusive, enabling straightforward integration into diverse object detection architectures. Experiments on the COCO dataset with multiple representative detectors show consistent improvements in accuracy. DetGain also demonstrates strong robustness under low-quality data and can be effectively combined with knowledge distillation techniques to further enhance performance, highlighting its potential as a general and complementary strategy for data-efficient object detection.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification</title>
<link>https://arxiv.org/abs/2511.14203</link>
<guid>https://arxiv.org/abs/2511.14203</guid>
<content:encoded><![CDATA[
arXiv:2511.14203v1 Announce Type: new 
Abstract: Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</title>
<link>https://arxiv.org/abs/2511.14208</link>
<guid>https://arxiv.org/abs/2511.14208</guid>
<content:encoded><![CDATA[
arXiv:2511.14208v1 Announce Type: new 
Abstract: Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</title>
<link>https://arxiv.org/abs/2511.14210</link>
<guid>https://arxiv.org/abs/2511.14210</guid>
<content:encoded><![CDATA[
arXiv:2511.14210v1 Announce Type: new 
Abstract: We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration</title>
<link>https://arxiv.org/abs/2511.14213</link>
<guid>https://arxiv.org/abs/2511.14213</guid>
<content:encoded><![CDATA[
arXiv:2511.14213v1 Announce Type: new 
Abstract: Blind face restoration (BFR) may correspond to multiple plausible high-quality (HQ) reconstructions under extremely low-quality (LQ) inputs. However, existing methods typically produce deterministic results, struggling to capture this one-to-many nature. In this paper, we propose a Measurement-Constrained Sampling (MCS) approach that enables diverse LQ face reconstructions conditioned on different textual prompts. Specifically, we formulate BFR as a measurement-constrained generative task by constructing an inverse problem through controlled degradations of coarse restorations, which allows posterior-guided sampling within text-to-image diffusion. Measurement constraints include both Forward Measurement, which ensures results align with input structures, and Reverse Measurement, which produces projection spaces, ensuring that the solution can align with various prompts. Experiments show that our MCS can generate prompt-aligned results and outperforms existing BFR methods. Codes will be released after acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model</title>
<link>https://arxiv.org/abs/2511.14223</link>
<guid>https://arxiv.org/abs/2511.14223</guid>
<content:encoded><![CDATA[
arXiv:2511.14223v1 Announce Type: new 
Abstract: This paper focuses on the task of speech-driven 3D facial animation, which aims to generate realistic and synchronized facial motions driven by speech inputs.Recent methods have employed audio-conditioned diffusion models for 3D facial animation, achieving impressive results in generating expressive and natural animations.However, these methods process the whole audio sequences in a single pass, which poses two major challenges: they tend to perform poorly when handling audio sequences that exceed the training horizon and will suffer from significant latency when processing long audio inputs. To address these limitations, we propose a novel autoregressive diffusion model that processes input audio in a streaming manner. This design ensures flexibility with varying audio lengths and achieves low latency independent of audio duration. Specifically, we select a limited number of past frames as historical motion context and combine them with the audio input to create a dynamic condition. This condition guides the diffusion process to iteratively generate facial motion frames, enabling real-time synthesis with high-quality results. Additionally, we implemented a real-time interactive demo, highlighting the effectiveness and efficiency of our approach. We will release the code at https://zju3dv.github.io/StreamingTalker/.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Passive Learning Trap: An Active Perception Strategy for Human Motion Prediction</title>
<link>https://arxiv.org/abs/2511.14237</link>
<guid>https://arxiv.org/abs/2511.14237</guid>
<content:encoded><![CDATA[
arXiv:2511.14237v1 Announce Type: new 
Abstract: Forecasting 3D human motion is an important embodiment of fine-grained understanding and cognition of human behavior by artificial agents. Current approaches excessively rely on implicit network modeling of spatiotemporal relationships and motion characteristics, falling into the passive learning trap that results in redundant and monotonous 3D coordinate information acquisition while lacking actively guided explicit learning mechanisms. To overcome these issues, we propose an Active Perceptual Strategy (APS) for human motion prediction, leveraging quotient space representations to explicitly encode motion properties while introducing auxiliary learning objectives to strengthen spatio-temporal modeling. Specifically, we first design a data perception module that projects poses into the quotient space, decoupling motion geometry from coordinate redundancy. By jointly encoding tangent vectors and Grassmann projections, this module simultaneously achieves geometric dimension reduction, semantic decoupling, and dynamic constraint enforcement for effective motion pose characterization. Furthermore, we introduce a network perception module that actively learns spatio-temporal dependencies through restorative learning. This module deliberately masks specific joints or injects noise to construct auxiliary supervision signals. A dedicated auxiliary learning network is designed to actively adapt and learn from perturbed information. Notably, APS is model agnostic and can be integrated with different prediction models to enhance active perceptual. The experimental results demonstrate that our method achieves the new state-of-the-art, outperforming existing methods by large margins: 16.3% on H3.6M, 13.9% on CMU Mocap, and 10.1% on 3DPW.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization</title>
<link>https://arxiv.org/abs/2511.14238</link>
<guid>https://arxiv.org/abs/2511.14238</guid>
<content:encoded><![CDATA[
arXiv:2511.14238v1 Announce Type: new 
Abstract: The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization</title>
<link>https://arxiv.org/abs/2511.14247</link>
<guid>https://arxiv.org/abs/2511.14247</guid>
<content:encoded><![CDATA[
arXiv:2511.14247v1 Announce Type: new 
Abstract: Multi-agents rely on accurate poses to share and align observations, enabling a collaborative perception of the environment. However, traditional GNSS-based localization often fails in GNSS-denied environments, making consistent feature alignment difficult in collaboration. To tackle this challenge, we propose a robust GNSS-free collaborative perception framework based on LiDAR localization. Specifically, we propose a lightweight Pose Generator with Confidence (PGC) to estimate compact pose and confidence representations. To alleviate the effects of localization errors, we further develop the Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT), which performs confidence-aware spatial alignment while capturing essential temporal context. Additionally, we present a new simulation dataset, V2VLoc, which can be adapted for both LiDAR localization and collaborative detection tasks. V2VLoc comprises three subsets: Town1Loc, Town4Loc, and V2VDet. Town1Loc and Town4Loc offer multi-traversal sequences for training in localization tasks, whereas V2VDet is specifically intended for the collaborative detection task. Extensive experiments conducted on the V2VLoc dataset demonstrate that our approach achieves state-of-the-art performance under GNSS-denied conditions. We further conduct extended experiments on the real-world V2V4Real dataset to validate the effectiveness and generalizability of PASTAT.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation</title>
<link>https://arxiv.org/abs/2511.14259</link>
<guid>https://arxiv.org/abs/2511.14259</guid>
<content:encoded><![CDATA[
arXiv:2511.14259v1 Announce Type: new 
Abstract: With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery</title>
<link>https://arxiv.org/abs/2511.14270</link>
<guid>https://arxiv.org/abs/2511.14270</guid>
<content:encoded><![CDATA[
arXiv:2511.14270v1 Announce Type: new 
Abstract: Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation</title>
<link>https://arxiv.org/abs/2511.14271</link>
<guid>https://arxiv.org/abs/2511.14271</guid>
<content:encoded><![CDATA[
arXiv:2511.14271v1 Announce Type: new 
Abstract: Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Free Lunch to Meet the Gap: Intermediate Domain Reconstruction for Cross-Domain Few-Shot Learning</title>
<link>https://arxiv.org/abs/2511.14279</link>
<guid>https://arxiv.org/abs/2511.14279</guid>
<content:encoded><![CDATA[
arXiv:2511.14279v1 Announce Type: new 
Abstract: Cross-Domain Few-Shot Learning (CDFSL) endeavors to transfer generalized knowledge from the source domain to target domains using only a minimal amount of training data, which faces a triplet of learning challenges in the meantime, i.e., semantic disjoint, large domain discrepancy, and data scarcity. Different from predominant CDFSL works focused on generalized representations, we make novel attempts to construct Intermediate Domain Proxies (IDP) with source feature embeddings as the codebook and reconstruct the target domain feature with this learned codebook. We then conduct an empirical study to explore the intrinsic attributes from perspectives of visual styles and semantic contents in intermediate domain proxies. Reaping benefits from these attributes of intermediate domains, we develop a fast domain alignment method to use these proxies as learning guidance for target domain feature transformation. With the collaborative learning of intermediate domain reconstruction and target feature transformation, our proposed model is able to surpass the state-of-the-art models by a margin on 8 cross-domain few-shot learning benchmarks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction</title>
<link>https://arxiv.org/abs/2511.14283</link>
<guid>https://arxiv.org/abs/2511.14283</guid>
<content:encoded><![CDATA[
arXiv:2511.14283v1 Announce Type: new 
Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration</title>
<link>https://arxiv.org/abs/2511.14286</link>
<guid>https://arxiv.org/abs/2511.14286</guid>
<content:encoded><![CDATA[
arXiv:2511.14286v1 Announce Type: new 
Abstract: In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories. During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data. However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone. Robust, automatic, and modality-agnostic bone surface registration is therefore clinically important. We propose NeuralBoneReg, a self-supervised, surface-based framework that registers bone surfaces using 3D point clouds as a modality-agnostic representation. NeuralBoneReg includes two modules: an implicit neural unsigned distance field (UDF) that learns the preoperative bone model, and an MLP-based registration module that performs global initialization and local refinement by generating transformation hypotheses to align the intraoperative point cloud with the neural UDF. Unlike SOTA supervised methods, NeuralBoneReg operates in a self-supervised manner, without requiring inter-subject training data. We evaluated NeuralBoneReg against baseline methods on two publicly available multi-modal datasets: a CT-ultrasound dataset of the fibula and tibia (UltraBones100k) and a CT-RGB-D dataset of spinal vertebrae (SpineDepth). The evaluation also includes a newly introduced CT--ultrasound dataset of cadaveric subjects containing femur and pelvis (UltraBones-Hip), which will be made publicly available. NeuralBoneReg matches or surpasses existing methods across all datasets, achieving mean RRE/RTE of 1.68{\deg}/1.86 mm on UltraBones100k, 1.88{\deg}/1.89 mm on UltraBones-Hip, and 3.79{\deg}/2.45 mm on SpineDepth. These results demonstrate strong generalizability across anatomies and modalities, providing robust and accurate cross-modal alignment for CAOS.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEN3D: Generating Domain-Free 3D Scenes from a Single Image</title>
<link>https://arxiv.org/abs/2511.14291</link>
<guid>https://arxiv.org/abs/2511.14291</guid>
<content:encoded><![CDATA[
arXiv:2511.14291v1 Announce Type: new 
Abstract: Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.14302</link>
<guid>https://arxiv.org/abs/2511.14302</guid>
<content:encoded><![CDATA[
arXiv:2511.14302v1 Announce Type: new 
Abstract: Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model</title>
<link>https://arxiv.org/abs/2511.14310</link>
<guid>https://arxiv.org/abs/2511.14310</guid>
<content:encoded><![CDATA[
arXiv:2511.14310v1 Announce Type: new 
Abstract: Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs</title>
<link>https://arxiv.org/abs/2511.14315</link>
<guid>https://arxiv.org/abs/2511.14315</guid>
<content:encoded><![CDATA[
arXiv:2511.14315v1 Announce Type: new 
Abstract: Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices</title>
<link>https://arxiv.org/abs/2511.14322</link>
<guid>https://arxiv.org/abs/2511.14322</guid>
<content:encoded><![CDATA[
arXiv:2511.14322v1 Announce Type: new 
Abstract: With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step by Step Network</title>
<link>https://arxiv.org/abs/2511.14329</link>
<guid>https://arxiv.org/abs/2511.14329</guid>
<content:encoded><![CDATA[
arXiv:2511.14329v1 Announce Type: new 
Abstract: Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding</title>
<link>https://arxiv.org/abs/2511.14336</link>
<guid>https://arxiv.org/abs/2511.14336</guid>
<content:encoded><![CDATA[
arXiv:2511.14336v1 Announce Type: new 
Abstract: A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs</title>
<link>https://arxiv.org/abs/2511.14343</link>
<guid>https://arxiv.org/abs/2511.14343</guid>
<content:encoded><![CDATA[
arXiv:2511.14343v1 Announce Type: new 
Abstract: Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</title>
<link>https://arxiv.org/abs/2511.14349</link>
<guid>https://arxiv.org/abs/2511.14349</guid>
<content:encoded><![CDATA[
arXiv:2511.14349v1 Announce Type: new 
Abstract: The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IBGS: Image-Based Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.14357</link>
<guid>https://arxiv.org/abs/2511.14357</guid>
<content:encoded><![CDATA[
arXiv:2511.14357v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements</title>
<link>https://arxiv.org/abs/2511.14361</link>
<guid>https://arxiv.org/abs/2511.14361</guid>
<content:encoded><![CDATA[
arXiv:2511.14361v1 Announce Type: new 
Abstract: Blinking is a vital physiological process that protects and maintains the health of the ocular surface. Objective assessment of eyelid movements remains challenging due to the complexity, cost, and limited clinical applicability of existing tools. This study presents the clinical validation of Bapp (Blink Application), a mobile application developed using the Flutter framework and integrated with Google ML Kit for on-device, real-time analysis of eyelid movements. The validation occurred using 45 videos from real patients, whose blinks were manually annotated by ophthalmology specialists from the Paulista School of Medicine of the Federal University of Sao Paulo (EPM-UNIFESP) to serve as the ground truth. Bapp's performance was evaluated using standard metrics, including Precision, Recall, and F1-Score, with results demonstrating 98.4% precision, 96.9% recall, and an overall accuracy of 98.3%. These outcomes confirm the reliability of Bapp as a portable, accessible, and objective tool for monitoring both normal and abnormal eyelid movements. The application offers a promising alternative to traditional manual blink counting, supporting continuous ocular health monitoring and postoperative evaluation in clinical environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</title>
<link>https://arxiv.org/abs/2511.14368</link>
<guid>https://arxiv.org/abs/2511.14368</guid>
<content:encoded><![CDATA[
arXiv:2511.14368v1 Announce Type: new 
Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection</title>
<link>https://arxiv.org/abs/2511.14371</link>
<guid>https://arxiv.org/abs/2511.14371</guid>
<content:encoded><![CDATA[
arXiv:2511.14371v1 Announce Type: new 
Abstract: Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background. Generally, detection performance heavily depends on the discriminative feature representation between target and background. Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection. Improving feature representation for detection under blur conditions remains challenging. In this paper, we propose a novel Joint Feature-Domain Deblurring and Detection end-to-end framework, dubbed JFD3. We design a dual-branch architecture with shared weights, where the clear branch guides the blurred branch to enhance discriminative feature representation. Specifically, we first introduce a lightweight feature restoration network, where features from the clear branch serve as feature-level supervision to guide the blurred branch, thereby enhancing its distinctive capability for detection. We then propose a frequency structure guidance module that refines the structure prior from the restoration network and integrates it into shallow detection layers to enrich target structural information. Finally, a feature consistency self-supervised loss is imposed between the dual-branch detection backbones, driving the blurred branch to approximate the feature representations of the clear one. Wealso construct a benchmark, named IRBlurUAV, containing 30,000 simulated and 4,118 real infrared UAV target images with diverse motion blur. Extensive experiments on IRBlurUAV demonstrate that JFD3 achieves superior detection performance while maintaining real-time efficiency.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents</title>
<link>https://arxiv.org/abs/2511.14376</link>
<guid>https://arxiv.org/abs/2511.14376</guid>
<content:encoded><![CDATA[
arXiv:2511.14376v1 Announce Type: new 
Abstract: International standards for biometric identity documents mandate strict compliance with pose requirements, including the square presentation of a subject's shoulders. However, the literature on automated quality assessment offers few quantitative methods for evaluating this specific attribute. This paper proposes a Shoulder Presentation Evaluation (SPE) algorithm to address this gap. The method quantifies shoulder yaw and roll using only the 3D coordinates of two shoulder landmarks provided by common pose estimation frameworks. The algorithm was evaluated on a dataset of 121 portrait images. The resulting SPE scores demonstrated a strong Pearson correlation (r approx. 0.80) with human-assigned labels. An analysis of the metric's filtering performance, using an adapted Error-versus-Discard methodology, confirmed its utility in identifying non-compliant samples. The proposed algorithm is a viable lightweight tool for automated compliance checking in enrolment systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.14386</link>
<guid>https://arxiv.org/abs/2511.14386</guid>
<content:encoded><![CDATA[
arXiv:2511.14386v1 Announce Type: new 
Abstract: Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition</title>
<link>https://arxiv.org/abs/2511.14391</link>
<guid>https://arxiv.org/abs/2511.14391</guid>
<content:encoded><![CDATA[
arXiv:2511.14391v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used for decision-making and planning in autonomous driving, showing promising reasoning capabilities and potential to generalize across diverse traffic situations. However, current LLM-based driving agents lack explicit mechanisms to enforce traffic rules and often struggle to reliably detect small, safety-critical objects such as traffic lights and signs. To address this limitation, we introduce TLS-Assist, a modular redundancy layer that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition. TLS-Assist converts detections into structured natural language messages that are injected into the LLM input, enforcing explicit attention to safety-critical cues. The framework is plug-and-play, model-agnostic, and supports both single-view and multi-view camera setups. We evaluate TLS-Assist in a closed-loop setup on the LangAuto benchmark in CARLA. The results demonstrate relative driving performance improvements of up to 14% over LMDrive and 7% over BEVDriver, while consistently reducing traffic light and sign infractions. We publicly release the code and models on https://github.com/iis-esslingen/TLS-Assist.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEDLAM2.0: Synthetic Humans and Cameras in Motion</title>
<link>https://arxiv.org/abs/2511.14394</link>
<guid>https://arxiv.org/abs/2511.14394</guid>
<content:encoded><![CDATA[
arXiv:2511.14394v1 Announce Type: new 
Abstract: Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stage Aware Diagnosis of Diabetic Retinopathy via Ordinal Regression</title>
<link>https://arxiv.org/abs/2511.14398</link>
<guid>https://arxiv.org/abs/2511.14398</guid>
<content:encoded><![CDATA[
arXiv:2511.14398v1 Announce Type: new 
Abstract: Diabetic Retinopathy (DR) has emerged as a major cause of preventable blindness in recent times. With timely screening and intervention, the condition can be prevented from causing irreversible damage. The work introduces a state-of-the-art Ordinal Regression-based DR Detection framework that uses the APTOS-2019 fundus image dataset. A widely accepted combination of preprocessing methods: Green Channel (GC) Extraction, Noise Masking, and CLAHE, was used to isolate the most relevant features for DR classification. Model performance was evaluated using the Quadratic Weighted Kappa, with a focus on agreement between results and clinical grading. Our Ordinal Regression approach attained a QWK score of 0.8992, setting a new benchmark on the APTOS dataset.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language as an Anchor: Preserving Relative Visual Geometry for Domain Incremental Learning</title>
<link>https://arxiv.org/abs/2511.14401</link>
<guid>https://arxiv.org/abs/2511.14401</guid>
<content:encoded><![CDATA[
arXiv:2511.14401v1 Announce Type: new 
Abstract: A key challenge in Domain Incremental Learning (DIL) is to continually learn under shifting distributions while preserving knowledge from previous domains. Existing methods face a fundamental dilemma. On one hand, projecting all domains into a single unified visual space leads to inter-domain interference and semantic distortion, as large shifts may vary with not only visual appearance but also underlying semantics. On the other hand, isolating domain-specific parameters causes knowledge fragmentation, creating "knowledge islands" that hamper knowledge reuse and exacerbate forgetting. To address this issue, we propose LAVA (Language-Anchored Visual Alignment), a novel DIL framework that replaces direct feature alignment with relative alignment driven by a text-based reference anchor. LAVA guides the visual representations of each incoming domain to preserve a consistent relative geometry, which is defined by mirroring the pairwise semantic similarities between the class names. This anchored geometric structure acts as a bridge across domains, enabling the retrieval of class-aware prior knowledge and facilitating robust feature aggregation. Extensive experiments on standard DIL benchmarks demonstrate that LAVA achieves significant performance improvements over state-of-the-arts. Code is available at https://github.com/ShuyiGeng/LAVA.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays</title>
<link>https://arxiv.org/abs/2511.14411</link>
<guid>https://arxiv.org/abs/2511.14411</guid>
<content:encoded><![CDATA[
arXiv:2511.14411v1 Announce Type: new 
Abstract: In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important. Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise. Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks. However, these methods are not reliable due to insufficient large-scale validation studies. In this paper, we proposed a novel framework Cranio-ID: First, an automatic annotation of landmarks on 2D skulls (which are X-ray scans of faces) with their respective optical images using our trained YOLO-pose models. Second, cross-modal matching by formulating these landmarks into graph representations and then finding semantic correspondence between graphs of these two modalities using cross-attention and optimal transport framework. Our proposed framework is validated on the S2F and CUHK datasets (CUHK dataset resembles with S2F dataset). Extensive experiments have been conducted to evaluate the performance of our proposed framework, which demonstrates significant improvements in both reliability and accuracy, as well as its effectiveness in cross-domain skull-to-face and sketch-to-face matching in forensic science.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to See Through a Baby's Eyes: Early Visual Diets Enable Robust Visual Intelligence in Humans and Machines</title>
<link>https://arxiv.org/abs/2511.14440</link>
<guid>https://arxiv.org/abs/2511.14440</guid>
<content:encoded><![CDATA[
arXiv:2511.14440v1 Announce Type: new 
Abstract: Newborns perceive the world with low-acuity, color-degraded, and temporally continuous vision, which gradually sharpens as infants develop. To explore the ecological advantages of such staged "visual diets", we train self-supervised learning (SSL) models on object-centric videos under constraints that simulate infant vision: grayscale-to-color (C), blur-to-sharp (A), and preserved temporal continuity (T)-collectively termed CATDiet. For evaluation, we establish a comprehensive benchmark across ten datasets, covering clean and corrupted image recognition, texture-shape cue conflict tests, silhouette recognition, depth-order classification, and the visual cliff paradigm. All CATDiet variants demonstrate enhanced robustness in object recognition, despite being trained solely on object-centric videos. Remarkably, models also exhibit biologically aligned developmental patterns, including neural plasticity changes mirroring synaptic density in macaque V1 and behaviors resembling infants' visual cliff responses. Building on these insights, CombDiet initializes SSL with CATDiet before standard training while preserving temporal continuity. Trained on object-centric or head-mounted infant videos, CombDiet outperforms standard SSL on both in-domain and out-of-domain object recognition and depth perception. Together, these results suggest that the developmental progression of early infant visual experience offers a powerful reverse-engineering framework for understanding the emergence of robust visual intelligence in machines. All code, data, and models will be publicly released.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding</title>
<link>https://arxiv.org/abs/2511.14446</link>
<guid>https://arxiv.org/abs/2511.14446</guid>
<content:encoded><![CDATA[
arXiv:2511.14446v1 Announce Type: new 
Abstract: Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval</title>
<link>https://arxiv.org/abs/2511.14449</link>
<guid>https://arxiv.org/abs/2511.14449</guid>
<content:encoded><![CDATA[
arXiv:2511.14449v1 Announce Type: new 
Abstract: This paper addresses the task of interactive, conversational text-to-image retrieval.
  Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.
  The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.
  Complementarily, the Image Refiner identifies perceptual gaps between generated images and user intentions, strategically reducing the visual-semantic discrepancy. By leveraging multi-turn dialogues, DIR-TIR provides superior controllability and fault tolerance compared to conventional single-query methods, significantly improving target image hit accuracy.
  Comprehensive experiments across diverse image datasets demonstrate our dialogue-based approach substantially outperforms initial-description-only baselines, while the synergistic module integration achieves both higher retrieval precision and enhanced interactive experience.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</title>
<link>https://arxiv.org/abs/2511.14469</link>
<guid>https://arxiv.org/abs/2511.14469</guid>
<content:encoded><![CDATA[
arXiv:2511.14469v1 Announce Type: new 
Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Subglacial Bed Topography from Sparse Radar with Physics-Guided Residuals</title>
<link>https://arxiv.org/abs/2511.14473</link>
<guid>https://arxiv.org/abs/2511.14473</guid>
<content:encoded><![CDATA[
arXiv:2511.14473v1 Announce Type: new 
Abstract: Accurate subglacial bed topography is essential for ice sheet modeling, yet radar observations are sparse and uneven. We propose a physics-guided residual learning framework that predicts bed thickness residuals over a BedMachine prior and reconstructs bed from the observed surface. A DeepLabV3+ decoder over a standard encoder (e.g.,ResNet-50) is trained with lightweight physics and data terms: multi-scale mass conservation, flow-aligned total variation, Laplacian damping, non-negativity of thickness, a ramped prior-consistency term, and a masked Huber fit to radar picks modulated by a confidence map. To measure real-world generalization, we adopt leakage-safe blockwise hold-outs (vertical/horizontal) with safety buffers and report metrics only on held-out cores. Across two Greenland sub-regions, our approach achieves strong test-core accuracy and high structural fidelity, outperforming U-Net, Attention U-Net, FPN, and a plain CNN. The residual-over-prior design, combined with physics, yields spatially coherent, physically plausible beds suitable for operational mapping under domain shift.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2D Gaussians Spatial Transport for Point-supervised Density Regression</title>
<link>https://arxiv.org/abs/2511.14477</link>
<guid>https://arxiv.org/abs/2511.14477</guid>
<content:encoded><![CDATA[
arXiv:2511.14477v1 Announce Type: new 
Abstract: This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation</title>
<link>https://arxiv.org/abs/2511.14481</link>
<guid>https://arxiv.org/abs/2511.14481</guid>
<content:encoded><![CDATA[
arXiv:2511.14481v1 Announce Type: new 
Abstract: Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM</title>
<link>https://arxiv.org/abs/2511.14499</link>
<guid>https://arxiv.org/abs/2511.14499</guid>
<content:encoded><![CDATA[
arXiv:2511.14499v1 Announce Type: new 
Abstract: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Aware Mamba Model for Multi-task Dense Prediction</title>
<link>https://arxiv.org/abs/2511.14503</link>
<guid>https://arxiv.org/abs/2511.14503</guid>
<content:encoded><![CDATA[
arXiv:2511.14503v1 Announce Type: new 
Abstract: Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images</title>
<link>https://arxiv.org/abs/2511.14518</link>
<guid>https://arxiv.org/abs/2511.14518</guid>
<content:encoded><![CDATA[
arXiv:2511.14518v1 Announce Type: new 
Abstract: Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement</title>
<link>https://arxiv.org/abs/2511.14521</link>
<guid>https://arxiv.org/abs/2511.14521</guid>
<content:encoded><![CDATA[
arXiv:2511.14521v1 Announce Type: new 
Abstract: Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation</title>
<link>https://arxiv.org/abs/2511.14530</link>
<guid>https://arxiv.org/abs/2511.14530</guid>
<content:encoded><![CDATA[
arXiv:2511.14530v1 Announce Type: new 
Abstract: Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details</title>
<link>https://arxiv.org/abs/2511.14539</link>
<guid>https://arxiv.org/abs/2511.14539</guid>
<content:encoded><![CDATA[
arXiv:2511.14539v1 Announce Type: new 
Abstract: Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction</title>
<link>https://arxiv.org/abs/2511.14540</link>
<guid>https://arxiv.org/abs/2511.14540</guid>
<content:encoded><![CDATA[
arXiv:2511.14540v1 Announce Type: new 
Abstract: This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title>
<link>https://arxiv.org/abs/2511.14554</link>
<guid>https://arxiv.org/abs/2511.14554</guid>
<content:encoded><![CDATA[
arXiv:2511.14554v1 Announce Type: new 
Abstract: Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Digital Pathology Models via Clustering Activations</title>
<link>https://arxiv.org/abs/2511.14558</link>
<guid>https://arxiv.org/abs/2511.14558</guid>
<content:encoded><![CDATA[
arXiv:2511.14558v1 Announce Type: new 
Abstract: We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.14582</link>
<guid>https://arxiv.org/abs/2511.14582</guid>
<content:encoded><![CDATA[
arXiv:2511.14582v1 Announce Type: new 
Abstract: Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2511.14588</link>
<guid>https://arxiv.org/abs/2511.14588</guid>
<content:encoded><![CDATA[
arXiv:2511.14588v1 Announce Type: new 
Abstract: White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities</title>
<link>https://arxiv.org/abs/2511.14599</link>
<guid>https://arxiv.org/abs/2511.14599</guid>
<content:encoded><![CDATA[
arXiv:2511.14599v1 Announce Type: new 
Abstract: The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts</title>
<link>https://arxiv.org/abs/2511.14601</link>
<guid>https://arxiv.org/abs/2511.14601</guid>
<content:encoded><![CDATA[
arXiv:2511.14601v1 Announce Type: new 
Abstract: Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation</title>
<link>https://arxiv.org/abs/2511.14604</link>
<guid>https://arxiv.org/abs/2511.14604</guid>
<content:encoded><![CDATA[
arXiv:2511.14604v1 Announce Type: new 
Abstract: Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology</title>
<link>https://arxiv.org/abs/2511.14613</link>
<guid>https://arxiv.org/abs/2511.14613</guid>
<content:encoded><![CDATA[
arXiv:2511.14613v1 Announce Type: new 
Abstract: A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&amp;E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&amp;E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap</title>
<link>https://arxiv.org/abs/2511.14620</link>
<guid>https://arxiv.org/abs/2511.14620</guid>
<content:encoded><![CDATA[
arXiv:2511.14620v1 Announce Type: new 
Abstract: Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as `intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</title>
<link>https://arxiv.org/abs/2511.14633</link>
<guid>https://arxiv.org/abs/2511.14633</guid>
<content:encoded><![CDATA[
arXiv:2511.14633v1 Announce Type: new 
Abstract: Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</title>
<link>https://arxiv.org/abs/2511.14639</link>
<guid>https://arxiv.org/abs/2511.14639</guid>
<content:encoded><![CDATA[
arXiv:2511.14639v1 Announce Type: new 
Abstract: Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT</title>
<link>https://arxiv.org/abs/2511.14649</link>
<guid>https://arxiv.org/abs/2511.14649</guid>
<content:encoded><![CDATA[
arXiv:2511.14649v1 Announce Type: new 
Abstract: Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATM'22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms</title>
<link>https://arxiv.org/abs/2511.14654</link>
<guid>https://arxiv.org/abs/2511.14654</guid>
<content:encoded><![CDATA[
arXiv:2511.14654v1 Announce Type: new 
Abstract: Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Image Resolution on Age Estimation with DeepFace and InsightFace</title>
<link>https://arxiv.org/abs/2511.14689</link>
<guid>https://arxiv.org/abs/2511.14689</guid>
<content:encoded><![CDATA[
arXiv:2511.14689v1 Announce Type: new 
Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring</title>
<link>https://arxiv.org/abs/2511.14698</link>
<guid>https://arxiv.org/abs/2511.14698</guid>
<content:encoded><![CDATA[
arXiv:2511.14698v1 Announce Type: new 
Abstract: Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images</title>
<link>https://arxiv.org/abs/2511.14702</link>
<guid>https://arxiv.org/abs/2511.14702</guid>
<content:encoded><![CDATA[
arXiv:2511.14702v1 Announce Type: new 
Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</title>
<link>https://arxiv.org/abs/2511.14712</link>
<guid>https://arxiv.org/abs/2511.14712</guid>
<content:encoded><![CDATA[
arXiv:2511.14712v1 Announce Type: new 
Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</title>
<link>https://arxiv.org/abs/2511.14716</link>
<guid>https://arxiv.org/abs/2511.14716</guid>
<content:encoded><![CDATA[
arXiv:2511.14716v1 Announce Type: new 
Abstract: Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</title>
<link>https://arxiv.org/abs/2511.14719</link>
<guid>https://arxiv.org/abs/2511.14719</guid>
<content:encoded><![CDATA[
arXiv:2511.14719v1 Announce Type: new 
Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Field-Based Approach for View Computation &amp; Data Exploration in 3D Urban Environments</title>
<link>https://arxiv.org/abs/2511.14742</link>
<guid>https://arxiv.org/abs/2511.14742</guid>
<content:encoded><![CDATA[
arXiv:2511.14742v1 Announce Type: new 
Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Large Language Models Are Good Noise Handlers in Engagement Analysis</title>
<link>https://arxiv.org/abs/2511.14749</link>
<guid>https://arxiv.org/abs/2511.14749</guid>
<content:encoded><![CDATA[
arXiv:2511.14749v1 Announce Type: new 
Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</title>
<link>https://arxiv.org/abs/2511.14751</link>
<guid>https://arxiv.org/abs/2511.14751</guid>
<content:encoded><![CDATA[
arXiv:2511.14751v1 Announce Type: new 
Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14760</link>
<guid>https://arxiv.org/abs/2511.14760</guid>
<content:encoded><![CDATA[
arXiv:2511.14760v1 Announce Type: new 
Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC Is a Vision Problem!</title>
<link>https://arxiv.org/abs/2511.14761</link>
<guid>https://arxiv.org/abs/2511.14761</guid>
<content:encoded><![CDATA[
arXiv:2511.14761v1 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm</title>
<link>https://arxiv.org/abs/2511.13760</link>
<guid>https://arxiv.org/abs/2511.13760</guid>
<content:encoded><![CDATA[
arXiv:2511.13760v1 Announce Type: cross 
Abstract: Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Create Legally Relevant Summaries and Analyses of Videos?</title>
<link>https://arxiv.org/abs/2511.13772</link>
<guid>https://arxiv.org/abs/2511.13772</guid>
<content:encoded><![CDATA[
arXiv:2511.13772v1 Announce Type: cross 
Abstract: Understanding the legally relevant factual basis of an event and conveying it through text is a key skill of legal professionals. This skill is important for preparing forms (e.g., insurance claims) or other legal documents (e.g., court claims), but often presents a challenge for laypeople. Current AI approaches aim to bridge this gap, but mostly rely on the user to articulate what has happened in text, which may be challenging for many. Here, we investigate the capability of large language models (LLMs) to understand and summarize events occurring in videos. We ask an LLM to summarize and draft legal letters, based on 120 YouTube videos showing legal issues in various domains. Overall, 71.7\% of the summaries were rated as of high or medium quality, which is a promising result, opening the door to a number of applications in e.g. access to justice.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration</title>
<link>https://arxiv.org/abs/2511.13787</link>
<guid>https://arxiv.org/abs/2511.13787</guid>
<content:encoded><![CDATA[
arXiv:2511.13787v1 Announce Type: cross 
Abstract: In this paper, we explore the transferability of SSL by addressing two central questions: (i) what is the representation transferability of SSL, and (ii) how can we effectively model this transferability? Transferability is defined as the ability of a representation learned from one task to support the objective of another.
  Inspired by the meta-learning paradigm, we construct multiple SSL tasks within each training batch to support explicitly modeling transferability. Based on empirical evidence and causal analysis, we find that although introducing task-level information improves transferability, it is still hindered by task conflict. To address this issue, we propose a Task Conflict Calibration (TC$^2$) method to alleviate the impact of task conflict. Specifically, it first splits batches to create multiple SSL tasks, infusing task-level information. Next, it uses a factor extraction network to produce causal generative factors for all tasks and a weight extraction network to assign dedicated weights to each sample, employing data reconstruction, orthogonality, and sparsity to ensure effectiveness. Finally, TC$^2$ calibrates sample representations during SSL training and integrates into the pipeline via a two-stage bi-level optimization framework to boost the transferability of learned representations. Experimental results on multiple downstream tasks demonstrate that our method consistently improves the transferability of SSL models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures</title>
<link>https://arxiv.org/abs/2511.13798</link>
<guid>https://arxiv.org/abs/2511.13798</guid>
<content:encoded><![CDATA[
arXiv:2511.13798v1 Announce Type: cross 
Abstract: Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection</title>
<link>https://arxiv.org/abs/2511.13880</link>
<guid>https://arxiv.org/abs/2511.13880</guid>
<content:encoded><![CDATA[
arXiv:2511.13880v1 Announce Type: cross 
Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar</title>
<link>https://arxiv.org/abs/2511.13922</link>
<guid>https://arxiv.org/abs/2511.13922</guid>
<content:encoded><![CDATA[
arXiv:2511.13922v1 Announce Type: cross 
Abstract: Real-time imaging sonar has become an important tool for underwater monitoring in environments where optical sensing is unreliable. Its broader use is constrained by two coupled challenges: highly limited uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) that affect up to 98% of frames. We present SCOPE, a self-supervised framework that jointly performs compression and artifact correction without clean-noise pairs or synthetic assumptions. SCOPE combines (i) Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with (ii) Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels. Evaluated on months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to <= 0.0118 bpp. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild. Results demonstrate that learning frequency-structured latents enables practical, low-bitrate sonar streaming with preserved signal details under real-world deployment conditions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoCGM: Poisson-Conditioned Generative Model for Sparse-View CT Reconstruction</title>
<link>https://arxiv.org/abs/2511.13967</link>
<guid>https://arxiv.org/abs/2511.13967</guid>
<content:encoded><![CDATA[
arXiv:2511.13967v1 Announce Type: cross 
Abstract: In computed tomography (CT), reducing the number of projection views is an effective strategy to lower radiation exposure and/or improve temporal resolution. However, this often results in severe aliasing artifacts and loss of structural details in reconstructed images, posing significant challenges for clinical applications. Inspired by the success of the Poisson Flow Generative Model (PFGM++) in natural image generation, we propose a PoCGM (Poisson-Conditioned Generative Model) to address the challenges of sparse-view CT reconstruction. Since PFGM++ was originally designed for unconditional generation, it lacks direct applicability to medical imaging tasks that require integrating conditional inputs. To overcome this limitation, the PoCGM reformulates PFGM++ into a conditional generative framework by incorporating sparse-view data as guidance during both training and sampling phases. By modeling the posterior distribution of full-view reconstructions conditioned on sparse observations, PoCGM effectively suppresses artifacts while preserving fine structural details. Qualitative and quantitative evaluations demonstrate that PoCGM outperforms the baselines, achieving improved artifact suppression, enhanced detail preservation, and reliable performance in dose-sensitive and time-critical imaging scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios</title>
<link>https://arxiv.org/abs/2511.13970</link>
<guid>https://arxiv.org/abs/2511.13970</guid>
<content:encoded><![CDATA[
arXiv:2511.13970v1 Announce Type: cross 
Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified but Fooled! Breaking Certified Defences with Ghost Certificates</title>
<link>https://arxiv.org/abs/2511.14003</link>
<guid>https://arxiv.org/abs/2511.14003</guid>
<content:encoded><![CDATA[
arXiv:2511.14003v1 Announce Type: cross 
Abstract: Certified defenses promise provable robustness guarantees. We study the malicious exploitation of probabilistic certification frameworks to better understand the limits of guarantee provisions. Now, the objective is to not only mislead a classifier, but also manipulate the certification process to generate a robustness guarantee for an adversarial input certificate spoofing. A recent study in ICLR demonstrated that crafting large perturbations can shift inputs far into regions capable of generating a certificate for an incorrect class. Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible. We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class ghost certificates. Extensive evaluations with the ImageNet demonstrate the ability to effectively bypass state-of-the-art certified defenses such as Densepure. Our work underscores the need to better understand the limits of robustness certification methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The CHASM-SWPC Dataset for Coronal Hole Detection &amp; Analysis</title>
<link>https://arxiv.org/abs/2511.14044</link>
<guid>https://arxiv.org/abs/2511.14044</guid>
<content:encoded><![CDATA[
arXiv:2511.14044v1 Announce Type: cross 
Abstract: Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009). In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches. Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks. The resulting masks constitute the CHASM-SWPC dataset, a high-quality dataset to train and test automated CH detection models, which is released with this paper. We developed CHASM (Coronal Hole Annotation using Semi-automatic Methods), a software tool for semi-automatic annotation that enables users to rapidly and accurately annotate SWPC maps. The CHASM tool enabled us to annotate 1,111 CH masks, comprising the CHASM-SWPC-1111 dataset. We then trained multiple CHRONNOS (Coronal Hole RecOgnition Neural Network Over multi-Spectral-data) architecture (Jarolim et al. 2021) neural networks using the CHASM-SWPC dataset and compared their performance. Training the CHRONNOS neural network on these data achieved an accuracy of 0.9805, a True Skill Statistic (TSS) of 0.6807, and an intersection-over-union (IoU) of 0.5668, which is higher than the original pretrained CHRONNOS model Jarolim et al. (2021) achieved an accuracy of 0.9708, a TSS of 0.6749, and an IoU of 0.4805, when evaluated on the CHASM-SWPC-1111 test set.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELiC: Efficient LiDAR Geometry Compression via Cross-Bit-depth Feature Propagation and Bag-of-Encoders</title>
<link>https://arxiv.org/abs/2511.14070</link>
<guid>https://arxiv.org/abs/2511.14070</guid>
<content:encoded><![CDATA[
arXiv:2511.14070v1 Announce Type: cross 
Abstract: Hierarchical LiDAR geometry compression encodes voxel occupancies from low to high bit-depths, yet prior methods treat each depth independently and re-estimate local context from coordinates at every level, limiting compression efficiency. We present ELiC, a real-time framework that combines cross-bit-depth feature propagation, a Bag-of-Encoders (BoE) selection scheme, and a Morton-order-preserving hierarchy. Cross-bit-depth propagation reuses features extracted at denser, lower depths to support prediction at sparser, higher depths. BoE selects, per depth, the most suitable coding network from a small pool, adapting capacity to observed occupancy statistics without training a separate model for each level. The Morton hierarchy maintains global Z-order across depth transitions, eliminating per-level sorting and reducing latency. Together these components improve entropy modeling and computation efficiency, yielding state-of-the-art compression at real-time throughput on Ford and SemanticKITTI. Code and models will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action</title>
<link>https://arxiv.org/abs/2511.14161</link>
<guid>https://arxiv.org/abs/2511.14161</guid>
<content:encoded><![CDATA[
arXiv:2511.14161v1 Announce Type: cross 
Abstract: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindCross: Fast New Subject Adaptation with Limited Data for Cross-subject Video Reconstruction from Brain Signals</title>
<link>https://arxiv.org/abs/2511.14196</link>
<guid>https://arxiv.org/abs/2511.14196</guid>
<content:encoded><![CDATA[
arXiv:2511.14196v1 Announce Type: cross 
Abstract: Reconstructing video from brain signals is an important brain decoding task. Existing brain decoding frameworks are primarily built on a subject-dependent paradigm, which requires large amounts of brain data for each subject. However, the expensive cost of collecting brain-video data causes severe data scarcity. Although some cross-subject methods being introduced, they often overfocus with subject-invariant information while neglecting subject-specific information, resulting in slow fine-tune-based adaptation strategy. To achieve fast and data-efficient new subject adaptation, we propose MindCross, a novel cross-subject framework. MindCross's N specific encoders and one shared encoder are designed to extract subject-specific and subject-invariant information, respectively. Additionally, a Top-K collaboration module is adopted to enhance new subject decoding with the knowledge learned from previous subjects' encoders. Extensive experiments on fMRI/EEG-to-video benchmarks demonstrate MindCross's efficacy and efficiency of cross-subject decoding and new subject adaptation using only one model.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going Places: Place Recognition in Artificial and Natural Systems</title>
<link>https://arxiv.org/abs/2511.14341</link>
<guid>https://arxiv.org/abs/2511.14341</guid>
<content:encoded><![CDATA[
arXiv:2511.14341v1 Announce Type: cross 
Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v1 Announce Type: cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention</title>
<link>https://arxiv.org/abs/2511.14515</link>
<guid>https://arxiv.org/abs/2511.14515</guid>
<content:encoded><![CDATA[
arXiv:2511.14515v1 Announce Type: cross 
Abstract: Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities</title>
<link>https://arxiv.org/abs/2511.14631</link>
<guid>https://arxiv.org/abs/2511.14631</guid>
<content:encoded><![CDATA[
arXiv:2511.14631v1 Announce Type: cross 
Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</title>
<link>https://arxiv.org/abs/2511.14691</link>
<guid>https://arxiv.org/abs/2511.14691</guid>
<content:encoded><![CDATA[
arXiv:2511.14691v1 Announce Type: cross 
Abstract: Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Topological Foundation of Learning and Memory</title>
<link>https://arxiv.org/abs/1103.1587</link>
<guid>https://arxiv.org/abs/1103.1587</guid>
<content:encoded><![CDATA[
arXiv:1103.1587v3 Announce Type: replace 
Abstract: We propose a formal foundation for cognition rooted in algebraic topology, built on a Homological Parity Principle. This posits that even-dimensional homology represents stable Structure/Context (e.g., generative models), while odd-dimensional homology represents dynamic Flow/Content (e.g., sensory/memory data). Cognition is governed by the Context-Content Uncertainty Principle (CCUP), a dynamical cycle aligning these parities. This framework distinguishes two modes: Inference (waking), where the scaffold predicts the flow (a Context-before-Content process); and Learning (sleep), an inverted Structure-before-Specificity process where memory traces sculpt the scaffold. This parity interpretation unifies cognitive functions like semantic and episodic memory and provides a structural generalization of existing theories, recasting Friston's Free Energy Principle and Tonini's Integrated Information in topological terms.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemCo: Toward Semantic Coherent Visual Relationship Forecasting</title>
<link>https://arxiv.org/abs/2107.01181</link>
<guid>https://arxiv.org/abs/2107.01181</guid>
<content:encoded><![CDATA[
arXiv:2107.01181v2 Announce Type: replace 
Abstract: Visual Relationship Forecasting (VRF) aims to anticipate relations among objects without observing future visual content. The task relies on capturing and modeling the semantic coherence in object interactions, as it underpins the evolution of events and scenes in videos. However, existing VRF datasets offer limited support for learning such coherence due to noisy annotations in the datasets and weak correlations between different actions and relationship transitions in subject-object pair. Furthermore, existing methods struggle to distinguish similar relationships and overfit to unchanging relationships in consecutive frames. To address these challenges, we present SemCoBench, a benchmark that emphasizes semantic coherence for visual relationship forecasting. Based on action labels and short-term subject-object pairs, SemCoBench decomposes relationship categories and dynamics by cleaning and reorganizing video datasets to ensure predicting semantic coherence in object interactions. In addition, we also present Semantic Coherent Transformer method (SemCoFormer) to model the semantic coherence with a Relationship Augmented Module (RAM) and a Coherence Reasoning Module (CRM). RAM is designed to distinguish similar relationships, and CRM facilitates the model's focus on the dynamics in relationships. The experimental results on SemCoBench demonstrate that modeling the semantic coherence is a key step toward reasonable, fine-grained, and diverse visual relationship forecasting, contributing to a more comprehensive understanding of video scenes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoReFun: Past-Movement Guided Motion Representation Learning for Future Motion Prediction and Understanding</title>
<link>https://arxiv.org/abs/2408.02091</link>
<guid>https://arxiv.org/abs/2408.02091</guid>
<content:encoded><![CDATA[
arXiv:2408.02091v2 Announce Type: replace 
Abstract: 3D human motion prediction aims to generate coherent future motions from observed sequences, yet existing end-to-end regression frameworks often fail to capture complex dynamics and tend to produce temporally inconsistent or static predictions-a limitation rooted in representation shortcutting, where models rely on superficial cues rather than learning meaningful motion structure. We propose a two-stage self-supervised framework that decouples representation learning from prediction. In the pretraining stage, the model performs unified past-future self-reconstruction, reconstructing the past sequence while recovering masked joints in the future sequence under full historical guidance. A velocity-based masking strategy selects highly dynamic joints, forcing the model to focus on informative motion components and internalize the statistical dependencies between past and future states without regression interference. In the fine-tuning stage, the pretrained model predicts the entire future sequence, now treated as fully masked, and is further equipped with a lightweight future-text prediction head for joint optimization of low-level motion prediction and high-level motion understanding. Experiments on Human3.6M, 3DPW, and AMASS show that our method reduces average prediction errors by 8.8% over state-of-the-art methods while achieving competitive future-motion understanding performance compared to LLM-based models. Code is available at: https://github.com/JunyuShi02/MoReFun
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Regions Are Equal: Attention-Guided Perturbation Network for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2408.07490</link>
<guid>https://arxiv.org/abs/2408.07490</guid>
<content:encoded><![CDATA[
arXiv:2408.07490v3 Announce Type: replace 
Abstract: In unsupervised image anomaly detection, reconstruction methods aim to train models to capture normal patterns comprehensively for normal data reconstruction. Yet, these models sometimes retain unintended reconstruction capacity for anomalous regions during inference, leading to missed detections. To mitigate this issue, existing works perturb normal samples in a sample-agnostic manner, uniformly adding noise across spatial locations before reconstructing the original. Despite promising results, they disregard the fact that foreground locations are inherently more critical for robust reconstruction. Motivated by this, we present a novel reconstruction framework named Attention-Guided Perturbation Network (AGPNet) for industrial anomaly detection. Its core idea is to add perturbations guided by a sample-aware attention mask to improve the learning of invariant normal patterns at important locations. AGPNet consists of two branches, \ie, a reconstruction branch and an auxiliary attention-based perturbation one. The reconstruction branch learns to reconstruct normal samples, while the auxiliary one aims to produce attention masks to guide the noise perturbation process for normal samples. By perturbing more aggressively at those important regions, we encourage the reconstruction branch to learn inherent normal patterns both comprehensively and robustly. Extensive experiments are conducted on several popular benchmarks covering MVTec-AD, VisA, and MVTec-3D, and show that AGPNet consistently obtains leading anomaly detection performance across a variety of setups, including few-shot, one-class, and multi-class ones.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LED: Light Enhanced Depth Estimation at Night</title>
<link>https://arxiv.org/abs/2409.08031</link>
<guid>https://arxiv.org/abs/2409.08031</guid>
<content:encoded><![CDATA[
arXiv:2409.08031v3 Announce Type: replace 
Abstract: Nighttime camera-based depth estimation is a highly challenging task, especially for autonomous driving applications, where accurate depth perception is essential for ensuring safe navigation. Models trained on daytime data often fail in the absence of precise but costly LiDAR. Even vision foundation models trained on large amounts of data are unreliable in low-light conditions. In this work, we aim to improve the reliability of perception systems at night time. To this end, we introduce Light Enhanced Depth (LED), a novel, cost-effective approach that significantly improves depth estimation in low-light environments by harnessing a pattern projected by high definition headlights available in modern vehicles. LED leads to significant performance boosts across multiple depth-estimation architectures (encoder-decoder, Adabins, DepthFormer, Depth Anything V2) both on synthetic and real datasets. Furthermore, increased performances beyond illuminated areas reveal a holistic enhancement in scene understanding. Finally, we release the Nighttime Synthetic Drive Dataset, a synthetic and photo-realistic nighttime dataset, which comprises 49,990 comprehensively annotated images.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning -- Object Detection and Semantic Segmentation: From Theory to Applications</title>
<link>https://arxiv.org/abs/2410.15584</link>
<guid>https://arxiv.org/abs/2410.15584</guid>
<content:encoded><![CDATA[
arXiv:2410.15584v3 Announce Type: replace 
Abstract: An in-depth exploration of object detection and semantic segmentation is provided, combining theoretical foundations with practical applications. State-of-the-art advancements in machine learning and deep learning are reviewed, focusing on convolutional neural networks (CNNs), YOLO architectures, and transformer-based approaches such as DETR. The integration of artificial intelligence (AI) techniques and large language models for enhancing object detection in complex environments is examined. Additionally, a comprehensive analysis of big data processing is presented, with emphasis on model optimization and performance evaluation metrics. By bridging the gap between traditional methods and modern deep learning frameworks, valuable insights are offered for researchers, data scientists, and engineers aiming to apply AI-driven methodologies to large-scale object detection tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVST: A Unified Framework for Training-free Localized Video Style Transfer</title>
<link>https://arxiv.org/abs/2410.20084</link>
<guid>https://arxiv.org/abs/2410.20084</guid>
<content:encoded><![CDATA[
arXiv:2410.20084v5 Announce Type: replace 
Abstract: This paper presents UniVST, a unified framework for localized video style transfer based on diffusion models. It operates without the need for training, offering a distinct advantage over existing diffusion methods that transfer style across entire videos. The endeavors of this paper comprise: (1) A point-matching mask propagation strategy that leverages the feature maps from the DDIM inversion. This streamlines the model's architecture by obviating the need for tracking models. (2) A training-free AdaIN-guided localized video stylization mechanism that operates at both the latent and attention levels. This balances content fidelity and style richness, mitigating the loss of localized details commonly associated with direct video stylization. (3) A sliding-window consistent smoothing scheme that harnesses optical flow within the pixel representation and refines predicted noise to update the latent space. This significantly enhances temporal consistency and diminishes artifacts in stylized video. Our proposed UniVST has been validated to be superior to existing methods in quantitative and qualitative metrics. It adeptly addresses the challenges of preserving the primary object's style while ensuring temporal consistency and detail preservation. Our code is available at https://github.com/QuanjianSong/UniVST.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2410.22995</link>
<guid>https://arxiv.org/abs/2410.22995</guid>
<content:encoded><![CDATA[
arXiv:2410.22995v2 Announce Type: replace 
Abstract: A hallmark of advanced artificial intelligence is the capacity to progress from passive visual perception to the strategic modification of visual information to facilitate complex reasoning. This advanced capability, however, remains critically underdeveloped in current Large Multi-modal Models (LMMs). The deficiency is often masked by evaluation metrics that prioritize final-answer accuracy, creating an illusion of competence where genuine reasoning is absent. Using the domain of geometric problem-solving as a precise instrument, we probe this issue through tasks that require constructing visual aids. To this end, we introduce \textbf{VisAidMath}, a challenging benchmark, and our novel Three-Layered Funnel Evaluation Framework. This framework moves beyond simple accuracy (ACCU) to scrutinize the generation of valid visual aids (PVA) and the soundness of subsequent reasoning steps (SPRS). Our extensive experiments on state-of-the-art models, including Doubao-Seed-1.6 and o4, reveal a profound ``Reasoning Illusion''. We observe that high surface-level accuracy conceals a catastrophic failure in the models' ability to produce valid visual aids or to reason from them. Our findings expose a fundamental schism between visual perception and logical deduction in modern LMMs. We host an evaluation platform at CodaBench for testing publicly. Homepage: https://nlp2ct.github.io/VisAidMathHomepage/ Evaluation: https://www.codabench.org/competitions/7634/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Fourier Filtering Network with Contrastive Learning for AAV-based Unaligned Bimodal Salient Object Detection</title>
<link>https://arxiv.org/abs/2411.03728</link>
<guid>https://arxiv.org/abs/2411.03728</guid>
<content:encoded><![CDATA[
arXiv:2411.03728v3 Announce Type: replace 
Abstract: Autonomous aerial vehicle (AAV)-based bi-modal salient object detection (BSOD) aims to segment salient objects in a scene utilizing complementary cues in unaligned RGB and thermal image pairs. However, the high computational expense of existing AAV-based BSOD models limits their applicability to real-world AAV devices. To address this problem, we propose an efficient Fourier filter network with contrastive learning that achieves both real-time and accurate performance. Specifically, we first design a semantic contrastive alignment loss to align the two modalities at the semantic level, which facilitates mutual refinement in a parameter-free way. Second, inspired by the fast Fourier transform that obtains global relevance in linear complexity, we propose synchronized alignment fusion, which aligns and fuses bi-modal features in the channel and spatial dimensions by a hierarchical filtering mechanism. Our proposed model, AlignSal, reduces the number of parameters by 70.0%, decreases the floating point operations by 49.4%, and increases the inference speed by 152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive experiments on the AAV RGB-T 2400 and seven bi-modal dense prediction datasets demonstrate that AlignSal achieves both real-time inference speed and better performance and generalizability compared to nineteen state-of-the-art models across most evaluation metrics. In addition, our ablation studies further verify AlignSal's potential in boosting the performance of existing aligned BSOD models on AAV-based unaligned data. The code is available at: https://github.com/JoshuaLPF/AlignSal.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iris: Integrating Language into Diffusion-based Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v4 Announce Type: replace 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisances. We demonstrate that language can enhance monocular depth estimation by providing an additional condition (rather than images alone) aligned with plausible 3D scenes, thereby reducing the solution space for depth estimation. This conditional distribution is learned during the text-to-image pre-training of diffusion models. To generate images under various viewpoints and layouts that precisely reflect textual descriptions, the model implicitly models object sizes, shapes, and scales, their spatial relationships, and the overall scene structure. In this paper, Iris, we investigate the benefits of our strategy to integrate text descriptions into training and inference of diffusion-based depth estimation models. We experiment with three different diffusion-based monocular depth estimators (Marigold, Lotus, and E2E-FT) and their variants. By training on HyperSim and Virtual KITTI, and evaluating on NYUv2, KITTI, ETH3D, ScanNet, and DIODE, we find that our strategy improves the overall monocular depth estimation accuracy, especially in small areas. It also improves the model's depth perception of specific regions described in the text. We find that by providing more details in the text, the depth prediction can be iteratively refined. Simultaneously, we find that language can act as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. Code and generated text data will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVias: Mitigate any Visual Bias</title>
<link>https://arxiv.org/abs/2412.06632</link>
<guid>https://arxiv.org/abs/2412.06632</guid>
<content:encoded><![CDATA[
arXiv:2412.06632v2 Announce Type: replace 
Abstract: Mitigating biases in computer vision models is an essential step towards the trustworthiness of artificial intelligence models. Existing bias mitigation methods focus on a small set of predefined biases, limiting their applicability in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach leveraging foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select those visual features defining the target class, resulting in a set of language-coded potential visual biases. We then translate this set of potential biases into vision-language embeddings and introduce an in-processing bias mitigation approach to prevent the model from encoding information related to them. Our experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks outperforming current state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2502.07631</link>
<guid>https://arxiv.org/abs/2502.07631</guid>
<content:encoded><![CDATA[
arXiv:2502.07631v3 Announce Type: replace 
Abstract: Perceiving the environment and its changes over time corresponds to two fundamental yet heterogeneous types of information: semantics and motion. Previous end-to-end autonomous driving works represent both types of information in a single feature vector. However, including motion related tasks, such as prediction and planning, impairs detection and tracking performance, a phenomenon known as negative transfer in multi-task learning. To address this issue, we propose Neural-Bayes motion decoding, a novel parallel detection, tracking, and prediction method that separates semantic and motion learning. Specifically, we employ a set of learned motion queries that operate in parallel with detection and tracking queries, sharing a unified set of recursively updated reference points. Moreover, we employ interactive semantic decoding to enhance information exchange in semantic tasks, promoting positive transfer. Experiments on the nuScenes dataset with UniAD and SparseDrive confirm the effectiveness of our divide and merge approach, resulting in performance improvements across perception, prediction, and planning. Our code is available at https://github.com/shenyinzhe/DMAD.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Noise Conditioning Necessary for Denoising Generative Models?</title>
<link>https://arxiv.org/abs/2502.13129</link>
<guid>https://arxiv.org/abs/2502.13129</guid>
<content:encoded><![CDATA[
arXiv:2502.13129v2 Announce Type: replace 
Abstract: It is widely believed that noise conditioning is indispensable for denoising diffusion models to work successfully. This work challenges this belief. Motivated by research on blind image denoising, we investigate a variety of denoising-based generative models in the absence of noise conditioning. To our surprise, most models exhibit graceful degradation, and in some cases, they even perform better without noise conditioning. We provide a theoretical analysis of the error caused by removing noise conditioning and demonstrate that our analysis aligns with empirical observations. We further introduce a noise-unconditional model that achieves a competitive FID of 2.23 on CIFAR-10, significantly narrowing the gap to leading noise-conditional models. We hope our findings will inspire the community to revisit the foundations and formulations of denoising generative models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Availability-aware Sensor Fusion via Unified Canonical Space</title>
<link>https://arxiv.org/abs/2503.07029</link>
<guid>https://arxiv.org/abs/2503.07029</guid>
<content:encoded><![CDATA[
arXiv:2503.07029v2 Announce Type: replace 
Abstract: Sensor fusion of camera, LiDAR, and 4-dimensional (4D) Radar has brought a significant performance improvement in autonomous driving. However, there still exist fundamental challenges: deeply coupled fusion methods assume continuous sensor availability, making them vulnerable to sensor degradation and failure, whereas sensor-wise cross-attention fusion methods struggle with computational cost and unified feature representation. This paper presents availability-aware sensor fusion (ASF), a novel method that employs unified canonical projection (UCP) to enable consistency in all sensor features for fusion and cross-attention across sensors along patches (CASAP) to enhance robustness of sensor fusion against sensor degradation and failure. As a result, the proposed ASF shows a superior object detection performance to the existing state-of-the-art fusion methods under various weather and sensor degradation (or failure) conditions. Extensive experiments on the K-Radar dataset demonstrate that ASF achieves improvements of 9.7% in AP BEV (87.2%) and 20.1% in AP 3D (73.6%) in object detection at IoU=0.5, while requiring a low computational cost. All codes are available at https://github.com/kaist-avelab/k-radar.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold Learning for Hyperspectral Images</title>
<link>https://arxiv.org/abs/2503.15016</link>
<guid>https://arxiv.org/abs/2503.15016</guid>
<content:encoded><![CDATA[
arXiv:2503.15016v3 Announce Type: replace 
Abstract: Traditional feature extraction and projection techniques, such as Principal Component Analysis, struggle to adequately represent X-Ray Transmission (XRT) Multi-Energy (ME) images, limiting the performance of neural networks in decision-making processes. To address this issue, we propose a method that approximates the dataset topology by constructing adjacency graphs using the Uniform Manifold Approximation and Projection. This approach captures nonlinear correlations within the data, significantly improving the performance of machine learning algorithms, particularly in processing Hyperspectral Images (HSI) from X-ray transmission spectroscopy. This technique not only preserves the global structure of the data but also enhances feature separability, leading to more accurate and robust classification results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Train Driver Performance as Key to Approval of Driverless Trains</title>
<link>https://arxiv.org/abs/2504.19735</link>
<guid>https://arxiv.org/abs/2504.19735</guid>
<content:encoded><![CDATA[
arXiv:2504.19735v3 Announce Type: replace 
Abstract: Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. The dataset with supplementing information and literature is published on https://data.fid-move.de/de/dataset/atosensedata
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdCare-VLM: Towards a Unified and Pre-aligned Latent Representation for Healthcare Video Understanding</title>
<link>https://arxiv.org/abs/2505.00275</link>
<guid>https://arxiv.org/abs/2505.00275</guid>
<content:encoded><![CDATA[
arXiv:2505.00275v2 Announce Type: replace 
Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized LLaVA-based multimodal large vision language model (LVLM) by introducing a unified visual latent space with pre-alignment to facilitate visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach</title>
<link>https://arxiv.org/abs/2505.05513</link>
<guid>https://arxiv.org/abs/2505.05513</guid>
<content:encoded><![CDATA[
arXiv:2505.05513v3 Announce Type: replace 
Abstract: Rice is an essential staple food worldwide that is important in promoting international trade, economic growth, and nutrition. Asian countries such as China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their significant contribution to the cultivation and utilization of rice. These nations are also known for cultivating different rice grains, including short and long grains. These sizes are further classified as basmati, jasmine, kainat saila, ipsala, arborio, etc., catering to diverse culinary preferences and cultural traditions. For both local and international trade, inspecting and maintaining the quality of rice grains to satisfy customers and preserve a country's reputation is necessary. Manual quality check and classification is quite a laborious and time-consuming process. It is also highly prone to mistakes. Therefore, an automatic solution must be proposed for the effective and efficient classification of different varieties of rice grains. This research paper presents an automatic framework based on a convolutional neural network (CNN) for classifying different varieties of rice grains. We evaluated the proposed model based on performance metrics such as accuracy, recall, precision, and F1-Score. The CNN model underwent rigorous training and validation, achieving a remarkable accuracy rate and a perfect area under each class's Receiver Operating Characteristic (ROC) curve. The confusion matrix analysis confirmed the model's effectiveness in distinguishing between the different rice varieties, indicating minimal misclassifications. Additionally, the integration of explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided valuable insights into the model's decision-making process, revealing how specific features of the rice grains influenced classification outcomes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logos as a Well-Tempered Pre-train for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2505.10481</link>
<guid>https://arxiv.org/abs/2505.10481</guid>
<content:encoded><![CDATA[
arXiv:2505.10481v2 Announce Type: replace 
Abstract: This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, although a certain number of datasets is available, the data for individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive available ISLR dataset by the number of signers, one of the most extensive datasets in size and vocabulary, and the largest RSL dataset. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target low-resource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models</title>
<link>https://arxiv.org/abs/2505.14454</link>
<guid>https://arxiv.org/abs/2505.14454</guid>
<content:encoded><![CDATA[
arXiv:2505.14454v2 Announce Type: replace 
Abstract: Video large language models (VideoLLM) excel at video understanding, but face efficiency challenges due to the quadratic complexity of abundant visual tokens. Our systematic analysis of token compression methods for VideoLLMs reveals two critical issues: (i) overlooking distinctive visual signals across frames, leading to information loss; (ii) suffering from implementation constraints, causing incompatibility with modern architectures or efficient operators. To address these challenges, we distill three design principles for VideoLLM token compression and propose a plug-and-play inference acceleration framework "Video Compression Commander" (VidCom2). By quantifying each frame's uniqueness, VidCom2 adaptively adjusts compression intensity across frames, effectively preserving essential information while reducing redundancy in video sequences. Extensive experiments across various VideoLLMs and benchmarks demonstrate the superior performance and efficiency of our VidCom2. With only 25% visual tokens, VidCom2 achieves 99.6% of the original performance on LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame Compression Adjustment strategy is compatible with other token compression methods to further improve their performance. Our code is available at https://github.com/xuyang-liu16/VidCom2.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Geology: Structural Geology Meets Deep Learning</title>
<link>https://arxiv.org/abs/2506.11164</link>
<guid>https://arxiv.org/abs/2506.11164</guid>
<content:encoded><![CDATA[
arXiv:2506.11164v2 Announce Type: replace 
Abstract: Reconstructing the structural geology and mineral composition of the first few kilometers of the Earth's subsurface from sparse or indirect surface observations remains a long-standing challenge with critical applications in mineral exploration, geohazard assessment, and geotechnical engineering. This inherently ill-posed problem is often addressed by classical geophysical inversion methods, which typically yield a single maximum-likelihood model that fails to capture the full range of plausible geology. The adoption of modern deep learning methods has been limited by the lack of large 3D training datasets. We address this gap with \textit{StructuralGeo}, a geological simulation engine that mimics eons of tectonic, magmatic, and sedimentary processes to generate a virtually limitless supply of realistic synthetic 3D lithological models. Using this dataset, we train both unconditional and conditional generative flow-matching models with a 3D attention U-net architecture. The resulting foundation model can reconstruct multiple plausible 3D scenarios from surface topography and sparse borehole data, depicting structures such as layers, faults, folds, and dikes. By sampling many reconstructions from the same observations, we introduce a probabilistic framework for estimating the size and extent of subsurface features. While the realism of the output is bounded by the fidelity of the training data to true geology, this combination of simulation and generative AI functions offers a flexible prior for probabilistic modeling, regional fine-tuning, and use as an AI-based regularizer in traditional geophysical inversion workflows.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branch, or Layer? Zeroth-Order Optimization for Continual Learning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.12409</link>
<guid>https://arxiv.org/abs/2506.12409</guid>
<content:encoded><![CDATA[
arXiv:2506.12409v2 Announce Type: replace 
Abstract: Vision-Language Continual Learning (VLCL) has attracted significant research attention for its robust capabilities, and the adoption of Parameter-Efficient Fine-Tuning (PEFT) strategies is enabling these models to achieve competitive performance with substantially reduced resource consumption. However, dominated First-Order (FO) optimization is prone to trap models in suboptimal local minima, especially in limited exploration subspace within PEFT. To overcome this challenge, this paper pioneers a systematic exploration of adopting Zeroth-Order (ZO) optimization for PEFT-based VLCL. We first identify the incompatibility of naive full-ZO adoption in VLCL due to optimization process instability. We then investigate the application of ZO optimization from a modality branch-wise to a fine-grained layer-wise across various training units to identify an optimal strategy. Besides, a key theoretical insight reveals that vision modality exhibit higher variance than language counterparts in VLCL during the ZO optimization process, and we propose a modality-aware ZO strategy, which adopts gradient sign normalization in ZO and constrains vision modality perturbation to further improve performance. Benefiting from the adoption of ZO optimization, PEFT-based VLCL fulfills better ability to escape local minima during the optimization process, extensive experiments on four benchmarks demonstrate that our method achieves state-of-the-art results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RelTopo: Multi-Level Relational Modeling for Driving Scene Topology Reasoning</title>
<link>https://arxiv.org/abs/2506.13553</link>
<guid>https://arxiv.org/abs/2506.13553</guid>
<content:encoded><![CDATA[
arXiv:2506.13553v3 Announce Type: replace 
Abstract: Accurate road topology reasoning is critical for autonomous driving, as it requires both perceiving road elements and understanding how lanes connect to each other (L2L) and to traffic elements (L2T). Existing methods often focus on either perception or L2L reasoning, leaving L2T underexplored and fall short of jointly optimizing perception and reasoning. Moreover, although topology prediction inherently involves relations, relational modeling itself is seldom incorporated into feature extraction or supervision. As humans naturally leverage contextual relationships to recognize road element and infer their connectivity, we posit that relational modeling can likewise benefit both perception and reasoning, and that these two tasks should be mutually enhancing. To this end, we propose RelTopo, a multi-level relational modeling approach that systematically integrates relational cues across three levels: 1) perception-level: a relation-aware lane detector with geometry-biased self-attention and curve-guided cross-attention enriches lane representations; 2) reasoning-level: relation-enhanced topology heads, including a geometry-enhanced L2L head and a cross-view L2T head, enhance topology inference via relational cues; and 3) supervision-level: a contrastive InfoNCE strategy regularizes relational embeddings. This design enables perception and reasoning to be optimized jointly. Extensive experiments on OpenLane-V2 demonstrate that RelTopo significantly improves both detection and topology reasoning, with gains of +3.1 in DET$_l$, +5.3 in TOP$_{ll}$, +4.9 in TOP$_{lt}$, and +4.4 overall in OLS, setting a new state-of-the-art. Code will be released.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration</title>
<link>https://arxiv.org/abs/2506.22242</link>
<guid>https://arxiv.org/abs/2506.22242</guid>
<content:encoded><![CDATA[
arXiv:2506.22242v2 Announce Type: replace 
Abstract: Leveraging diverse robotic data for pretraining remains a critical challenge. Existing methods typically model the dataset's action distribution using simple observations as inputs. However, these inputs are often incomplete, resulting in a dispersed conditional action distribution-an issue we refer to as coordinate system chaos and state chaos. This inconsistency significantly hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel approach that effectively integrates 4D information into the input to mitigate these sources of chaos. Our model introduces depth and temporal information into visual features with sequential RGB-D inputs, aligning the coordinate systems of the robot and the scene. This alignment endows the model with strong spatiotemporal reasoning capabilities while minimizing training overhead. Additionally, we introduce memory bank sampling, a frame sampling strategy designed to extract informative frames from historical images, further improving effectiveness and efficiency. Experimental results demonstrate that our pretraining method and architectural components substantially enhance model performance. In both simulated and real-world experiments, our model achieves a significant increase in success rate over OpenVLA. To further assess spatial perception and generalization to novel views, we introduce MV-Bench, a multi-view simulation benchmark. Our model consistently outperforms existing methods, demonstrating stronger spatial understanding and adaptability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.23982</link>
<guid>https://arxiv.org/abs/2506.23982</guid>
<content:encoded><![CDATA[
arXiv:2506.23982v3 Announce Type: replace 
Abstract: Personalization, while extensively studied in conventional autonomous driving pipelines, has been largely overlooked in the context of end-to-end autonomous driving (E2EAD), despite its critical role in fostering user trust, safety perception, and real-world adoption. A primary bottleneck is the absence of large-scale real-world datasets that systematically capture driving preferences, severely limiting the development and evaluation of personalized E2EAD models. In this work, we introduce the first large-scale real-world dataset explicitly curated for personalized E2EAD, integrating comprehensive scene topology with rich dynamic context derived from agent dynamics and semantics inferred via a fine-tuned vision-language model (VLM). We propose a hybrid annotation pipeline that combines behavioral analysis, rule-and-distribution-based heuristics, and subjective semantic modeling guided by VLM reasoning, with final refinement through human-in-the-loop verification. Building upon this dataset, we introduce the first standardized benchmark for systematically evaluating personalized E2EAD models. Empirical evaluations on state-of-the-art architectures demonstrate that incorporating personalized driving preferences significantly improves behavioral alignment with human demonstrations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UVLM: Benchmarking Video Language Model for Underwater World Understanding</title>
<link>https://arxiv.org/abs/2507.02373</link>
<guid>https://arxiv.org/abs/2507.02373</guid>
<content:encoded><![CDATA[
arXiv:2507.02373v2 Announce Type: replace 
Abstract: Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning few-step posterior samplers by unfolding and distillation of diffusion models</title>
<link>https://arxiv.org/abs/2507.02686</link>
<guid>https://arxiv.org/abs/2507.02686</guid>
<content:encoded><![CDATA[
arXiv:2507.02686v2 Announce Type: replace 
Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed</title>
<link>https://arxiv.org/abs/2507.03779</link>
<guid>https://arxiv.org/abs/2507.03779</guid>
<content:encoded><![CDATA[
arXiv:2507.03779v2 Announce Type: replace 
Abstract: Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at https://github.com/KevinZ0217/fast_dinov2
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation</title>
<link>https://arxiv.org/abs/2507.15243</link>
<guid>https://arxiv.org/abs/2507.15243</guid>
<content:encoded><![CDATA[
arXiv:2507.15243v2 Announce Type: replace 
Abstract: Despite the progress in cross-domain few-shot learning, a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, coalescent projection, as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method, combined with self-supervised transformations, that relies solely on the base domain to prepare the network to encounter unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain-shift problem of the BSCD-FSL benchmark. Our code is published at \href{https://github.com/Naeem-Paeedeh/CPLSR}{https://github.com/Naeem-Paeedeh/CPLSR}.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Promise of RL for Autoregressive Image Editing</title>
<link>https://arxiv.org/abs/2508.01119</link>
<guid>https://arxiv.org/abs/2508.01119</guid>
<content:encoded><![CDATA[
arXiv:2508.01119v3 Announce Type: replace 
Abstract: While image generation techniques are now capable of producing high-quality images that respect prompts which span multiple sentences, the task of text-guided image editing remains a challenge. Even edit requests that consist of only a few words often fail to be executed correctly. We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2508.01293</link>
<guid>https://arxiv.org/abs/2508.01293</guid>
<content:encoded><![CDATA[
arXiv:2508.01293v2 Announce Type: replace 
Abstract: Multiple Instance Learning (MIL) is the leading approach for whole slide image (WSI) classification, enabling efficient analysis of gigapixel pathology slides. Recent work has introduced vision-language models (VLMs) into MIL pipelines to incorporate medical knowledge through text-based class descriptions rather than simple class names. However, when these methods rely on large language models (LLMs) to generate clinical descriptions or use fixed-length prompts to represent complex pathology concepts, the limited token capacity of VLMs often constrains the expressiveness and richness of the encoded class information. Additionally, descriptions generated solely by LLMs may lack domain grounding and fine-grained medical specificity, leading to suboptimal alignment with visual features. To address these challenges, we propose a vision-language MIL framework with two key contributions: (1) A grounded multi-agent description generation system that leverages curated pathology textbooks and agent specialization (e.g., morphology, spatial context) to produce accurate and diverse clinical descriptions; (2) A text encoding strategy using a list of descriptions rather than a single prompt, capturing fine-grained and complementary clinical signals for better alignment with visual features. Integrated into a VLM-MIL pipeline, our approach shows improved performance over single-prompt class baselines and achieves results comparable to state-of-the-art models, as demonstrated on renal and lung cancer datasets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval</title>
<link>https://arxiv.org/abs/2508.01711</link>
<guid>https://arxiv.org/abs/2508.01711</guid>
<content:encoded><![CDATA[
arXiv:2508.01711v2 Announce Type: replace 
Abstract: Text-to-video retrieval requires precise alignment between language and temporally rich audio-video signals. However, existing methods often emphasize visual cues while underutilizing audio semantics or relying on coarse fusion strategies, resulting in suboptimal multimodal representations. We introduce GAIS, a retrieval framework that strengthens multimodal alignment from both representation and regularization perspectives. First, a Frame-level Gated Fusion (FGF) module adaptively integrates audio-visual features under textual guidance, enabling fine-grained temporal selection of informative frames. Second, a Semantic Variance-Scaled Perturbation (SVSP) mechanism regularizes the text embedding space by controlling perturbation magnitude in a semantics-aware manner. These two modules are complementary: FGF minimizes modality gaps through selective fusion, while SVSP improves embedding stability and discrimination. Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX demonstrate that GAIS consistently outperforms strong baselines across multiple retrieval metrics while maintaining notable computational efficiency.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image Compression</title>
<link>https://arxiv.org/abs/2508.02051</link>
<guid>https://arxiv.org/abs/2508.02051</guid>
<content:encoded><![CDATA[
arXiv:2508.02051v2 Announce Type: replace 
Abstract: Distributed multi-stage image compression -- where visual content traverses multiple processing nodes under varying quality requirements -- poses challenges. Progressive methods enable bitstream truncation but underutilize available compute resources; successive compression repeats costly pixel-domain operations and suffers cumulative quality loss and inefficiency; fixed-parameter models lack post-encoding flexibility. In this work, we developed the Hierarchical Cascade Framework (HCF) that achieves high rate-distortion performance and better computational efficiency through direct latent-space transformations across network nodes in distributed multi-stage image compression systems. Under HCF, we introduced policy-driven quantization control to optimize rate-distortion trade-offs, and established the edge quantization principle through differential entropy analysis. The configuration based on this principle demonstrates up to 0.6dB PSNR gains over other configurations. When comprehensively evaluated on the Kodak, CLIC, and CLIC2020-mobile datasets, HCF outperforms successive-compression methods by up to 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU memory, and 90.0% execution time. It also outperforms state-of-the-art progressive compression methods by up to 12.64% BD-Rate on Kodak and enables retraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on CLIC2020-mobile.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlotMatch: Distilling Object-Centric Representations for Unsupervised Video Segmentation</title>
<link>https://arxiv.org/abs/2508.03411</link>
<guid>https://arxiv.org/abs/2508.03411</guid>
<content:encoded><![CDATA[
arXiv:2508.03411v3 Announce Type: replace 
Abstract: Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on three datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running up to 2.7x faster. Moreover, our student surpasses all other state-of-the-art unsupervised video segmentation models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions</title>
<link>https://arxiv.org/abs/2508.05430</link>
<guid>https://arxiv.org/abs/2508.05430</guid>
<content:encoded><![CDATA[
arXiv:2508.05430v2 Announce Type: replace 
Abstract: Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, such as the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on the MS COCO and ImageNet-1k benchmarks validate that second-order methods, such as FIxLIP, outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models, e.g. CLIP vs. SigLIP-2.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMOL-MapSeg: Show Me One Label as prompt</title>
<link>https://arxiv.org/abs/2508.05501</link>
<guid>https://arxiv.org/abs/2508.05501</guid>
<content:encoded><![CDATA[
arXiv:2508.05501v2 Announce Type: replace 
Abstract: Historical maps offer valuable insights into changes on Earth's surface but pose challenges for modern segmentation models due to inconsistent visual styles and symbols. While deep learning models such as UNet and pre-trained foundation models perform well in domains like autonomous driving and medical imaging, they struggle with the variability of historical maps, where similar concepts appear in diverse forms. To address this issue, we propose On-Need Declarative (OND) knowledge-based prompting, a method that provides explicit image-label pair prompts to guide models in linking visual patterns with semantic concepts. This enables users to define and segment target concepts on demand, supporting flexible, concept-aware segmentation. Our approach replaces the prompt encoder of the Segment Anything Model (SAM) with the OND prompting mechanism and fine-tunes it on historical maps, creating SMOL-MapSeg (Show Me One Label). Unlike existing SAM-based fine-tuning methods that are class-agnostic or restricted to fixed classes, SMOL-MapSeg supports class-aware segmentation across arbitrary datasets. Experiments show that SMOL-MapSeg accurately segments user-defined classes and substantially outperforms baseline models. Furthermore, it demonstrates strong generalization even with minimal training data, highlighting its potential for scalable and adaptable historical map analysis.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset</title>
<link>https://arxiv.org/abs/2508.06537</link>
<guid>https://arxiv.org/abs/2508.06537</guid>
<content:encoded><![CDATA[
arXiv:2508.06537v2 Announce Type: replace 
Abstract: Object detection models are typically trained on datasets like ImageNet, COCO, and PASCAL VOC, which focus on everyday objects. However, these lack signal sparsity found in non-commercial domains. MobilTelesco, a smartphone-based astrophotography dataset, addresses this by providing sparse night-sky images. We benchmark several detection models on it, highlighting challenges under feature-deficient conditions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Understanding 3D Vision: the Role of Gaussian Curvature</title>
<link>https://arxiv.org/abs/2508.11825</link>
<guid>https://arxiv.org/abs/2508.11825</guid>
<content:encoded><![CDATA[
arXiv:2508.11825v2 Announce Type: replace 
Abstract: Recent advances in computer vision have predominantly relied on data-driven approaches that leverage deep learning and large-scale datasets. Deep neural networks have achieved remarkable success in tasks such as stereo matching and monocular depth reconstruction. However, these methods lack explicit models of 3D geometry that can be directly analyzed, transferred across modalities, or systematically modified for controlled experimentation. We investigate the role of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being an invariant quantity under change of observers or coordinate systems, we demonstrate using the Middlebury stereo dataset that it offers a sparse and compact description of 3D surfaces. Furthermore, we show a strong correlation between the performance rank of top state-of-the-art stereo and monocular methods and the low total absolute Gaussian curvature. We propose that this property can serve as a geometric prior to improve future 3D reconstruction algorithms.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
<link>https://arxiv.org/abs/2508.11999</link>
<guid>https://arxiv.org/abs/2508.11999</guid>
<content:encoded><![CDATA[
arXiv:2508.11999v2 Announce Type: replace 
Abstract: With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Governance-Ready Small Language Models for Medical Imaging: Prompting, Abstention, and PACS Integration</title>
<link>https://arxiv.org/abs/2508.13378</link>
<guid>https://arxiv.org/abs/2508.13378</guid>
<content:encoded><![CDATA[
arXiv:2508.13378v3 Announce Type: replace 
Abstract: Small Language Models (SLMs) are a practical option for narrow, workflow-relevant medical imaging utilities where privacy, latency, and cost dominate. We present a governance-ready recipe that combines prompt scaffolds, calibrated abstention, and standards-compliant integration into Picture Archiving and Communication Systems (PACS). Our focus is the assistive task of AP/PA view tagging for chest radiographs. Using four deployable SLMs (Qwen2.5-VL, MiniCPM-V, Gemma 7B, LLaVA 7B) on NIH Chest X-ray, we provide illustrative evidence: reflection-oriented prompts benefit lighter models, whereas stronger baselines are less sensitive. Beyond accuracy, we operationalize abstention, expected calibration error, and oversight burden, and we map outputs to DICOM tags, HL7 v2 messages, and FHIR ImagingStudy. The contribution is a prompt-first deployment framework, an operations playbook for calibration, logging, and change management, and a clear pathway from pilot utilities to reader studies without over-claiming clinical validation. We additionally specify a human-factors RACI, stratified calibration for dataset shift, and an auditable evidence pack to support local governance reviews.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</title>
<link>https://arxiv.org/abs/2508.14153</link>
<guid>https://arxiv.org/abs/2508.14153</guid>
<content:encoded><![CDATA[
arXiv:2508.14153v2 Announce Type: replace 
Abstract: Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning significantly enhances text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models (SAM). Code is available at https://github.com/hustvl/LENS.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RynnEC: Bringing MLLMs into Embodied World</title>
<link>https://arxiv.org/abs/2508.14160</link>
<guid>https://arxiv.org/abs/2508.14160</guid>
<content:encoded><![CDATA[
arXiv:2508.14160v2 Announce Type: replace 
Abstract: We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data</title>
<link>https://arxiv.org/abs/2509.08571</link>
<guid>https://arxiv.org/abs/2509.08571</guid>
<content:encoded><![CDATA[
arXiv:2509.08571v2 Announce Type: replace 
Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level projections, but radar observations are sparse and uneven. We introduce GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built from surface observables (elevation, velocity, mass balance) are augmented with gradient features and polynomial trends to capture both local variability and broad structure. To handle data gaps, we employ a hybrid loss that combines confidence-weighted radar supervision with dynamically balanced regularization. Applied to three Greenland subregions, GraphTopoNet outperforms interpolation, convolutional, and graph-based baselines, reducing error by up to 60 percent while preserving fine-scale glacial features. The resulting bed maps improve reliability for operational modeling, supporting agencies engaged in climate forecasting and policy. More broadly, GraphTopoNet shows how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Wise Correspondence Prediction between Manga Line Art Images</title>
<link>https://arxiv.org/abs/2509.09501</link>
<guid>https://arxiv.org/abs/2509.09501</guid>
<content:encoded><![CDATA[
arXiv:2509.09501v3 Announce Type: replace 
Abstract: Understanding region-wise correspondences between manga line art images is fundamental for high-level manga processing, supporting downstream tasks such as line art colorization and in-between frame generation. Unlike natural images that contain rich visual cues, manga line art consists only of sparse black-and-white strokes, making it challenging to determine which regions correspond across images. In this work, we introduce a new task: predicting region-wise correspondence between raw manga line art images without any annotations. To address this problem, we propose a Transformer-based framework trained on large-scale, automatically generated region correspondences. The model learns to suppress noisy matches and strengthen consistent structural relationships, resulting in robust patch-level feature alignment within and across images. During inference, our method segments each line art and establishes coherent region-level correspondences through edge-aware clustering and region matching. We construct manually annotated benchmarks for evaluation, and experiments across multiple datasets demonstrate both high patch-level accuracy and strong region-level correspondence performance, achieving 78.4-84.4% region-level accuracy. These results highlight the potential of our method for real-world manga and animation applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition</title>
<link>https://arxiv.org/abs/2509.10815</link>
<guid>https://arxiv.org/abs/2509.10815</guid>
<content:encoded><![CDATA[
arXiv:2509.10815v3 Announce Type: replace 
Abstract: Previous work has made use of a parameterized plane curve polynomial representation for mathematical handwriting, with the polynomials represented in a Legendre or Legendre-Sobolev graded basis. This provides a compact geometric representation for the digital ink. Preliminary results have also been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the trade-offs between basis choice and polynomial degree to achieve accurate modeling with a low computational cost. To do this, we consider the condition number for polynomial evaluation in these bases and bound how the various inner products give norms for the variations between symbols.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11853</link>
<guid>https://arxiv.org/abs/2509.11853</guid>
<content:encoded><![CDATA[
arXiv:2509.11853v2 Announce Type: replace 
Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Sharper Object Boundaries in Self-Supervised Depth Estimation</title>
<link>https://arxiv.org/abs/2509.15987</link>
<guid>https://arxiv.org/abs/2509.15987</guid>
<content:encoded><![CDATA[
arXiv:2509.15987v2 Announce Type: replace 
Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</title>
<link>https://arxiv.org/abs/2509.17707</link>
<guid>https://arxiv.org/abs/2509.17707</guid>
<content:encoded><![CDATA[
arXiv:2509.17707v2 Announce Type: replace 
Abstract: Background: The standardisation of Intermodal Loading Units (ILUs), including containers, semi-trailers, and swap bodies, has transformed global trade, yet efficient and robust identification remains an operational bottleneck in ports and terminals. Objective: To map Computer Vision (CV) methods for ILU identification, clarify terminology, summarise the evolution of proposed approaches, and highlight research gaps, future directions and their potential effects on terminal operations. Methods: Following PRISMA-ScR, we searched Google Scholar and dblp for English-language studies with quantitative results. After dual reviewer screening, the studies were charted across methods, datasets, and evaluation metrics. Results: 63 empirical studies on CV-based solutions for the ILU identification task, published between 1990 and 2025 were reviewed. Methodological evolution of ILU identification solutions, datasets, evaluation of the proposed methods and future research directions are summarised. A shift from static (e.g. OCR-gates) to vehicle mounted camera setups, which enables precise monitoring is observed. The reported results for end-to-end accuracy range from 5% to 96%. Conclusions: We propose standardised terminology, advocate for open-access datasets, codebases and model weights to enable fair evaluation and define future work directions. The shift from static to dynamic camera settings introduces new challenges that have transformative potential for transportation and logistics. However, the lack of public benchmark datasets, open-access code, and standardised terminology hinders the advancements in this field. As for the future work, we suggest addressing the new challenges emerged from vehicle mounted cameras, exploring synthetic data generation, refining the multi-stage methods into unified end-to-end models to reduce complexity, and focusing on contextless text recognition.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference</title>
<link>https://arxiv.org/abs/2509.19082</link>
<guid>https://arxiv.org/abs/2509.19082</guid>
<content:encoded><![CDATA[
arXiv:2509.19082v2 Announce Type: replace 
Abstract: Sa2VA is a recent model for language-guided dense grounding in images and video that achieves state-of-the-art results on multiple segmentation benchmarks and that has become widely popular. However, we found that Sa2VA does not perform according to its full potential for referring video object segmentation tasks. We identify inconsistencies between training and inference procedures as the key factor holding it back. To mitigate this issue, we propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and improves the results. In fact, Sa2VA-i sets a new state of the art for multiple video benchmarks and achieves improvements of up to +11.6 J&amp;F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the original Sa2VA-26B model on the MeViS benchmark. We hope that this work will show the importance of seemingly trivial implementation details and that it will provide valuable insights for the referring video segmentation field. We provide the code and updated models at https://github.com/kumuji/sa2va-i
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rasterized Steered Mixture of Experts for Efficient 2D Image Regression</title>
<link>https://arxiv.org/abs/2510.05814</link>
<guid>https://arxiv.org/abs/2510.05814</guid>
<content:encoded><![CDATA[
arXiv:2510.05814v2 Announce Type: replace 
Abstract: The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning for Image Captioning through Improved Image-Text Alignment</title>
<link>https://arxiv.org/abs/2510.06009</link>
<guid>https://arxiv.org/abs/2510.06009</guid>
<content:encoded><![CDATA[
arXiv:2510.06009v2 Announce Type: replace 
Abstract: Generating accurate and coherent image captions in a continual learning setting remains a major challenge due to catastrophic forgetting and the difficulty of aligning evolving visual concepts with language over time. In this work, we propose a novel multi-loss framework for continual image captioning that integrates semantic guidance through prompt-based continual learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone, our approach combines standard cross-entropy loss with three additional components: (1) a prompt-based cosine similarity loss that aligns image embeddings with synthetically constructed prompts encoding objects, attributes, and actions; (2) a CLIP-style loss that promotes alignment between image embeddings and target caption embedding; and (3) a language-guided contrastive loss that employs a triplet loss to enhance class-level discriminability between tasks. Notably, our approach introduces no additional overhead at inference time and requires no prompts during caption generation. We find that this approach mitigates catastrophic forgetting, while achieving better semantic caption alignment compared to state-of-the-art methods. The code can be found via the following link: https://github.com/Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</title>
<link>https://arxiv.org/abs/2510.12089</link>
<guid>https://arxiv.org/abs/2510.12089</guid>
<content:encoded><![CDATA[
arXiv:2510.12089v2 Announce Type: replace 
Abstract: Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN</title>
<link>https://arxiv.org/abs/2510.13137</link>
<guid>https://arxiv.org/abs/2510.13137</guid>
<content:encoded><![CDATA[
arXiv:2510.13137v2 Announce Type: replace 
Abstract: This study investigates the performance of 3D Convolutional Neural Networks (3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences, LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy, computational efficiency, and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2% more processing time per frame compared to LSTMs, which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNNLSTM model shows decent performance, which suggests that context-dependent architecture selection is crucial for practical implementation.This project provides professional benchmarks for developing assistive technologies, highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.13675</link>
<guid>https://arxiv.org/abs/2510.13675</guid>
<content:encoded><![CDATA[
arXiv:2510.13675v2 Announce Type: replace 
Abstract: Open-domain visual entity recognition aims to identify and link entities depicted in images to a vast and evolving set of real-world concepts, such as those found in Wikidata. Unlike conventional classification tasks with fixed label sets, it operates under open-set conditions, where most target entities are unseen during training and exhibit long-tail distributions. This makes the task inherently challenging due to limited supervision, high visual ambiguity, and the need for semantic disambiguation. We propose a Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both images and text descriptions into a shared semantic space grounded by structured information from Wikidata. By abstracting visual and textual inputs to a conceptual level, the model leverages entity descriptions, type hierarchies, and relational context to support zero-shot entity recognition. We evaluate our approach on the OVEN benchmark, a large-scale open-domain visual recognition dataset with Wikidata IDs as the label space. Our experiments show that using visual, textual, and structured knowledge greatly improves accuracy, especially for rare and unseen entities. Our smallest model improves the accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite being 35 times smaller.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Human Evaluations in Gesture Generation: Insights from a Community-Driven State-of-the-Art Benchmark</title>
<link>https://arxiv.org/abs/2511.01233</link>
<guid>https://arxiv.org/abs/2511.01233</guid>
<content:encoded><![CDATA[
arXiv:2511.01233v2 Announce Type: replace 
Abstract: We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models -- each trained by its original authors -- across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies -- enabling new evaluations without model reimplementation required -- alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[
arXiv:2511.03132v2 Announce Type: replace 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior</title>
<link>https://arxiv.org/abs/2511.05403</link>
<guid>https://arxiv.org/abs/2511.05403</guid>
<content:encoded><![CDATA[
arXiv:2511.05403v2 Announce Type: replace 
Abstract: The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps With Sub-Meter Imagery</title>
<link>https://arxiv.org/abs/2511.07231</link>
<guid>https://arxiv.org/abs/2511.07231</guid>
<content:encoded><![CDATA[
arXiv:2511.07231v3 Announce Type: replace 
Abstract: Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant neural networks and equivarification</title>
<link>https://arxiv.org/abs/1906.07172</link>
<guid>https://arxiv.org/abs/1906.07172</guid>
<content:encoded><![CDATA[
arXiv:1906.07172v5 Announce Type: replace-cross 
Abstract: Equivariant neural networks are a class of neural networks designed to preserve symmetries inherent in the data. In this paper, we introduce a general method for modifying a neural network to enforce equivariance, a process we refer to as equivarification. We further show that group convolutional neural networks (G-CNNs) arise as a special case of our framework.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Sample Complexity Bounds for Diffusion Model Training</title>
<link>https://arxiv.org/abs/2311.13745</link>
<guid>https://arxiv.org/abs/2311.13745</guid>
<content:encoded><![CDATA[
arXiv:2311.13745v4 Announce Type: replace-cross 
Abstract: Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the sample complexity of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an exponential improvement in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SF-Loc: A Visual Mapping and Geo-Localization System based on Sparse Visual Structure Frames</title>
<link>https://arxiv.org/abs/2412.01500</link>
<guid>https://arxiv.org/abs/2412.01500</guid>
<content:encoded><![CDATA[
arXiv:2412.01500v3 Announce Type: replace-cross 
Abstract: For high-level geo-spatial applications and intelligent robotics, accurate global pose information is of crucial importance. Map-aided localization is a universal approach to overcome the limitations of global navigation satellite system (GNSS) in challenging environments. However, current solutions face challenges in terms of mapping flexibility, storage burden and re-localization performance. In this work, we present SF-Loc, a lightweight visual mapping and map-aided localization system, whose core idea is the map representation based on sparse frames with dense but compact depth, termed as visual structure frames. In the mapping phase, multi-sensor dense bundle adjustment (MS-DBA) is applied to construct geo-referenced visual structure frames. The local co-visbility is checked to keep the map sparsity and achieve incremental mapping. In the localization phase, coarse-to-fine vision-based localization is performed, in which multi-frame information and the map distribution are fully integrated. To be specific, the concept of spatially smoothed similarity (SSS) is proposed to overcome the place ambiguity, and pairwise frame matching is applied for efficient and robust pose estimation. Experimental results on the cross-season dataset verify the effectiveness of the system. In complex urban road scenarios, the map size is down to 3 MB per kilometer and stable decimeter-level re-localization can be achieved. The code will be made open-source soon (https://github.com/GREAT-WHU/SF-Loc).
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Explainability for Weakly Supervised Segmentation in Medical PE Detection</title>
<link>https://arxiv.org/abs/2412.07384</link>
<guid>https://arxiv.org/abs/2412.07384</guid>
<content:encoded><![CDATA[
arXiv:2412.07384v2 Announce Type: replace-cross 
Abstract: Pulmonary Embolism (PE) are a leading cause of cardiovascular death. Computed tomographic pulmonary angiography (CTPA) is the gold standard for PE diagnosis, with growing interest in AI-based diagnostic assistance. However, these algorithms are limited by scarce fine-grained annotations of thromboembolic burden. We address this challenge with iExplain, a weakly supervised learning algorithm that transforms coarse image-level annotations into detailed pixel-level PE masks through iterative model explainability. Our approach generates soft segmentation maps used to mask detected regions, enabling the process to repeat and discover additional embolisms that would be missed in a single pass. This iterative refinement effectively captures complete PE regions and detects multiple distinct embolisms. Models trained on these automatically generated annotations achieve excellent PE detection performance, with significant improvements at each iteration. We demonstrate iExplain's effectiveness on the RSPECT augmented dataset, achieving results comparable to strongly supervised methods while outperforming existing weakly supervised methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching</title>
<link>https://arxiv.org/abs/2412.18911</link>
<guid>https://arxiv.org/abs/2412.18911</guid>
<content:encoded><![CDATA[
arXiv:2412.18911v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, aiming to skip the computation for unimportant tokens while still computing the important ones. In this paper, we propose to carefully check the effectiveness in token-wise feature caching with the following two questions: (1) Is it really necessary to compute the so-called "important" tokens in each step? (2) Are so-called important tokens really important? Surprisingly, this paper gives some counter-intuition answers, demonstrating that consistently computing the selected ``important tokens'' in all steps is not necessary. The selection of the so-called ``important tokens'' is often ineffective, and even sometimes shows inferior performance than random selection. Based on these observations, this paper introduces dual feature caching referred to as DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively and selects the tokens for computing randomly. Extensive experimental results demonstrate the effectiveness of our method in DiT, PixArt, FLUX, and OpenSora, demonstrating significant improvements than the previous token-wise feature caching.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model</title>
<link>https://arxiv.org/abs/2506.01196</link>
<guid>https://arxiv.org/abs/2506.01196</guid>
<content:encoded><![CDATA[
arXiv:2506.01196v2 Announce Type: replace-cross 
Abstract: We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeeDe3DGS: Speedy Deformable 3D Gaussian Splatting with Temporal Pruning and Motion Grouping</title>
<link>https://arxiv.org/abs/2506.07917</link>
<guid>https://arxiv.org/abs/2506.07917</guid>
<content:encoded><![CDATA[
arXiv:2506.07917v2 Announce Type: replace-cross 
Abstract: Dynamic extensions of 3D Gaussian Splatting (3DGS) achieve high-quality reconstructions through neural motion fields, but per-Gaussian neural inference makes these models computationally expensive. Building on DeformableGS, we introduce Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), which bridges this efficiency-fidelity gap through three complementary modules: Temporal Sensitivity Pruning (TSP) removes low-impact Gaussians via temporally aggregated sensitivity analysis, Temporal Sensitivity Sampling (TSS) perturbs timestamps to suppress floaters and improve temporal coherence, and GroupFlow distills the learned deformation field into shared SE(3) transformations for efficient groupwise motion. On the 50 dynamic scenes in MonoDyGauBench, integrating TSP and TSS into DeformableGS accelerates rendering by 6.78$\times$ on average while maintaining neural-field fidelity and using 10$\times$ fewer primitives. Adding GroupFlow culminates in 13.71$\times$ faster rendering and 2.53$\times$ shorter training, surpassing all baselines in speed while preserving superior image quality.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Medical Imaging: A Review and Outlook</title>
<link>https://arxiv.org/abs/2506.09095</link>
<guid>https://arxiv.org/abs/2506.09095</guid>
<content:encoded><![CDATA[
arXiv:2506.09095v4 Announce Type: replace-cross 
Abstract: Foundation models (FMs) are changing the way medical images are analyzed by learning from large collections of unlabeled data. Instead of relying on manually annotated examples, FMs are pre-trained to learn general-purpose visual features that can later be adapted to specific clinical tasks with little additional supervision. In this review, we examine how FMs are being developed and applied in pathology, radiology, and ophthalmology, drawing on evidence from over 150 studies. We explain the core components of FM pipelines, including model architectures, self-supervised learning methods, and strategies for downstream adaptation. We also review how FMs are being used in each imaging domain and compare design choices across applications. Finally, we discuss key challenges and open questions to guide future research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODE$_t$(ODE$_l$): Shortcutting the Time and the Length in Diffusion and Flow Models for Faster Sampling</title>
<link>https://arxiv.org/abs/2506.21714</link>
<guid>https://arxiv.org/abs/2506.21714</guid>
<content:encoded><![CDATA[
arXiv:2506.21714v3 Announce Type: replace-cross 
Abstract: Continuous normalizing flows (CNFs) and diffusion models (DMs) generate high-quality data from a noise distribution. However, their sampling process demands multiple iterations to solve an ordinary differential equation (ODE) with high computational complexity. State-of-the-art methods focus on reducing the number of discrete time steps during sampling to improve efficiency. In this work, we explore a complementary direction in which the quality-complexity tradeoff can also be controlled in terms of the neural network length. We achieve this by rewiring the blocks in the transformer-based architecture to solve an inner discretized ODE w.r.t. its depth. Then, we apply a length consistency term during flow matching training, and as a result, the sampling can be performed with an arbitrary number of time steps and transformer blocks. Unlike others, our ODE$_t$(ODE$_l$) approach is solver-agnostic in time dimension and reduces both latency and, importantly, memory usage. CelebA-HQ and ImageNet generation experiments show a latency reduction of up to $2\times$ in the most efficient sampling mode, and FID improvement of up to $2.8$ points for high-quality sampling when applied to prior methods. We open-source our code and checkpoints at github.com/gudovskiy/odelt.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent</title>
<link>https://arxiv.org/abs/2507.11461</link>
<guid>https://arxiv.org/abs/2507.11461</guid>
<content:encoded><![CDATA[
arXiv:2507.11461v2 Announce Type: replace-cross 
Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed points, which have recently gained attention for learning image regularization functionals, particularly in settings involving Gaussian fidelities, where assumptions on the forward operator ensure contractiveness of standard (proximal) Gradient Descent operators. In this work, we extend the application of DEQs to Poisson inverse problems, where the data fidelity term is more appropriately modeled by the Kullback--Leibler divergence. To this end, we introduce a novel DEQ formulation based on Mirror Descent defined in terms of a tailored non-Euclidean geometry that naturally adapts with the structure of the data term. This enables the learning of neural regularizers within a principled training framework. We derive sufficient conditions and establish refined convergence results based on the Kurdyka--Lojasiewicz framework for subanalytic functions with non-closed domains to guarantee the convergence of the learned reconstruction scheme and propose computational strategies that enable both efficient training and parameter-free inference. Numerical experiments show that our method outperforms traditional model-based approaches and it is comparable to the performance of Bregman Plug-and-Play methods, while mitigating their typical drawbacks, such as time-consuming tuning of hyper-parameters. The code is publicly available at https://github.com/christiandaniele/DEQ-MD.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification</title>
<link>https://arxiv.org/abs/2507.15487</link>
<guid>https://arxiv.org/abs/2507.15487</guid>
<content:encoded><![CDATA[
arXiv:2507.15487v3 Announce Type: replace-cross 
Abstract: Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency domain information, which is crucial for accurate lesion classification in medical imaging. However, effectively integrating multi-sequence MRI data for robust 3D lesion classification remains a challenge. In this paper, we propose DeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel framework designed to extract decoupled representations and adaptively fuse spatial and spectral features for lesion classification. DeSamba introduces a Decoupled Representation Learning Module (DRLM) that decouples features from different MRI sequences through self-reconstruction and cross-reconstruction, and a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet, enabling dynamic fusion of spectral and spatial information based on lesion characteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On a six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1 accuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external validation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On a spondylitis dataset (n=251) involving a challenging binary classification task, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal and external validation sets, respectively. Ablation studies demonstrate that both DRLM and SAMB significantly contribute to overall performance, with over 10% relative improvement compared to the baseline. Our results highlight the potential of DeSamba as a generalizable and effective solution for 3D lesion classification in multi-sequence medical imaging.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
arXiv:2507.21503v2 Announce Type: replace-cross 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Squeezed Diffusion Models</title>
<link>https://arxiv.org/abs/2508.14871</link>
<guid>https://arxiv.org/abs/2508.14871</guid>
<content:encoded><![CDATA[
arXiv:2508.14871v2 Announce Type: replace-cross 
Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning</title>
<link>https://arxiv.org/abs/2508.15874</link>
<guid>https://arxiv.org/abs/2508.15874</guid>
<content:encoded><![CDATA[
arXiv:2508.15874v2 Announce Type: replace-cross 
Abstract: Vision-centric hierarchical embodied models have demonstrated strong potential. However, existing methods lack spatial awareness capabilities, limiting their effectiveness in bridging visual plans to actionable control in complex environments. To address this problem, we propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic manipulation framework via explicit spatial modeling and reasoning. Specifically, we first design a spatial-conditioned embodied video generation module to model spatially guided predictions through the spatial plan table. Then, we propose a flow-based action prediction module to infer executable actions with coordination. Finally, we propose a spatial reasoning feedback policy to refine the spatial plan table via dual-stage replanning. Extensive experiments show that SP substantially outperforms state-of-the-art baselines, achieving over 33% improvement on Meta-World and over 25% improvement on iTHOR, demonstrating strong effectiveness across 23 embodied control tasks. We additionally evaluate SP in real-world robotic experiments to verify its practical viability. SP enhances the practicality of embodied models for robotic control applications. Code and checkpoints are maintained at https://plantpotatoonmoon.github.io/SpatialPolicy/.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DepthVision: Enabling Robust Vision-Language Models with GAN-Based LiDAR-to-RGB Synthesis for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.07463</link>
<guid>https://arxiv.org/abs/2509.07463</guid>
<content:encoded><![CDATA[
arXiv:2509.07463v2 Announce Type: replace-cross 
Abstract: Ensuring reliable autonomous operation when visual input is degraded remains a key challenge in intelligent vehicles and robotics. We present DepthVision, a multimodal framework that enables Vision--Language Models (VLMs) to exploit LiDAR data without any architectural changes or retraining. DepthVision synthesizes dense, RGB-like images from sparse LiDAR point clouds using a conditional GAN with an integrated refiner, and feeds these into off-the-shelf VLMs through their standard visual interface. A Luminance-Aware Modality Adaptation (LAMA) module fuses synthesized and real camera images by dynamically weighting each modality based on ambient lighting, compensating for degradation such as darkness or motion blur. This design turns LiDAR into a drop-in visual surrogate when RGB becomes unreliable, effectively extending the operational envelope of existing VLMs. We evaluate DepthVision on real and simulated datasets across multiple VLMs and safety-critical tasks, including vehicle-in-the-loop experiments. The results show substantial improvements in low-light scene understanding over RGB-only baselines while preserving full compatibility with frozen VLM architectures. These findings demonstrate that LiDAR-guided RGB synthesis is a practical pathway for integrating range sensing into modern vision-language systems for autonomous driving.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Atlas Graphs for Dynamic Scene Decomposition and Editing</title>
<link>https://arxiv.org/abs/2509.16336</link>
<guid>https://arxiv.org/abs/2509.16336</guid>
<content:encoded><![CDATA[
arXiv:2509.16336v3 Announce Type: replace-cross 
Abstract: Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.
  Project Page: https://princeton-computational-imaging.github.io/nag/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clone Deterministic 3D Worlds</title>
<link>https://arxiv.org/abs/2510.26782</link>
<guid>https://arxiv.org/abs/2510.26782</guid>
<content:encoded><![CDATA[
arXiv:2510.26782v2 Announce Type: replace-cross 
Abstract: A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future physical state of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. However, existing world models often focus on random generation of open worlds, but neglect the need for high-fidelity modeling of deterministic scenarios (such as fixed-map mazes and static space robot navigation). In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone a deterministic 3D world. 1) Through diagnostic experiment, we quantitatively demonstrate that high-fidelity cloning is feasible and the primary bottleneck for long-horizon fidelity is the geometric structure of the latent representation, not the dynamics model itself. 2) Building on this insight, we show that applying temporal contrastive learning principle as a geometric regularization can effectively curate a latent space that better reflects the underlying physical state manifold, demonstrating that contrastive constraints can serve as a powerful inductive bias for stable world modeling; we call this approach Geometrically-Regularized World Models (GRWM). At its core is a lightweight geometric regularization module that can be seamlessly integrated into standard autoencoders, reshaping their latent space to provide a stable foundation for effective dynamics modeling. By focusing on representation quality, GRWM offers a simple yet powerful pipeline for improving world model fidelity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction</title>
<link>https://arxiv.org/abs/2511.08955</link>
<guid>https://arxiv.org/abs/2511.08955</guid>
<content:encoded><![CDATA[
arXiv:2511.08955v2 Announce Type: replace-cross 
Abstract: Simulating microstructure evolution (MicroEvo) is vital for materials design but demands high numerical accuracy, efficiency, and physical fidelity. Although recent studies on deep learning (DL) offer a promising alternative to traditional solvers, the field lacks standardized benchmarks. Existing studies are flawed due to a lack of comparing specialized MicroEvo DL models with state-of-the-art spatio-temporal architectures, an overemphasis on numerical accuracy over physical fidelity, and a failure to analyze error propagation over time. To address these gaps, we introduce MicroEvoEval, the first comprehensive benchmark for image-based microstructure evolution prediction. We evaluate 14 models, encompassing both domain-specific and general-purpose architectures, across four representative MicroEvo tasks with datasets specifically structured for both short- and long-term assessment. Our multi-faceted evaluation framework goes beyond numerical accuracy and computational cost, incorporating a curated set of structure-preserving metrics to assess physical fidelity. Our extensive evaluations yield several key insights. Notably, we find that modern architectures (e.g., VMamba), not only achieve superior long-term stability and physical fidelity but also operate with an order-of-magnitude greater computational efficiency. The results highlight the necessity of holistic evaluation and identify these modern architectures as a highly promising direction for developing efficient and reliable surrogate models in data-driven materials science.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition</title>
<link>https://arxiv.org/abs/2511.05893</link>
<guid>https://arxiv.org/abs/2511.05893</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-rank sparse regression, face recognition, Histogram of Oriented Hessian, hybrid descriptor, structured noise

<br /><br />Summary: 
This paper addresses the challenges in face recognition posed by occlusion and illumination variations, presenting a Hybrid second-order gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model. The authors identify limitations in existing methods related to feature representation and structured corruption modeling. To enhance feature extraction, they introduce the Histogram of Oriented Hessian (HOH), which captures second-order geometric characteristics like curvature and ridge patterns. This is combined with first-order gradient histograms to create the Hybrid second-order gradient Histogram (H2H), resulting in a more robust local descriptor that improves structural discriminability under challenging conditions. The H2H features are then integrated into an advanced version of the Sparse Regularized Nuclear Norm based Matrix Regression (SR_NMR) model, which applies a global low-rank constraint on the residual matrix to leverage cross-sample correlations in structured noise. The H2H-GLRSR model demonstrates superior discrimination and robustness compared to existing methods. Experimental evaluations on benchmark datasets reveal that this novel approach significantly outperforms state-of-the-art regression-based classifiers in terms of recognition accuracy and computational efficiency. <div>
arXiv:2511.05893v2 Announce Type: replace 
Abstract: Low-rank sparse regression models have been widely adopted in face recognition due to their robustness against occlusion and illumination variations. However, existing methods often suffer from insufficient feature representation and limited modeling of structured corruption across samples. To address these issues, this paper proposes a Hybrid second-order gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model. First, we propose the Histogram of Oriented Hessian (HOH) to capture second-order geometric characteristics such as curvature and ridge patterns. By fusing HOH and first-order gradient histograms, we construct a unified local descriptor, termed the Hybrid second-order gradient Histogram (H2H), which enhances structural discriminability under challenging conditions. Subsequently, the H2H features are incorporated into an extended version of the Sparse Regularized Nuclear Norm based Matrix Regression (SR\_NMR) model, where a global low-rank constraint is imposed on the residual matrix to exploit cross-sample correlations in structured noise. The resulting H2H-GLRSR model achieves superior discrimination and robustness. Experimental results on benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art regression-based classifiers in both recognition accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory</title>
<link>https://arxiv.org/abs/2511.05966</link>
<guid>https://arxiv.org/abs/2511.05966</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot, industrial anomaly detection, multimodal, hypergraph, structural commonality<br /><br />Summary:<br /><br />This paper addresses the challenge of few-shot multimodal industrial anomaly detection, where limited training samples hinder the ability to capture diverse test sample patterns. To overcome this, the authors propose CIF (Commonality In Few), a novel unsupervised method that extracts structural commonality from few training samples. CIF employs hypergraphs to model higher-order correlations and capture intra-class structural information, storing this prior knowledge in a memory bank. The method includes a semantic-aware hypergraph construction module designed specifically for single-semantic industrial images to guide memory bank creation. It also integrates a training-free hypergraph message passing module that updates test sample features to close the distribution gap with memory bank features. Additionally, a hyperedge-guided memory search module is introduced to leverage structural information during memory retrieval, reducing false positive rates. Experimental validation on the MVTec 3D-AD and Eyecandies datasets demonstrates that CIF outperforms state-of-the-art methods under few-shot conditions. The paper’s contributions highlight the effectiveness of utilizing higher-order structural commonality and hypergraph-based techniques to improve anomaly detection performance with very limited training data. The authors have made their code publicly available at the provided GitHub repository. <div>
arXiv:2511.05966v2 Announce Type: replace 
Abstract: Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at https://github.com/Sunny5250/CIF.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling</title>
<link>https://arxiv.org/abs/2511.06194</link>
<guid>https://arxiv.org/abs/2511.06194</guid>
<content:encoded><![CDATA[
<div> NURBS, 3D CAD, large language model, text-to-CAD, geometric fidelity

<br /><br />Summary:  
This paper introduces NURBGen, a novel framework designed to generate high-fidelity 3D CAD models directly from natural language text using Non-Uniform Rational B-Splines (NURBS). Existing text-to-CAD systems are limited by their reliance on meshes or scarce design-history data, whereas NURBGen translates free-form textual descriptions into JSON representations of NURBS parameters, such as control points, knot vectors, degrees, and rational weights. These JSON outputs can then be converted directly into CAD Boundary Representation (BRep) format using Python. To improve robustness and reduce complexity, the authors propose a hybrid representation that combines untrimmed NURBS with analytic primitives, effectively handling trimmed surfaces and degenerate regions. The paper also introduces partABC, a curated subset of the ABC dataset, which includes individual CAD components annotated with detailed captions generated through an automated annotation pipeline. Evaluation results demonstrate that NURBGen surpasses prior methods in terms of geometric fidelity and dimensional accuracy, with expert assessments confirming its effectiveness across diverse prompts. The authors plan to release both the code and dataset publicly to support further research and application development in text-driven CAD modeling. <div>
arXiv:2511.06194v2 Announce Type: replace 
Abstract: Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (\textit{i.e}, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-Aware Mixture of Experts with Log-Logistic Survival Modeling for Whole-Slide Images</title>
<link>https://arxiv.org/abs/2511.06266</link>
<guid>https://arxiv.org/abs/2511.06266</guid>
<content:encoded><![CDATA[
<div> Quantile-Gated Patch Selection, Graph-Guided Clustering, Hierarchical Context Attention, Expert-Driven Mixture of Log-Logistics, Survival Prediction<br /><br />Summary:<br /><br />This study tackles the challenge of accurate survival prediction from gigapixel-resolution histopathology whole-slide images (WSIs), which feature strong spatial heterogeneity and complex survival distributions. The authors propose a novel computational pathology framework incorporating four key innovations: (1) Quantile-Gated Patch Selection dynamically identifies regions on WSIs relevant to prognosis, enhancing feature extraction; (2) Graph-Guided Clustering groups patches based on spatial and morphological similarities, capturing meaningful tissue relationships; (3) Hierarchical Context Attention models both local tissue interactions and the broader slide-level context, allowing for improved interpretability and contextual understanding; (4) an Expert-Driven Mixture of Log-Logistics module flexibly models intricate survival distributions, addressing limitations of existing survival models. Evaluations on large TCGA cohorts demonstrate state-of-the-art performance with time-dependent concordance indices of 0.644 for Lung Adenocarcinoma (LUAD), 0.751 for Kidney Renal Clear Cell Carcinoma (KIRC), and 0.752 for Breast Invasive Carcinoma (BRCA). This approach consistently outperforms both histology-only and multimodal baseline methods. Furthermore, the framework improves calibration and interpretability, marking a significant advancement toward personalized cancer prognosis using WSIs. <div>
arXiv:2511.06266v3 Announce Type: replace 
Abstract: Accurate survival prediction from histopathology whole-slide images (WSIs) remains challenging due to their gigapixel resolution, strong spatial heterogeneity, and complex survival distributions. We introduce a comprehensive computational pathology framework that addresses these limitations through four complementary innovations: (1) Quantile-Gated Patch Selection for dynamically identifying prognostically relevant regions, (2) Graph-Guided Clustering to group patches by spatial and morphological similarity, (3) Hierarchical Context Attention to model both local tissue interactions and global slide-level context, and (4) an Expert-Driven Mixture of Log-Logistics module that flexibly models complex survival distributions. Across large TCGA cohorts, our method achieves state-of-the-art performance, yielding time-dependent concordance indices of 0.644 on LUAD, 0.751 on KIRC, and 0.752 on BRCA, consistently outperforming both histology-only and multimodal baselines. The framework further provides improved calibration and interpretability, advancing the use of WSIs for personalized cancer prognosis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.06298</link>
<guid>https://arxiv.org/abs/2511.06298</guid>
<content:encoded><![CDATA[
<div> Keywords: multispectral object detection, frequency-domain features, Kolmogorov-Arnold Network, UAV scale variation, feature fusion  

<br /><br />Summary: This paper introduces a novel approach named Spatial and Frequency Feature Reconstruction (SFFR) for multispectral object detection, particularly in UAV applications. 1) Unlike previous methods that focused mainly on spatial-domain fusion with CNNs or Transformers, SFFR uniquely incorporates frequency-domain features, enhancing feature representation. 2) The method is built around the Kolmogorov-Arnold Network (KAN), which simultaneously reconstructs complementary spatial and frequency features for improved fusion. 3) Two core modules are proposed: the Frequency Component Exchange KAN (FCEKAN), which implements an innovative selective exchange of frequency components between RGB and IR modalities to enhance cross-modal complementarity and consistency; and the Multi-Scale Gaussian KAN (MSGKAN), which uses multi-scale Gaussian basis functions to model nonlinear spatial features and address scale variations caused by different UAV flight altitudes. 4) These modules are complementary—FCEKAN captures frequency features effectively, while MSGKAN models spatial semantics, collectively enhancing robustness and adaptability. 5) Extensive experiments on SeaDroneSee, DroneVehicle, and DVTOD datasets demonstrate the superior performance of SFFR in UAV multispectral object detection tasks. The authors commit to releasing the code publicly to facilitate further research and application. <div>
arXiv:2511.06298v3 Announce Type: replace 
Abstract: Recent multispectral object detection methods have primarily focused on spatial-domain feature fusion based on CNNs or Transformers, while the potential of frequency-domain feature remains underexplored. In this work, we propose a novel Spatial and Frequency Feature Reconstruction method (SFFR) method, which leverages the spatial-frequency feature representation mechanisms of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary representations in both spatial and frequency domains prior to feature fusion. The core components of SFFR are the proposed Frequency Component Exchange KAN (FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN introduces an innovative selective frequency component exchange strategy that effectively enhances the complementarity and consistency of cross-modal features based on the frequency feature of RGB and IR images. The MSGKAN module demonstrates excellent nonlinear feature modeling capability in the spatial domain. By leveraging multi-scale Gaussian basis functions, it effectively captures the feature variations caused by scale changes at different UAV flight altitudes, significantly enhancing the model's adaptability and robustness to scale variations. It is experimentally validated that our proposed FCEKAN and MSGKAN modules are complementary and can effectively capture the frequency and spatial semantic features respectively for better feature fusion. Extensive experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the superior performance and significant advantages of the proposed method in UAV multispectral object perception task. Code will be available at https://github.com/qchenyu1027/SFFR.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports</title>
<link>https://arxiv.org/abs/2511.06499</link>
<guid>https://arxiv.org/abs/2511.06499</guid>
<content:encoded><![CDATA[
<div> Keywords: sports intelligence, multimodal models, reasoning chains, dataset, benchmarking

<br /><br />Summary: This paper introduces SportR, the first large-scale benchmark for evaluating multi-sport reasoning capabilities in multimodal large language models (MLLMs). It identifies three critical capabilities necessary for sports understanding: fine-grained visual perception, application of sport rule knowledge, and effective grounding of that knowledge in visual evidence. Existing benchmarks are limited as they often focus on individual sports or lack the depth of reasoning and visual grounding required. SportR addresses this with a dataset comprising 5,017 images and 2,101 videos, organized around a hierarchy of question-answer pairs that assess reasoning from simple to complex tasks. For advanced inquiries, it includes 7,118 human-authored Chain of Thought (CoT) annotations, supporting multi-step reasoning. The benchmark features both image and video modalities with manual bounding box annotations for visual grounding assessment. Experiments reveal that current state-of-the-art models struggle with the benchmarks' most challenging tasks. Although training on the dataset through Supervised Fine-Tuning and Reinforcement Learning yields improvements, the results remain insufficient, illustrating a substantial gap in model capabilities. SportR thus sets a new research direction for the sports reasoning community. <div>
arXiv:2511.06499v2 Announce Type: replace 
Abstract: Deeply understanding sports requires an intricate blend of fine-grained visual perception and rule-based reasoning - a challenge that pushes the limits of current multimodal models. To succeed, models must master three critical capabilities: perceiving nuanced visual details, applying abstract sport rule knowledge, and grounding that knowledge in specific visual evidence. Current sports benchmarks either cover single sports or lack the detailed reasoning chains and precise visual grounding needed to robustly evaluate these core capabilities in a multi-sport context. To address this gap, we introduce SportR, the first multi-sports large-scale benchmark designed to train and evaluate MLLMs on the fundamental reasoning required for sports intelligence. Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable granular evaluation, we structure our benchmark around a progressive hierarchy of question-answer (QA) pairs designed to probe reasoning at increasing depths - from simple infraction identification to complex penalty prediction. For the most advanced tasks requiring multi-step reasoning, such as determining penalties or explaining tactics, we provide 7,118 high-quality, human-authored Chain of Thought (CoT) annotations. In addition, our benchmark incorporates both image and video modalities and provides manual bounding box annotations to test visual grounding in the image part directly. Extensive experiments demonstrate the profound difficulty of our benchmark. State-of-the-art baseline models perform poorly on our most challenging tasks. While training on our data via Supervised Fine-Tuning and Reinforcement Learning improves these scores, they remain relatively low, highlighting a significant gap in current model capabilities. SportR presents a new challenge for the community, providing a critical resource to drive future research in multimodal sports reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers</title>
<link>https://arxiv.org/abs/2511.06848</link>
<guid>https://arxiv.org/abs/2511.06848</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, Knowledge Distillation, Feature-based Distillation, Representational Mismatch, Frequency Analysis  

<br /><br />Summary: This paper addresses the unexpected failure of feature-based knowledge distillation methods when applied to Vision Transformers (ViTs), contrasting with their success in CNN compression. The authors introduce a novel analytical framework named "distillation dynamics" that integrates frequency spectrum analysis, information entropy metrics, and activation magnitude tracking to study this phenomenon. Their investigation uncovers a unique U-shaped information processing pattern in ViTs, characterized by an initial compression phase followed by expansion in later layers. The core issue identified is a representational paradigm mismatch between teacher and student models, where teachers use distributed, high-dimensional encoding strategies that the smaller student models cannot replicate due to limited channel capacity. This mismatch leads to negative transfer, with late-layer feature alignment actively degrading student performance. The study highlights that naive feature mimicry is insufficient for effective knowledge transfer in ViTs. Instead, successful distillation approaches must acknowledge and accommodate these fundamental representational constraints. The findings offer theoretical insights crucial for designing robust ViT compression strategies. To aid further research, the authors have released all source code and experimental logs at the provided GitHub repository. <div>
arXiv:2511.06848v2 Announce Type: replace 
Abstract: While feature-based knowledge distillation has proven highly effective for compressing CNNs, these techniques unexpectedly fail when applied to Vision Transformers (ViTs), often performing worse than simple logit-based distillation. We provide the first comprehensive analysis of this phenomenon through a novel analytical framework termed as "distillation dynamics", combining frequency spectrum analysis, information entropy metrics, and activation magnitude tracking. Our investigation reveals that ViTs exhibit a distinctive U-shaped information processing pattern: initial compression followed by expansion. We identify the root cause of negative transfer in feature distillation: a fundamental representational paradigm mismatch between teacher and student models. Through frequency-domain analysis, we show that teacher models employ distributed, high-dimensional encoding strategies in later layers that smaller student models cannot replicate due to limited channel capacity. This mismatch causes late-layer feature alignment to actively harm student performance. Our findings reveal that successful knowledge transfer in ViTs requires moving beyond naive feature mimicry to methods that respect these fundamental representational constraints, providing essential theoretical guidance for designing effective ViTs compression strategies. All source code and experimental logs are provided at https://github.com/thy960112/Distillation-Dynamics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion</title>
<link>https://arxiv.org/abs/2511.07978</link>
<guid>https://arxiv.org/abs/2511.07978</guid>
<content:encoded><![CDATA[
<div> Point cloud completion, transformer decoder, density-agnostic, class-aware, geometric features<br /><br />Summary: Point cloud completion focuses on reconstructing missing geometric details from incomplete 3D scans, which are often limited due to occlusions or sensor viewpoints. Current methods generally assume fixed input or output densities and sometimes rely on image-based data, limiting their effectiveness in real-world settings with variable point sparsity and limited supervision. The proposed approach, called Density-agnostic and Class-aware Network (DANCE), specifically targets completion of only the missing regions while maintaining the observed geometry intact. DANCE employs ray-based sampling from multiple viewpoints to generate candidate points for the missing areas. A transformer decoder refines these candidate point positions and predicts opacity scores that determine their validity for inclusion in the final output surface. To guide the completion semantically, a lightweight classification head is trained directly on geometric features, allowing category-consistent completion without the need for external image supervision. Extensive evaluations on benchmarks such as PCN and MVP demonstrate that DANCE surpasses existing state-of-the-art methods in both accuracy and structural consistency. Additionally, DANCE shows robustness to varying input densities and noise levels, highlighting its suitability for practical applications with diverse scanning conditions. <div>
arXiv:2511.07978v2 Announce Type: replace 
Abstract: Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.08291</link>
<guid>https://arxiv.org/abs/2511.08291</guid>
<content:encoded><![CDATA[
<div> Keywords: SynWeather, weather synthesis, multi-variable, multi-region, Diffusion Transformer<br /><br />Summary:<br />1. The paper introduces SynWeather, a novel dataset aimed at Unified Multi-region and Multi-variable Weather Observation Data Synthesis, addressing limitations in current meteorological data modeling.<br />2. SynWeather encompasses four key representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, ensuring broad geographical coverage.<br />3. The dataset includes high-resolution observations of essential weather variables, specifically Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature, to facilitate comprehensive weather analysis.<br />4. To overcome the shortcomings of deterministic models that often produce over-smoothed results and ignore cross-variable complementarity, the authors propose SynWeatherDiff, a probabilistic weather synthesis model based on the Diffusion Transformer framework.<br />5. Experimental results on SynWeather demonstrate that SynWeatherDiff outperforms both task-specific and general weather modeling methods, showcasing its effectiveness in unified, multi-variable, and multi-region weather data synthesis. <div>
arXiv:2511.08291v3 Announce Type: replace 
Abstract: With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Negative Flips via Margin Preserving Training</title>
<link>https://arxiv.org/abs/2511.08322</link>
<guid>https://arxiv.org/abs/2511.08322</guid>
<content:encoded><![CDATA[
<div> Keywords: negative flips, margin calibration, image classification, focal distillation loss, model update<br /><br />Summary: Minimizing inconsistencies across successive versions of AI models, particularly in image classification, is critical to maintain reliability. A common inconsistency, known as negative flips, occurs when an updated model misclassifies samples that were correctly predicted by a previous model. This problem intensifies as new classes are added over time, reducing class margins and causing conflicting learning patterns that degrade original class performance. To address this, the authors introduce a novel method that preserves the original model margins while training the updated model. Their approach explicitly enforces a larger relative margin between old and new classes via a margin-calibration term applied to the logits. However, strictly enforcing logit margin constraints can reduce accuracy on new classes compared to models trained independently. To balance this, they propose integrating a double-source focal distillation loss combining knowledge from both the previous model and a newly trained independent model. This allows the learning of suitable decision margins from old and new data under margin calibration. Extensive experiments conducted on standard image classification benchmarks show that the proposed method consistently decreases the negative flip rate while maintaining high overall accuracy, demonstrating its effectiveness in reducing inconsistencies during model updates. <div>
arXiv:2511.08322v2 Announce Type: replace 
Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</title>
<link>https://arxiv.org/abs/2511.07947</link>
<guid>https://arxiv.org/abs/2511.07947</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, model extraction attacks, watermarking, resilience, Class-Feature Watermarks 

<br /><br />Summary: Machine learning models are valuable but face threats from model extraction attacks (MEA) that replicate their functionality via black-box queries. Model watermarking is a strategy to protect these models by embedding forensic markers; however, current techniques largely focus on evading MEAs through representation entanglement, leaving vulnerabilities against sequential attacks and watermark removal strategies. This study identifies that the effectiveness of existing removal methods is compromised by entanglement. To tackle this issue, the researchers introduce the Watermark Removal attacK (WRK), which successfully navigates entanglement constraints by leveraging decision boundaries impacted by watermark artifacts, achieving a substantial reduction in watermark success rates (over 88.79%). For enhanced protection, the study proposes Class-Feature Watermarks (CFW), utilizing class-level artifacts to bolster resilience. CFW innovatively forms a synthetic class from out-of-domain samples, eradicating weak decision boundaries. This method balances MEA transferability and stability after MEA attacks. Experimental results indicate that CFW consistently surpasses existing methodologies, maintaining a watermark success rate of at least 70.15% in extracted models under combined MEA and WRK challenges, all while ensuring the functionality of the protected models remains intact. <div>
arXiv:2511.07947v2 Announce Type: replace-cross 
Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Psychological stress during Examination and its estimation by handwriting in answer script</title>
<link>https://arxiv.org/abs/2511.11633</link>
<guid>https://arxiv.org/abs/2511.11633</guid>
<content:encoded><![CDATA[
<div> Keywords: graphology, artificial intelligence, psychological stress, handwritten examination, sentiment analysis

<br /><br />Summary: This research introduces a novel approach that merges graphology with artificial intelligence to measure psychological stress levels in students by examining their handwritten exam scripts. Utilizing Optical Character Recognition (OCR) and transformer-based sentiment analysis, the study presents a data-driven alternative to conventional grading systems, offering enhanced insights into students' cognitive and emotional states during examinations. The proposed system employs high-resolution image processing, specifically TrOCR, combined with sentiment entropy fusion techniques based on RoBERTa models, ultimately leading to the creation of a numerical Stress Index. This innovative framework ensures robustness through a five-model voting mechanism along with unsupervised anomaly detection methods, paving the way for advancements in academic forensics. By analyzing the nuances of handwriting, the research aims to provide educators with a deeper understanding of student stress levels, which can inform better pedagogical strategies and support systems during exams, enhancing the overall educational experience for students. <div>
arXiv:2511.11633v1 Announce Type: new 
Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time pothole detection with onboard sensors and camera on vehicles</title>
<link>https://arxiv.org/abs/2511.11643</link>
<guid>https://arxiv.org/abs/2511.11643</guid>
<content:encoded><![CDATA[
<div> Keywords: potholes, road conditions, SVM classifier, real-time detection, vehicle sensors

<br /><br />Summary: Road conditions significantly impact daily commutes, especially with the increasing number of vehicles. Frequent assessment of road conditions is essential to ensure smooth traffic flow. Small cracks can develop into larger potholes due to temperature changes and vehicular pressure. This paper addresses the challenge of detecting potholes in real-time using onboard sensors in vehicles. The research implements a Support Vector Machine (SVM) classifier to identify potholes, achieving a notable accuracy of 98.1%. The data for this study was collected over a 2 km stretch of a local road that contained 26 potholes. The successful detection of potholes using this method can facilitate better data analysis and management of pothole-related issues on a larger scale. The potential for real-time detection not only aids in immediate responses to road hazards but also contributes to long-term infrastructure planning and maintenance. The code used in this research is accessible on GitHub, promoting further exploration and application of the findings in various contexts to enhance road safety and reduce maintenance costs. <div>
arXiv:2511.11643v1 Announce Type: new 
Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model</title>
<link>https://arxiv.org/abs/2511.11659</link>
<guid>https://arxiv.org/abs/2511.11659</guid>
<content:encoded><![CDATA[
<div> Keywords: cultivated land ecosystems, habitat classification, ultra-high-resolution remote sensing, Dynamic-Weighted Feature Fusion Network, multi-layer feature fusion<br /><br />Summary:<br /><br />This study addresses critical gaps in habitat classification for cultivated land ecosystems, including the absence of a standardized system and incomplete habitat type coverage. To tackle these issues, the authors developed a high-quality, ultra-high-resolution remote sensing image dataset annotated across 15 categories of cultivated land habitats. They proposed a novel Dynamic-Weighted Feature Fusion Network (DWFF-Net) that leverages a frozen-parameter DINOv3 encoder to extract foundational features. The model introduces a data-level adaptive dynamic weighting strategy to effectively fuse features by analyzing relationships between category images and feature maps. The decoder uses a dynamic weight computation network to integrate multi-layer features thoroughly. A hybrid loss function optimizes the training process, enhancing segmentation accuracy. Experimental results demonstrate that DWFF-Net achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming baseline models by 0.021 and 0.0161, respectively. Ablation studies validate the complementary effect of multi-layer feature fusion, particularly improving IoU for challenging micro-habitat categories such as field ridges. Overall, this framework enables sub-meter precision habitat identification and mapping at low cost, providing strong technical support for fine-grained habitat monitoring in cultivated landscapes. <div>
arXiv:2511.11659v1 Announce Type: new 
Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.11662</link>
<guid>https://arxiv.org/abs/2511.11662</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Image Segmentation, Few-Shot Learning, Geodesic Distance, Edge-Aware, Prototype Extraction<br /><br />Summary:  
1. The article addresses the challenge of medical image segmentation, which typically requires large annotated datasets that are difficult to obtain, hindering clinical applications.  
2. It focuses on few-shot segmentation methods that aim to perform well with minimal training examples but suffer from poor boundary delineation, especially when anatomically similar regions lack sufficient spatial context.  
3. The authors propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships using edge-aware geodesic distance learning to better respect anatomical boundaries.  
4. AGENet uses computationally lightweight geometric modeling rather than relying on complex networks or architectures, making it efficient and suitable for clinical scenarios with limited data.  
5. The framework includes three core components: (i) an edge-aware geodesic distance learning module enhanced via iterative Fast Marching refinement, (ii) adaptive prototype extraction capturing both global and local boundary features through spatially-weighted aggregation, and (iii) adaptive parameter learning to tailor the model to different organ characteristics.  
6. Extensive experiments on various medical imaging datasets show that AGENet improves segmentation performance compared to state-of-the-art methods, notably reducing boundary errors while maintaining computational efficiency.  
7. This balance of accuracy and efficiency makes AGENet particularly valuable for clinical applications requiring precise segmentation with limited annotated data. <div>
arXiv:2511.11662v1 Announce Type: new 
Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance</title>
<link>https://arxiv.org/abs/2511.11700</link>
<guid>https://arxiv.org/abs/2511.11700</guid>
<content:encoded><![CDATA[
<div> few-shot learning, point cloud segmentation, zero-shot inference, cross-attention, textual guidance<br /><br />Summary:<br /><br />This paper addresses the limitations of existing few-shot 3D point cloud semantic segmentation methods, which typically rely on a two-stage learning process involving pre-training, limiting flexibility and adaptability. To overcome these issues, the authors propose EPSegFZ, a novel pre-training-free network designed for both few-shot and zero-shot scenarios. The model integrates three main components: the Prototype-Enhanced Registers Attention (ProERA) module and Dual Relative Positional Encoding (DRPE) for enhanced feature extraction and precise query-prototype matching without pre-training, and the Language-Guided Prototype Embedding (LGPE) module, which incorporates textual annotations from the support set to boost few-shot performance and enable effective zero-shot inference. By leveraging additional textual information, the model better exploits the support data beyond visual cues alone. Extensive experiments demonstrate that EPSegFZ surpasses the current state-of-the-art methods, achieving improvements of 5.68% on the S3DIS benchmark and 3.82% on ScanNet. Overall, the proposed approach enhances the adaptability, performance, and zero-shot capabilities of point cloud semantic segmentation without the dependency on pre-training stages. <div>
arXiv:2511.11700v1 Announce Type: new 
Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement</title>
<link>https://arxiv.org/abs/2511.11702</link>
<guid>https://arxiv.org/abs/2511.11702</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene, affordance segmentation, geometric reasoning, task-aware, SceneFun3D  

<br /><br />Summary:  
The paper introduces Task-Aware 3D Scene-level Affordance segmentation (TASA), aimed at enabling embodied agents to understand and interact within complex 3D environments. The authors note existing methods typically focus on object-level affordances or simply apply 2D predictions to 3D, overlooking valuable geometric information from point clouds and leading to high computational demands. TASA seeks to address these shortcomings by employing a geometry-optimized framework that integrates 2D semantic cues with 3D geometric reasoning through a coarse-to-fine approach. A key feature of TASA is its task-aware 2D affordance detection module, which identifies manipulable points based on language and visual inputs, assisting in the selection of relevant views for tasks. Additionally, a 3D affordance refinement module allows for the combination of 2D semantic priors with local 3D geometry, yielding more accurate and spatially coherent 3D affordance masks. The experiments conducted on the SceneFun3D dataset demonstrate that the TASA framework significantly enhances both accuracy and efficiency in scene-level affordance segmentation compared to baseline methods. <div>
arXiv:2511.11702v1 Announce Type: new 
Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LE-CapsNet: A Light and Enhanced Capsule Network</title>
<link>https://arxiv.org/abs/2511.11708</link>
<guid>https://arxiv.org/abs/2511.11708</guid>
<content:encoded><![CDATA[
<div> Capsule Networks, LE-CapsNet, CIFAR-10, AffNIST, affine transformations  

<br /><br />Summary:  
This paper introduces LE-CapsNet, a lightweight and enhanced version of the Capsule Network (CapsNet) designed to address efficiency and accuracy limitations of traditional CapsNet classifiers. First, the authors highlight CapsNet’s advantages over Convolutional Neural Networks (CNNs), such as superior detection of overlapping categories and improved accuracy on transformed images. However, they note that CapsNet suffers from slower inference speed, high resource consumption, and a large number of parameters, alongside somewhat lower accuracy compared to CNNs. To counter these issues, LE-CapsNet uses only 3.8 million weights, enabling it to perform inference approximately four times faster than the original CapsNet. The model achieves a 76.73% accuracy rate on the CIFAR-10 dataset, indicating competitive performance with fewer resources. Additionally, LE-CapsNet demonstrates enhanced robustness to affine transformations, outperforming CapsNet with a 94.3% accuracy on the AffNIST dataset compared to CapsNet’s 90.52%. Overall, the study presents LE-CapsNet as an effective and efficient alternative to original CapsNet models, providing both faster processing times and improved accuracy on standard and transformed image recognition tasks. <div>
arXiv:2511.11708v1 Announce Type: new 
Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target-Balanced Score Distillation</title>
<link>https://arxiv.org/abs/2511.11710</link>
<guid>https://arxiv.org/abs/2511.11710</guid>
<content:encoded><![CDATA[
<div> Score Distillation Sampling, Negative Prompts, Texture Realism, Shape Distortion, Multi-objective Optimization

<br /><br />Summary: This work addresses limitations of Score Distillation Sampling (SDS) used in 3D asset generation from pretrained 2D text-to-image diffusion models, which typically suffers from over-saturation and over-smoothing. Recent improvements incorporating negative prompts have shown a trade-off between limited texture optimization and improved texture at the cost of shape distortions. The authors conduct a systematic analysis revealing that this trade-off is largely influenced by the use of Target Negative Prompts (TNP), which embed target information and enhance texture fidelity but cause geometric inaccuracies. To overcome this challenge, they propose Target-Balanced Score Distillation (TBSD), which formulates the generation task as a multi-objective optimization problem. TBSD introduces an adaptive balancing strategy that effectively mitigates the trade-off between texture quality and shape accuracy. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, delivering 3D assets featuring high-fidelity textures alongside geometrically accurate shapes. This approach advances the quality and realism of 3D models generated via diffusion priors, potentially impacting domains like gaming, virtual reality, and digital content creation. <div>
arXiv:2511.11710v1 Announce Type: new 
Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition</title>
<link>https://arxiv.org/abs/2511.11716</link>
<guid>https://arxiv.org/abs/2511.11716</guid>
<content:encoded><![CDATA[
<div> Keywords: CNN compression, Tucker factorization, rank selection, CompressNAS, model efficiency<br /><br />Summary: Deploying Deep Convolutional Neural Networks (CNNs) on resource-constrained devices like microcontrollers (MCUs) and lightweight Neural Processing Units (NPUs) is challenging due to the large size and computational demand of these networks. Low-rank tensor decomposition methods, such as Tucker factorization, offer a way to reduce model parameters and operations while maintaining reasonable accuracy. However, prior methods often select decomposition ranks locally without considering global trade-offs between compression and accuracy. To address this, the paper introduces CompressNAS, a framework inspired by MicroNAS, that formulates rank selection as a global search problem. CompressNAS features a fast accuracy estimator that rapidly evaluates candidate tensor decompositions, enabling exhaustive and efficient exploration of rank combinations under given memory and accuracy constraints. Experimental results show that on ImageNet, CompressNAS compresses ResNet-18 by 8 times with less than a 4% drop in accuracy. On the COCO dataset, it achieves a 2x compression of YOLOv5s without accuracy loss and a 2x compression of YOLOv5n with only a 2.5% accuracy drop. Additionally, the authors introduce a new family of compressed models named STResNet, demonstrating competitive performance relative to other efficient models. This work advances CNN deployment on constrained hardware by improved global rank optimization for low-rank compression. <div>
arXiv:2511.11716v1 Announce Type: new 
Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks</title>
<link>https://arxiv.org/abs/2511.11720</link>
<guid>https://arxiv.org/abs/2511.11720</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV networks, test-time adaptation, semantic segmentation, prompt-guided adaptation, collaborative learning<br /><br />Summary:<br /><br />1. Low-altitude UAV networks depend on robust semantic segmentation to support integrated sensing, communication, and control among diverse agents, but segmentation models degrade under varying weather, lighting, and viewpoints.<br /><br />2. Resource-constrained UAVs cannot perform traditional gradient-based test-time adaptation, while resource-rich UAVs adapt independently, leading to inefficient use of shared knowledge.<br /><br />3. AdaptFly is introduced as a novel prompt-guided test-time adaptation framework that adjusts segmentation models without updating weights, enabling efficient adaptation across UAVs.<br /><br />4. The framework supports two modes: lightweight token-prompt retrieval from a shared global memory for resource-limited UAVs, and gradient-free sparse visual prompt optimization using Covariance Matrix Adaptation Evolution Strategy for resource-massive UAVs.<br /><br />5. An activation-statistic detector triggers adaptation, and a cross-UAV knowledge pool consolidates prompt knowledge for fleet-wide collaboration with minimal bandwidth demands.<br /><br />6. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse environmental conditions, demonstrate that AdaptFly significantly enhances segmentation accuracy and robustness compared to static models and existing state-of-the-art test-time adaptation methods.<br /><br />7. Overall, AdaptFly offers a practical and communication-efficient approach to resilient perception in emerging low-altitude UAV economies. <div>
arXiv:2511.11720v1 Announce Type: new 
Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video</title>
<link>https://arxiv.org/abs/2511.11725</link>
<guid>https://arxiv.org/abs/2511.11725</guid>
<content:encoded><![CDATA[
<div> Children’s word learning, egocentric data, masked autoencoder, biologically plausible masking, word-referent mapping<br /><br />Summary:<br /><br />1. Children typically begin learning their first words between 6 and 9 months of age by associating spoken words with visual referents. 2. Learning word meanings without prior knowledge is challenging because a word could refer to numerous objects, their parts, or attributes within the environment. 3. This study uses longitudinal, egocentric, and ecologically valid data from the experience of a single child to model this learning process. 4. The authors propose a self-supervised, biologically plausible strategy using a masked autoencoder visual backbone that incorporates information about the human eye’s blind spot to create a new masking method. 5. This novel masking approach aims to replicate how the human brain fills in missing visual information, contrasting with standard random masking strategies that lack biological justification. 6. The pretrained encoder from this model is integrated into a contrastive learning video-text framework to learn word-referent mappings effectively. 7. Extensive evaluations demonstrate that this biologically inspired masking strategy performs at least as well as traditional random masking methods for learning word-referent mappings in cross-situational and temporally extended episodes. <div>
arXiv:2511.11725v1 Announce Type: new 
Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion</title>
<link>https://arxiv.org/abs/2511.11730</link>
<guid>https://arxiv.org/abs/2511.11730</guid>
<content:encoded><![CDATA[
<div> multimodal spatial omics, graph convolutional network, contrastive learning, adaptive integration, histopathological images<br /><br />Summary:<br /><br />1. The paper addresses the challenge of effectively integrating multimodal spatial omics data, such as transcriptomics, proteomics, and epigenomics, with high-resolution histopathological images to better understand tissue complexity and biological mechanisms.<br /><br />2. It highlights problems arising from heterogeneity across biological and imaging modalities, semantic differences, resolution mismatches, and biological perturbations during sample preparation that complicate data fusion.<br /><br />3. To overcome these issues, the authors propose GROVER, a novel framework that uses a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to extract nonlinear, modality-specific embeddings while preserving spatial information.<br /><br />4. A spot-feature-pair contrastive learning strategy is introduced to explicitly optimize alignment and correspondence between modalities at each spatial spot.<br /><br />5. Additionally, GROVER incorporates a dynamic expert routing mechanism that adaptively selects the most informative modalities for each spatial spot and suppresses noisy or low-quality data.<br /><br />6. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms existing state-of-the-art methods, offering a robust, reliable tool for adaptive multimodal spatial omics integration. <div>
arXiv:2511.11730v1 Announce Type: new 
Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing DeepFakes via Hyperspectral Domain Mapping</title>
<link>https://arxiv.org/abs/2511.11732</link>
<guid>https://arxiv.org/abs/2511.11732</guid>
<content:encoded><![CDATA[
<div> Generative models, Diffusion models, Hyperspectral imaging, Deepfake detection, FaceForensics++ dataset  

<br /><br />Summary:  
1. Modern generative and diffusion models create highly realistic images that can deceive both human observers and advanced automated detection systems.  
2. Most current detection techniques analyze images only in the RGB color space, limiting their ability to capture subtle manipulation artifacts.  
3. The authors introduce HSI-Detect, a novel two-stage pipeline that first reconstructs a dense 31-channel hyperspectral image from a standard RGB input and then performs manipulation detection within this richer spectral domain.  
4. By expanding the input representation into hyperspectral bands, HSI-Detect amplifies the visibility of manipulation artifacts that are either weak or invisible in traditional RGB channels, especially within specific frequency bands.  
5. Experimental evaluation on the FaceForensics++ dataset demonstrates that HSI-Detect consistently outperforms RGB-only detection baselines, confirming the effectiveness of spectral-domain mapping techniques for enhancing Deepfake detection accuracy. <div>
arXiv:2511.11732v1 Announce Type: new 
Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward bilipshiz geometric models</title>
<link>https://arxiv.org/abs/2511.11735</link>
<guid>https://arxiv.org/abs/2511.11735</guid>
<content:encoded><![CDATA[
<div> Keywords: point clouds, bi-Lipschitz equivalence, Procrustes Matching, Gromov Wasserstein, invariant neural networks<br /><br />Summary:<br /><br />This paper investigates whether neural networks designed to be invariant under symmetries of point clouds—specifically permutations and rigid motions—preserve natural symmetry-aware distances through bi-Lipschitz equivalence. Two primary symmetry-aware metrics on point clouds are studied: the Procrustes Matching (PM) metric and the Hard Gromov Wasserstein distance. The authors demonstrate that these two distances are not bi-Lipschitz equivalent, which implies that popular invariant neural networks do not maintain bi-Lipschitz properties with respect to the PM metric. To address this limitation, the paper proposes modifications to the existing invariant networks that enable them to satisfy bi-Lipschitz guarantees. The practical benefits of these modified bi-Lipschitz models are illustrated through initial experiments focused on the task of finding correspondences between 3D point clouds, where the proposed approach shows improvements over standard invariant models. This work contributes to the understanding of metric preservation in equivariant learning and highlights the advantages of enforcing bi-Lipschitz constraints for tasks involving point cloud symmetry and matching. <div>
arXiv:2511.11735v1 Announce Type: new 
Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</title>
<link>https://arxiv.org/abs/2511.11751</link>
<guid>https://arxiv.org/abs/2511.11751</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, neurosymbolic frameworks, visual grounding, concept-rulenet, interpretable reasoning<br /><br />Summary: Modern vision-language models (VLMs) achieve high predictive accuracy but often lack transparency and tend to hallucinate facts, especially with out-of-distribution data. To address these challenges, the authors propose Concept-RuleNet, a multi-agent neurosymbolic system that integrates visual grounding with transparent symbolic reasoning. The system begins with a multimodal concept generator that extracts discriminative visual concepts directly from a representative subset of training images, thereby reducing dependency on task labels and mitigating label bias. These concepts are then used to condition symbol discovery, anchoring symbolic representations in actual image statistics. A large language model reasoner agent composes these symbols into executable first-order logic rules, providing interpretable reasoning pathways. During inference, a vision verifier agent assesses the presence of each symbol and, alongside black-box neural model outputs, triggers rule execution to produce predictions with explicit explanations. The approach was validated across five benchmark datasets, including two challenging medical imaging and three underrepresented natural image tasks. Results show that Concept-RuleNet improves state-of-the-art neurosymbolic baselines by an average of 5% and reduces hallucinated symbols in rules by up to 50%, demonstrating both enhanced accuracy and interpretability. <div>
arXiv:2511.11751v1 Announce Type: new 
Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition</title>
<link>https://arxiv.org/abs/2511.11754</link>
<guid>https://arxiv.org/abs/2511.11754</guid>
<content:encoded><![CDATA[
<div> Transformer, Sparse Attention, Feature Selection, Encoder-Decoder, Synthetic Image Generation<br /><br />Summary:<br /><br />1. The paper proposes a novel variation of the Transformer architecture called Batch Transformers that operates in an implicit sparse attention style. <br />2. Unlike traditional Transformers which attend to entire sequences or batches across all dimensions, this model focuses attention selectively on "important" dimensions or primary components, effectively performing feature selection. <br />3. This selective attention significantly reduces the bottleneck size in encoder-decoder artificial neural network (ANN) architectures, leading to more efficient computation. <br />4. The proposed architecture is evaluated on a synthetic image generation task related to face recognition, specifically under challenges of makeup and occlusion. <br />5. The method demonstrates the ability to increase variability and richness in a limited original dataset, thus enhancing performance in scenarios with constrained or altered training data. <div>
arXiv:2511.11754v1 Announce Type: new 
Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing</title>
<link>https://arxiv.org/abs/2511.11780</link>
<guid>https://arxiv.org/abs/2511.11780</guid>
<content:encoded><![CDATA[
<div> Text-to-Image Generation, Reinforcement Learning, Task Decomposition, Vision-Language Models, Multi-Expert Systems<br /><br />Summary:<br /><br />Recent advances in single-shot text-to-image generation models have struggled with processing long, compositional prompts common in creative workflows. Image-POSER addresses this by introducing a reflective reinforcement learning framework that orchestrates a diverse set of pretrained text-to-image and image-to-image expert models. It manages complex, long-form prompts end-to-end by dynamically decomposing tasks into manageable subtasks. At each step, alignment between generated content and prompts is supervised through structured feedback provided by a vision-language model acting as a critic. By modeling image synthesis and editing as a Markov Decision Process, Image-POSER learns adaptive expert pipelines that combine the strengths of multiple models non-trivially. Experimental results demonstrate that Image-POSER outperforms existing baseline and state-of-the-art models across standard and custom benchmarks, achieving superior alignment, fidelity, and aesthetics. Human evaluations consistently prefer its outputs, highlighting the system’s practical effectiveness. Overall, the framework showcases how reinforcement learning can enable AI systems to autonomously decompose, reorder, and integrate visual models, paving the way for general-purpose visual assistants capable of comprehensive and creative image generation and editing. <div>
arXiv:2511.11780v1 Announce Type: new 
Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction</title>
<link>https://arxiv.org/abs/2511.11824</link>
<guid>https://arxiv.org/abs/2511.11824</guid>
<content:encoded><![CDATA[
<div> Keywords: single-object tracking, temporal transformer, trajectory prediction, real-time inference, identity propagation  

<br /><br />Summary:  
This paper introduces SOTFormer, a novel temporal transformer designed to address challenges in single-object tracking and short-term motion forecasting, particularly under conditions like occlusion, scale variation, and temporal drift. Unlike previous approaches that rely on recurrent or stacked temporal encoders, SOTFormer leverages a minimal constant-memory design that integrates object detection, tracking, and trajectory prediction into a unified end-to-end framework. The model features a ground-truth-primed memory and a burn-in anchor loss to explicitly stabilize the initialization and ensure consistent identity propagation over time. A single lightweight temporal-attention layer is utilized to refine object embeddings across frames efficiently, enabling real-time inference with fixed GPU memory usage. Evaluated on the Mini-LaSOT benchmark (using 20% of the data), SOTFormer achieves a competitive 76.3 AUC score while running at 53.7 frames per second with only 4.3 GB of VRAM, which demonstrates its efficiency and practicality. Furthermore, SOTFormer outperforms existing transformer-based tracking models such as TrackFormer and MOTRv2, especially in challenging scenarios involving fast object motion, significant scale changes, and occlusions. This makes SOTFormer a promising approach for robust, real-time single-object tracking and motion forecasting applications. <div>
arXiv:2511.11824v1 Announce Type: new 
Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning</title>
<link>https://arxiv.org/abs/2511.11837</link>
<guid>https://arxiv.org/abs/2511.11837</guid>
<content:encoded><![CDATA[
<div> Machining process planning, dynamic graph learning, 3D geometry, transformer, operation sequence prediction<br /><br />Summary:<br /><br />1. Machining process planning (MP) involves complex dependencies among part features and machining operations that evolve dynamically as machining progresses.<br />2. Existing dynamic graph learning (DGL) methods model spatio-temporal dependencies in machining but lack the incorporation of three-dimensional (3D) geometric information, leading to limited domain awareness.<br />3. The paper introduces MP-GFormer, a novel 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL via an attention mechanism.<br />4. MP-GFormer uses StereoLithography (STL) surface meshes to represent the 3D geometry of parts after each machining operation, with initial designs represented by boundary representation (B-rep) methods.<br />5. The approach was evaluated on a synthesized dataset, showing significant improvements of 24% in main operation prediction accuracy and 36% in sub-operation prediction accuracy compared to state-of-the-art methods, demonstrating enhanced performance by incorporating 3D geometric awareness into dynamic graph learning for MP. <div>
arXiv:2511.11837v1 Announce Type: new 
Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defending Unauthorized Model Merging via Dual-Stage Weight Protection</title>
<link>https://arxiv.org/abs/2511.11851</link>
<guid>https://arxiv.org/abs/2511.11851</guid>
<content:encoded><![CDATA[
<div> Keywords: model merging, intellectual property, weight protection, task fidelity, merging compatibility  

<br /><br />Summary:  
The paper addresses the rising issue of unauthorized model merging, where individuals combine fine-tuned pretrained models into new multi-capability models without permission, causing intellectual property violations and sabotaging model ownership and accountability. To counter this, the authors propose MergeGuard, a dual-stage proactive framework designed to protect model weights and disrupt merging compatibility, while preserving the original model's performance. The first stage involves redistributing task-relevant information evenly across model layers using L2-regularized optimization to ensure important gradients are dispersed, preventing straightforward merging. The second stage injects structured perturbations that misalign task subspaces and break curvature compatibility in the loss landscape, which causes destructive interference in merged models. Experiments conducted on vision models (ViT-L-14) and various language models (Llama2, Gemma2, Mistral) demonstrate the effectiveness of MergeGuard, showing a reduction of up to 90% in accuracy for merged models. At the same time, the protected models maintain high task fidelity with less than 1.5% performance degradation. Overall, MergeGuard offers a robust defense against unauthorized merging by reshaping the geometry of model parameters to undermine merging compatibility without compromising the original model’s utility. <div>
arXiv:2511.11851v1 Announce Type: new 
Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision</title>
<link>https://arxiv.org/abs/2511.11864</link>
<guid>https://arxiv.org/abs/2511.11864</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, signed distance function, boundary preservation, FocusSDF, loss function

<br /><br />Summary: Medical image segmentation is crucial for accurate diagnosis and effective treatment in clinical settings. However, many current segmentation models struggle with preserving boundary details because they do not explicitly incorporate boundary information. To tackle this challenge, the authors present FocusSDF, a novel loss function based on signed distance functions (SDFs) that enhances boundary awareness by assigning higher weights to pixels near lesion or organ boundaries. This adaptive weighting guides the network to focus on critical boundary regions during training. The study performs comprehensive evaluations comparing FocusSDF against five state-of-the-art segmentation models, including MedSAM, a foundation model for medical segmentation. Evaluations are conducted across multiple datasets featuring diverse segmentation tasks such as cerebral aneurysm, stroke, liver, and breast tumor segmentation, employing various imaging modalities. Additionally, FocusSDF is compared to four existing distance-based loss functions to assess its effectiveness rigorously. The experimental results demonstrate consistent and superior performance of FocusSDF over current distance transform-based loss functions, confirming its ability to improve boundary preservation in medical image segmentation across different organs and imaging types. <div>
arXiv:2511.11864v1 Announce Type: new 
Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)</title>
<link>https://arxiv.org/abs/2511.11882</link>
<guid>https://arxiv.org/abs/2511.11882</guid>
<content:encoded><![CDATA[
<div> Keywords: muskox detection, synthetic imagery, object detection models, zero-shot learning, few-shot learning  

<br /><br />Summary:  
This study focuses on improving wildlife population estimates, specifically muskoxen in Arctic regions, by enhancing deep learning object detection models (ODMs) using synthetic imagery (SI). Traditional survey methods like aerial counts and GNSS tracking are resource intensive and logistically challenging. The limited availability of real image data hinders the robustness of ODMs for sparsely distributed species such as muskoxen. The researchers tested a baseline model trained exclusively on real images against 5 zero-shot (ZS) and 5 few-shot (FS) models that incorporated increasing amounts of synthetic images. For ZS models, which used no real images during training, the inclusion of synthetic imagery markedly improved detection performance, with precision, recall, and F1 scores increasing and then plateauing once SI surpassed 100% of the baseline training data size. FS models, combining real and synthetic images, showed improved recall and marginally higher overall accuracy compared to models trained only on real data, though these gains were not statistically significant. The findings highlight the effectiveness of synthetic imagery for training ODMs under limited data conditions, enabling monitoring of rare or inaccessible species and increasing survey frequency. This approach allows ODM development without initial real data and refinement over time as real images become available. <div>
arXiv:2511.11882v1 Announce Type: new 
Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation</title>
<link>https://arxiv.org/abs/2511.11890</link>
<guid>https://arxiv.org/abs/2511.11890</guid>
<content:encoded><![CDATA[
<div> High-resolution imaging, CUDA, segmentation, GPU acceleration, HPC<br /><br />Summary:<br /><br />This article presents Harpia, a new CUDA-based processing library integrated with Annotat3D, designed to address the challenges posed by large 3D volumetric imaging datasets generated by techniques like X-ray tomography and advanced microscopy. Harpia supports scalable and interactive segmentation workflows tailored for high-performance computing (HPC) and remote-access environments. Key features of Harpia include strict memory management and native chunked execution, which allow it to handle datasets larger than the memory capacity of a single GPU effectively. The library incorporates a comprehensive set of GPU-accelerated tools for filtering, annotation, and quantification, enabling faster and more efficient processing. Experimental comparisons show that Harpia significantly outperforms popular existing frameworks such as NVIDIA cuCIM and scikit-image in speed, memory utilization, and scalability. Additionally, Harpia offers an interactive, human-in-the-loop interface that enhances collaborative scientific imaging tasks in environments shared by multiple users. By efficiently managing GPU resources and supporting remote and scalable workflows, Harpia advances the state of the art in volumetric image segmentation and processing within HPC infrastructures. <div>
arXiv:2511.11890v1 Announce Type: new 
Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks</title>
<link>https://arxiv.org/abs/2511.11898</link>
<guid>https://arxiv.org/abs/2511.11898</guid>
<content:encoded><![CDATA[
<div> Vision-language models, medical imaging, prompt optimization, automated prompting, clinical AI<br /><br />Summary:<br /><br />Vision-language foundation models (VLMs) hold promise for various imaging tasks but tend to underperform in medical settings. Traditional improvement methods like model fine-tuning require extensive domain-specific data and computational resources, while manual prompt engineering lacks generalizability and accessibility in medical institutions. To address these issues, the authors adapt the Declarative Self-improving Python (DSPy) framework for structured, automated prompt optimization tailored to medical vision-language systems. They develop prompting pipelines for five diverse medical imaging tasks spanning radiology, gastroenterology, and dermatology. The evaluation includes 10 open-source VLMs tested with four different prompt optimization methods. Results show a median relative improvement of 53% over zero-shot baseline prompts, with exceptional gains ranging between 300% and 3,400% on tasks where initial zero-shot performance was low. These findings underscore the effectiveness of automated prompt optimization in enhancing clinical image interpretation by reducing reliance on manual prompt engineering. This approach allows clinicians to concentrate more on patient care rather than on prompt crafting. Additionally, the methodology offers scalability, preserves data privacy, and improves performance on publicly available models. The authors publicly release their evaluation pipelines to foster reproducible research in specialized medical vision-language tasks at the provided GitHub repository. <div>
arXiv:2511.11898v1 Announce Type: new 
Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PI-NAIM: Path-Integrated Neural Adaptive Imputation Model</title>
<link>https://arxiv.org/abs/2511.11908</link>
<guid>https://arxiv.org/abs/2511.11908</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, missing modality, imputation, neural networks, mortality prediction<br /><br />Summary: The article addresses the common problem of missing modality data in medical imaging and multi-modal clinical diagnostics, which hinders accurate analysis. It introduces PI-NAIM, an innovative dual-path architecture that dynamically routes samples based on the complexity of missing data, ensuring efficient and accurate imputation. The proposed framework consists of three key components: (1) intelligent path routing that channels simple missingness cases to statistical imputation using MICE, while directing complex cases to neural network-based imputation via GAIN enhanced with temporal analysis; (2) cross-path attention fusion that combines outputs from both imputation paths by leveraging missingness-aware embeddings, improving overall data representation; (3) end-to-end joint optimization aiming to enhance both imputation accuracy and performance on downstream clinical prediction tasks. The model is evaluated extensively on the MIMIC-III dataset and various multimodal benchmarks, demonstrating superior imputation performance with an RMSE of 0.108, outperforming baseline methods that range from 0.119 to 0.152. Furthermore, the system achieves significant improvements in downstream mortality prediction, reaching an AUROC of 0.812. PI-NAIM’s design supports easy integration into existing vision and sensor data pipelines, making it a practical, unified tool for handling incomplete or corrupted inputs in real-world healthcare scenarios. The implementation code is openly available for community use. <div>
arXiv:2511.11908v1 Announce Type: new 
Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</title>
<link>https://arxiv.org/abs/2511.11910</link>
<guid>https://arxiv.org/abs/2511.11910</guid>
<content:encoded><![CDATA[
<div> Keywords: long video understanding, multimodal large language models, Query-aware Token Selector, vision token selection, temporal localization  

<br /><br />Summary: The paper addresses the challenge of long video understanding within multimodal large language models (MLLMs), where the growing number of vision tokens causes significant computational cost and latency. To tackle this, the authors introduce Query-aware Token Selector (QTSplus), a lightweight module that selectively filters important visual tokens relevant to a given text query. QTSplus operates by scoring tokens through cross-attention, predicting an instance-specific token retention budget based on query complexity, and selecting the top-n tokens using a differentiable straight-through estimator during training and a hard gate at inference. Additionally, a lightweight re-encoder preserves temporal sequence information through absolute timing, allowing precise second-level localization while maintaining a global view of the video. Integrated into the Qwen2.5-VL model, QTSplus achieves up to an 89% reduction in vision token volume and decreases end-to-end latency by 28% on long videos. Evaluation on eight long video benchmarks confirms that QTSplus maintains near-parity accuracy with the original model and surpasses it significantly on temporal direction and order tasks by +20.5 and +5.6 points respectively. These results demonstrate that QTSplus is a general and effective approach for scaling MLLMs to handle real-world, long-duration video understanding tasks efficiently. The authors commit to releasing all codes, datasets, and trained models publicly. <div>
arXiv:2511.11910v1 Announce Type: new 
Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing</title>
<link>https://arxiv.org/abs/2511.11944</link>
<guid>https://arxiv.org/abs/2511.11944</guid>
<content:encoded><![CDATA[
<div> event cameras, dehazing, diffusion model, high dynamic range, drone dataset<br /><br />Summary: This work addresses the challenge of clear imaging under hazy conditions, where traditional methods operating on RGB frames are limited by dynamic range, causing loss of structure and illumination details. For the first time, the authors propose using event cameras for dehazing, leveraging their significantly higher high dynamic range (120 dB versus 60 dB) and microsecond latency that are well-suited for capturing hazy scenes. Since paired HDR event and RGB data are scarce, the paper introduces an event-guided diffusion model that incorporates strong generative priors to restore clear images from hazy inputs by transferring HDR information from events to RGB frames. A novel event-guided module is designed to map sparse HDR event features such as edges and corners into the diffusion model's latent space, providing structural guidance that enhances visual realism and reduces semantic drift during image generation. To support real-world evaluation, the authors collect a novel drone dataset recorded in heavy haze conditions (AQI=341) with synchronized RGB and event sensors. Experimental results on two established benchmarks and the new dataset demonstrate that their approach achieves state-of-the-art dehazing performance, highlighting the effectiveness of combining event-based sensing with diffusion modeling for challenging visual restoration tasks. <div>
arXiv:2511.11944v1 Announce Type: new 
Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs</title>
<link>https://arxiv.org/abs/2511.11959</link>
<guid>https://arxiv.org/abs/2511.11959</guid>
<content:encoded><![CDATA[
<div> Keywords: U-Net, semantic segmentation, petroglyphs, attention mechanisms, BEGL loss<br /><br />Summary:<br /><br />1. The study compares three U-Net-based deep learning architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites.<br />2. The architectures tested are: (1) BEGL-UNet using Border-Enhanced Gaussian Loss; (2) Attention-Residual BEGL-UNet incorporating residual blocks and gated attention; (3) Spatial Channel Attention BEGL-UNet employing spatial-channel attention modules based on the Convolutional Block Attention Module.<br />3. All models utilize the BEGL loss function, which combines binary cross-entropy with Gaussian edge enhancement to improve segmentation accuracy on edges.<br />4. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex in Piauí, Brazil, using a 5-fold cross-validation strategy.<br />5. Attention-Residual BEGL-UNet achieved the best performance with a Dice Score of 0.710, validation loss of 0.067, and recall of 0.854.<br />6. Spatial Channel Attention BEGL-UNet showed comparable results with a Dice Score of 0.707 and recall of 0.857.<br />7. The baseline BEGL-UNet recorded a Dice Score of 0.690.<br />8. The integration of attention mechanisms improved Dice Scores by approximately 2.5–2.9% over the baseline.<br />9. These findings underscore the effectiveness of attention-based enhancements for digital preservation of archaeological heritage through improved semantic segmentation of petroglyph images. <div>
arXiv:2511.11959v1 Announce Type: new 
Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Po\c{c}o da Bebidinha Archaeological Complex, Piau\'i, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology</title>
<link>https://arxiv.org/abs/2511.11984</link>
<guid>https://arxiv.org/abs/2511.11984</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained glomerular subtyping, vision-language models, few-shot learning, pathology-specialized models, multimodal representation

<br /><br />Summary:  
This study addresses the challenge of fine-grained glomerular subtyping in kidney biopsy interpretation, where clinically valuable labeled data is scarce. The authors frame the problem as a few-shot learning task and systematically evaluate both pathology-specialized and general-purpose vision-language models (VLMs) under limited supervision. The evaluation metrics include classification performance (accuracy, AUC, F1) and representational geometry, focusing on feature alignment between image and text embeddings and subtype separability. The research investigates the impact of shot count, model architecture, domain-specific knowledge, and adaptation strategy on the performance and structure of learned multimodal representations. Results demonstrate that pathology-specialized vision-language backbones combined with vanilla fine-tuning outperform other approaches, even with as few as 4-8 labeled examples per subtype. These models effectively distinguish between different glomerular subtypes and exhibit significant improvements in both discrimination and calibration, with further supervision leading to incremental gains. Additionally, the study highlights that discrimination between positive and negative samples is as critical as image-text alignment for diagnostic accuracy. Overall, the findings provide practical guidance for model selection, adaptation strategies, and annotation effort allocation in clinical settings characterized by limited labeled data. <div>
arXiv:2511.11984v1 Announce Type: new 
Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups</title>
<link>https://arxiv.org/abs/2511.11989</link>
<guid>https://arxiv.org/abs/2511.11989</guid>
<content:encoded><![CDATA[
<div> Identity-Preserving Personalized Generation, Dual-Line Inference, Identity Adaptive Fusion, Identity Aggregation Prepending, semantic consistency<br /><br />Summary: This paper addresses the limitations of existing Identity-Preserving Personalized Generation (IPPG) methods that overly focus on facial regions, resulting in outputs dominated by facial close-ups and suffering from weak visual narrativity and poor semantic consistency, particularly under complex text prompts. The core issue is the conflict between identity (ID) feature embeddings and semantic expressiveness in generative models. To overcome this, the authors propose a novel IPPG method that enables generation beyond facial close-ups by synergistically optimizing identity fidelity and scene-level semantic creation. The key innovation is the Dual-Line Inference (DLI) pipeline, which separates identity and semantic information to resolve representation conflicts typical of single-path architectures. Additionally, they introduce an Identity Adaptive Fusion (IdAF) strategy that postpones the fusion of ID and semantic features to the noise prediction stage, employing adaptive attention fusion and noise decision masking to prevent identity embeddings from interfering with semantic content without manual masking. Complementing these improvements, an Identity Aggregation Prepending (IdAP) module aggregates identity information to replace random initialization, enhancing identity preservation. Experimental results demonstrate stable, effective performance without manual masking or fine-tuning. The framework is designed as a plug-and-play module, making it easy to incorporate into existing IPPG systems, thereby expanding the generation capacity from face close-ups to full character-scene compositions for film production and personalized content creation. <div>
arXiv:2511.11989v1 Announce Type: new 
Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks</title>
<link>https://arxiv.org/abs/2511.11993</link>
<guid>https://arxiv.org/abs/2511.11993</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, transfer attacks, parameter optimization, transformation-based attacks, dynamic parameter optimization<br /><br />Summary: This paper addresses vulnerabilities in deep neural networks, focusing on transformation-based transfer attacks. It identifies critical shortcomings in prior works: (1) Most studies evaluate attacks only at low iterations, which misrepresents performance at higher iteration counts where behavior differs significantly. (2) Uniform parameter settings are used across different surrogate models, iterations, and tasks, severely limiting attack transferability. (3) Traditional optimization of transformation parameters employs grid search, resulting in high computational complexity O(m^n) that restricts parameter tuning efforts. To tackle these issues, the authors empirically investigate various transformations and discover three dynamic patterns of transferability relative to parameter strength. They introduce the Concentric Decay Model (CDM) to effectively interpret these patterns. Leveraging these insights, they propose a Dynamic Parameter Optimization (DPO) method based on a rise-then-fall pattern in transferability, which reduces optimization complexity substantially to O(nlogm). Extensive experiments verify that DPO significantly enhances transferability of transformation-based attacks across different surrogate models, iterations, and tasks. This work offers a more nuanced understanding and efficient optimization strategy that meaningfully advances the effectiveness of such adversarial attacks against deep neural networks. <div>
arXiv:2511.11993v1 Announce Type: new 
Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation</title>
<link>https://arxiv.org/abs/2511.12005</link>
<guid>https://arxiv.org/abs/2511.12005</guid>
<content:encoded><![CDATA[
<div> Keywords: Lithography Segmentation, SEM Images, Human-in-the-Loop, Coarse-to-Fine Network, Point-wise Refinement<br /><br />Summary: Accurate segmentation of lithography scanning electron microscope (SEM) images is essential for process control and optimizing semiconductor manufacturing. The paper addresses the challenge of achieving pixel-level groove contour delineation with robust performance across various pattern geometries and process windows. Existing methods fall short in precision and robustness, limiting their usability. To overcome these issues, the authors propose LithoSeg, a novel coarse-to-fine segmentation network specifically designed for lithography images. The approach consists of two stages: a coarse segmentation stage employing a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM), which enhances robustness with minimal supervision. In the fine stage, the problem of 2D segmentation is transformed into a 1D regression task by sampling groove-normal profiles from the coarse mask. Point-wise refinement is then applied using a lightweight multilayer perceptron (MLP) to improve accuracy. LithoSeg demonstrates superior performance compared to previous methods in terms of segmentation accuracy and metrology precision, while requiring significantly less supervision. This makes LithoSeg a promising solution for enhancing real-world lithography metrology and semiconductor manufacturing yield. <div>
arXiv:2511.12005v1 Announce Type: new 
Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy</title>
<link>https://arxiv.org/abs/2511.12006</link>
<guid>https://arxiv.org/abs/2511.12006</guid>
<content:encoded><![CDATA[
<div> adaptation, microscopy, deep learning, adversarial, uncertainty<br /><br />Summary:<br /><br />1. This article addresses the challenge of transferring deep learning models in microscopy to new instruments or imaging conditions, where models often fail due to domain shifts.  
2. It critiques the conventional adversarial domain adaptation (ADDA) approach that retrains entire networks, potentially disrupting learned semantic representations.  
3. The authors propose a novel paradigm whereby only the earliest convolutional layers are adapted, while deeper layers remain frozen, preserving semantic features and improving transfer reliability.  
4. Building on this insight, they introduce SIT-ADDA-Auto, a self-configuring framework that combines shallow-layer adversarial alignment with predictive uncertainty to automatically select the optimal depth for adaptation, requiring no target labels.  
5. They validate SIT-ADDA across various conditions, including exposure and illumination changes, cross-instrument transfer, and multiple staining protocols, demonstrating improved reconstruction and segmentation performance compared to both full-encoder adaptation and non-adversarial baselines.  
6. The approach also reduces semantic feature drift, supporting robust and label-free adaptation in microscopy.  
7. The work establishes a practical design rule and offers a recipe suitable for field deployment, with publicly available code to facilitate adoption. <div>
arXiv:2511.12006v1 Announce Type: new 
Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis</title>
<link>https://arxiv.org/abs/2511.12018</link>
<guid>https://arxiv.org/abs/2511.12018</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic safety, Post-Encroachment Time (PET), multi-camera system, real-time analysis, edge computing  

<br /><br />Summary:  
1. The paper addresses the challenge of traffic safety analysis at signalized intersections, highlighting the limitations of traditional crash-based studies due to data sparsity and latency.  
2. It proposes a novel multi-camera computer vision framework that performs real-time safety assessment by computing Post-Encroachment Time (PET), demonstrated at the H Street and Broadway intersection in Chula Vista, California.  
3. The system uses four synchronized cameras providing continuous coverage, with each frame processed on NVIDIA Jetson AGX Xavier edge devices using YOLOv11 segmentation to detect vehicle polygons.  
4. These detected vehicles are mapped to a unified bird's-eye view through homography matrices to enable alignment across overlapping camera views.  
5. The authors developed a pixel-level PET algorithm that avoids fixed cell reliance, enabling fine-grained, accurate hazard visualization via dynamic heatmaps with a spatial resolution of 3.3 sq-cm.  
6. Timestamped vehicle and PET data are stored in an SQL database, supporting long-term monitoring.  
7. Experimental results show the framework can identify high-risk regions with sub-second precision and sustain real-time throughput (~2.68 FPS) on edge devices.  
8. The system generates an 800 x 800 pixel logarithmic heatmap for hazard visualization.  
9. Overall, the study validates a decentralized, scalable, and replicable methodology for high-resolution real-time intersection safety evaluation using vision-based PET analysis. <div>
arXiv:2511.12018v1 Announce Type: new 
Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2511.12020</link>
<guid>https://arxiv.org/abs/2511.12020</guid>
<content:encoded><![CDATA[
<div> Weakly-Supervised Referring Expression Comprehension, Generalized Referring Expression, Hyperbolic Geometry, Euclidean Similarity, Hybrid Similarity Module  

<br /><br />Summary:  
This paper introduces the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC) to address limitations of existing WREC methods that assume a one-to-one mapping between expressions and targets. WGREC allows expressions to refer to zero, one, or multiple objects, reflecting more realistic scenarios. Two key challenges arise in WGREC: supervisory signal ambiguity caused by weak image-level supervision, and semantic representation collapse due to Euclidean similarity clustering related concepts indistinctly. To overcome these, the authors propose LIHE, a two-stage framework comprising Referential Decoupling and Referent Grounding. The first stage predicts the number of referents and splits complex expressions into simpler sub-expressions. The second stage uses HEMix, a novel hybrid similarity module that combines Euclidean proximity for precise alignment with hyperbolic geometry to capture hierarchical relationships, preventing semantic collapse while maintaining fine-grained distinctions. LIHE establishes the first effective weakly supervised WGREC baseline on datasets gRefCOCO and Ref-ZOM. Additionally, the HEMix module consistently improves performance on standard REC benchmarks, enhancing IoU@0.5 scores by up to 2.5%. The source code is publicly available, facilitating further research in generalized referring expression comprehension under weak supervision. <div>
arXiv:2511.12020v1 Announce Type: new 
Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging</title>
<link>https://arxiv.org/abs/2511.12024</link>
<guid>https://arxiv.org/abs/2511.12024</guid>
<content:encoded><![CDATA[
<div> Keywords: lensless imaging, diffusion priors, null-space diffusion distillation, photorealistic reconstruction, ground-truth-free<br /><br />Summary:<br /><br />This paper addresses photorealistic image reconstruction in lensless cameras, highlighting limitations of current methods that rely on paired lensless-lensed supervision, which can cause biases due to domain mismatch. To overcome this, the authors focus on ground-truth-free diffusion priors, but observe that typical diffusion formulations fail under the noisy and ill-posed conditions of lensless deconvolution. They propose separating enforcement in range-space from null-space diffusion-prior updates as a solution to achieve stable, realistic reconstructions. Building on this insight, the paper introduces Null-Space Diffusion Distillation (NSDD), a novel single-pass model that distills the null-space component from an iterative DDNM+ solver, conditioned on lensless measurements and a range-space anchor. NSDD effectively maintains measurement consistency while producing photorealistic results without needing paired supervision. Experimental results on Lensless-FFHQ and PhlatCam datasets show NSDD is highly efficient, ranking second in speed after Wiener filtering, and offers near-teacher-level perceptual quality, outperforming other methods like DPS and classical convex approaches. These findings demonstrate a practical and faster approach for high-quality, ground-truth-free lensless imaging reconstruction. <div>
arXiv:2511.12024v1 Announce Type: new 
Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2511.12026</link>
<guid>https://arxiv.org/abs/2511.12026</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical tracking, multimodal dataset, point status, text-guided tracking, computer-assisted surgery  

<br /><br />Summary:  
Accurate point tracking in surgical environments is particularly difficult due to complex visual interferences such as smoke occlusion, specular reflections, and tissue deformation. Existing surgical tracking datasets often contain coordinate data but lack semantic context needed to analyze and understand tracking failures. To address this gap, the authors introduce VL-SurgPT, the first large-scale multimodal dataset combining visual tracking data with textual descriptions of point status in surgical scenes. The dataset includes 908 in vivo video clips, with 754 clips dedicated to tissue tracking involving 17,171 annotated points across five challenging scenarios, and 154 clips for instrument tracking covering seven instrument types with detailed keypoint annotations. The study benchmarks eight state-of-the-art tracking methods on this dataset and proposes TG-SurgPT, a novel text-guided tracking approach that leverages semantic descriptions to boost tracking robustness under difficult visual conditions. Experimental results show that integrating point status information significantly improves tracking accuracy and reliability, especially in adverse conditions where traditional vision-only methods underperform. By connecting visual and linguistic modalities, VL-SurgPT facilitates the development of context-aware tracking systems that are critical for advancing computer-assisted surgery technologies capable of sustaining high performance during complex intraoperative scenarios. <div>
arXiv:2511.12026v1 Announce Type: new 
Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</title>
<link>https://arxiv.org/abs/2511.12027</link>
<guid>https://arxiv.org/abs/2511.12027</guid>
<content:encoded><![CDATA[
<div> Long-video understanding, Multimodal Large Language Models, Global-Context-Aware Agent, Episodic Memory, Video-MME benchmark<br /><br />Summary: Long-video understanding challenges existing Multimodal Large Language Models (MLLMs) due to token limitations and difficulty in capturing long-term temporal dependencies. To overcome these issues, the paper introduces GCAgent, a novel Global-Context-Aware Agent framework designed for comprehensive understanding of long videos. The core innovation lies in the Schematic and Narrative Episodic Memory, which structurally models events along with their causal and temporal relationships into an organized context, effectively resolving the problem of long-term dependency. GCAgent operates through a multi-stage Perception-Action-Reflection cycle, leveraging a Memory Manager that retrieves relevant episodic contexts to enable robust and context-aware inference. Extensive experiments demonstrate that GCAgent significantly improves long-video understanding, with up to a 23.5% accuracy increase on the Video-MME Long split compared to a strong MLLM baseline. Additionally, the method achieves state-of-the-art performance among comparable 7B-scale MLLMs, attaining 73.4% accuracy on the Long split and achieving the highest overall average accuracy of 71.9% on the Video-MME benchmark. These results validate the effectiveness of the agent-based reasoning paradigm and the structured memory approach for cognitively inspired long-video understanding. <div>
arXiv:2511.12027v1 Announce Type: new 
Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation</title>
<link>https://arxiv.org/abs/2511.12030</link>
<guid>https://arxiv.org/abs/2511.12030</guid>
<content:encoded><![CDATA[
<div> hand-object pose estimation, 3D pose, visual-physical cues, candidate pose aggregation, physical plausibility<br /><br />Summary:<br /><br />Estimating the 3D poses of hands and objects from a single RGB image is a challenging task critical for applications in augmented reality and human-computer interaction. Existing methods primarily focus on visual cues, which often lead to physically implausible results such as interpenetration or lack of realistic contact. Prior attempts to incorporate physics reasoning generally rely on post-optimization steps or non-differentiable physics engines, which hinder visual consistency and prevent end-to-end training. To address these issues, the authors propose a novel framework that jointly integrates both visual and physical cues for enhanced hand-object pose estimation. The framework features two main innovations: first, joint visual-physical cue learning, where the model simultaneously extracts 2D visual features and 3D physical cues for richer representation of interactions; second, candidate pose aggregation, a refinement process that consolidates multiple diffusion-generated pose candidates using both visual and physical information to produce a final pose estimate that is consistent with both appearance and physics. Experimental results demonstrate that this method significantly improves over current state-of-the-art techniques in terms of accuracy and physical realism of the estimated poses. <div>
arXiv:2511.12030v1 Announce Type: new 
Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Masked Image Generation with Knowledge-Augmented Token Representations</title>
<link>https://arxiv.org/abs/2511.12032</link>
<guid>https://arxiv.org/abs/2511.12032</guid>
<content:encoded><![CDATA[
<div> Masked image generation, semantic dependencies, knowledge graphs, token-level representation, image synthesis<br /><br />Summary:<br /><br />This paper introduces KA-MIG, a Knowledge-Augmented Masked Image Generation framework designed to improve masked image generation (MIG) by incorporating explicit token-level semantic knowledge. Existing MIG methods struggle to learn semantic dependencies effectively because visual tokens individually lack clear meanings, and sequences are often long. To overcome this, KA-MIG integrates prior knowledge through three types of token knowledge graphs: a co-occurrence graph and a semantic similarity graph (positive graphs), as well as a position-token incompatibility graph (negative graph). These graphs provide rich, structured semantic dependencies extracted from training data. The framework employs a graph-aware encoder to utilize these prior knowledge graphs to enhance the learning of token and position-aware representations. A lightweight fusion mechanism then combines these enriched representations with existing MIG models. By leveraging such explicit semantic priors, KA-MIG significantly improves the model’s ability to capture semantic relationships between tokens, leading to higher fidelity and more coherent image generation. Experimental evaluations on class-conditional image generation tasks using the ImageNet dataset demonstrate that KA-MIG outperforms current MIG methods, confirming the benefits of knowledge augmentation in masked image generation. <div>
arXiv:2511.12032v1 Announce Type: new 
Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Multimodal Representation Learning with Missing Modalities</title>
<link>https://arxiv.org/abs/2511.12034</link>
<guid>https://arxiv.org/abs/2511.12034</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, representation alignment, missing modalities, anchor shift, CalMRL<br /><br />Summary:<br /><br />1. This paper addresses the challenge in multimodal representation learning where traditional methods require all modalities to be present, limiting the use of datasets with missing data.<br /><br />2. The authors introduce a theoretical perspective called the anchor shift, where observed modalities align with a local anchor that shifts from the optimal alignment achievable when all modalities are available.<br /><br />3. To correct this misalignment, they propose CalMRL, a method that calibrates incomplete alignments by modeling the imputation of missing modalities using prior knowledge and inter-modal relationships at the representation level.<br /><br />4. CalMRL employs a bi-step learning approach with a closed-form solution for the posterior distribution of shared latent variables, resolving optimization challenges.<br /><br />5. The paper provides theoretical validation of convergence and anchor shift mitigation, alongside extensive experiments demonstrating CalMRL’s superior performance and enhanced flexibility to incorporate incomplete multimodal data, which was previously unmanageable with existing methods. <div>
arXiv:2511.12034v1 Announce Type: new 
Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images</title>
<link>https://arxiv.org/abs/2511.12040</link>
<guid>https://arxiv.org/abs/2511.12040</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, low-resolution images, reference-guided feature enhancement, Gaussian primitives, texture-aware density control  

<br /><br />Summary:  
The paper introduces SRSplat, a novel feed-forward framework designed for high-resolution 3D scene reconstruction from sparse, low-resolution (LR) input images, addressing challenges commonly faced in autonomous driving and embodied AI applications. Existing methods struggle with recovering fine texture details from LR inputs due to missing high-frequency information. SRSplat overcomes this by jointly utilizing both external high-quality reference images and internal texture cues. A scene-specific reference gallery is constructed per scene using Multimodal Large Language Models (MLLMs) and diffusion models, serving as the external high-quality reference. The core innovation includes the Reference-Guided Feature Enhancement (RGFE) module, which aligns and fuses features from the LR images and their corresponding reference images to enhance texture detail. A decoder is then trained to predict Gaussian primitives based on the fused multi-view features. To improve reconstruction fidelity, the Texture-Aware Density Control (TADC) module adaptively adjusts Gaussian density based on the texture richness present in the LR inputs. Extensive experiments on datasets such as RealEstate10K, ACID, and DTU demonstrate that SRSplat surpasses existing methods, showing robust cross-dataset and cross-resolution generalization, making it highly effective for practical low-resolution 3D reconstruction scenarios. <div>
arXiv:2511.12040v1 Announce Type: new 
Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification</title>
<link>https://arxiv.org/abs/2511.12044</link>
<guid>https://arxiv.org/abs/2511.12044</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, non-IID data, histopathological images, diffusion models, stain distribution alignment  

<br /><br />Summary:  
This paper addresses the challenge of non-IID (non-independent and identically distributed) data in federated learning (FL), specifically focusing on histopathological images which suffer from feature distribution shifts. Unlike many prior works that tackle non-IID issues through model updates, the authors propose a novel method called Federated Stain Distribution Alignment (FedSDA) that aims to normalize data distributions across clients. FedSDA leverages diffusion models, known for their capability to fit complex data distributions, combined with stain separation techniques to extract critical features related to the non-IID properties of histopathological data. By aligning the stain distributions of all clients with a common target distribution in the FL framework, the method reduces heterogeneity and improves learning performance. Importantly, FedSDA avoids the privacy leakage risks associated with training diffusion models on raw client data by adopting an alternative alignment strategy that protects sensitive information. Extensive experiments demonstrate that FedSDA outperforms existing baseline methods that either focus on model update strategies or address non-IID data from a distributional perspective. The method offers practical insights and valuable improvements for computational pathology applications, contributing to more robust and privacy-preserving federated learning in medical imaging contexts. <div>
arXiv:2511.12044v1 Announce Type: new 
Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging</title>
<link>https://arxiv.org/abs/2511.12047</link>
<guid>https://arxiv.org/abs/2511.12047</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformer, medical image analysis, Degree-Corrected Mixed-Membership model, self-attention, interpretability

<br /><br />Summary:  
The paper introduces DCMM-Transformer, a novel Vision Transformer (ViT) architecture designed specifically for medical image analysis. Unlike existing ViTs that overlook latent anatomical groupings such as organs, tissues, and pathological regions, this approach integrates a Degree-Corrected Mixed-Membership (DCMM) model directly into self-attention as an additive bias. Prior methods like SBM-Transformer employed stochastic binary masking, which caused issues including non-differentiability, unstable training, and inability to flexibly capture complex community structures. In contrast, DCMM-Transformer ensures full differentiability and interpretable modeling of community structures and degree heterogeneity. The effectiveness of the approach is validated through comprehensive experiments on multiple diverse medical imaging datasets—including brain, chest, breast, and ocular images—where it demonstrated superior performance and generalization. Additionally, the learned group structures and the structured modulation of attention enhance model interpretability by producing attention maps that are both anatomically meaningful and semantically coherent. This improvement in interpretability is particularly valuable for clinical applications where understanding model decision-making is critical. Overall, DCMM-Transformer represents a significant advance for ViT architectures in medical imaging by embedding domain-relevant structures in a mathematically principled and practical way. <div>
arXiv:2511.12047v1 Announce Type: new 
Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training</title>
<link>https://arxiv.org/abs/2511.12048</link>
<guid>https://arxiv.org/abs/2511.12048</guid>
<content:encoded><![CDATA[
<div> Deepfakes, DeiT, progressive training, data augmentation, deepfake detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting deepfakes, which pose significant risks to digital media integrity. 2. It introduces DeiTFake, a detection model based on the DeiT transformer architecture, coupled with a novel two-stage progressive training strategy that increases augmentation complexity over time. 3. The training process begins with a transfer-learning phase using standard data augmentations, followed by a fine-tuning phase that incorporates advanced affine and deepfake-specific augmentations to better capture manipulation cues. 4. DeiT's built-in knowledge distillation mechanism helps in identifying subtle artifacts from manipulated images, thereby improving detection robustness. 5. Evaluated on the large-scale OpenForensics dataset comprising 190,335 images, DeiTFake achieves impressive accuracy scores of 98.71% after the first training stage and 99.22% after fine-tuning, with an AUROC of 0.9997, outperforming existing baselines. 6. The study also conducts detailed analyses on the impact of different augmentations and training schedules, providing practical insights and benchmarks for future research in facial deepfake detection. <div>
arXiv:2511.12048v1 Announce Type: new 
Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization</title>
<link>https://arxiv.org/abs/2511.12054</link>
<guid>https://arxiv.org/abs/2511.12054</guid>
<content:encoded><![CDATA[
<div> Cross-view geo-localization, unsupervised learning, adversarial bridging, graph calibration, pseudo-label refinement  

<br /><br />Summary:  
This paper addresses the challenging task of cross-view geo-localization (CVGL), which involves matching images taken from different viewpoints, such as drone and satellite images. Traditional supervised approaches deliver strong results but require extensive pairwise annotations, limiting their practicality. To overcome this, the authors propose UniABG, a novel dual-stage unsupervised framework designed to enhance scalability without labeled data. UniABG integrates two key components: View-Aware Adversarial Bridging (VAAB) and Heterogeneous Graph Filtering Calibration (HGFC). VAAB aims to learn view-invariant feature representations and improve the reliability of pseudo-labels through adversarial training, reducing the domain gap between views. HGFC further refines the cross-view matches by building heterogeneous graphs that capture structural relationships, thus calibrating and validating correspondences more effectively. Experimental results on benchmark datasets University-1652 and SUES-200 demonstrate that UniABG significantly improves average precision (AP) in unsupervised settings by +10.63% and +16.73% respectively, even outperforming some supervised baselines. The approach represents a significant advancement in unsupervised CVGL by addressing noisy pseudo-label issues and enhancing cross-view feature alignment. The authors also provide an open-source codebase for reproducibility and future research. <div>
arXiv:2511.12054v1 Announce Type: new 
Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling</title>
<link>https://arxiv.org/abs/2511.12056</link>
<guid>https://arxiv.org/abs/2511.12056</guid>
<content:encoded><![CDATA[
<div> Video generation, Diffusion transformer, Pipelining, GPU parallelism, Latency reduction<br /><br />Summary: This paper addresses the challenges of slow inference speeds and high memory consumption in diffusion transformer (DiT) based video generation models. It introduces PipeDiT, a novel pipelining framework designed to accelerate video generation through three main innovations. First, PipeSP, a pipelining algorithm for sequence parallelism, enables overlapping computation of latent generation and communication across multiple GPUs to reduce inference latency. Second, the authors propose DeDiVAE, which separates the diffusion and variational autoencoder (VAE) modules into two distinct GPU groups, allowing parallel execution that reduces both memory usage and latency. Third, an attention co-processing (Aco) technique is introduced to better utilize GPU resources within the VAE group, further decreasing video generation time. PipeDiT was integrated into OpenSoraPlan and HunyuanVideo, two leading open-source video generation frameworks. Extensive experiments on two 8-GPU systems demonstrate that PipeDiT achieves speedups ranging from 1.06x to 4.02x across varying resolution and timestep settings compared to these state-of-the-art methods. This work thus presents an effective approach to improve practical deployment of DiT-based video generation through advanced GPU pipelining and resource optimization. <div>
arXiv:2511.12056v1 Announce Type: new 
Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity</title>
<link>https://arxiv.org/abs/2511.12061</link>
<guid>https://arxiv.org/abs/2511.12061</guid>
<content:encoded><![CDATA[
<div> Trajectory similarity, contrastive learning, hierarchical representation, GPS augmentation, computational efficiency<br /><br />Summary:<br /><br />Trajectory similarity computation is essential for tasks such as clustering, prediction, and anomaly detection. Existing learning-based approaches have three main limitations: they insufficiently model both movement dynamics and multi-scale hierarchical structure; rely on costly point-wise encoding; and utilize augmentations that distort the physical meaning of trajectories. To overcome these challenges, the paper proposes MovSemCL, a contrastive learning framework focusing on movement semantics for trajectory similarity. MovSemCL converts raw GPS trajectories into movement-semantic features and segments them into patches. It uses intra- and inter-patch attentions to capture both local and global patterns efficiently, resulting in hierarchical representation while reducing computational costs. The framework incorporates a curvature-guided augmentation strategy that preserves important segments like turns and intersections and masks redundant parts, ensuring physically plausible augmented views. Experiments conducted on real-world datasets demonstrate that MovSemCL outperforms state-of-the-art methods, achieving mean ranks near the ideal value of 1 in similarity search tasks. It also improves heuristic approximation accuracy by up to 20.3% and decreases inference latency by up to 43.4%, highlighting its effectiveness and efficiency. <div>
arXiv:2511.12061v1 Announce Type: new 
Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal</title>
<link>https://arxiv.org/abs/2511.12066</link>
<guid>https://arxiv.org/abs/2511.12066</guid>
<content:encoded><![CDATA[
<div> Purple fringing, Longitudinal Chromatic Aberration, deep learning, Chromatic-Aware Coordinate Transformation, 5D Look-Up Table  
  
<br /><br />Summary:  
The paper addresses the problem of purple fringing, a common lens artifact caused by Longitudinal Chromatic Aberration (LCA) that negatively impacts image clarity. Traditional methods rely on expensive apochromatic (APO) lenses and handcrafted feature extraction, lacking data-driven solutions. To overcome these limitations, the authors propose DCA-LUT, the first deep learning-based framework specifically designed for purple fringing removal. Central to this framework is a novel Chromatic-Aware Coordinate Transformation (CA-CT) module that learns an adaptive color space, effectively isolating the purple fringe effects into a dedicated channel. This isolation facilitates the precise learning of the "purple fringe channel," which subsequently guides the restoration of the luminance channel for accurate image correction. The approach includes applying a 5D Look-Up Table (5D LUT) for nonlinear color mapping, enabling efficient and robust color correction. To support training and evaluation, the authors created PF-Synth, a large-scale synthetic purple fringing dataset. Extensive experiments conducted on both synthetic and real-world images demonstrate that DCA-LUT achieves state-of-the-art performance, outperforming existing methods in removing purple fringing artifacts while maintaining image fidelity and visual quality. <div>
arXiv:2511.12066v1 Announce Type: new 
Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound</title>
<link>https://arxiv.org/abs/2511.12077</link>
<guid>https://arxiv.org/abs/2511.12077</guid>
<content:encoded><![CDATA[
<div> emotion understanding, audio-visual models, large language models, art emotion, cross-modal learning  

<br /><br />Summary: This paper addresses the challenge of emotion understanding in Large Language Models (LLMs), emphasizing the importance of interpreting emotion conveyed by art through both visual and auditory elements. Existing methods largely focus on human-centered or single-modality approaches and rely heavily on extensive audio pretraining, limiting their scalability. The authors propose VAEmotionLLM, a novel two-stage framework that enables a Visual Language Model (VLM) to "hear by seeing" with minimal audio pretraining and to comprehend emotion across different modalities. Stage 1, called Vision-Guided Audio Alignment (VG-Align), distills a frozen visual pathway into a new audio pathway by aligning next-token distributions from synchronized audio-video clips, thus circumventing the need for massive audio datasets. Stage 2 introduces the Cross-Modal Emotion Adapter (EmoAdapter), which includes the Emotion Enhancer and Emotion Supervisor components to inject emotion-sensitive adjustments and apply emotion supervision, improving cross-modal emotion understanding. The paper also presents ArtEmoBenchmark, a new art-centric dataset designed to evaluate emotion understanding in audio-only, visual-only, and audio-visual conditions. Experiments demonstrate that VAEmotionLLM achieves state-of-the-art performance across all these settings, and ablation studies confirm the complementary benefits of its proposed components. <div>
arXiv:2511.12077v1 Announce Type: new 
Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point Cloud Quantization through Multimodal Prompting for 3D Understanding</title>
<link>https://arxiv.org/abs/2511.12079</link>
<guid>https://arxiv.org/abs/2511.12079</guid>
<content:encoded><![CDATA[
arXiv:2511.12079v1 Announce Type: new 
Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning</title>
<link>https://arxiv.org/abs/2511.12082</link>
<guid>https://arxiv.org/abs/2511.12082</guid>
<content:encoded><![CDATA[
arXiv:2511.12082v1 Announce Type: new 
Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving</title>
<link>https://arxiv.org/abs/2511.12084</link>
<guid>https://arxiv.org/abs/2511.12084</guid>
<content:encoded><![CDATA[
arXiv:2511.12084v1 Announce Type: new 
Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning</title>
<link>https://arxiv.org/abs/2511.12090</link>
<guid>https://arxiv.org/abs/2511.12090</guid>
<content:encoded><![CDATA[
arXiv:2511.12090v1 Announce Type: new 
Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio</title>
<link>https://arxiv.org/abs/2511.12095</link>
<guid>https://arxiv.org/abs/2511.12095</guid>
<content:encoded><![CDATA[
arXiv:2511.12095v1 Announce Type: new 
Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2511.12097</link>
<guid>https://arxiv.org/abs/2511.12097</guid>
<content:encoded><![CDATA[
arXiv:2511.12097v1 Announce Type: new 
Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT</title>
<link>https://arxiv.org/abs/2511.12098</link>
<guid>https://arxiv.org/abs/2511.12098</guid>
<content:encoded><![CDATA[
arXiv:2511.12098v1 Announce Type: new 
Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12099</link>
<guid>https://arxiv.org/abs/2511.12099</guid>
<content:encoded><![CDATA[
arXiv:2511.12099v1 Announce Type: new 
Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation</title>
<link>https://arxiv.org/abs/2511.12100</link>
<guid>https://arxiv.org/abs/2511.12100</guid>
<content:encoded><![CDATA[
arXiv:2511.12100v1 Announce Type: new 
Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation</title>
<link>https://arxiv.org/abs/2511.12103</link>
<guid>https://arxiv.org/abs/2511.12103</guid>
<content:encoded><![CDATA[
arXiv:2511.12103v1 Announce Type: new 
Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery</title>
<link>https://arxiv.org/abs/2511.12104</link>
<guid>https://arxiv.org/abs/2511.12104</guid>
<content:encoded><![CDATA[
arXiv:2511.12104v1 Announce Type: new 
Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection</title>
<link>https://arxiv.org/abs/2511.12107</link>
<guid>https://arxiv.org/abs/2511.12107</guid>
<content:encoded><![CDATA[
arXiv:2511.12107v1 Announce Type: new 
Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images</title>
<link>https://arxiv.org/abs/2511.12110</link>
<guid>https://arxiv.org/abs/2511.12110</guid>
<content:encoded><![CDATA[
arXiv:2511.12110v1 Announce Type: new 
Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.12117</link>
<guid>https://arxiv.org/abs/2511.12117</guid>
<content:encoded><![CDATA[
arXiv:2511.12117v1 Announce Type: new 
Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description</title>
<link>https://arxiv.org/abs/2511.12131</link>
<guid>https://arxiv.org/abs/2511.12131</guid>
<content:encoded><![CDATA[
arXiv:2511.12131v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware</title>
<link>https://arxiv.org/abs/2511.12136</link>
<guid>https://arxiv.org/abs/2511.12136</guid>
<content:encoded><![CDATA[
arXiv:2511.12136v1 Announce Type: new 
Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.12142</link>
<guid>https://arxiv.org/abs/2511.12142</guid>
<content:encoded><![CDATA[
arXiv:2511.12142v1 Announce Type: new 
Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain</title>
<link>https://arxiv.org/abs/2511.12150</link>
<guid>https://arxiv.org/abs/2511.12150</guid>
<content:encoded><![CDATA[
arXiv:2511.12150v1 Announce Type: new 
Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing</title>
<link>https://arxiv.org/abs/2511.12151</link>
<guid>https://arxiv.org/abs/2511.12151</guid>
<content:encoded><![CDATA[
arXiv:2511.12151v1 Announce Type: new 
Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function</title>
<link>https://arxiv.org/abs/2511.12162</link>
<guid>https://arxiv.org/abs/2511.12162</guid>
<content:encoded><![CDATA[
arXiv:2511.12162v1 Announce Type: new 
Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective</title>
<link>https://arxiv.org/abs/2511.12170</link>
<guid>https://arxiv.org/abs/2511.12170</guid>
<content:encoded><![CDATA[
arXiv:2511.12170v1 Announce Type: new 
Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixAR: Mixture Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2511.12181</link>
<guid>https://arxiv.org/abs/2511.12181</guid>
<content:encoded><![CDATA[
arXiv:2511.12181v1 Announce Type: new 
Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis</title>
<link>https://arxiv.org/abs/2511.12193</link>
<guid>https://arxiv.org/abs/2511.12193</guid>
<content:encoded><![CDATA[
arXiv:2511.12193v1 Announce Type: new 
Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System</title>
<link>https://arxiv.org/abs/2511.12196</link>
<guid>https://arxiv.org/abs/2511.12196</guid>
<content:encoded><![CDATA[
arXiv:2511.12196v1 Announce Type: new 
Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&amp;Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation</title>
<link>https://arxiv.org/abs/2511.12200</link>
<guid>https://arxiv.org/abs/2511.12200</guid>
<content:encoded><![CDATA[
arXiv:2511.12200v1 Announce Type: new 
Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs</title>
<link>https://arxiv.org/abs/2511.12201</link>
<guid>https://arxiv.org/abs/2511.12201</guid>
<content:encoded><![CDATA[
arXiv:2511.12201v1 Announce Type: new 
Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image</title>
<link>https://arxiv.org/abs/2511.12202</link>
<guid>https://arxiv.org/abs/2511.12202</guid>
<content:encoded><![CDATA[
arXiv:2511.12202v1 Announce Type: new 
Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction</title>
<link>https://arxiv.org/abs/2511.12204</link>
<guid>https://arxiv.org/abs/2511.12204</guid>
<content:encoded><![CDATA[
arXiv:2511.12204v1 Announce Type: new 
Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR</title>
<link>https://arxiv.org/abs/2511.12206</link>
<guid>https://arxiv.org/abs/2511.12206</guid>
<content:encoded><![CDATA[
arXiv:2511.12206v1 Announce Type: new 
Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</title>
<link>https://arxiv.org/abs/2511.12207</link>
<guid>https://arxiv.org/abs/2511.12207</guid>
<content:encoded><![CDATA[
arXiv:2511.12207v1 Announce Type: new 
Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\epsilon$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention</title>
<link>https://arxiv.org/abs/2511.12215</link>
<guid>https://arxiv.org/abs/2511.12215</guid>
<content:encoded><![CDATA[
arXiv:2511.12215v1 Announce Type: new 
Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Suppressing VLM Hallucinations with Spectral Representation Filtering</title>
<link>https://arxiv.org/abs/2511.12220</link>
<guid>https://arxiv.org/abs/2511.12220</guid>
<content:encoded><![CDATA[
arXiv:2511.12220v1 Announce Type: new 
Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Inversion Attack Against Deep Hashing</title>
<link>https://arxiv.org/abs/2511.12233</link>
<guid>https://arxiv.org/abs/2511.12233</guid>
<content:encoded><![CDATA[
arXiv:2511.12233v1 Announce Type: new 
Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets</title>
<link>https://arxiv.org/abs/2511.12255</link>
<guid>https://arxiv.org/abs/2511.12255</guid>
<content:encoded><![CDATA[
arXiv:2511.12255v1 Announce Type: new 
Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment</title>
<link>https://arxiv.org/abs/2511.12256</link>
<guid>https://arxiv.org/abs/2511.12256</guid>
<content:encoded><![CDATA[
arXiv:2511.12256v1 Announce Type: new 
Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation</title>
<link>https://arxiv.org/abs/2511.12259</link>
<guid>https://arxiv.org/abs/2511.12259</guid>
<content:encoded><![CDATA[
arXiv:2511.12259v1 Announce Type: new 
Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12263</link>
<guid>https://arxiv.org/abs/2511.12263</guid>
<content:encoded><![CDATA[
arXiv:2511.12263v1 Announce Type: new 
Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks</title>
<link>https://arxiv.org/abs/2511.12267</link>
<guid>https://arxiv.org/abs/2511.12267</guid>
<content:encoded><![CDATA[
arXiv:2511.12267v1 Announce Type: new 
Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.12270</link>
<guid>https://arxiv.org/abs/2511.12270</guid>
<content:encoded><![CDATA[
arXiv:2511.12270v1 Announce Type: new 
Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</title>
<link>https://arxiv.org/abs/2511.12280</link>
<guid>https://arxiv.org/abs/2511.12280</guid>
<content:encoded><![CDATA[
arXiv:2511.12280v1 Announce Type: new 
Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.12291</link>
<guid>https://arxiv.org/abs/2511.12291</guid>
<content:encoded><![CDATA[
arXiv:2511.12291v1 Announce Type: new 
Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method</title>
<link>https://arxiv.org/abs/2511.12301</link>
<guid>https://arxiv.org/abs/2511.12301</guid>
<content:encoded><![CDATA[
arXiv:2511.12301v1 Announce Type: new 
Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors</title>
<link>https://arxiv.org/abs/2511.12304</link>
<guid>https://arxiv.org/abs/2511.12304</guid>
<content:encoded><![CDATA[
arXiv:2511.12304v1 Announce Type: new 
Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Time in Static Classifiers</title>
<link>https://arxiv.org/abs/2511.12321</link>
<guid>https://arxiv.org/abs/2511.12321</guid>
<content:encoded><![CDATA[
arXiv:2511.12321v1 Announce Type: new 
Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.12331</link>
<guid>https://arxiv.org/abs/2511.12331</guid>
<content:encoded><![CDATA[
arXiv:2511.12331v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground Plane Projection for Improved Traffic Analytics at Intersections</title>
<link>https://arxiv.org/abs/2511.12342</link>
<guid>https://arxiv.org/abs/2511.12342</guid>
<content:encoded><![CDATA[
arXiv:2511.12342v1 Announce Type: new 
Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[
arXiv:2511.12346v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI-Generated Image Detection RewardBench</title>
<link>https://arxiv.org/abs/2511.12363</link>
<guid>https://arxiv.org/abs/2511.12363</guid>
<content:encoded><![CDATA[
arXiv:2511.12363v1 Announce Type: new 
Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.12365</link>
<guid>https://arxiv.org/abs/2511.12365</guid>
<content:encoded><![CDATA[
arXiv:2511.12365v1 Announce Type: new 
Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Reasoning Segmentation for Images and Videos</title>
<link>https://arxiv.org/abs/2511.12368</link>
<guid>https://arxiv.org/abs/2511.12368</guid>
<content:encoded><![CDATA[
arXiv:2511.12368v1 Announce Type: new 
Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Changes in Real Time: Online Scene Change Detection with Multi-View Fusion</title>
<link>https://arxiv.org/abs/2511.12370</link>
<guid>https://arxiv.org/abs/2511.12370</guid>
<content:encoded><![CDATA[
arXiv:2511.12370v1 Announce Type: new 
Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models</title>
<link>https://arxiv.org/abs/2511.12371</link>
<guid>https://arxiv.org/abs/2511.12371</guid>
<content:encoded><![CDATA[
arXiv:2511.12371v1 Announce Type: new 
Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification</title>
<link>https://arxiv.org/abs/2511.12382</link>
<guid>https://arxiv.org/abs/2511.12382</guid>
<content:encoded><![CDATA[
arXiv:2511.12382v1 Announce Type: new 
Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Quantum-Based Architectures for Robust Diagnostics</title>
<link>https://arxiv.org/abs/2511.12386</link>
<guid>https://arxiv.org/abs/2511.12386</guid>
<content:encoded><![CDATA[
arXiv:2511.12386v1 Announce Type: new 
Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.12389</link>
<guid>https://arxiv.org/abs/2511.12389</guid>
<content:encoded><![CDATA[
arXiv:2511.12389v1 Announce Type: new 
Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting</title>
<link>https://arxiv.org/abs/2511.12400</link>
<guid>https://arxiv.org/abs/2511.12400</guid>
<content:encoded><![CDATA[
arXiv:2511.12400v1 Announce Type: new 
Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.12405</link>
<guid>https://arxiv.org/abs/2511.12405</guid>
<content:encoded><![CDATA[
arXiv:2511.12405v1 Announce Type: new 
Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection</title>
<link>https://arxiv.org/abs/2511.12410</link>
<guid>https://arxiv.org/abs/2511.12410</guid>
<content:encoded><![CDATA[
arXiv:2511.12410v1 Announce Type: new 
Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Rotation-only Imaging Geometry: Rotation Estimation</title>
<link>https://arxiv.org/abs/2511.12415</link>
<guid>https://arxiv.org/abs/2511.12415</guid>
<content:encoded><![CDATA[
arXiv:2511.12415v1 Announce Type: new 
Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance</title>
<link>https://arxiv.org/abs/2511.12419</link>
<guid>https://arxiv.org/abs/2511.12419</guid>
<content:encoded><![CDATA[
arXiv:2511.12419v1 Announce Type: new 
Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation</title>
<link>https://arxiv.org/abs/2511.12422</link>
<guid>https://arxiv.org/abs/2511.12422</guid>
<content:encoded><![CDATA[
arXiv:2511.12422v1 Announce Type: new 
Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning</title>
<link>https://arxiv.org/abs/2511.12428</link>
<guid>https://arxiv.org/abs/2511.12428</guid>
<content:encoded><![CDATA[
arXiv:2511.12428v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion</title>
<link>https://arxiv.org/abs/2511.12432</link>
<guid>https://arxiv.org/abs/2511.12432</guid>
<content:encoded><![CDATA[
arXiv:2511.12432v1 Announce Type: new 
Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning</title>
<link>https://arxiv.org/abs/2511.12438</link>
<guid>https://arxiv.org/abs/2511.12438</guid>
<content:encoded><![CDATA[
arXiv:2511.12438v1 Announce Type: new 
Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training</title>
<link>https://arxiv.org/abs/2511.12446</link>
<guid>https://arxiv.org/abs/2511.12446</guid>
<content:encoded><![CDATA[
arXiv:2511.12446v1 Announce Type: new 
Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding</title>
<link>https://arxiv.org/abs/2511.12449</link>
<guid>https://arxiv.org/abs/2511.12449</guid>
<content:encoded><![CDATA[
arXiv:2511.12449v1 Announce Type: new 
Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions</title>
<link>https://arxiv.org/abs/2511.12452</link>
<guid>https://arxiv.org/abs/2511.12452</guid>
<content:encoded><![CDATA[
arXiv:2511.12452v1 Announce Type: new 
Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Layout: LLM-driven Co-optimization for Interior Layout</title>
<link>https://arxiv.org/abs/2511.12474</link>
<guid>https://arxiv.org/abs/2511.12474</guid>
<content:encoded><![CDATA[
arXiv:2511.12474v1 Announce Type: new 
Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning</title>
<link>https://arxiv.org/abs/2511.12480</link>
<guid>https://arxiv.org/abs/2511.12480</guid>
<content:encoded><![CDATA[
arXiv:2511.12480v1 Announce Type: new 
Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2511.12498</link>
<guid>https://arxiv.org/abs/2511.12498</guid>
<content:encoded><![CDATA[
arXiv:2511.12498v1 Announce Type: new 
Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visible Structure Retrieval for Lightweight Image-Based Relocalisation</title>
<link>https://arxiv.org/abs/2511.12503</link>
<guid>https://arxiv.org/abs/2511.12503</guid>
<content:encoded><![CDATA[
arXiv:2511.12503v1 Announce Type: new 
Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2511.12511</link>
<guid>https://arxiv.org/abs/2511.12511</guid>
<content:encoded><![CDATA[
arXiv:2511.12511v1 Announce Type: new 
Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics</title>
<link>https://arxiv.org/abs/2511.12525</link>
<guid>https://arxiv.org/abs/2511.12525</guid>
<content:encoded><![CDATA[
arXiv:2511.12525v1 Announce Type: new 
Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</title>
<link>https://arxiv.org/abs/2511.12528</link>
<guid>https://arxiv.org/abs/2511.12528</guid>
<content:encoded><![CDATA[
arXiv:2511.12528v1 Announce Type: new 
Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding</title>
<link>https://arxiv.org/abs/2511.12530</link>
<guid>https://arxiv.org/abs/2511.12530</guid>
<content:encoded><![CDATA[
arXiv:2511.12530v1 Announce Type: new 
Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12547</link>
<guid>https://arxiv.org/abs/2511.12547</guid>
<content:encoded><![CDATA[
arXiv:2511.12547v1 Announce Type: new 
Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis</title>
<link>https://arxiv.org/abs/2511.12554</link>
<guid>https://arxiv.org/abs/2511.12554</guid>
<content:encoded><![CDATA[
arXiv:2511.12554v1 Announce Type: new 
Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition</title>
<link>https://arxiv.org/abs/2511.12559</link>
<guid>https://arxiv.org/abs/2511.12559</guid>
<content:encoded><![CDATA[
arXiv:2511.12559v1 Announce Type: new 
Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection</title>
<link>https://arxiv.org/abs/2511.12572</link>
<guid>https://arxiv.org/abs/2511.12572</guid>
<content:encoded><![CDATA[
arXiv:2511.12572v1 Announce Type: new 
Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection</title>
<link>https://arxiv.org/abs/2511.12575</link>
<guid>https://arxiv.org/abs/2511.12575</guid>
<content:encoded><![CDATA[
arXiv:2511.12575v1 Announce Type: new 
Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction</title>
<link>https://arxiv.org/abs/2511.12578</link>
<guid>https://arxiv.org/abs/2511.12578</guid>
<content:encoded><![CDATA[
arXiv:2511.12578v1 Announce Type: new 
Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting</title>
<link>https://arxiv.org/abs/2511.12588</link>
<guid>https://arxiv.org/abs/2511.12588</guid>
<content:encoded><![CDATA[
arXiv:2511.12588v1 Announce Type: new 
Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&amp;E-stained data further confirms the scalability of the proposed method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Representation for Lane Topology Reasoning</title>
<link>https://arxiv.org/abs/2511.12590</link>
<guid>https://arxiv.org/abs/2511.12590</guid>
<content:encoded><![CDATA[
arXiv:2511.12590v1 Announce Type: new 
Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seg-VAR: Image Segmentation with Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2511.12594</link>
<guid>https://arxiv.org/abs/2511.12594</guid>
<content:encoded><![CDATA[
arXiv:2511.12594v1 Announce Type: new 
Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet</title>
<link>https://arxiv.org/abs/2511.12602</link>
<guid>https://arxiv.org/abs/2511.12602</guid>
<content:encoded><![CDATA[
arXiv:2511.12602v1 Announce Type: new 
Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixels or Positions? Benchmarking Modalities in Group Activity Recognition</title>
<link>https://arxiv.org/abs/2511.12606</link>
<guid>https://arxiv.org/abs/2511.12606</guid>
<content:encoded><![CDATA[
arXiv:2511.12606v1 Announce Type: new 
Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine</title>
<link>https://arxiv.org/abs/2511.12607</link>
<guid>https://arxiv.org/abs/2511.12607</guid>
<content:encoded><![CDATA[
arXiv:2511.12607v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding</title>
<link>https://arxiv.org/abs/2511.12614</link>
<guid>https://arxiv.org/abs/2511.12614</guid>
<content:encoded><![CDATA[
arXiv:2511.12614v1 Announce Type: new 
Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3Net: Context-Contrast Network for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2511.12627</link>
<guid>https://arxiv.org/abs/2511.12627</guid>
<content:encoded><![CDATA[
arXiv:2511.12627v1 Announce Type: new 
Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation</title>
<link>https://arxiv.org/abs/2511.12631</link>
<guid>https://arxiv.org/abs/2511.12631</guid>
<content:encoded><![CDATA[
arXiv:2511.12631v1 Announce Type: new 
Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoising Vision Transformer Autoencoder with Spectral Self-Regularization</title>
<link>https://arxiv.org/abs/2511.12633</link>
<guid>https://arxiv.org/abs/2511.12633</guid>
<content:encoded><![CDATA[
arXiv:2511.12633v1 Announce Type: new 
Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Knowledge Intervention Prompt Tuning for Medical Image Classification</title>
<link>https://arxiv.org/abs/2511.12639</link>
<guid>https://arxiv.org/abs/2511.12639</guid>
<content:encoded><![CDATA[
arXiv:2511.12639v1 Announce Type: new 
Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</title>
<link>https://arxiv.org/abs/2511.12653</link>
<guid>https://arxiv.org/abs/2511.12653</guid>
<content:encoded><![CDATA[
arXiv:2511.12653v1 Announce Type: new 
Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis</title>
<link>https://arxiv.org/abs/2511.12658</link>
<guid>https://arxiv.org/abs/2511.12658</guid>
<content:encoded><![CDATA[
arXiv:2511.12658v1 Announce Type: new 
Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans</title>
<link>https://arxiv.org/abs/2511.12662</link>
<guid>https://arxiv.org/abs/2511.12662</guid>
<content:encoded><![CDATA[
arXiv:2511.12662v1 Announce Type: new 
Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality</title>
<link>https://arxiv.org/abs/2511.12671</link>
<guid>https://arxiv.org/abs/2511.12671</guid>
<content:encoded><![CDATA[
arXiv:2511.12671v1 Announce Type: new 
Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis</title>
<link>https://arxiv.org/abs/2511.12675</link>
<guid>https://arxiv.org/abs/2511.12675</guid>
<content:encoded><![CDATA[
arXiv:2511.12675v1 Announce Type: new 
Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections</title>
<link>https://arxiv.org/abs/2511.12676</link>
<guid>https://arxiv.org/abs/2511.12676</guid>
<content:encoded><![CDATA[
arXiv:2511.12676v1 Announce Type: new 
Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection</title>
<link>https://arxiv.org/abs/2511.12691</link>
<guid>https://arxiv.org/abs/2511.12691</guid>
<content:encoded><![CDATA[
arXiv:2511.12691v1 Announce Type: new 
Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.12693</link>
<guid>https://arxiv.org/abs/2511.12693</guid>
<content:encoded><![CDATA[
arXiv:2511.12693v1 Announce Type: new 
Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-VMamba: Explainable Vision Mamba</title>
<link>https://arxiv.org/abs/2511.12694</link>
<guid>https://arxiv.org/abs/2511.12694</guid>
<content:encoded><![CDATA[
arXiv:2511.12694v1 Announce Type: new 
Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Through Occlusion: Framework for Open World Amodal Counting</title>
<link>https://arxiv.org/abs/2511.12702</link>
<guid>https://arxiv.org/abs/2511.12702</guid>
<content:encoded><![CDATA[
arXiv:2511.12702v1 Announce Type: new 
Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling</title>
<link>https://arxiv.org/abs/2511.12708</link>
<guid>https://arxiv.org/abs/2511.12708</guid>
<content:encoded><![CDATA[
arXiv:2511.12708v1 Announce Type: new 
Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning</title>
<link>https://arxiv.org/abs/2511.12735</link>
<guid>https://arxiv.org/abs/2511.12735</guid>
<content:encoded><![CDATA[
arXiv:2511.12735v1 Announce Type: new 
Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct Visual Grounding by Directing Attention of Visual Tokens</title>
<link>https://arxiv.org/abs/2511.12738</link>
<guid>https://arxiv.org/abs/2511.12738</guid>
<content:encoded><![CDATA[
arXiv:2511.12738v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests</title>
<link>https://arxiv.org/abs/2511.12740</link>
<guid>https://arxiv.org/abs/2511.12740</guid>
<content:encoded><![CDATA[
arXiv:2511.12740v1 Announce Type: new 
Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Saliency-Guided Contrastive Embeddings</title>
<link>https://arxiv.org/abs/2511.12744</link>
<guid>https://arxiv.org/abs/2511.12744</guid>
<content:encoded><![CDATA[
arXiv:2511.12744v1 Announce Type: new 
Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion</title>
<link>https://arxiv.org/abs/2511.12757</link>
<guid>https://arxiv.org/abs/2511.12757</guid>
<content:encoded><![CDATA[
arXiv:2511.12757v1 Announce Type: new 
Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.12767</link>
<guid>https://arxiv.org/abs/2511.12767</guid>
<content:encoded><![CDATA[
arXiv:2511.12767v1 Announce Type: new 
Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Optimal-Transport Harmonization on Edge Devices</title>
<link>https://arxiv.org/abs/2511.12785</link>
<guid>https://arxiv.org/abs/2511.12785</guid>
<content:encoded><![CDATA[
arXiv:2511.12785v1 Announce Type: new 
Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation</title>
<link>https://arxiv.org/abs/2511.12801</link>
<guid>https://arxiv.org/abs/2511.12801</guid>
<content:encoded><![CDATA[
arXiv:2511.12801v1 Announce Type: new 
Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2511.12810</link>
<guid>https://arxiv.org/abs/2511.12810</guid>
<content:encoded><![CDATA[
arXiv:2511.12810v1 Announce Type: new 
Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGA: Source Attribution of Generative AI Videos</title>
<link>https://arxiv.org/abs/2511.12834</link>
<guid>https://arxiv.org/abs/2511.12834</guid>
<content:encoded><![CDATA[
arXiv:2511.12834v1 Announce Type: new 
Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Finetuning Improves Reasoning Between Frames</title>
<link>https://arxiv.org/abs/2511.12868</link>
<guid>https://arxiv.org/abs/2511.12868</guid>
<content:encoded><![CDATA[
arXiv:2511.12868v1 Announce Type: new 
Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>View-aware Cross-modal Distillation for Multi-view Action Recognition</title>
<link>https://arxiv.org/abs/2511.12870</link>
<guid>https://arxiv.org/abs/2511.12870</guid>
<content:encoded><![CDATA[
arXiv:2511.12870v1 Announce Type: new 
Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</title>
<link>https://arxiv.org/abs/2511.12878</link>
<guid>https://arxiv.org/abs/2511.12878</guid>
<content:encoded><![CDATA[
arXiv:2511.12878v1 Announce Type: new 
Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings</title>
<link>https://arxiv.org/abs/2511.12880</link>
<guid>https://arxiv.org/abs/2511.12880</guid>
<content:encoded><![CDATA[
arXiv:2511.12880v1 Announce Type: new 
Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation</title>
<link>https://arxiv.org/abs/2511.12893</link>
<guid>https://arxiv.org/abs/2511.12893</guid>
<content:encoded><![CDATA[
arXiv:2511.12893v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing 3D Scenes in Native High Dynamic Range</title>
<link>https://arxiv.org/abs/2511.12895</link>
<guid>https://arxiv.org/abs/2511.12895</guid>
<content:encoded><![CDATA[
arXiv:2511.12895v1 Announce Type: new 
Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI</title>
<link>https://arxiv.org/abs/2511.12899</link>
<guid>https://arxiv.org/abs/2511.12899</guid>
<content:encoded><![CDATA[
arXiv:2511.12899v1 Announce Type: new 
Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.12908</link>
<guid>https://arxiv.org/abs/2511.12908</guid>
<content:encoded><![CDATA[
arXiv:2511.12908v1 Announce Type: new 
Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.12909</link>
<guid>https://arxiv.org/abs/2511.12909</guid>
<content:encoded><![CDATA[
arXiv:2511.12909v1 Announce Type: new 
Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explore How to Inject Beneficial Noise in MLLMs</title>
<link>https://arxiv.org/abs/2511.12917</link>
<guid>https://arxiv.org/abs/2511.12917</guid>
<content:encoded><![CDATA[
arXiv:2511.12917v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</title>
<link>https://arxiv.org/abs/2511.12919</link>
<guid>https://arxiv.org/abs/2511.12919</guid>
<content:encoded><![CDATA[
arXiv:2511.12919v1 Announce Type: new 
Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Photographic Control for Scene-Consistent Video Cinematic Editing</title>
<link>https://arxiv.org/abs/2511.12921</link>
<guid>https://arxiv.org/abs/2511.12921</guid>
<content:encoded><![CDATA[
arXiv:2511.12921v1 Announce Type: new 
Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes</title>
<link>https://arxiv.org/abs/2511.12932</link>
<guid>https://arxiv.org/abs/2511.12932</guid>
<content:encoded><![CDATA[
arXiv:2511.12932v1 Announce Type: new 
Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</title>
<link>https://arxiv.org/abs/2511.12935</link>
<guid>https://arxiv.org/abs/2511.12935</guid>
<content:encoded><![CDATA[
arXiv:2511.12935v1 Announce Type: new 
Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios</title>
<link>https://arxiv.org/abs/2511.12938</link>
<guid>https://arxiv.org/abs/2511.12938</guid>
<content:encoded><![CDATA[
arXiv:2511.12938v1 Announce Type: new 
Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking</title>
<link>https://arxiv.org/abs/2511.12939</link>
<guid>https://arxiv.org/abs/2511.12939</guid>
<content:encoded><![CDATA[
arXiv:2511.12939v1 Announce Type: new 
Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention</title>
<link>https://arxiv.org/abs/2511.12940</link>
<guid>https://arxiv.org/abs/2511.12940</guid>
<content:encoded><![CDATA[
arXiv:2511.12940v1 Announce Type: new 
Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.12956</link>
<guid>https://arxiv.org/abs/2511.12956</guid>
<content:encoded><![CDATA[
arXiv:2511.12956v1 Announce Type: new 
Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics</title>
<link>https://arxiv.org/abs/2511.12962</link>
<guid>https://arxiv.org/abs/2511.12962</guid>
<content:encoded><![CDATA[
arXiv:2511.12962v1 Announce Type: new 
Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models</title>
<link>https://arxiv.org/abs/2511.12964</link>
<guid>https://arxiv.org/abs/2511.12964</guid>
<content:encoded><![CDATA[
arXiv:2511.12964v1 Announce Type: new 
Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12968</link>
<guid>https://arxiv.org/abs/2511.12968</guid>
<content:encoded><![CDATA[
arXiv:2511.12968v1 Announce Type: new 
Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fr\'echet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology</title>
<link>https://arxiv.org/abs/2511.12969</link>
<guid>https://arxiv.org/abs/2511.12969</guid>
<content:encoded><![CDATA[
arXiv:2511.12969v1 Announce Type: new 
Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&amp;E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning</title>
<link>https://arxiv.org/abs/2511.12976</link>
<guid>https://arxiv.org/abs/2511.12976</guid>
<content:encoded><![CDATA[
arXiv:2511.12976v1 Announce Type: new 
Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes</title>
<link>https://arxiv.org/abs/2511.12977</link>
<guid>https://arxiv.org/abs/2511.12977</guid>
<content:encoded><![CDATA[
arXiv:2511.12977v1 Announce Type: new 
Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach</title>
<link>https://arxiv.org/abs/2511.12978</link>
<guid>https://arxiv.org/abs/2511.12978</guid>
<content:encoded><![CDATA[
arXiv:2511.12978v1 Announce Type: new 
Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective</title>
<link>https://arxiv.org/abs/2511.12988</link>
<guid>https://arxiv.org/abs/2511.12988</guid>
<content:encoded><![CDATA[
arXiv:2511.12988v1 Announce Type: new 
Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection</title>
<link>https://arxiv.org/abs/2511.12992</link>
<guid>https://arxiv.org/abs/2511.12992</guid>
<content:encoded><![CDATA[
arXiv:2511.12992v1 Announce Type: new 
Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching</title>
<link>https://arxiv.org/abs/2511.12998</link>
<guid>https://arxiv.org/abs/2511.12998</guid>
<content:encoded><![CDATA[
arXiv:2511.12998v1 Announce Type: new 
Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medal S: Spatio-Textual Prompt Model for Medical Segmentation</title>
<link>https://arxiv.org/abs/2511.13001</link>
<guid>https://arxiv.org/abs/2511.13001</guid>
<content:encoded><![CDATA[
arXiv:2511.13001v1 Announce Type: new 
Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinite-Story: A Training-Free Consistent Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.13002</link>
<guid>https://arxiv.org/abs/2511.13002</guid>
<content:encoded><![CDATA[
arXiv:2511.13002v1 Announce Type: new 
Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias</title>
<link>https://arxiv.org/abs/2511.13005</link>
<guid>https://arxiv.org/abs/2511.13005</guid>
<content:encoded><![CDATA[
arXiv:2511.13005v1 Announce Type: new 
Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis</title>
<link>https://arxiv.org/abs/2511.13011</link>
<guid>https://arxiv.org/abs/2511.13011</guid>
<content:encoded><![CDATA[
arXiv:2511.13011v1 Announce Type: new 
Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2511.13013</link>
<guid>https://arxiv.org/abs/2511.13013</guid>
<content:encoded><![CDATA[
arXiv:2511.13013v1 Announce Type: new 
Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues</title>
<link>https://arxiv.org/abs/2511.13015</link>
<guid>https://arxiv.org/abs/2511.13015</guid>
<content:encoded><![CDATA[
arXiv:2511.13015v1 Announce Type: new 
Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeanFlow Transformers with Representation Autoencoders</title>
<link>https://arxiv.org/abs/2511.13019</link>
<guid>https://arxiv.org/abs/2511.13019</guid>
<content:encoded><![CDATA[
arXiv:2511.13019v1 Announce Type: new 
Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction</title>
<link>https://arxiv.org/abs/2511.13020</link>
<guid>https://arxiv.org/abs/2511.13020</guid>
<content:encoded><![CDATA[
arXiv:2511.13020v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2511.13026</link>
<guid>https://arxiv.org/abs/2511.13026</guid>
<content:encoded><![CDATA[
arXiv:2511.13026v1 Announce Type: new 
Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2511.13031</link>
<guid>https://arxiv.org/abs/2511.13031</guid>
<content:encoded><![CDATA[
arXiv:2511.13031v1 Announce Type: new 
Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts</title>
<link>https://arxiv.org/abs/2511.13032</link>
<guid>https://arxiv.org/abs/2511.13032</guid>
<content:encoded><![CDATA[
arXiv:2511.13032v1 Announce Type: new 
Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data</title>
<link>https://arxiv.org/abs/2511.13036</link>
<guid>https://arxiv.org/abs/2511.13036</guid>
<content:encoded><![CDATA[
arXiv:2511.13036v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization</title>
<link>https://arxiv.org/abs/2511.13039</link>
<guid>https://arxiv.org/abs/2511.13039</guid>
<content:encoded><![CDATA[
arXiv:2511.13039v1 Announce Type: new 
Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation</title>
<link>https://arxiv.org/abs/2511.13047</link>
<guid>https://arxiv.org/abs/2511.13047</guid>
<content:encoded><![CDATA[
arXiv:2511.13047v1 Announce Type: new 
Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViSS-R1: Self-Supervised Reinforcement Video Reasoning</title>
<link>https://arxiv.org/abs/2511.13054</link>
<guid>https://arxiv.org/abs/2511.13054</guid>
<content:encoded><![CDATA[
arXiv:2511.13054v1 Announce Type: new 
Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries</title>
<link>https://arxiv.org/abs/2511.13055</link>
<guid>https://arxiv.org/abs/2511.13055</guid>
<content:encoded><![CDATA[
arXiv:2511.13055v1 Announce Type: new 
Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation</title>
<link>https://arxiv.org/abs/2511.13063</link>
<guid>https://arxiv.org/abs/2511.13063</guid>
<content:encoded><![CDATA[
arXiv:2511.13063v1 Announce Type: new 
Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustGait: Robustness Analysis for Appearance Based Gait Recognition</title>
<link>https://arxiv.org/abs/2511.13065</link>
<guid>https://arxiv.org/abs/2511.13065</guid>
<content:encoded><![CDATA[
arXiv:2511.13065v1 Announce Type: new 
Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.13079</link>
<guid>https://arxiv.org/abs/2511.13079</guid>
<content:encoded><![CDATA[
arXiv:2511.13079v1 Announce Type: new 
Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations</title>
<link>https://arxiv.org/abs/2511.13081</link>
<guid>https://arxiv.org/abs/2511.13081</guid>
<content:encoded><![CDATA[
arXiv:2511.13081v1 Announce Type: new 
Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images</title>
<link>https://arxiv.org/abs/2511.13099</link>
<guid>https://arxiv.org/abs/2511.13099</guid>
<content:encoded><![CDATA[
arXiv:2511.13099v1 Announce Type: new 
Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation</title>
<link>https://arxiv.org/abs/2511.13102</link>
<guid>https://arxiv.org/abs/2511.13102</guid>
<content:encoded><![CDATA[
arXiv:2511.13102v1 Announce Type: new 
Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2511.13105</link>
<guid>https://arxiv.org/abs/2511.13105</guid>
<content:encoded><![CDATA[
arXiv:2511.13105v1 Announce Type: new 
Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Level Dataset Distillation for Medical Image Enhancement</title>
<link>https://arxiv.org/abs/2511.13106</link>
<guid>https://arxiv.org/abs/2511.13106</guid>
<content:encoded><![CDATA[
arXiv:2511.13106v1 Announce Type: new 
Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2511.13108</link>
<guid>https://arxiv.org/abs/2511.13108</guid>
<content:encoded><![CDATA[
arXiv:2511.13108v1 Announce Type: new 
Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing</title>
<link>https://arxiv.org/abs/2511.13110</link>
<guid>https://arxiv.org/abs/2511.13110</guid>
<content:encoded><![CDATA[
arXiv:2511.13110v1 Announce Type: new 
Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining</title>
<link>https://arxiv.org/abs/2511.13113</link>
<guid>https://arxiv.org/abs/2511.13113</guid>
<content:encoded><![CDATA[
arXiv:2511.13113v1 Announce Type: new 
Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features</title>
<link>https://arxiv.org/abs/2511.13115</link>
<guid>https://arxiv.org/abs/2511.13115</guid>
<content:encoded><![CDATA[
arXiv:2511.13115v1 Announce Type: new 
Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model</title>
<link>https://arxiv.org/abs/2511.13121</link>
<guid>https://arxiv.org/abs/2511.13121</guid>
<content:encoded><![CDATA[
arXiv:2511.13121v1 Announce Type: new 
Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Point Joint Representation for Effective Trajectory Similarity Learning</title>
<link>https://arxiv.org/abs/2511.13125</link>
<guid>https://arxiv.org/abs/2511.13125</guid>
<content:encoded><![CDATA[
arXiv:2511.13125v1 Announce Type: new 
Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</title>
<link>https://arxiv.org/abs/2511.13127</link>
<guid>https://arxiv.org/abs/2511.13127</guid>
<content:encoded><![CDATA[
arXiv:2511.13127v1 Announce Type: new 
Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</title>
<link>https://arxiv.org/abs/2511.13132</link>
<guid>https://arxiv.org/abs/2511.13132</guid>
<content:encoded><![CDATA[
arXiv:2511.13132v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation</title>
<link>https://arxiv.org/abs/2511.13135</link>
<guid>https://arxiv.org/abs/2511.13135</guid>
<content:encoded><![CDATA[
arXiv:2511.13135v1 Announce Type: new 
Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection</title>
<link>https://arxiv.org/abs/2511.13138</link>
<guid>https://arxiv.org/abs/2511.13138</guid>
<content:encoded><![CDATA[
arXiv:2511.13138v1 Announce Type: new 
Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2511.13145</link>
<guid>https://arxiv.org/abs/2511.13145</guid>
<content:encoded><![CDATA[
arXiv:2511.13145v1 Announce Type: new 
Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.13150</link>
<guid>https://arxiv.org/abs/2511.13150</guid>
<content:encoded><![CDATA[
arXiv:2511.13150v1 Announce Type: new 
Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration</title>
<link>https://arxiv.org/abs/2511.13168</link>
<guid>https://arxiv.org/abs/2511.13168</guid>
<content:encoded><![CDATA[
arXiv:2511.13168v1 Announce Type: new 
Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THIR: Topological Histopathological Image Retrieval</title>
<link>https://arxiv.org/abs/2511.13170</link>
<guid>https://arxiv.org/abs/2511.13170</guid>
<content:encoded><![CDATA[
arXiv:2511.13170v1 Announce Type: new 
Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.13175</link>
<guid>https://arxiv.org/abs/2511.13175</guid>
<content:encoded><![CDATA[
arXiv:2511.13175v1 Announce Type: new 
Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenTract: Generative Global Tractography</title>
<link>https://arxiv.org/abs/2511.13183</link>
<guid>https://arxiv.org/abs/2511.13183</guid>
<content:encoded><![CDATA[
arXiv:2511.13183v1 Announce Type: new 
Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework</title>
<link>https://arxiv.org/abs/2511.13189</link>
<guid>https://arxiv.org/abs/2511.13189</guid>
<content:encoded><![CDATA[
arXiv:2511.13189v1 Announce Type: new 
Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Spatial Reasoning with Object-Centric 3D Rollout</title>
<link>https://arxiv.org/abs/2511.13190</link>
<guid>https://arxiv.org/abs/2511.13190</guid>
<content:encoded><![CDATA[
arXiv:2511.13190v1 Announce Type: new 
Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Birth of a Painting: Differentiable Brushstroke Reconstruction</title>
<link>https://arxiv.org/abs/2511.13191</link>
<guid>https://arxiv.org/abs/2511.13191</guid>
<content:encoded><![CDATA[
arXiv:2511.13191v1 Announce Type: new 
Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2511.13195</link>
<guid>https://arxiv.org/abs/2511.13195</guid>
<content:encoded><![CDATA[
arXiv:2511.13195v1 Announce Type: new 
Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Ultrasound Screen Detection</title>
<link>https://arxiv.org/abs/2511.13197</link>
<guid>https://arxiv.org/abs/2511.13197</guid>
<content:encoded><![CDATA[
arXiv:2511.13197v1 Announce Type: new 
Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.13204</link>
<guid>https://arxiv.org/abs/2511.13204</guid>
<content:encoded><![CDATA[
arXiv:2511.13204v1 Announce Type: new 
Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer</title>
<link>https://arxiv.org/abs/2511.13208</link>
<guid>https://arxiv.org/abs/2511.13208</guid>
<content:encoded><![CDATA[
arXiv:2511.13208v1 Announce Type: new 
Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale</title>
<link>https://arxiv.org/abs/2511.13211</link>
<guid>https://arxiv.org/abs/2511.13211</guid>
<content:encoded><![CDATA[
arXiv:2511.13211v1 Announce Type: new 
Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid-Domain Adaptative Representation Learning for Gaze Estimation</title>
<link>https://arxiv.org/abs/2511.13222</link>
<guid>https://arxiv.org/abs/2511.13222</guid>
<content:encoded><![CDATA[
arXiv:2511.13222v1 Announce Type: new 
Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</title>
<link>https://arxiv.org/abs/2511.13232</link>
<guid>https://arxiv.org/abs/2511.13232</guid>
<content:encoded><![CDATA[
arXiv:2511.13232v1 Announce Type: new 
Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2511.13242</link>
<guid>https://arxiv.org/abs/2511.13242</guid>
<content:encoded><![CDATA[
arXiv:2511.13242v1 Announce Type: new 
Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention</title>
<link>https://arxiv.org/abs/2511.13249</link>
<guid>https://arxiv.org/abs/2511.13249</guid>
<content:encoded><![CDATA[
arXiv:2511.13249v1 Announce Type: new 
Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models</title>
<link>https://arxiv.org/abs/2511.13259</link>
<guid>https://arxiv.org/abs/2511.13259</guid>
<content:encoded><![CDATA[
arXiv:2511.13259v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges</title>
<link>https://arxiv.org/abs/2511.13261</link>
<guid>https://arxiv.org/abs/2511.13261</guid>
<content:encoded><![CDATA[
arXiv:2511.13261v1 Announce Type: new 
Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression</title>
<link>https://arxiv.org/abs/2511.13264</link>
<guid>https://arxiv.org/abs/2511.13264</guid>
<content:encoded><![CDATA[
arXiv:2511.13264v1 Announce Type: new 
Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</title>
<link>https://arxiv.org/abs/2511.13269</link>
<guid>https://arxiv.org/abs/2511.13269</guid>
<content:encoded><![CDATA[
arXiv:2511.13269v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models</title>
<link>https://arxiv.org/abs/2511.13276</link>
<guid>https://arxiv.org/abs/2511.13276</guid>
<content:encoded><![CDATA[
arXiv:2511.13276v1 Announce Type: new 
Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.13278</link>
<guid>https://arxiv.org/abs/2511.13278</guid>
<content:encoded><![CDATA[
arXiv:2511.13278v1 Announce Type: new 
Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</title>
<link>https://arxiv.org/abs/2511.13282</link>
<guid>https://arxiv.org/abs/2511.13282</guid>
<content:encoded><![CDATA[
arXiv:2511.13282v1 Announce Type: new 
Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing</title>
<link>https://arxiv.org/abs/2511.13283</link>
<guid>https://arxiv.org/abs/2511.13283</guid>
<content:encoded><![CDATA[
arXiv:2511.13283v1 Announce Type: new 
Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design</title>
<link>https://arxiv.org/abs/2511.13285</link>
<guid>https://arxiv.org/abs/2511.13285</guid>
<content:encoded><![CDATA[
arXiv:2511.13285v1 Announce Type: new 
Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.13297</link>
<guid>https://arxiv.org/abs/2511.13297</guid>
<content:encoded><![CDATA[
arXiv:2511.13297v1 Announce Type: new 
Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.13309</link>
<guid>https://arxiv.org/abs/2511.13309</guid>
<content:encoded><![CDATA[
arXiv:2511.13309v1 Announce Type: new 
Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer Vision based group activity detection and action spotting</title>
<link>https://arxiv.org/abs/2511.13315</link>
<guid>https://arxiv.org/abs/2511.13315</guid>
<content:encoded><![CDATA[
arXiv:2511.13315v1 Announce Type: new 
Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection</title>
<link>https://arxiv.org/abs/2511.13344</link>
<guid>https://arxiv.org/abs/2511.13344</guid>
<content:encoded><![CDATA[
arXiv:2511.13344v1 Announce Type: new 
Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images</title>
<link>https://arxiv.org/abs/2511.13353</link>
<guid>https://arxiv.org/abs/2511.13353</guid>
<content:encoded><![CDATA[
arXiv:2511.13353v1 Announce Type: new 
Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</title>
<link>https://arxiv.org/abs/2511.13387</link>
<guid>https://arxiv.org/abs/2511.13387</guid>
<content:encoded><![CDATA[
arXiv:2511.13387v1 Announce Type: new 
Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)</title>
<link>https://arxiv.org/abs/2511.13397</link>
<guid>https://arxiv.org/abs/2511.13397</guid>
<content:encoded><![CDATA[
arXiv:2511.13397v1 Announce Type: new 
Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing</title>
<link>https://arxiv.org/abs/2511.13399</link>
<guid>https://arxiv.org/abs/2511.13399</guid>
<content:encoded><![CDATA[
arXiv:2511.13399v1 Announce Type: new 
Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Color Is It? A Text-Interference Multimodal Hallucination Benchmark</title>
<link>https://arxiv.org/abs/2511.13400</link>
<guid>https://arxiv.org/abs/2511.13400</guid>
<content:encoded><![CDATA[
arXiv:2511.13400v1 Announce Type: new 
Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source</title>
<link>https://arxiv.org/abs/2511.13417</link>
<guid>https://arxiv.org/abs/2511.13417</guid>
<content:encoded><![CDATA[
arXiv:2511.13417v1 Announce Type: new 
Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task</title>
<link>https://arxiv.org/abs/2511.13420</link>
<guid>https://arxiv.org/abs/2511.13420</guid>
<content:encoded><![CDATA[
arXiv:2511.13420v1 Announce Type: new 
Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSE: A Flow-based Mapping Between Shapes</title>
<link>https://arxiv.org/abs/2511.13431</link>
<guid>https://arxiv.org/abs/2511.13431</guid>
<content:encoded><![CDATA[
arXiv:2511.13431v1 Announce Type: new 
Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</title>
<link>https://arxiv.org/abs/2511.13442</link>
<guid>https://arxiv.org/abs/2511.13442</guid>
<content:encoded><![CDATA[
arXiv:2511.13442v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling</title>
<link>https://arxiv.org/abs/2511.13478</link>
<guid>https://arxiv.org/abs/2511.13478</guid>
<content:encoded><![CDATA[
arXiv:2511.13478v1 Announce Type: new 
Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE</title>
<link>https://arxiv.org/abs/2511.13488</link>
<guid>https://arxiv.org/abs/2511.13488</guid>
<content:encoded><![CDATA[
arXiv:2511.13488v1 Announce Type: new 
Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Guided Invariance Probing of Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.13494</link>
<guid>https://arxiv.org/abs/2511.13494</guid>
<content:encoded><![CDATA[
arXiv:2511.13494v1 Announce Type: new 
Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping the Vanishing and Transformation of Urban Villages in China</title>
<link>https://arxiv.org/abs/2511.13507</link>
<guid>https://arxiv.org/abs/2511.13507</guid>
<content:encoded><![CDATA[
arXiv:2511.13507v1 Announce Type: new 
Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2511.13533</link>
<guid>https://arxiv.org/abs/2511.13533</guid>
<content:encoded><![CDATA[
arXiv:2511.13533v1 Announce Type: new 
Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew</title>
<link>https://arxiv.org/abs/2511.13535</link>
<guid>https://arxiv.org/abs/2511.13535</guid>
<content:encoded><![CDATA[
arXiv:2511.13535v1 Announce Type: new 
Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse</title>
<link>https://arxiv.org/abs/2511.13539</link>
<guid>https://arxiv.org/abs/2511.13539</guid>
<content:encoded><![CDATA[
arXiv:2511.13539v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks</title>
<link>https://arxiv.org/abs/2511.13545</link>
<guid>https://arxiv.org/abs/2511.13545</guid>
<content:encoded><![CDATA[
arXiv:2511.13545v1 Announce Type: new 
Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.13552</link>
<guid>https://arxiv.org/abs/2511.13552</guid>
<content:encoded><![CDATA[
arXiv:2511.13552v1 Announce Type: new 
Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation</title>
<link>https://arxiv.org/abs/2511.13571</link>
<guid>https://arxiv.org/abs/2511.13571</guid>
<content:encoded><![CDATA[
arXiv:2511.13571v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.13575</link>
<guid>https://arxiv.org/abs/2511.13575</guid>
<content:encoded><![CDATA[
arXiv:2511.13575v1 Announce Type: new 
Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images</title>
<link>https://arxiv.org/abs/2511.13586</link>
<guid>https://arxiv.org/abs/2511.13586</guid>
<content:encoded><![CDATA[
arXiv:2511.13586v1 Announce Type: new 
Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping</title>
<link>https://arxiv.org/abs/2511.13587</link>
<guid>https://arxiv.org/abs/2511.13587</guid>
<content:encoded><![CDATA[
arXiv:2511.13587v1 Announce Type: new 
Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.13607</link>
<guid>https://arxiv.org/abs/2511.13607</guid>
<content:encoded><![CDATA[
arXiv:2511.13607v1 Announce Type: new 
Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AtlasMorph: Learning conditional deformable templates for brain MRI</title>
<link>https://arxiv.org/abs/2511.13609</link>
<guid>https://arxiv.org/abs/2511.13609</guid>
<content:encoded><![CDATA[
arXiv:2511.13609v1 Announce Type: new 
Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tissue Aware Nuclei Detection and Classification Model for Histopathology Images</title>
<link>https://arxiv.org/abs/2511.13615</link>
<guid>https://arxiv.org/abs/2511.13615</guid>
<content:encoded><![CDATA[
arXiv:2511.13615v1 Announce Type: new 
Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio</title>
<link>https://arxiv.org/abs/2511.13618</link>
<guid>https://arxiv.org/abs/2511.13618</guid>
<content:encoded><![CDATA[
arXiv:2511.13618v1 Announce Type: new 
Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpha Divergence Losses for Biometric Verification</title>
<link>https://arxiv.org/abs/2511.13621</link>
<guid>https://arxiv.org/abs/2511.13621</guid>
<content:encoded><![CDATA[
arXiv:2511.13621v1 Announce Type: new 
Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $\alpha$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $\alpha>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $\alpha$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2511.13644</link>
<guid>https://arxiv.org/abs/2511.13644</guid>
<content:encoded><![CDATA[
arXiv:2511.13644v1 Announce Type: new 
Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2511.13647</link>
<guid>https://arxiv.org/abs/2511.13647</guid>
<content:encoded><![CDATA[
arXiv:2511.13647v1 Announce Type: new 
Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&amp;A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</title>
<link>https://arxiv.org/abs/2511.13648</link>
<guid>https://arxiv.org/abs/2511.13648</guid>
<content:encoded><![CDATA[
arXiv:2511.13648v1 Announce Type: new 
Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Distillation Meets Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13649</link>
<guid>https://arxiv.org/abs/2511.13649</guid>
<content:encoded><![CDATA[
arXiv:2511.13649v1 Announce Type: new 
Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</title>
<link>https://arxiv.org/abs/2511.13655</link>
<guid>https://arxiv.org/abs/2511.13655</guid>
<content:encoded><![CDATA[
arXiv:2511.13655v1 Announce Type: new 
Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</title>
<link>https://arxiv.org/abs/2511.13684</link>
<guid>https://arxiv.org/abs/2511.13684</guid>
<content:encoded><![CDATA[
arXiv:2511.13684v1 Announce Type: new 
Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</title>
<link>https://arxiv.org/abs/2511.13704</link>
<guid>https://arxiv.org/abs/2511.13704</guid>
<content:encoded><![CDATA[
arXiv:2511.13704v1 Announce Type: new 
Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine</title>
<link>https://arxiv.org/abs/2511.13713</link>
<guid>https://arxiv.org/abs/2511.13713</guid>
<content:encoded><![CDATA[
arXiv:2511.13713v1 Announce Type: new 
Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</title>
<link>https://arxiv.org/abs/2511.13714</link>
<guid>https://arxiv.org/abs/2511.13714</guid>
<content:encoded><![CDATA[
arXiv:2511.13714v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segment Anything Across Shots: A Method and Benchmark</title>
<link>https://arxiv.org/abs/2511.13715</link>
<guid>https://arxiv.org/abs/2511.13715</guid>
<content:encoded><![CDATA[
arXiv:2511.13715v1 Announce Type: new 
Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Spatial Intelligence with Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2511.13719</link>
<guid>https://arxiv.org/abs/2511.13719</guid>
<content:encoded><![CDATA[
arXiv:2511.13719v1 Announce Type: new 
Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Basics: Let Denoising Generative Models Denoise</title>
<link>https://arxiv.org/abs/2511.13720</link>
<guid>https://arxiv.org/abs/2511.13720</guid>
<content:encoded><![CDATA[
arXiv:2511.13720v1 Announce Type: new 
Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding</title>
<link>https://arxiv.org/abs/2511.11634</link>
<guid>https://arxiv.org/abs/2511.11634</guid>
<content:encoded><![CDATA[
arXiv:2511.11634v1 Announce Type: cross 
Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature</title>
<link>https://arxiv.org/abs/2511.11639</link>
<guid>https://arxiv.org/abs/2511.11639</guid>
<content:encoded><![CDATA[
arXiv:2511.11639v1 Announce Type: cross 
Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Slow - Motion Video Synthesis for Basketball Using Frame Interpolation</title>
<link>https://arxiv.org/abs/2511.11644</link>
<guid>https://arxiv.org/abs/2511.11644</guid>
<content:encoded><![CDATA[
arXiv:2511.11644v1 Announce Type: cross 
Abstract: Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2511.11664</link>
<guid>https://arxiv.org/abs/2511.11664</guid>
<content:encoded><![CDATA[
arXiv:2511.11664v1 Announce Type: cross 
Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Preserving for Continual Multitask Learning</title>
<link>https://arxiv.org/abs/2511.11676</link>
<guid>https://arxiv.org/abs/2511.11676</guid>
<content:encoded><![CDATA[
arXiv:2511.11676v1 Announce Type: cross 
Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications</title>
<link>https://arxiv.org/abs/2511.11679</link>
<guid>https://arxiv.org/abs/2511.11679</guid>
<content:encoded><![CDATA[
arXiv:2511.11679v1 Announce Type: cross 
Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP</title>
<link>https://arxiv.org/abs/2511.11680</link>
<guid>https://arxiv.org/abs/2511.11680</guid>
<content:encoded><![CDATA[
arXiv:2511.11680v1 Announce Type: cross 
Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation</title>
<link>https://arxiv.org/abs/2511.11681</link>
<guid>https://arxiv.org/abs/2511.11681</guid>
<content:encoded><![CDATA[
arXiv:2511.11681v1 Announce Type: cross 
Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stratified Knowledge-Density Super-Network for Scalable Vision Transformers</title>
<link>https://arxiv.org/abs/2511.11683</link>
<guid>https://arxiv.org/abs/2511.11683</guid>
<content:encoded><![CDATA[
arXiv:2511.11683v1 Announce Type: cross 
Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v1 Announce Type: cross 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.11690</link>
<guid>https://arxiv.org/abs/2511.11690</guid>
<content:encoded><![CDATA[
arXiv:2511.11690v1 Announce Type: cross 
Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2511.11692</link>
<guid>https://arxiv.org/abs/2511.11692</guid>
<content:encoded><![CDATA[
arXiv:2511.11692v1 Announce Type: cross 
Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation</title>
<link>https://arxiv.org/abs/2511.11693</link>
<guid>https://arxiv.org/abs/2511.11693</guid>
<content:encoded><![CDATA[
arXiv:2511.11693v1 Announce Type: cross 
Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL</title>
<link>https://arxiv.org/abs/2511.11696</link>
<guid>https://arxiv.org/abs/2511.11696</guid>
<content:encoded><![CDATA[
arXiv:2511.11696v1 Announce Type: cross 
Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Vision-Language Math Reasoning via Rendered Text</title>
<link>https://arxiv.org/abs/2511.11704</link>
<guid>https://arxiv.org/abs/2511.11704</guid>
<content:encoded><![CDATA[
arXiv:2511.11704v1 Announce Type: cross 
Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs</title>
<link>https://arxiv.org/abs/2511.11705</link>
<guid>https://arxiv.org/abs/2511.11705</guid>
<content:encoded><![CDATA[
arXiv:2511.11705v1 Announce Type: cross 
Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling</title>
<link>https://arxiv.org/abs/2511.11706</link>
<guid>https://arxiv.org/abs/2511.11706</guid>
<content:encoded><![CDATA[
arXiv:2511.11706v1 Announce Type: cross 
Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets</title>
<link>https://arxiv.org/abs/2511.11713</link>
<guid>https://arxiv.org/abs/2511.11713</guid>
<content:encoded><![CDATA[
arXiv:2511.11713v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 3D Surrogate Modeling for Data Center Thermal Management</title>
<link>https://arxiv.org/abs/2511.11722</link>
<guid>https://arxiv.org/abs/2511.11722</guid>
<content:encoded><![CDATA[
arXiv:2511.11722v1 Announce Type: cross 
Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm</title>
<link>https://arxiv.org/abs/2511.11727</link>
<guid>https://arxiv.org/abs/2511.11727</guid>
<content:encoded><![CDATA[
arXiv:2511.11727v1 Announce Type: cross 
Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain</title>
<link>https://arxiv.org/abs/2511.11753</link>
<guid>https://arxiv.org/abs/2511.11753</guid>
<content:encoded><![CDATA[
arXiv:2511.11753v1 Announce Type: cross 
Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review</title>
<link>https://arxiv.org/abs/2511.11777</link>
<guid>https://arxiv.org/abs/2511.11777</guid>
<content:encoded><![CDATA[
arXiv:2511.11777v1 Announce Type: cross 
Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinate Descent for Network Linearization</title>
<link>https://arxiv.org/abs/2511.11781</link>
<guid>https://arxiv.org/abs/2511.11781</guid>
<content:encoded><![CDATA[
arXiv:2511.11781v1 Announce Type: cross 
Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment</title>
<link>https://arxiv.org/abs/2511.11787</link>
<guid>https://arxiv.org/abs/2511.11787</guid>
<content:encoded><![CDATA[
arXiv:2511.11787v1 Announce Type: cross 
Abstract: Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.11831</link>
<guid>https://arxiv.org/abs/2511.11831</guid>
<content:encoded><![CDATA[
arXiv:2511.11831v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production</title>
<link>https://arxiv.org/abs/2511.11880</link>
<guid>https://arxiv.org/abs/2511.11880</guid>
<content:encoded><![CDATA[
arXiv:2511.11880v1 Announce Type: cross 
Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction</title>
<link>https://arxiv.org/abs/2511.11899</link>
<guid>https://arxiv.org/abs/2511.11899</guid>
<content:encoded><![CDATA[
arXiv:2511.11899v1 Announce Type: cross 
Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering</title>
<link>https://arxiv.org/abs/2511.11930</link>
<guid>https://arxiv.org/abs/2511.11930</guid>
<content:encoded><![CDATA[
arXiv:2511.11930v1 Announce Type: cross 
Abstract: In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts</title>
<link>https://arxiv.org/abs/2511.11934</link>
<guid>https://arxiv.org/abs/2511.11934</guid>
<content:encoded><![CDATA[
arXiv:2511.11934v1 Announce Type: cross 
Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images</title>
<link>https://arxiv.org/abs/2511.11937</link>
<guid>https://arxiv.org/abs/2511.11937</guid>
<content:encoded><![CDATA[
arXiv:2511.11937v1 Announce Type: cross 
Abstract: Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as "black boxes," we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting Fine-Tuning Examples by Quizzing VLMs</title>
<link>https://arxiv.org/abs/2511.12002</link>
<guid>https://arxiv.org/abs/2511.12002</guid>
<content:encoded><![CDATA[
arXiv:2511.12002v1 Announce Type: cross 
Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12008</link>
<guid>https://arxiv.org/abs/2511.12008</guid>
<content:encoded><![CDATA[
arXiv:2511.12008v1 Announce Type: cross 
Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space</title>
<link>https://arxiv.org/abs/2511.12035</link>
<guid>https://arxiv.org/abs/2511.12035</guid>
<content:encoded><![CDATA[
arXiv:2511.12035v1 Announce Type: cross 
Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning</title>
<link>https://arxiv.org/abs/2511.12046</link>
<guid>https://arxiv.org/abs/2511.12046</guid>
<content:encoded><![CDATA[
arXiv:2511.12046v1 Announce Type: cross 
Abstract: Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding</title>
<link>https://arxiv.org/abs/2511.12140</link>
<guid>https://arxiv.org/abs/2511.12140</guid>
<content:encoded><![CDATA[
arXiv:2511.12140v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variation-Bounded Loss for Noise-Tolerant Learning</title>
<link>https://arxiv.org/abs/2511.12143</link>
<guid>https://arxiv.org/abs/2511.12143</guid>
<content:encoded><![CDATA[
arXiv:2511.12143v1 Announce Type: cross 
Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.12149</link>
<guid>https://arxiv.org/abs/2511.12149</guid>
<content:encoded><![CDATA[
arXiv:2511.12149v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps</title>
<link>https://arxiv.org/abs/2511.12212</link>
<guid>https://arxiv.org/abs/2511.12212</guid>
<content:encoded><![CDATA[
arXiv:2511.12212v1 Announce Type: cross 
Abstract: This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos</title>
<link>https://arxiv.org/abs/2511.12241</link>
<guid>https://arxiv.org/abs/2511.12241</guid>
<content:encoded><![CDATA[
arXiv:2511.12241v1 Announce Type: cross 
Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network</title>
<link>https://arxiv.org/abs/2511.12248</link>
<guid>https://arxiv.org/abs/2511.12248</guid>
<content:encoded><![CDATA[
arXiv:2511.12248v1 Announce Type: cross 
Abstract: Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems</title>
<link>https://arxiv.org/abs/2511.12257</link>
<guid>https://arxiv.org/abs/2511.12257</guid>
<content:encoded><![CDATA[
arXiv:2511.12257v1 Announce Type: cross 
Abstract: This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks</title>
<link>https://arxiv.org/abs/2511.12265</link>
<guid>https://arxiv.org/abs/2511.12265</guid>
<content:encoded><![CDATA[
arXiv:2511.12265v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification</title>
<link>https://arxiv.org/abs/2511.12268</link>
<guid>https://arxiv.org/abs/2511.12268</guid>
<content:encoded><![CDATA[
arXiv:2511.12268v1 Announce Type: cross 
Abstract: Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data. We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata. A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes. Haemoglobin-sensitive indices, texture features, and spectral-shape measures were extracted and fused with deep and clinical features. Multiple machine-learning models were assessed with patient-wise validation. We further introduce an incremental heuristic meta-learner (IHML) that combines calibrated base classifiers through probabilistic stacking and patient-level posterior smoothing. On an unseen patient split, the proposed framework achieved a macro F1 of 66.23% and an accuracy of 64.56%. Results demonstrate that hyperspectral reconstruction and uncertainty-aware meta-learning substantially improve robustness for real-world oral lesion screening.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAA-MIL: A Novel Framework for Classification of Oral Cytology</title>
<link>https://arxiv.org/abs/2511.12269</link>
<guid>https://arxiv.org/abs/2511.12269</guid>
<content:encoded><![CDATA[
arXiv:2511.12269v1 Announce Type: cross 
Abstract: Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging</title>
<link>https://arxiv.org/abs/2511.12373</link>
<guid>https://arxiv.org/abs/2511.12373</guid>
<content:encoded><![CDATA[
arXiv:2511.12373v1 Announce Type: cross 
Abstract: In the field of medical imaging, AI-assisted techniques such as object detection, segmentation, and classification are widely employed to alleviate the workload of physicians and doctors. However, single-task models are predominantly used, overlooking the shared information across tasks. This oversight leads to inefficiencies in real-life applications. In this work, we propose MTMed3D, a novel end-to-end Multi-task Transformer-based model to address the limitations of single-task models by jointly performing 3D detection, segmentation, and classification in medical imaging. Our model uses a Transformer as the shared encoder to generate multi-scale features, followed by CNN-based task-specific decoders. The proposed framework was evaluated on the BraTS 2018 and 2019 datasets, achieving promising results across all three tasks, especially in detection, where our method achieves better results than prior works. Additionally, we compare our multi-task model with equivalent single-task variants trained separately. Our multi-task model significantly reduces computational costs and achieves faster inference speed while maintaining comparable performance to the single-task models, highlighting its efficiency advantage. To the best of our knowledge, this is the first work to leverage Transformers for multi-task learning that simultaneously covers detection, segmentation, and classification tasks in 3D medical imaging, presenting its potential to enhance diagnostic processes. The code is available at https://github.com/fanlimua/MTMed3D.git.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEMIST: \underline{DE}coupled \underline{M}ulti-stream latent d\underline{I}ffusion for Quantitative Myelin Map \underline{S}yn\underline{T}hesis</title>
<link>https://arxiv.org/abs/2511.12396</link>
<guid>https://arxiv.org/abs/2511.12396</guid>
<content:encoded><![CDATA[
arXiv:2511.12396v1 Announce Type: cross 
Abstract: Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \textbf{semantic} tokens via cross-attention, (ii) \textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at https://github.com/MedICL-VU/MS-Synthesis-3DcLDM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BSO: Binary Spiking Online Optimization Algorithm</title>
<link>https://arxiv.org/abs/2511.12502</link>
<guid>https://arxiv.org/abs/2511.12502</guid>
<content:encoded><![CDATA[
arXiv:2511.12502v1 Announce Type: cross 
Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear time small coresets for k-mean clustering of segments with applications</title>
<link>https://arxiv.org/abs/2511.12564</link>
<guid>https://arxiv.org/abs/2511.12564</guid>
<content:encoded><![CDATA[
arXiv:2511.12564v1 Announce Type: cross 
Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</title>
<link>https://arxiv.org/abs/2511.12609</link>
<guid>https://arxiv.org/abs/2511.12609</guid>
<content:encoded><![CDATA[
arXiv:2511.12609v1 Announce Type: cross 
Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting upcoming visual features during eye movements yields scene representations aligned with human visual cortex</title>
<link>https://arxiv.org/abs/2511.12715</link>
<guid>https://arxiv.org/abs/2511.12715</guid>
<content:encoded><![CDATA[
arXiv:2511.12715v1 Announce Type: cross 
Abstract: Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Generalisation of Learned Reconstruction Frameworks</title>
<link>https://arxiv.org/abs/2511.12730</link>
<guid>https://arxiv.org/abs/2511.12730</guid>
<content:encoded><![CDATA[
arXiv:2511.12730v1 Announce Type: cross 
Abstract: Ensuring proper generalization is a critical challenge in applying data-driven methods for solving inverse problems in imaging, as neural networks reconstructing an image must perform well across varied datasets and acquisition geometries. In X-ray Computed Tomography (CT), convolutional neural networks (CNNs) are widely used to filter the projection data but are ill-suited for this task as they apply grid-based convolutions to the sinogram, which inherently lies on a line manifold, not a regular grid. The CNNs, unaware of the geometry, are implicitly tied to it and require an excessive amount of parameters as they must infer the relations between measurements from the data rather than from prior information.
  The contribution of this paper is twofold. First, we introduce a graph data structure to represent CT acquisition geometries and tomographic data, providing a detailed explanation of the graph's structure for circular, cone-beam geometries. Second, we propose GLM, a hybrid neural network architecture that leverages both graph and grid convolutions to process tomographic data.
  We demonstrate that GLM outperforms CNNs when performance is quantified in terms of structural similarity and peak signal-to-noise ratio, despite the fact that GLM uses only a fraction of the trainable parameters. Compared to CNNs, GLM also requires significantly less training time and memory, and its memory requirements scale better. Crucially, GLM demonstrates robust generalization to unseen variations in the acquisition geometry, like when training only on fully sampled CT data and then testing on sparse-view CT data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainNormalizer: Anatomy-Informed Pseudo-Healthy Brain Reconstruction from Tumor MRI via Edge-Guided ControlNet</title>
<link>https://arxiv.org/abs/2511.12853</link>
<guid>https://arxiv.org/abs/2511.12853</guid>
<content:encoded><![CDATA[
arXiv:2511.12853v1 Announce Type: cross 
Abstract: Brain tumors are among the most clinically significant neurological diseases and remain a major cause of morbidity and mortality due to their aggressive growth and structural heterogeneity. As tumors expand, they induce substantial anatomical deformation that disrupts both local tissue organization and global brain architecture, complicating diagnosis, treatment planning, and surgical navigation. Yet a subject-specific reference of how the brain would appear without tumor-induced changes is fundamentally unobtainable in clinical practice. We present BrainNormalizer, an anatomy-informed diffusion framework that reconstructs pseudo-healthy MRIs directly from tumorous scans by conditioning the generative process on boundary cues extracted from the subject's own anatomy. This boundary-guided conditioning enables anatomically plausible pseudo-healthy reconstruction without requiring paired non-tumorous and tumorous scans. BrainNormalizer employs a two-stage training strategy. The pretrained diffusion model is first adapted through inpainting-based fine-tuning on tumorous and non-tumorous scans. Next, an edge-map-guided ControlNet branch is trained to inject fine-grained anatomical contours into the frozen decoder while preserving learned priors. During inference, a deliberate misalignment strategy pairs tumorous inputs with non-tumorous prompts and mirrored contralateral edge maps, leveraging hemispheric correspondence to guide reconstruction. On the BraTS2020 dataset, BrainNormalizer achieves strong quantitative performance and qualitatively produces anatomically plausible reconstructions in tumor-affected regions while retaining overall structural coherence. BrainNormalizer provides clinically reliable anatomical references for treatment planning and supports new research directions in counterfactual modeling and tumor-induced deformation analysis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
arXiv:2511.12861v1 Announce Type: cross 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Mean Flow in Hilbert Space</title>
<link>https://arxiv.org/abs/2511.12898</link>
<guid>https://arxiv.org/abs/2511.12898</guid>
<content:encoded><![CDATA[
arXiv:2511.12898v1 Announce Type: cross 
Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration</title>
<link>https://arxiv.org/abs/2511.12930</link>
<guid>https://arxiv.org/abs/2511.12930</guid>
<content:encoded><![CDATA[
arXiv:2511.12930v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.12937</link>
<guid>https://arxiv.org/abs/2511.12937</guid>
<content:encoded><![CDATA[
arXiv:2511.12937v1 Announce Type: cross 
Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation</title>
<link>https://arxiv.org/abs/2511.12961</link>
<guid>https://arxiv.org/abs/2511.12961</guid>
<content:encoded><![CDATA[
arXiv:2511.12961v1 Announce Type: cross 
Abstract: Event cameras, by virtue of their working principle, directly encode motion within a scene. Many learning-based and model-based methods exist that estimate event-based optical flow, however the temporally dense yet spatially sparse nature of events poses significant challenges. To address these issues, contrast maximization (CM) is a prominent model-based optimization methodology that estimates the motion trajectories of events within an event volume by optimally warping them. Since its introduction, the CM framework has undergone a series of refinements by the computer vision community. Nonetheless, it remains a highly non-convex optimization problem. In this paper, we introduce a novel biologically-inspired hybrid CM method for event-based optical flow estimation that couples visual and inertial motion cues. Concretely, we propose the use of orientation maps, derived from camera 3D velocities, as priors to guide the CM process. The orientation maps provide directional guidance and constrain the space of estimated motion trajectories. We show that this orientation-guided formulation leads to improved robustness and convergence in event-based optical flow estimation. The evaluation of our approach on the MVSEC, DSEC, and ECD datasets yields superior accuracy scores over the state of the art.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization</title>
<link>https://arxiv.org/abs/2511.12982</link>
<guid>https://arxiv.org/abs/2511.12982</guid>
<content:encoded><![CDATA[
arXiv:2511.12982v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks</title>
<link>https://arxiv.org/abs/2511.12985</link>
<guid>https://arxiv.org/abs/2511.12985</guid>
<content:encoded><![CDATA[
arXiv:2511.12985v1 Announce Type: cross 
Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Vision-Guided Crop Yield Estimation</title>
<link>https://arxiv.org/abs/2511.12999</link>
<guid>https://arxiv.org/abs/2511.12999</guid>
<content:encoded><![CDATA[
arXiv:2511.12999v1 Announce Type: cross 
Abstract: Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.13009</link>
<guid>https://arxiv.org/abs/2511.13009</guid>
<content:encoded><![CDATA[
arXiv:2511.13009v1 Announce Type: cross 
Abstract: We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of breast cancer sites using deformation-aware graph neural network</title>
<link>https://arxiv.org/abs/2511.13082</link>
<guid>https://arxiv.org/abs/2511.13082</guid>
<content:encoded><![CDATA[
arXiv:2511.13082v1 Announce Type: cross 
Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements</title>
<link>https://arxiv.org/abs/2511.13087</link>
<guid>https://arxiv.org/abs/2511.13087</guid>
<content:encoded><![CDATA[
arXiv:2511.13087v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications</title>
<link>https://arxiv.org/abs/2511.13131</link>
<guid>https://arxiv.org/abs/2511.13131</guid>
<content:encoded><![CDATA[
arXiv:2511.13131v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIGEON: VLM-Driven Object Navigation via Points of Interest Selection</title>
<link>https://arxiv.org/abs/2511.13207</link>
<guid>https://arxiv.org/abs/2511.13207</guid>
<content:encoded><![CDATA[
arXiv:2511.13207v1 Announce Type: cross 
Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering and Mitigating Transient Blindness in Multimodal Model Editing</title>
<link>https://arxiv.org/abs/2511.13243</link>
<guid>https://arxiv.org/abs/2511.13243</guid>
<content:encoded><![CDATA[
arXiv:2511.13243v1 Announce Type: cross 
Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAP: A Discrete-token Autoregressive Planner for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.13306</link>
<guid>https://arxiv.org/abs/2511.13306</guid>
<content:encoded><![CDATA[
arXiv:2511.13306v1 Announce Type: cross 
Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Grounded Enhancement for Visual Document Retrieval</title>
<link>https://arxiv.org/abs/2511.13415</link>
<guid>https://arxiv.org/abs/2511.13415</guid>
<content:encoded><![CDATA[
arXiv:2511.13415v1 Announce Type: cross 
Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust in Vision-Language Models: Insights from a Participatory User Workshop</title>
<link>https://arxiv.org/abs/2511.13458</link>
<guid>https://arxiv.org/abs/2511.13458</guid>
<content:encoded><![CDATA[
arXiv:2511.13458v1 Announce Type: cross 
Abstract: With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v1 Announce Type: cross 
Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention</title>
<link>https://arxiv.org/abs/2511.13679</link>
<guid>https://arxiv.org/abs/2511.13679</guid>
<content:encoded><![CDATA[
arXiv:2511.13679v1 Announce Type: cross 
Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</title>
<link>https://arxiv.org/abs/2511.13689</link>
<guid>https://arxiv.org/abs/2511.13689</guid>
<content:encoded><![CDATA[
arXiv:2511.13689v1 Announce Type: cross 
Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation</title>
<link>https://arxiv.org/abs/2105.06421</link>
<guid>https://arxiv.org/abs/2105.06421</guid>
<content:encoded><![CDATA[
arXiv:2105.06421v4 Announce Type: replace 
Abstract: Facial emotion recognition (FER) is a fine-grained problem where the value of transfer learning is often assumed. We first quantify this assumption and show that, on AffectNet, training from random initialization with sufficiently strong augmentation consistently matches or surpasses fine-tuning from ImageNet. Motivated by this result, we propose Hybrid Multi-Task Learning (HMTL) for FER in the wild. HMTL augments supervised learning (SL) with self-supervised learning (SSL) objectives during training, while keeping the inference-time model unchanged. We instantiate HMTL with two tailored pretext tasks, puzzling and inpainting with a perceptual loss, that encourage part-aware and expression-relevant features. On AffectNet, both HMTL variants achieve state-of-the-art accuracy in the eight-emotion setting without any additional pretraining data, and they provide larger gains under low-data regimes. Compared with conventional SSL pretraining, HMTL yields stronger downstream performance. Beyond FER, the same strategy improves fine-grained facial analysis tasks, including head pose estimation and gender recognition. These results suggest that aligned SSL auxiliaries are an effective and simple way to strengthen supervised fine-grained facial representation without adding extra computation cost during inference time.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HIBMatch: Hypergraph Information Bottleneck for Semi-supervised Alzheimer's Progression</title>
<link>https://arxiv.org/abs/2303.10390</link>
<guid>https://arxiv.org/abs/2303.10390</guid>
<content:encoded><![CDATA[
arXiv:2303.10390v2 Announce Type: replace 
Abstract: Alzheimer's disease progression prediction is critical for patients with early Mild Cognitive Impairment (MCI) to enable timely intervention and improve their quality of life. While existing progression prediction techniques demonstrate potential with multimodal data, they are highly limited by their reliance on labelled data and fail to account for a key element of future progression prediction: not all features extracted at the current moment may be relevant for predicting progression several years later. To address these limitations in the literature, we design a novel semi-supervised multimodal learning hypergraph architecture, termed HIBMatch, by harnessing hypergraph knowledge based on information bottleneck and consistency regularisation strategies. Firstly, our framework utilises hypergraphs to represent multimodal data, encompassing both imaging and non-imaging modalities. Secondly, to harmonise relevant information from the currently captured data for future MCI conversion prediction, we propose a Hypergraph Information Bottleneck (HIB) that discriminates against irrelevant information, thereby focusing exclusively on harmonising relevant information for future MCI conversion prediction. Thirdly, our method enforces consistency regularisation between the HIB and a discriminative classifier to enhance the robustness and generalisation capabilities of HIBMatch under both topological and feature perturbations. Finally, to fully exploit the unlabeled data, HIBMatch incorporates a cross-modal contrastive loss for data efficiency. Extensive experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed HIBMatch framework surpasses existing state-of-the-art methods in Alzheimer's disease prognosis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection</title>
<link>https://arxiv.org/abs/2305.13625</link>
<guid>https://arxiv.org/abs/2305.13625</guid>
<content:encoded><![CDATA[
arXiv:2305.13625v3 Announce Type: replace 
Abstract: The increasingly pervasive facial recognition (FR) systems raise serious concerns about personal privacy, especially for billions of users who have publicly shared their photos on social media. Several attempts have been made to protect individuals from being identified by unauthorized FR systems utilizing adversarial attacks to generate encrypted face images. However, existing methods suffer from poor visual quality or low attack success rates, which limit their utility. Recently, diffusion models have achieved tremendous success in image generation. In this work, we ask: can diffusion models be used to generate adversarial examples to improve both visual quality and attack performance? We propose DiffProtect, which utilizes a diffusion autoencoder to generate semantically meaningful perturbations on FR systems. Extensive experiments demonstrate that DiffProtect produces more natural-looking encrypted images than state-of-the-art methods while achieving significantly higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the CelebA-HQ and FFHQ datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A comprehensive and easy-to-use multi-domain multi-task medical imaging meta-dataset</title>
<link>https://arxiv.org/abs/2404.16000</link>
<guid>https://arxiv.org/abs/2404.16000</guid>
<content:encoded><![CDATA[
arXiv:2404.16000v2 Announce Type: replace 
Abstract: While the field of medical image analysis has undergone a transformative shift with the integration of machine learning techniques, the main challenge of these techniques is often the scarcity of large, diverse, and well-annotated datasets. Medical images vary in format, size, and other parameters and therefore require extensive preprocessing and standardization, for usage in machine learning. Addressing these challenges, we introduce the Medical Imaging Meta-Dataset (MedIMeta), a novel multi-domain, multi-task meta-dataset. MedIMeta contains 19 medical imaging datasets spanning 10 different domains and encompassing 54 distinct medical tasks, all of which are standardized to the same format and readily usable in PyTorch or other ML frameworks. We perform a technical validation of MedIMeta, demonstrating its utility through fully supervised and cross-domain few-shot learning baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lane Graph Extraction from Aerial Imagery via Lane Segmentation Refinement with Diffusion Models</title>
<link>https://arxiv.org/abs/2405.00620</link>
<guid>https://arxiv.org/abs/2405.00620</guid>
<content:encoded><![CDATA[
arXiv:2405.00620v2 Announce Type: replace 
Abstract: The lane graph is critical for applications such as autonomous driving and lane-level route planning. While previous research has focused on extracting lane-level graphs from aerial imagery using convolutional neural networks (CNNs) followed by post-processing segmentation-to-graph algorithms, these methods often face challenges in producing sharp and complete segmentation masks. Challenges such as occlusions, variations in lighting, and changes in road texture can lead to incomplete and inaccurate lane masks, resulting in poor-quality lane graphs. To address these challenges, we propose a novel approach that refines the lane masks, output by a CNN, using diffusion models. Experimental results on a publicly available dataset demonstrate that our method outperforms existing methods based solely on CNNs or diffusion models, particularly in terms of graph connectivity. Our lane mask refinement approach enhances the quality of the extracted lane graph, yielding gains of approximately 1.5\% in GEO F1 and 3.5\% in TOPO F1 scores over the best-performing CNN-based method, and improvements of 28\% and 34\%, respectively, compared to a prior diffusion-based approach. Both GEO F1 and TOPO F1 scores are critical metrics for evaluating lane graph quality. Additionally, ablation studies are conducted to evaluate the individual components of our approach, providing insights into their respective contributions and effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-free meets 3D priors: Novel View Synthesis from a Single Image with Pretrained Diffusion Guidance</title>
<link>https://arxiv.org/abs/2408.06157</link>
<guid>https://arxiv.org/abs/2408.06157</guid>
<content:encoded><![CDATA[
arXiv:2408.06157v5 Announce Type: replace 
Abstract: Recent 3D novel view synthesis (NVS) methods often require extensive 3D data for training, and also typically lack generalization beyond the training distribution. Moreover, they tend to be object centric and struggle with complex and intricate scenes. Conversely, 3D-free methods can generate text-controlled views of complex, in-the-wild scenes using a pretrained stable diffusion model without the need for a large amount of 3D-based training data, but lack camera control. In this paper, we introduce a method capable of generating camera-controlled viewpoints from a single input image, by combining the benefits of 3D-free and 3D-based approaches. Our method excels in handling complex and diverse scenes without extensive training or additional 3D and multiview data. It leverages widely available pretrained NVS models for weak guidance, integrating this knowledge into a 3D-free view synthesis style approach, along with enriching the CLIP vision-language space with 3D camera angle information, to achieve the desired results. Experimental results demonstrate that our method outperforms existing models in both qualitative and quantitative evaluations, achieving high-fidelity, consistent novel view synthesis at desired camera angles across a wide variety of scenes while maintaining accurate, natural detail representation and image clarity across various viewpoints. We also support our method with a comprehensive analysis of 2D image generation models and the 3D space, providing a solid foundation and rationale for our solution.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BadVim: Unveiling Backdoor Threats in Visual State Space Model</title>
<link>https://arxiv.org/abs/2408.11679</link>
<guid>https://arxiv.org/abs/2408.11679</guid>
<content:encoded><![CDATA[
arXiv:2408.11679v3 Announce Type: replace 
Abstract: Visual State Space Models (VSSM) have shown remarkable performance in various computer vision tasks. However, backdoor attacks pose significant security challenges, causing compromised models to predict target labels when specific triggers are present while maintaining normal behavior on benign samples. In this paper, we investigate the robustness of VSSMs against backdoor attacks. Specifically, we delicately design a novel framework for VSSMs, dubbed BadVim, which utilizes low-rank perturbations on state-wise to uncover their impact on state transitions during training. By poisoning only $0.3\%$ of the training data, our attacks cause any trigger-embedded input to be misclassified to the targeted class with a high attack success rate (over 97%) at inference time. Our findings suggest that the state-space representation property of VSSMs, which enhances model capability, may also contribute to its vulnerability to backdoor attacks. Our attack exhibits effectiveness across three datasets, even bypassing state-of-the-art defenses against such attacks. Extensive experiments show that the backdoor robustness of VSSMs is comparable to that of Transformers (ViTs) and superior to that of Convolutional Neural Networks (CNNs). We believe our findings will prompt the community to reconsider the trade-offs between performance and robustness in model design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation and Smoothing Affect Explanation Quality More Than the Choice of Perturbation-based XAI Method for Image Explanations</title>
<link>https://arxiv.org/abs/2409.04116</link>
<guid>https://arxiv.org/abs/2409.04116</guid>
<content:encoded><![CDATA[
arXiv:2409.04116v3 Announce Type: replace 
Abstract: Perturbation-based post-hoc image explanation methods are commonly used to explain image prediction models. These methods perturb parts of the input to measure how those parts affect the output. Since the methods only require the input and output, they can be applied to any model, making them a popular choice to explain black-box models. While many different methods exist and have been compared with one another, it remains poorly understood which parameters of the different methods are responsible for their varying performance.
  This work uses the Randomized Input Sampling for Explanations (RISE) method as a baseline to evaluate many combinations of mask sampling, segmentation techniques, smoothing, attribution calculation, and per-segment or per-pixel attribution, using a proxy metric. The results show that attribution calculation, which is frequently the focus of other works, has little impact on the results. Conversely, segmentation and per-pixel attribution, rarely examined parameters, have a significant impact.
  The implementation of and data gathered in this work are available online: https://github.com/guspih/post-hoc-image-perturbation and https://bit.ly/smooth-mask-perturbation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Watermarking Method for Latent Diffusion Models via Low-Rank Adaptation and Dynamic Loss Weighting</title>
<link>https://arxiv.org/abs/2410.20202</link>
<guid>https://arxiv.org/abs/2410.20202</guid>
<content:encoded><![CDATA[
arXiv:2410.20202v2 Announce Type: replace 
Abstract: The rapid proliferation of Deep Neural Networks (DNNs) is driving a surge in model watermarking technologies, as the trained models themselves constitute valuable intellectual property. Existing watermarking approaches primarily focus on modifying model parameters or altering sampling behaviors. However, with the emergence of increasingly large models, improving the efficiency of watermark embedding becomes essential to manage increasing computational demands. Prioritizing efficiency not only optimizes resource utilization, making the watermarking process more applicable for large models, but also mitigates potential degradation of model performance. In this paper, we propose an efficient watermarking method for Latent Diffusion Models (LDMs) based on Low-Rank Adaptation (LoRA). The core idea is to introduce trainable low-rank parameters into the frozen LDM to embed watermark, thereby preserving the integrity of the original model weights. Furthermore, a dynamic loss weight scheduler is designed to adaptively balance the objectives of generative quality and watermark fidelity, enabling the model to achieve effective watermark embedding with minimal impact on quality of the generated images. Experimental results show that the proposed method ensures fast and accurate watermark embedding and a high quality of the generated images, at the same time maintaining a level of robustness aligned - in some cases superior - with state-of-the-art approaches. Moreover, the method generalizes well across different datasets and base LDMs. Codes are available at: https://github.com/MrDongdongLin/EW-LoRA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models</title>
<link>https://arxiv.org/abs/2410.20595</link>
<guid>https://arxiv.org/abs/2410.20595</guid>
<content:encoded><![CDATA[
arXiv:2410.20595v4 Announce Type: replace 
Abstract: In volcano monitoring, effective recognition of seismic events is essential for understanding volcanic activity and raising timely warning alerts. Traditional methods rely on manual analysis, which can be subjective and labor-intensive. Furthermore, current automatic approaches often tackle detection and classification separately, mostly rely on single station information and generally require tailored preprocessing and representations to perform predictions. These limitations often hinder their application to real-time monitoring and utilization across different volcano conditions. This study introduces a novel approach that utilizes Semantic Segmentation models to automate seismic event recognition by applying a straight forward transformation of multi-channel 1D signals into 2D representations, enabling their use as images. Our framework employs a data-driven, end-to-end design that integrates multi-station seismic data with minimal preprocessing, performing both detection and classification simultaneously for five seismic event classes. We evaluated four state-of-the-art segmentation models (UNet, UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events recorded at four different Chilean volcanoes: Nevados del Chill\'an Volcanic Complex, Laguna del Maule, Villarrica and Puyehue-Cord\'on Caulle. Among these models, the UNet architecture was identified as the most effective model, achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and 0.88, respectively, and demonstrating superior noise robustness and model flexibility to unseen volcano datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2411.02747</link>
<guid>https://arxiv.org/abs/2411.02747</guid>
<content:encoded><![CDATA[
arXiv:2411.02747v3 Announce Type: replace 
Abstract: Monocular 3D object detection has attracted great attention due to simplicity and low cost. Existing methods typically follow conventional 2D detection paradigms, first locating object centers and then predicting 3D attributes via neighboring features. However, these methods predominantly rely on progressive cross-scale feature aggregation and focus solely on local information, which may result in a lack of global awareness and the omission of small-scale objects. In addition, due to large variation in object scales across different scenes and depths, inaccurate receptive fields often lead to background noise and degraded feature representation. To address these issues, we introduces MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Specifically, EH-FAM employs multi-head attention with a global receptive field to extract semantic features for small-scale objects and leverages lightweight convolutional modules to efficiently aggregate visual features across different scales. The ASRH encodes 2D bounding box dimensions and then fuses scale features with the semantic features aggregated by EH-FAM through a scale-semantic feature fusion module. The scale-semantic feature fusion module guides ASRH in learning dynamic receptive field offsets, incorporating scale priors into 3D position prediction for better scale-awareness. Extensive experiments on the KITTI and Waymo datasets demonstrate that MonoASRH achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Long-Tailed Learning: Insights from an Architectural Perspective</title>
<link>https://arxiv.org/abs/2411.06098</link>
<guid>https://arxiv.org/abs/2411.06098</guid>
<content:encoded><![CDATA[
arXiv:2411.06098v4 Announce Type: replace 
Abstract: Long-Tailed (LT) recognition has been widely studied to tackle the challenge of imbalanced data distributions in real-world applications. However, the design of neural architectures for LT settings has received limited attention, despite evidence showing that architecture choices can substantially affect performance. This paper aims to bridge the gap between LT challenges and neural network design by providing an in-depth analysis of how various architectures influence LT performance. Specifically, we systematically examine the effects of key network components on LT handling, such as topology, convolutions, and activation functions. Based on these observations, we propose two convolutional operations optimized for improved performance. Recognizing that operation interactions are also crucial to network effectiveness, we apply Neural Architecture Search (NAS) to facilitate efficient exploration. We propose LT-DARTS, a NAS method with a novel search space and search strategy specifically designed for LT data. Experimental results demonstrate that our approach consistently outperforms existing architectures across multiple LT datasets, achieving parameter-efficient, state-of-the-art results when integrated with current LT methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Angles Improves Feature Distillation</title>
<link>https://arxiv.org/abs/2411.15239</link>
<guid>https://arxiv.org/abs/2411.15239</guid>
<content:encoded><![CDATA[
arXiv:2411.15239v3 Announce Type: replace 
Abstract: Knowledge distillation methods compress models by training a student network using the classification outputs of a high quality teacher model, but can fail to effectively transfer the properties of computer vision foundation models from the teacher to the student. While it has been recently shown that feature distillation$\unicode{x2013}$where a teacher model's output features are replicated instead$\unicode{x2013}$can reproduce performance for foundation models across numerous downstream tasks, they fall short in matching critical properties such as robustness and out-of-distribution (OOD) detection performance. This paper overcomes this shortcoming by introducing Cosine-similarity Preserving Compression (CosPress), a feature distillation technique that learns a mapping to compress the latent space of the teacher model into the smaller latent space of the student, by preserving the cosine similarities between image embeddings. This enables direct optimisation of the student network and produces a more faithful reproduction of the teacher's properties. It is shown that distillation with CosPress on a variety of datasets, including ImageNet, produces more accurate models with greater performance on generalisability, robustness and OOD detection benchmarks, and that this technique provides a competitive pathway for training highly performant lightweight models on small datasets. Code is available at github.com/emannix/cospress.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration</title>
<link>https://arxiv.org/abs/2411.17686</link>
<guid>https://arxiv.org/abs/2411.17686</guid>
<content:encoded><![CDATA[
arXiv:2411.17686v4 Announce Type: replace 
Abstract: The quadratic complexity of Multimodal Large Language Models (MLLMs) with respect to context length poses significant computational and memory challenges, hindering their real-world deployment. In the paper, we devise a ''filter-correlate-compress'' framework to accelerate the MLLM by systematically optimizing multimodal context length during prefilling. The framework first implements FiCoCo-V, a training-free method operating within the vision encoder. It employs a redundancy-based token discard mechanism that uses a novel integrated metric to accurately filter out redundant visual tokens. To mitigate information loss, the framework introduces a correlation-based information recycling mechanism that allows preserved tokens to selectively recycle information from correlated discarded tokens with a self-preserving compression, thereby preventing the dilution of their own core content. The framework's FiCoCo-L variant further leverages task-aware textual priors to perform token reduction directly within the LLM decoder. Extensive experiments demonstrate that the FiCoCo series effectively accelerates a range of MLLMs, achieves up to 14.7x FLOPs reduction with 93.6% performance retention. Our methods consistently outperform state-of-the-art training-free approaches, showcasing effectiveness and generalizability across model architectures, sizes, and tasks without requiring retraining. Code: https://github.com/kawhiiiileo/FiCoCo
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density-aware global-local attention network for point cloud segmentation</title>
<link>https://arxiv.org/abs/2412.00489</link>
<guid>https://arxiv.org/abs/2412.00489</guid>
<content:encoded><![CDATA[
arXiv:2412.00489v2 Announce Type: replace 
Abstract: 3D point cloud segmentation has a wide range of applications in areas such as autonomous driving, augmented reality, virtual reality and digital twins. The point cloud data collected in real scenes often contain small objects and categories with small sample sizes, which are difficult to handle by existing networks. In this regard, we propose a point cloud segmentation network that fuses local attention based on density perception with global attention. The core idea is to increase the effective receptive field of each point while reducing the loss of information about small objects in dense areas. Specifically, we divide different sized windows for local areas with different densities to compute attention within the window. Furthermore, we consider each local area as an independent token for the global attention of the entire input. A category-response loss is also proposed to balance the processing of different categories and sizes of objects. In particular, we set up an additional fully connected layer in the middle of the network for prediction of the presence of object categories, and construct a binary cross-entropy loss to respond to the presence of categories in the scene. In experiments, our method achieves competitive results in semantic segmentation and part segmentation tasks on several publicly available datasets. Experiments on point cloud data obtained from complex real-world scenes filled with tiny objects also validate the strong segmentation capability of our method for small objects as well as small sample categories.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models</title>
<link>https://arxiv.org/abs/2412.11050</link>
<guid>https://arxiv.org/abs/2412.11050</guid>
<content:encoded><![CDATA[
arXiv:2412.11050v4 Announce Type: replace 
Abstract: Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, a cross-modal alignment training method for embedding models with hard and semi-hard negative mining, and a fast querying and retrieval pipeline based on K-Means clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoBDA: Towards Bezier Deformable Attention for Road Topology Understanding</title>
<link>https://arxiv.org/abs/2412.18951</link>
<guid>https://arxiv.org/abs/2412.18951</guid>
<content:encoded><![CDATA[
arXiv:2412.18951v3 Announce Type: replace 
Abstract: Understanding road topology is crucial for autonomous driving. This paper introduces TopoBDA (Topology with Bezier Deformable Attention), a novel approach that enhances road topology comprehension by leveraging Bezier Deformable Attention (BDA). TopoBDA processes multi-camera 360-degree imagery to generate Bird's Eye View (BEV) features, which are refined through a transformer decoder employing BDA. BDA utilizes Bezier control points to drive the deformable attention mechanism, improving the detection and representation of elongated and thin polyline structures, such as lane centerlines. Additionally, TopoBDA integrates two auxiliary components: an instance mask formulation loss and a one-to-many set prediction loss strategy, to further refine centerline detection and enhance road topology understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate that TopoBDA outperforms existing methods, achieving state-of-the-art results in centerline detection and topology reasoning. TopoBDA also achieves the best results on the OpenLane-V1 dataset in 3D lane detection. Further experiments on integrating multi-modal data -- such as LiDAR, radar, and SDMap -- show that multimodal inputs can further enhance performance in road topology understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach</title>
<link>https://arxiv.org/abs/2501.01042</link>
<guid>https://arxiv.org/abs/2501.01042</guid>
<content:encoded><![CDATA[
arXiv:2501.01042v3 Announce Type: replace 
Abstract: Video-based multimodal large language models (V-MLLMs) have shown vulnerability to adversarial examples in video-text multimodal tasks. However, the transferability of adversarial videos to unseen models - a common and practical real-world scenario - remains unexplored. In this paper, we pioneer an investigation into the transferability of adversarial video samples across V-MLLMs. We find that existing adversarial attack methods face significant limitations when applied in black-box settings for V-MLLMs, which we attribute to the following shortcomings: (1) lacking generalization in perturbing video features, (2) focusing only on sparse key-frames, and (3) failing to integrate multimodal information. To address these limitations and deepen the understanding of V-MLLM vulnerabilities in black-box scenarios, we introduce the Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an image-based multimodal large language model (I-MLLM) as a surrogate model to craft adversarial video samples. Multimodal interactions and spatiotemporal information are integrated to disrupt video representations within the latent space, improving adversarial transferability. Additionally, a perturbation propagation technique is introduced to handle different unknown frame sampling strategies. Experimental results demonstrate that our method can generate adversarial examples that exhibit strong transferability across different V-MLLMs on multiple video-text multimodal tasks. Compared to white-box attacks on these models, our black-box attacks (using BLIP-2 as a surrogate model) achieve competitive performance, with average attack success rate (AASR) of 57.98% on MSVD-QA and 58.26% on MSRVTT-QA for Zero-Shot VideoQA tasks, respectively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DehazeGS: Seeing Through Fog with 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2501.03659</link>
<guid>https://arxiv.org/abs/2501.03659</guid>
<content:encoded><![CDATA[
arXiv:2501.03659v5 Announce Type: replace 
Abstract: Current novel view synthesis methods are typically designed for high-quality and clean input images. However, in foggy scenes, scattering and attenuation can significantly degrade the quality of rendering. Although NeRF-based dehazing approaches have been developed, their reliance on deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Furthermore, NeRF's implicit representation limits its ability to recover fine-grained details from hazy scenes. To overcome these limitations, we propose learning an explicit Gaussian representation to explain the formation mechanism of foggy images through a physically forward rendering process. Our method, DehazeGS, reconstructs and renders fog-free scenes using only multi-view foggy images as input. Specifically, based on the atmospheric scattering model, we simulate the formation of fog by establishing the transmission function directly onto Gaussian primitives via depth-to-transmission mapping. During training, we jointly learn the atmospheric light and scattering coefficients while optimizing the Gaussian representation of foggy scenes. At inference time, we remove the effects of scattering and attenuation in Gaussian distributions and directly render the scene to obtain dehazed views. Experiments on both real-world and synthetic foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance. visualizations are available at https://dehazegs.github.io/
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUSLO: General and Unified Structured Light Optimization</title>
<link>https://arxiv.org/abs/2501.14659</link>
<guid>https://arxiv.org/abs/2501.14659</guid>
<content:encoded><![CDATA[
arXiv:2501.14659v2 Announce Type: replace 
Abstract: Structured light (SL) 3D reconstruction captures the precise surface shape of objects, providing high-accuracy 3D data essential for industrial inspection and cultural heritage digitization. However, existing methods suffer from two key limitations: reliance on scene-specific calibration with manual parameter tuning, and optimization frameworks tailored to specific SL patterns, limiting their generalizability across varied scenarios. We propose General and Unified Structured Light Optimization (GUSLO), a novel framework addressing these issues through two coordinated innovations: (1) single-shot calibration via 2D triangulation-based interpolation that converts sparse matches into dense correspondence fields, and (2) artifact-aware photometric adaptation via explicit transfer functions, balancing generalization and color fidelity. We conduct diverse experiments covering binary, speckle, and color-coded settings. Results show that GUSLO consistently improves accuracy and cross-encoding robustness over conventional methods in challenging industrial and cultural scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding</title>
<link>https://arxiv.org/abs/2502.10810</link>
<guid>https://arxiv.org/abs/2502.10810</guid>
<content:encoded><![CDATA[
arXiv:2502.10810v2 Announce Type: replace 
Abstract: Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://github.com/sotayang/SVBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>