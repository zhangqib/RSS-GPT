<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title>
<link>https://arxiv.org/abs/2512.19711</link>
<guid>https://arxiv.org/abs/2512.19711</guid>
<content:encoded><![CDATA[
<div> Keywords: Connected Autonomous Vehicles, Adversarial Attacks, Anamorphic Art, Object Detection, V2X Communication  

<br /><br />Summary:  
1. The paper addresses vulnerabilities in connected autonomous vehicles (CAVs) that depend on vision-based deep neural networks (DNNs) and low-latency Vehicle-to-Everything (V2X) communication for navigation.  
2. It introduces PHANTOM, a novel framework that uses anamorphic art to create perspective-dependent physical adversarial examples that deceive object detectors.  
3. PHANTOM works under black-box conditions without access to the internal model and shows strong transferability across four popular object detector architectures: YOLOv5, SSD, Faster R-CNN, and RetinaNet.  
4. Extensive testing in the CARLA simulation environment demonstrates that PHANTOM achieves over 90% attack success under ideal conditions and maintains 60-80% effectiveness even under challenging speeds, weather, and lighting scenarios.  
5. The attack activates when the target vehicle is within 6-10 meters, leaving insufficient time for safe countermeasures.  
6. Beyond fooling individual vehicles, PHANTOM causes network-wide disruption by triggering false emergency messages that spread through V2X communications.  
7. Co-simulations using SUMO and OMNeT++ indicate a 68-89% increase in Peak Age of Information, significantly degrading safety-critical communication in CAV systems.  
8. These findings reveal critical security weaknesses in both the perception and communication components of connected autonomous vehicle ecosystems. <div>
arXiv:2512.19711v1 Announce Type: new 
Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating the Past, Present and Future from a Motion-Blurred Image</title>
<link>https://arxiv.org/abs/2512.19817</link>
<guid>https://arxiv.org/abs/2512.19817</guid>
<content:encoded><![CDATA[
<div> Keywords: motion blur, video diffusion model, scene dynamics, video reconstruction, camera trajectory<br /><br />Summary:<br /><br />This paper investigates how a motion-blurred image can reveal information about a scene's past, present, and future states despite the typical degradation of visual details caused by blur. The authors critique prior methods that rely on handcrafted priors or specialized network architectures, noting their limitations in capturing complex scene dynamics and inability to recover events before or after the image capture. They propose a novel approach that utilizes a pre-trained video diffusion model trained on vast internet-scale datasets. This model repurposes implicit video and image priors to generate videos that disclose dynamic scene details during the exposure and predict what likely occurred immediately before and after the blurred frame. The approach is demonstrated to be robust and versatile, outperforming previous techniques in video reconstruction from a single blurred image. Moreover, it generalizes well to real-world, challenging images taken in the wild. Beyond video recovery, the method supports further downstream tasks such as estimating camera trajectories, object motions, and reconstructing the dynamic 3D structure of scenes. The code and datasets are made publicly accessible at the provided project website, encouraging further research and application development in this domain. <div>
arXiv:2512.19817v1 Announce Type: new 
Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Refocus with Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.19823</link>
<guid>https://arxiv.org/abs/2512.19823</guid>
<content:encoded><![CDATA[
<div> Keywords: post-capture refocusing, video diffusion models, focal stack, smartphone photography, focus editing<br /><br />Summary:  
1. The paper addresses the challenge of autofocus systems often capturing unintended subjects and users wanting to refocus images after capture.  
2. It introduces a novel method leveraging video diffusion models to generate realistic post-capture refocusing effects from a single defocused image.  
3. This method creates a perceptually accurate focal stack represented as a video sequence, allowing interactive refocusing on any part of the scene.  
4. A large-scale focal stack dataset has been released, featuring images captured under varied real-world smartphone conditions, to support this research and future advancements.  
5. Experimental results show that this approach outperforms existing methods in terms of perceptual quality and robustness in challenging scenarios, enabling enhanced focus-editing possibilities in everyday photography.  
6. The authors also provide public access to their code and dataset, encouraging further research and development in post-capture focus manipulation. <div>
arXiv:2512.19823v1 Announce Type: new 
Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RANSAC Scoring Functions: Analysis and Reality Check</title>
<link>https://arxiv.org/abs/2512.19850</link>
<guid>https://arxiv.org/abs/2512.19850</guid>
<content:encoded><![CDATA[
<div> RANSAC, geometric fitting, scoring function, MAGSAC++, robust estimation<br /><br />Summary:<br /><br />This work revisits the problem of assigning score functions to candidate geometric models, a central part of RANSAC algorithms used for robust geometric fitting. Traditionally, the geometric error scoring function is derived from a Gaussian noise probabilistic model; the authors extend this to accommodate spherical noise distributions. In robust settings with outliers modeled as a uniform distribution, they demonstrate that using a threshold-based parameterization unifies likelihood-based and robust M-estimators, along with their local optimization processes. The paper critically analyzes MAGSAC++, an influential method that achieves top benchmark performance but is shown to rest on unsound derivation principles. Notably, the authors prove that the MAGSAC++ scoring function is numerically equivalent to a simpler Gaussian-uniform likelihood model within their proposed framework. Furthermore, they introduce an experimental methodology for evaluating scoring functions using either large validation sets or small random ones in expectation. Their experiments reveal no performance differences among various scoring functions—including those leveraging learned inlier distributions—and show that MAGSAC++ is neither better performing nor less sensitive to threshold hyperparameter choices than simpler alternatives. This comprehensive theoretical and experimental reassessment is vital for guiding future research in improving robust geometric fitting approaches or adapting them to other problem domains. <div>
arXiv:2512.19850v1 Announce Type: new 
Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</title>
<link>https://arxiv.org/abs/2512.19871</link>
<guid>https://arxiv.org/abs/2512.19871</guid>
<content:encoded><![CDATA[
<div> 3D Panoptic Occupancy Prediction, geometric consistency, hybrid view-transformation, Gaussian depth representation, edge awareness  

<br /><br />Summary:  
3D Panoptic Occupancy Prediction involves reconstructing a detailed volumetric scene map by assigning semantic classes and instance identities to all occupied regions in 3D space, enabling fine-grained spatial understanding. Achieving this level of detail requires accurate geometric reasoning and maintaining spatial consistency within complex environments. Existing methods often face challenges in preserving precise geometry and accurately delineating the spatial extent of 3D instances, which are crucial for effective panoptic separation. To address these challenges, the paper introduces HyGE-Occ, a novel framework designed to enhance geometric consistency and boundary awareness through a hybrid view-transformation approach. HyGE-Occ combines a continuous Gaussian-based depth representation with a discretized depth-bin formulation to produce bird’s-eye view (BEV) features that exhibit improved structural coherence. Additionally, it integrates edge maps extracted from these BEV features as auxiliary inputs to better learn edge cues, further refining instance boundaries. Evaluations on the Occ3D-nuScenes dataset demonstrate that HyGE-Occ surpasses previous methods, showcasing superior 3D geometric reasoning and more accurate panoptic occupancy predictions. This approach provides a promising direction for advancing fine-grained 3D scene understanding in complex real-world scenarios. <div>
arXiv:2512.19871v1 Announce Type: new 
Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</title>
<link>https://arxiv.org/abs/2512.19918</link>
<guid>https://arxiv.org/abs/2512.19918</guid>
<content:encoded><![CDATA[
<div> Keywords: UI2Code, Widget2Code, widget benchmark, WidgetDSL, WidgetFactory<br /><br />Summary:<br /><br />User interface to code (UI2Code) typically focuses on generating executable code for web pages and mobile screens, but app widgets remain underexplored due to their compact, context-free, and iconography-dense layouts under strict spatial constraints. Unlike web or mobile UIs, widgets lack accessible markup because their designs are proprietary, making paired image-code datasets unavailable. This paper formalizes the Widget-to-Code (Widget2Code) problem and introduces a novel image-only widget benchmark equipped with fine-grained, multi-dimensional evaluation metrics to assess model performance.<br /><br />Benchmark experiments reveal that generalized multimodal large language models (MLLMs) currently outperform specialized UI2Code methods but still struggle to produce reliable and visually consistent code for widgets. To tackle these challenges, the authors develop a new baseline that enhances both perceptual understanding and structured code generation. Perceptually, they follow widget design principles to assemble atomic components into complete layouts, incorporating icon retrieval and reusable visualization modules. System-wise, they design WidgetFactory, an end-to-end infrastructure featuring a framework-independent, widget-specific domain-specific language called WidgetDSL, alongside a compiler translating WidgetDSL into multiple front-end frameworks (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to meet compactness constraints, greatly improving visual fidelity and establishing a strong foundation for future Widget2Code research. <div>
arXiv:2512.19918v1 Announce Type: new 
Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Brain Surface and Volume Registration</title>
<link>https://arxiv.org/abs/2512.19928</link>
<guid>https://arxiv.org/abs/2512.19928</guid>
<content:encoded><![CDATA[
<div> Keywords: brain MRI registration, cortical surface, volumetric alignment, deep learning, spherical coordinate space  

<br /><br />Summary: Accurate registration of brain MRI scans is crucial for neuroscientific studies involving cross-subject analysis, requiring alignment of both the cortical surface and interior brain volume. Traditional approaches treat volumetric and surface registration separately, resulting in inconsistencies that hinder downstream analyses. The proposed method, NeurAlign, introduces a deep learning framework that jointly registers 3D brain MRI images by integrating both cortical and subcortical regions using a unified volume-and-surface-based representation. This is achieved by leveraging an intermediate spherical coordinate space that bridges anatomical surface topology with volumetric anatomy, enabling consistent and anatomically precise alignment. By incorporating spherical registration into the learning process, NeurAlign maintains geometric coherence across volume and surface domains. Experimental validation on diverse datasets demonstrates that NeurAlign consistently surpasses classical and machine learning registration techniques, improving Dice scores by up to 7 points while preserving regular deformation fields. Moreover, it significantly reduces inference time, being orders of magnitude faster than standard methods, and offers enhanced usability by requiring only the MRI scan as input. Overall, NeurAlign advances the state of the art by delivering superior accuracy, rapid processing, and streamlined application for joint cortical and subcortical brain MRI registration. <div>
arXiv:2512.19928v1 Announce Type: new 
Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vehicle-centric Perception via Multimodal Structured Pre-training</title>
<link>https://arxiv.org/abs/2512.19934</link>
<guid>https://arxiv.org/abs/2512.19934</guid>
<content:encoded><![CDATA[
<div> Vehicle-centric perception, pre-training, masked token reconstruction, structured priors, large-scale dataset<br /><br />Summary: Vehicle-centric perception is vital for intelligent systems like surveillance, transportation, and autonomous driving. Existing methods struggle with learning vehicle-related knowledge during pre-training, limiting their ability to generate effective vehicle perception representations. VehicleMAE-V2 addresses this by leveraging vehicle-related multimodal structured priors to guide masked token reconstruction, enhancing generalizable vehicle-centric features. The model introduces three novel modules: Symmetry-guided Mask Module (SMM), which utilizes vehicle symmetry to select high-quality masked patches and reduce redundancy; Contour-guided Representation Module (CRM), which aligns contour features with reconstructed features to preserve holistic vehicle structure in pixel-level reconstruction; and Semantics-guided Representation Module (SRM), which employs contrastive learning and cross-modal distillation to align image-text features, mitigating semantic confusion during reconstruction. To facilitate pre-training, the Autobot4M dataset was created, containing roughly 4 million vehicle images paired with 12,693 text descriptions, providing diverse and large-scale data. Extensive experiments across five downstream tasks demonstrate that VehicleMAE-V2 achieves superior performance and generalization capabilities, highlighting its effectiveness in improving vehicle-centric perception representation learning. <div>
arXiv:2512.19934v1 Announce Type: new 
Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block-Recurrent Dynamics in Vision Transformers</title>
<link>https://arxiv.org/abs/2512.19941</link>
<guid>https://arxiv.org/abs/2512.19941</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, Block-Recurrent Hypothesis, Raptor, dynamical systems, representational similarity matrices<br /><br />Summary:<br /><br />1. The paper addresses the need for a mechanistic understanding of Vision Transformers (ViTs) by proposing the Block-Recurrent Hypothesis (BRH), which conceptualizes ViT depth as a recurrent structure where a small number of distinct blocks are reused instead of many unique layers.  
2. Analysis of representational similarity matrices across layers shows phase-like contiguous blocks, suggesting that computation phases may be reusable.  
3. To test this, the authors introduce Raptor, a block-recurrent surrogate model trained to approximate pretrained ViTs by repeatedly applying fewer blocks. They find that stochastic depth and training encourage such recurrent structures, improving Raptor’s fit.  
4. Empirically, a Raptor model with only 2 recurrent blocks achieves 96% of the linear probe accuracy on ImageNet-1k from DINOv2, at roughly the same computational cost as the original large ViT, supporting BRH’s validity.  
5. Leveraging BRH enables Dynamical Interpretability, revealing that ViT computations converge directionally into class-specific angular basins with stable, self-correcting trajectories, exhibit token-specific late dynamics with class token sharp reorientations and patch token coherence, and collapse to low-rank updates consistent with low-dimensional attractor dynamics. This points toward a low-complexity, principled dynamical systems framework for understanding ViTs. <div>
arXiv:2512.19941v1 Announce Type: new 
Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction</title>
<link>https://arxiv.org/abs/2512.19943</link>
<guid>https://arxiv.org/abs/2512.19943</guid>
<content:encoded><![CDATA[
<div> Keywords: 360-degree panoramas, image editing, Vision-Language Model, Transformer diffusion model, data generation pipeline<br /><br />Summary:<br /><br />This paper addresses the challenges of instruction-based image editing specifically for 360° panoramas, which often suffer from implausible results in both equirectangular and perspective projections. To overcome these obstacles, the authors introduce SE360, a novel framework designed for multi-condition guided object editing in 360° panoramic images. Central to SE360 is a coarse-to-fine autonomous data generation pipeline that operates without manual labeling. This pipeline utilizes a Vision-Language Model (VLM) alongside adaptive projection adjustments to perform hierarchical analysis, resulting in holistic segmentation of objects in their physical context. Such a method ensures that the produced data pairs maintain semantic meaning and geometric consistency, even when derived from unlabeled panoramas. To further enhance data realism and reduce the risk of the editing model overfitting to erase artifacts, the authors propose a cost-effective two-stage data refinement strategy. Leveraging the refined dataset, a Transformer-based diffusion model is trained to enable flexible object editing in 360° panoramas, guided by text prompts, masks, or reference images. Experimental results demonstrate that SE360 surpasses existing state-of-the-art methods in both visual quality and semantic accuracy of the edited panoramas. <div>
arXiv:2512.19943v1 Announce Type: new 
Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Much 3D Do Video Foundation Models Encode?</title>
<link>https://arxiv.org/abs/2512.19949</link>
<guid>https://arxiv.org/abs/2512.19949</guid>
<content:encoded><![CDATA[
<div> 3D understanding, Video Foundation Models, model-agnostic framework, video generation, 3D benchmarking<br /><br />Summary:<br /><br />1. The paper investigates whether global 3D understanding naturally emerges in Video Foundation Models (VidFMs) trained solely on large-scale 2D video data, considering videos as continuous 2D projections of 3D worlds. <br /><br />2. The authors introduce the first model-agnostic framework designed to measure the 3D awareness of various VidFMs. This framework estimates multiple 3D properties by applying shallow read-outs on the features extracted from these models, without requiring any 3D-specific training. <br /><br />3. The study conducts a comprehensive evaluation of several state-of-the-art VidFMs, revealing their capacity to implicitly encode strong 3D representations of objects and scenes despite the absence of explicit 3D supervision during training. <br /><br />4. Remarkably, some video generation models demonstrate better 3D understanding than specialized large models explicitly trained for 3D tasks, highlighting the surprising effectiveness of large-scale video pretraining. <br /><br />5. The findings and the provided 3D benchmarking results serve as valuable guidance for the future development and scaling of models that aim to integrate 3D perception from video data, promoting more scalable and generalizable 3D understanding in machine learning systems. <div>
arXiv:2512.19949v1 Announce Type: new 
Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes</title>
<link>https://arxiv.org/abs/2512.19954</link>
<guid>https://arxiv.org/abs/2512.19954</guid>
<content:encoded><![CDATA[
<div> Keywords: HistoWAS, spatial analysis, pathology, whole slide images, Kidney Precision Medicine Project<br /><br />Summary:<br /><br />1. HistoWAS (Histology-Wide Association Study) is a computational framework developed to analyze tissue spatial organization and link it to clinical outcomes. <br />2. It enhances traditional tissue feature metrics by integrating 30 topological and spatial features inspired by Geographic Information Systems (GIS) point pattern analysis, enabling detailed quantification of tissue micro-architecture. <br />3. The framework includes an association study engine modeled after Phenome-Wide Association Studies (PheWAS), conducting mass univariate regression for each feature while applying rigorous statistical corrections for robust results. <br />4. As a demonstration, HistoWAS was applied to 385 PAS-stained whole slide images from 206 participants in the Kidney Precision Medicine Project (KPMP), analyzing 102 features in total—72 conventional object-level features plus 30 spatial features. <br />5. The study highlights HistoWAS's capability to provide novel insights into tissue structure-clinical outcome relationships by leveraging advanced spatial metrics, and the code and data are publicly available on GitHub for reproducibility and further research. <div>
arXiv:2512.19954v1 Announce Type: new 
Abstract: High-throughput "pathomic" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2512.19982</link>
<guid>https://arxiv.org/abs/2512.19982</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiple Instance Learning, Transformer, Whole Slide Image, Attention Mechanism, Computational Pathology

<br /><br />Summary:  
This paper addresses limitations in current multiple instance learning (MIL) methods for computational pathology, particularly their inability to fully capture complex semantic relationships among instances in whole slide images (WSI). Existing Transformer-based MIL approaches, while capable of modeling instance dependencies, suffer from quadratic computational complexity that restricts their scalability to large WSIs. Additionally, fixed-scale attention mechanisms struggle with the varying tumor region scales and fail to incorporate distance-based decay effects for patch relevance. To overcome these, the authors propose the window scale decay MIL (WSD-MIL), which improves the modeling of tumor regions at multiple scales and enhances computational efficiency. WSD-MIL features a window scale decay attention module that uses a cluster-based sampling strategy to reduce computation and progressively decays attention window sizes to capture local instance relationships at different scales. It also incorporates a squeeze-and-excitation based region gate module that dynamically adjusts window weights to better integrate global information. Experimental validation on CAMELYON16 and TCGA-BRCA datasets demonstrates that WSD-MIL achieves state-of-the-art diagnostic accuracy while reducing computational memory usage by 62%. The authors plan to make their code publicly available to facilitate further research and application. <div>
arXiv:2512.19982v1 Announce Type: new 
Abstract: In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</title>
<link>https://arxiv.org/abs/2512.19989</link>
<guid>https://arxiv.org/abs/2512.19989</guid>
<content:encoded><![CDATA[
<div> Keywords: Guava disease detection, CNN, Gradient Boosting Machine, Anthracnose, Fruit Fly infection<br /><br />Summary: This paper focuses on improving guava disease detection in Bangladesh, a country where guava cultivation plays a vital economic role. The study addresses major threats to guava quality and productivity, specifically Anthracnose and fruit fly infections. To facilitate detection, a new Guava Fruit Disease Dataset 2024 (GFDD24) was developed, containing images of healthy guava fruits as well as those affected by the diseases, collected from plantations in Rajshahi and Pabna regions. The researchers propose a hybrid modeling approach combining Convolutional Neural Networks (CNN) with traditional machine learning methods, specifically utilizing an ensemble that integrates CNN with a Gradient Boosting Machine. This cascade framework aims to leverage the feature extraction power of CNNs and the classification strength of gradient boosting to enhance disease identification accuracy. The study reports achieving approximately 99.99% classification accuracy on the locally sourced dataset, indicating the model's robustness and effectiveness. The proposed ensemble model and CNN-ML cascade system demonstrate strong potential for implementation in real-time agricultural monitoring, helping farmers in Bangladesh detect guava diseases early, thereby reducing crop losses and supporting sustainable agricultural development. This research contributes to advancing precision agriculture through AI-based disease monitoring tools tailored to local crop conditions. <div>
arXiv:2512.19989v1 Announce Type: new 
Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</title>
<link>https://arxiv.org/abs/2512.19990</link>
<guid>https://arxiv.org/abs/2512.19990</guid>
<content:encoded><![CDATA[
<div> Cross-resolution, land cover mapping, weak supervision, diffusion model, transformer

<br /><br />Summary:  
This paper addresses the challenge of cross-resolution land cover mapping, where the goal is to generate high-resolution semantic maps using only coarse or low-resolution supervision. Existing weakly supervised methods face difficulties due to the resolution mismatch, resulting in noisy labels and poor mapping accuracy. To overcome these issues, the authors propose DDTM, a dual-branch framework that separates local semantic refinement from global contextual reasoning. The first branch leverages a diffusion-based model to gradually improve fine-scale local details under coarse supervision. The second branch utilizes a transformer architecture to capture and enforce long-range spatial consistency over broad areas. To further enhance performance, a pseudo-label confidence evaluation module is introduced to identify and mitigate noise caused by cross-resolution inconsistencies, allowing the model to selectively trust reliable supervisory signals. Extensive experiments on the Chesapeake Bay benchmark demonstrate that DDTM achieves a new state-of-the-art mean intersection-over-union (mIoU) score of 66.52%, significantly outperforming previous weakly supervised techniques. The authors also provide open access to their code, enabling reproducibility and further research. This work presents a promising direction for improving weakly supervised land cover mapping under challenging resolution discrepancies. <div>
arXiv:2512.19990v1 Announce Type: new 
Abstract: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.20000</link>
<guid>https://arxiv.org/abs/2512.20000</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image animation, Modular Image-to-Video Adapter, motion control, training efficiency  

<br /><br />Summary: Diffusion models (DMs) have recently shown strong results in photorealistic image and video generation but face significant challenges in image animation tasks. The primary difficulties stem from the high dimensionality of video data, which limits available training samples, causing DMs to memorize rather than accurately follow motion prompts. Additionally, these models struggle to generalize innovations in motion patterns not included in training data, and fine-tuning for new patterns with limited samples is not well studied. To overcome these challenges, the authors propose the Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to existing pre-trained diffusion models. Each MIVA module is specialized to capture a single motion pattern, and multiple modules can be combined in parallel for more complex motion specification. MIVA modules can be efficiently trained with as few as approximately ten samples on consumer-grade GPUs, greatly reducing resource demands. At inference, users can explicitly control motion by choosing one or several MIVAs, removing the need for complex prompt engineering. Extensive experiments demonstrate that MIVA not only enables more precise and diverse motion control but also maintains or even improves the generation quality compared to models trained on much larger datasets. <div>
arXiv:2512.20000v1 Announce Type: new 
Abstract: Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</title>
<link>https://arxiv.org/abs/2512.20011</link>
<guid>https://arxiv.org/abs/2512.20011</guid>
<content:encoded><![CDATA[
<div> Keywords: pavement defect detection, dataset standardization, object detection, benchmark, zero-shot transfer

<br /><br />Summary:  
This paper addresses challenges in automated pavement defect detection caused by inconsistencies in existing datasets, such as varied annotation styles, distress type definitions, and formats. To overcome these issues, the authors present a comprehensive benchmark dataset that unifies multiple publicly available datasets into a single standardized collection. The dataset contains 52,747 images from seven countries, annotated with 135,277 bounding boxes covering 13 distinct pavement distress types. It captures wide real-world variability, including differences in image quality, resolution, viewing angles, and weather conditions, making it a unique resource for robust training and evaluation. The dataset's effectiveness is validated by benchmarking state-of-the-art object detection models like YOLOv8-YOLOv12, Faster R-CNN, and DETR, which demonstrate competitive performance across diverse environments. Standardizing class definitions and annotation formats establishes this benchmark as the first globally representative dataset for pavement defect detection. This foundation facilitates fair and consistent comparison between detection models and enables zero-shot transfer learning to new, unseen environments, promoting broader applicability and robustness of automated pavement defect detection systems. <div>
arXiv:2512.20011v1 Announce Type: new 
Abstract: Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.20013</link>
<guid>https://arxiv.org/abs/2512.20013</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, language-guided segmentation, LaSeRS dataset, SegEarth-R2, geospatial reasoning<br /><br />Summary:  
This paper addresses the challenge of effectively grounding complex language instructions to pixels in remote sensing (RS) images, which is vital for applications such as disaster response and environmental monitoring. Existing models struggle with complex geospatial reasoning involving hierarchical granularity, multiple targets, reasoning, and linguistic variability. To overcome these limitations, the authors introduce LaSeRS, the first large-scale dataset designed specifically to train and evaluate models across these four critical dimensions of language-guided segmentation, moving beyond simplified commands to better mirror real-world complexity. Additionally, the paper presents SegEarth-R2, a Multimodal Large Language Model (MLLM) architecture tailored for comprehensive language-guided segmentation in RS tasks. SegEarth-R2 features two key innovations: a spatial attention supervision mechanism for precise localization of small objects and their parts, and a versatile segmentation query mechanism capable of handling both single-target and multi-target instructions efficiently. Experimental results demonstrate that SegEarth-R2 achieves state-of-the-art performance on the LaSeRS benchmark as well as other established datasets, establishing a robust baseline for future geospatial segmentation research. The authors plan to publicly release all data and code to facilitate further advances in this important domain. <div>
arXiv:2512.20013v1 Announce Type: new 
Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</title>
<link>https://arxiv.org/abs/2512.20025</link>
<guid>https://arxiv.org/abs/2512.20025</guid>
<content:encoded><![CDATA[
<div> Keywords: distracted driving detection, dual-view inputs, spatiotemporal action recognition, SlowFast-R50, multimodal fusion<br /><br />Summary:<br /><br />This study explores the benefit of incorporating road-facing camera views alongside traditional driver-facing footage to improve distracted driving detection accuracy in naturalistic conditions. The researchers utilize synchronized dual-camera recordings gathered from real-world driving scenarios. They benchmark three prominent spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50, evaluating each under two input configurations—driver-facing only and stacked dual-view inputs. Results reveal that adding contextual road-facing input can yield accuracy improvements, but the extent varies depending on the model architecture. Specifically, the single-pathway SlowOnly-R50 model achieved a significant 9.8% accuracy increase with dual-view inputs, while the dual-pathway SlowFast-R50 model actually suffered a 7.2% performance drop due to representational conflicts caused by multi-view integration. This indicates that simply adding more visual context is not inherently beneficial; instead, architectures must be explicitly designed to handle fusion of multi-view data to prevent interference. These findings represent one of the first systematic comparisons of single- and dual-view distraction detection systems on naturalistic driving data. The study highlights the importance of fusion-aware architectural design for future multimodal driver monitoring to effectively leverage environmental context. <div>
arXiv:2512.20025v1 Announce Type: new 
Abstract: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</title>
<link>https://arxiv.org/abs/2512.20026</link>
<guid>https://arxiv.org/abs/2512.20026</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Multimodal Medical Diagnosis, Dynamic Graph Construction, Disentangled Feature Subspaces, Relational Fusion Engine<br /><br />Summary:<br /><br />The article addresses limitations in current graph neural networks (GNNs) applied to multimodal medical diagnosis, specifically their reliance on a single static graph created from undifferentiated features which restricts patient-specific pathology modeling. To overcome this, the authors propose the Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN), which moves beyond the single-graph approach by learning a multifaceted graph profile derived from semantically disentangled feature subspaces. The framework first employs a multi-dimensional discriminator to reveal latent graph-aware patterns within the data. These extracted patterns facilitate the dynamic construction of multiple activation graphs rather than a single static graph, enabling a richer and more nuanced relational representation. These multifaceted activation graphs are subsequently aggregated and contextualized through a relational fusion engine, enhancing the robustness and accuracy of medical diagnosis. The effectiveness of MAPI-GNN is validated through extensive experiments on two distinct medical diagnosis tasks encompassing over 1300 patient samples. Results demonstrate that MAPI-GNN significantly outperforms current state-of-the-art methods, highlighting the benefits of dynamic graph construction and feature disentanglement in medical GNN applications. This work offers a new paradigm for leveraging complex patient data for improved diagnostic performance in clinical settings. <div>
arXiv:2512.20026v1 Announce Type: new 
Abstract: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.20029</link>
<guid>https://arxiv.org/abs/2512.20029</guid>
<content:encoded><![CDATA[
<div> Compositional Zero-Shot Learning, Hyperbolic Embeddings, Hierarchical Entailment, Cross-Modal Attention, Semantic Hierarchy<br /><br />Summary:<br /><br />1. The paper addresses Compositional Zero-Shot Learning (CZSL), which focuses on recognizing unseen combinations of states and objects by leveraging known primitives. 2. Existing methods generally neglect hierarchical structures such as semantic hierarchies of primitives and conceptual hierarchies between primitives and compositions, and those that incorporate hierarchies using Euclidean space struggle to scale to large taxonomies due to volume growth limitations affecting generalization. 3. To overcome these challenges, the authors propose H2em, a framework that learns hierarchical embeddings in hyperbolic space, which more naturally represents tree-like taxonomies with lower distortion. 4. The method introduces two specialized loss functions: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce hierarchical relations, and a Discriminative Alignment Loss with hard negative mining to enhance fine-grained discrimination by increasing geodesic distances between semantically similar compositions. 5. Additionally, H2em features Hyperbolic Cross-Modal Attention to perform instance-aware fusion of information across modalities within hyperbolic geometry. 6. Extensive experiments on three benchmark datasets demonstrate that H2em achieves state-of-the-art performance in both closed-world and open-world CZSL settings, with code planned for public release. <div>
arXiv:2512.20029v1 Announce Type: new 
Abstract: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</title>
<link>https://arxiv.org/abs/2512.20032</link>
<guid>https://arxiv.org/abs/2512.20032</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Speech Recognition, Mandarin Lip-reading, Pinyin, Large Language Model, Multi-task Learning  

<br /><br />Summary:  
Visual Speech Recognition (VSR) focuses on transcribing spoken words by analyzing silent lip-motion videos, which presents significant challenges in Mandarin due to ambiguous visemes and the abundance of homophones. The authors propose VALLR-Pin, a novel two-stage framework that adapts the existing English-based VALLR architecture for Mandarin. The first stage involves a shared video encoder connected to dual decoders that simultaneously predict Chinese characters and their Pinyin romanization, enabling multi-task learning to build robust visual-semantic representations. In the second stage, during inference, multiple candidate Chinese text sequences generated by the text decoder are combined with Pinyin outputs into a prompt and fed into a large language model (LLM). This approach leverages explicit phonetic context to resolve ambiguities and correct homophone-induced errors. Moreover, the LLM is fine-tuned on synthetic noisy data generated from imperfect Pinyin-text pairs, derived from intermediate model checkpoints, to learn the error patterns unique to VALLR-Pin. Ultimately, VALLR-Pin integrates visual features with phonetic and linguistic contexts, yielding improved lip-reading accuracy for Mandarin by effectively addressing the language’s intrinsic challenges. <div>
arXiv:2512.20032v1 Announce Type: new 
Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs</title>
<link>https://arxiv.org/abs/2512.20033</link>
<guid>https://arxiv.org/abs/2512.20033</guid>
<content:encoded><![CDATA[
<div> Keywords: FlashLips, lip-sync, mask-free, latent-space editor, audio-to-pose transformer<br /><br />Summary:<br /><br />1. FlashLips is a two-stage, mask-free lip-sync system designed to achieve real-time lip synchronization at over 100 FPS on a single GPU, while maintaining visual quality comparable to larger state-of-the-art models. <br />2. The first stage is a compact latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained solely with reconstruction losses without relying on GANs or diffusion models. <br />3. To eliminate the need for explicit masks at inference time, a self-supervision approach is used by generating mouth-altered variants of the target image as pseudo ground truth, enabling the network to learn to localize edits specifically to the lips region and preserve the rest of the image. <br />4. The second stage is an audio-to-pose transformer model trained with a flow-matching objective, which predicts lips-pose vectors directly from speech signals. <br />5. Together, these stages create a stable and straightforward pipeline that combines deterministic image reconstruction with reliable audio-driven lip movement control, delivering both high perceptual quality and faster-than-real-time performance. <div>
arXiv:2512.20033v1 Announce Type: new 
Abstract: We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</title>
<link>https://arxiv.org/abs/2512.20042</link>
<guid>https://arxiv.org/abs/2512.20042</guid>
<content:encoded><![CDATA[
<div> Keywords: image captioning, contextual depth, multimodal pipeline, semantic search, fine-tuned Qwen3<br /><br />Summary:<br /><br />Real-world image captions typically miss important contextual information such as event background, temporal details, outcomes, and named entities that are not visually obvious, limiting their usefulness in fields requiring rich descriptions like journalism and education. To tackle this challenge, the authors propose a multimodal pipeline that supplements visual data with external textual knowledge. The system begins by retrieving semantically similar images using BEIT-3 (trained on Flickr30k-384 and COCO-384) and SigLIP So-384. It then reranks these images based on geometric alignment using ORB and SIFT features to ensure contextual relevance. Subsequently, contextual information is extracted from related articles through semantic search methods. This enriched context is combined with base captions generated by Instruct BLIP (Vicuna-7B) by employing a fine-tuned Qwen3 model using QLoRA, enabling the generation of event-enriched, context-aware image descriptions. Evaluation on the OpenEvents v1 dataset demonstrates that this approach produces significantly more informative captions than traditional captioning methods. The results highlight the pipeline's potential for applications demanding deeper visual and textual understanding, making it valuable for domains such as digital archives, education, and journalism. <div>
arXiv:2512.20042v1 Announce Type: new 
Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Learned Image Compression for Machine Perception</title>
<link>https://arxiv.org/abs/2512.20070</link>
<guid>https://arxiv.org/abs/2512.20070</guid>
<content:encoded><![CDATA[
<div> Keywords: progressive image compression, machine perception, learned codecs, trit-plane coding, adaptive decoding controller<br /><br />Summary: Recent advances in learned image codecs have shifted focus from human to machine perception, addressing the unique requirements of machine-oriented image compression. This paper introduces PICM-Net, a novel progressive learned image compression codec specifically designed for machine perception by leveraging trit-plane coding, which allows fine granular scalability (FGS). Unlike traditional codecs that serve human viewers, PICM-Net prioritizes rate-distortion characteristics tailored for downstream machine tasks such as classification. The work systematically examines latent prioritization strategies to better align with machine perception needs. To adapt dynamically to real-world scenarios, an adaptive decoding controller is proposed that determines the necessary decoding level at inference time, optimizing the bitstream decoding to meet predefined confidence thresholds of machine predictions. Extensive experiments validate that PICM-Net efficiently supports progressive transmission with multiple quality levels from a single bitstream while maintaining high accuracy in downstream classification tasks. This establishes a new paradigm in machine-aware image compression, combining progressive scalability with machine-centric performance and adaptability, making it a significant step forward in learned codecs for intelligent systems requiring flexible, confidence-driven data transmission. <div>
arXiv:2512.20070v1 Announce Type: new 
Abstract: Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts</title>
<link>https://arxiv.org/abs/2512.20088</link>
<guid>https://arxiv.org/abs/2512.20088</guid>
<content:encoded><![CDATA[
<div> Keywords: fashion style classification, item region pooling, gated feature fusion, dual-backbone architecture, feature extractor<br /><br />Summary:<br /><br />1. Fashion style classification is challenging due to large intra-style visual variations and visually similar styles.  
2. Styles are conveyed not only by global appearance but also by the attributes of individual items and their combinations.  
3. The study proposes an item region-based fashion style classification network (IRSN) that focuses on item-specific features and their combinations alongside global features.  
4. IRSN leverages item region pooling (IRP) to extract features from each item region separately, and utilizes gated feature fusion (GFF) to combine these features effectively.  
5. The feature extractor is enhanced with a dual-backbone architecture combining a domain-specific extractor and a general extractor pre-trained on a large-scale image-text dataset.  
6. Experimental results demonstrate that applying IRSN to six popular backbones such as EfficientNet, ConvNeXt, and Swin Transformer consistently improves style classification accuracy.  
7. The improvements average 6.9% and up to 14.5% on the FashionStyle14 dataset, and average 7.6% and up to 15.1% on the ShowniqV3 dataset.  
8. Visualization analysis further confirms that IRSN models better capture subtle differences among similar style classes compared to baseline models. <div>
arXiv:2512.20088v1 Announce Type: new 
Abstract: Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.20104</link>
<guid>https://arxiv.org/abs/2512.20104</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Activation Functions, Model Optimizers, BiLSTM, ConvLSTM<br /><br />Summary:<br /><br />Human Activity Recognition (HAR) is essential in healthcare and surveillance for enabling timely decisions and automation. This study addresses a gap by examining how the combination of Activation Functions (ReLU, Sigmoid, Tanh) and Model Optimizers (SGD, Adam, RMSprop, Adagrad) impacts HAR performance, an area previously underexplored compared to architecture designs. The research evaluates these combinations using two recurrent deep learning models—BiLSTM and ConvLSTM—on medically relevant activity classes selected from HMDB51 and UCF101 datasets, chosen for their healthcare relevance. Experimental results reveal that ConvLSTM consistently outperforms BiLSTM on both datasets. ConvLSTM paired with Adam or RMSprop optimizers achieves accuracy up to 99.00%, showcasing its strong spatio-temporal learning and stability. In contrast, BiLSTM attains about 98.00% accuracy on UCF101 but exhibits significantly reduced performance (~60.00%) on HMDB51, indicating poorer robustness and sensitivity to different activation functions and optimizers. Overall, the study provides valuable practical recommendations for optimizing HAR systems, emphasizing ConvLSTM with Adam or RMSprop for real-world healthcare environments where quick, precise activity recognition is critical. <div>
arXiv:2512.20104v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</title>
<link>https://arxiv.org/abs/2512.20105</link>
<guid>https://arxiv.org/abs/2512.20105</guid>
<content:encoded><![CDATA[
<div> LiDAR point clouds, autonomous driving, 3D layout, ControlNet, simulation  

<br /><br />Summary:  
Generating realistic and diverse LiDAR point clouds is vital for autonomous driving simulation. Previous methods often fail to deliver high-quality point clouds while maintaining versatile controllability, due to the complexity of LiDAR data versus the simplicity of control inputs. To overcome this, LiDARDraft introduces the use of 3D layouts as an intermediate representation bridging user inputs and LiDAR data. These 3D layouts can be easily created from various inputs such as textual descriptions, images, and sketches. The method encodes text, images, and point clouds into a unified 3D layout format, which is then converted into semantic and depth control signals to direct generation. A rangemap-based ControlNet architecture is employed to generate LiDAR point clouds under precise, pixel-level guidance. This design enables robust and controllable generation of LiDAR data, supporting "simulation from scratch" where entire self-driving environments can be constructed from arbitrary descriptions and visuals. Overall, LiDARDraft greatly enhances flexibility and fidelity in LiDAR simulation for autonomous vehicle development. <div>
arXiv:2512.20105v1 Announce Type: new 
Abstract: Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</title>
<link>https://arxiv.org/abs/2512.20107</link>
<guid>https://arxiv.org/abs/2512.20107</guid>
<content:encoded><![CDATA[
<div> Keywords: Novel view synthesis, bidirectional transformer, diffusion model, image rendering, multi-view learning  

<br /><br />Summary:  
This paper addresses the challenge of novel view synthesis (NVS), which involves generating photorealistic and geometrically consistent images of a scene from new camera angles using only a limited number of posed images. Existing deterministic methods perform well on observed areas but tend to blur unseen regions, while diffusion-based generative models create plausible content at the cost of expensive training and inference times. To overcome these limitations, the authors propose a hybrid architecture combining a bidirectional transformer that encodes multi-view image tokens and Plücker-ray embeddings into a unified latent space. The model leverages two specialized heads: a feed-forward regression head responsible for rendering pixels in regions where the geometry is well defined, and a masked autoregressive diffusion head designed to fill in occluded or unseen areas with plausible details. This end-to-end trainable framework integrates photometric and diffusion losses without relying on explicit 3D inductive biases, making it scalable to diverse scenes. Experimental results show that this hybrid approach achieves state-of-the-art image quality while drastically reducing rendering times by about tenfold compared to fully generative diffusion methods, demonstrating an effective balance between speed and visual fidelity in novel view synthesis. <div>
arXiv:2512.20107v1 Announce Type: new 
Abstract: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</title>
<link>https://arxiv.org/abs/2512.20113</link>
<guid>https://arxiv.org/abs/2512.20113</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal attention network, bridge deck delamination, radar and thermography fusion, uncertainty quantification, class imbalance<br /><br />Summary:<br /><br />1. The paper addresses the challenge of automated inspection for deteriorating civil infrastructure, specifically detecting bridge deck delamination by overcoming limitations inherent in single sensory modalities. <br />2. It proposes a multimodal attention network that fuses Ground Penetrating Radar temporal patterns with Infrared Thermography spatial features, leveraging temporal attention for radar data and spatial attention for thermal inputs. <br />3. Cross-modal fusion with learnable embeddings is introduced to uncover complementary defect patterns that are not detectable by individual sensors alone. <br />4. The framework incorporates uncertainty quantification techniques, including Monte Carlo dropout and learned variance estimation, allowing decomposition of uncertainty into epistemic and aleatoric components, which facilitates safer decision-making in critical contexts. <br />5. Extensive experiments on five bridge datasets demonstrate that the method outperforms single-modal and simple fusion baselines in terms of accuracy and AUC, with ablation studies confirming the importance of cross-modal attention and multi-head attention for improved calibration. <br />6. The uncertainty quantification reduces calibration error and enables selective prediction by filtering uncertain cases. <br />7. However, attention mechanisms are susceptible to majority class collapse in cases of extreme class imbalance, indicating the need for specialized handling in such scenarios. <br />8. The proposed system remains efficient and suitable for real-time deployment, with clearly characterized strengths and limitations for practical bridge inspection. <div>
arXiv:2512.20113v1 Announce Type: new 
Abstract: Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</title>
<link>https://arxiv.org/abs/2512.20117</link>
<guid>https://arxiv.org/abs/2512.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio-Visual Segmentation, Multi-source Entanglement, Audio-Visual Misalignment, Contrastive Learning, Cross-Attention<br /><br />Summary: Audio-Visual Segmentation (AVS) is tasked with precisely localizing sound-producing objects by integrating auditory and visual data at the pixel level. Existing AVS methods struggle with multi-source entanglement, which causes the models to bias louder or larger objects while neglecting weaker, smaller, or multiple co-occurring sources. Additionally, audio-visual misalignment further impairs segmentation accuracy. To overcome these limitations, the proposed framework DDAVS introduces a novel approach consisting of two main components. First, to handle multi-source entanglement, DDAVS uses learnable queries that extract distinct audio semantics and anchor them within a structured semantic space, formed by an audio prototype memory bank. This mechanism employs contrastive learning to boost the discriminability and robustness of the audio semantics. Second, to address audio-visual misalignment, DDAVS incorporates a delayed bidirectional alignment mechanism using dual cross-attention with staggered modality interaction, which enhances the robustness of multimodal alignment. Extensive experiments on AVS-Objects and VPO benchmarks demonstrate that DDAVS surpasses existing methods across single-source, multi-source, and multi-instance settings. The results confirm DDAVS’s efficacy and its strong generalization capability in complex real-world AVS scenarios. <div>
arXiv:2512.20117v1 Announce Type: new 
Abstract: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer</title>
<link>https://arxiv.org/abs/2512.20120</link>
<guid>https://arxiv.org/abs/2512.20120</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, pruning, Hessian-guided, token pruning, edge deployment<br /><br />Summary:  
This paper presents HEART-ViT, a novel pruning framework designed to optimize Vision Transformers (ViTs) by addressing their high computational costs and redundancy. HEART-ViT uniquely combines token and attention head pruning in a unified, second-order, input-adaptive manner using Hessian-vector products to estimate curvature-weighted sensitivities. This approach allows principled pruning decisions that respect explicit loss budgets, improving over prior methods that treat tokens or heads separately and rely on heuristics. The research reveals that token pruning contributes most to computational savings, while head pruning fine-tunes redundancy removal; their combination delivers superior efficiency and accuracy trade-offs. On benchmark datasets such as ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16 models, HEART-ViT achieves up to 49.4% reduction in FLOPs, 36% lower latency, and 46% higher throughput, while maintaining or exceeding baseline accuracy after fine-tuning—demonstrated by a 4.7% accuracy recovery at 40% token pruning. Beyond theory, the framework is successfully deployed on edge devices like the AGX Orin, showing significant real-world gains in inference speed and energy efficiency. HEART-ViT thus bridges theoretical insight and practical deployment, offering a first-of-its-kind curvature-driven, accuracy-preserving, and edge-efficient pruning method for ViTs. <div>
arXiv:2512.20120v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</title>
<link>https://arxiv.org/abs/2512.20128</link>
<guid>https://arxiv.org/abs/2512.20128</guid>
<content:encoded><![CDATA[
<div> Keywords: millimeter-wave radar, human pose estimation, spatio-temporal modeling, cross-view fusion, velocity loss<br /><br />Summary:<br />1. This paper introduces milliMamba, a novel framework for 2D human pose estimation (HPE) using millimeter-wave radar, which provides privacy and lighting-invariant sensing compared to traditional RGB sensors.<br />2. Radar signals tend to be sparse due to specular reflections, posing challenges for robust feature extraction; milliMamba addresses this by jointly modeling spatio-temporal dependencies in both feature extraction and decoding.<br />3. The framework employs a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer radar sequences while maintaining linear computational complexity.<br />4. A Spatio-Temporal-Cross Attention decoder is used to predict joint coordinates across multiple frames, allowing the model to utilize contextual information from neighboring frames and joints to infer missing keypoints.<br />5. To improve motion smoothness, the training process incorporates a velocity loss in addition to the standard keypoint loss.<br />6. Experimental results on the TransHuPR and HuPR radar datasets show that milliMamba significantly outperforms baseline methods by 11.0 AP and 14.6 AP respectively, while keeping computational complexity reasonable.<br />7. The authors have made the source code available on GitHub for further research and development. <div>
arXiv:2512.20128v1 Announce Type: new 
Abstract: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</title>
<link>https://arxiv.org/abs/2512.20148</link>
<guid>https://arxiv.org/abs/2512.20148</guid>
<content:encoded><![CDATA[
<div> Keywords: apple pose estimation, 3D Gaussian Splatting, occlusion, automated annotation, orchard automation<br /><br />Summary:<br /><br />Automating tasks in orchards involves significant challenges, particularly due to environmental variability and occlusions that obscure key apple features like the calyx. Traditional pose estimation methods depend on such key points for annotation, which is problematic in occluded conditions, leading to conflicting or missing annotations. This study introduces a novel pipeline utilizing 3D Gaussian Splatting to reconstruct orchard scenes, which simplifies annotations and automates their projection onto images, significantly reducing manual labeling effort. The pipeline required only 105 manual annotations to generate 28,191 training labels, representing a 99.6% reduction in manual workload. Experimental results showed that training the model on fruits with up to 95% occlusion yielded the best performance, achieving an F1 score of 0.927 on original images and 0.970 on rendered images. Variation in training dataset size had minimal impact on pose estimation accuracy and F1 score. The study also found that fruits with less occlusion had more accurate position estimates, while orientation estimation of apples remained a challenge for the tested method. Overall, this approach enhances dataset creation efficiency for apple pose estimation in occluded environments but highlights limitations in orientation learning. <div>
arXiv:2512.20148v1 Announce Type: new 
Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\leq95\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoDi -- an exemplar-conditioned diffusion model for low-shot counting</title>
<link>https://arxiv.org/abs/2512.20153</link>
<guid>https://arxiv.org/abs/2512.20153</guid>
<content:encoded><![CDATA[
<div> Low-shot object counting, latent diffusion, density maps, exemplar conditioning, object localization  

<br /><br />Summary:  
1. The paper introduces CoDi, a latent diffusion-based method for low-shot object counting, which aims to estimate object counts in images with few or no annotated exemplars at test time.  
2. Existing density-based counters perform well on total counts in dense regions with small objects but struggle with localization, while point-detection-based counters offer better localization but falter in images with very large object numbers due to limited pre-trained queries.  
3. CoDi produces high-quality density maps that allow accurate object location detection via non-maxima suppression, overcoming the limitations of prior methods that relied on upsampling or tiling.  
4. The core innovation is an exemplar-based conditioning module that extracts and integrates object prototypes into the latent diffusion denoising network’s intermediate layers, enhancing location accuracy.  
5. Experimental results demonstrate CoDi’s superiority: it reduces mean absolute error (MAE) by 15%, 13%, and 10% in few-shot, one-shot, and reference-less scenarios respectively on the FSC benchmark and outperforms the leading method on the MCAC benchmark by 44% MAE, establishing new state-of-the-art performance.  

The code for CoDi is publicly available at https://github.com/gsustar/CoDi. <div>
arXiv:2512.20153v1 Announce Type: new 
Abstract: Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</title>
<link>https://arxiv.org/abs/2512.20157</link>
<guid>https://arxiv.org/abs/2512.20157</guid>
<content:encoded><![CDATA[
<div> multi-teacher distillation, vision foundation models, Mixture-of-Experts, knowledge distillation, data efficiency<br /><br />Summary:<br /><br />This paper investigates multi-teacher distillation for vision foundation models, focusing on improving learning dynamics and data efficiency while reducing computational cost. The authors propose Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge simultaneously from two teacher models, SigLIP2 and DINOv3, into a Mixture-of-Experts student architecture. They introduce an Asymmetric Relation-Knowledge Distillation loss designed to preserve the geometric properties of each teacher model, enabling effective and balanced knowledge transfer. The paper also presents token-balanced batching, a technique that packs images of varying resolutions into sequences with uniform token budgets. This method stabilizes the learning process across different resolutions without compromising performance. Additionally, the study leverages hierarchical clustering and sampling of training data — traditionally used in self-supervised learning — to significantly improve sample efficiency compared to random sampling for multi-teacher distillation. Combining these innovations, the authors assemble the OpenLVD200M dataset, a 200 million image corpus optimized for multi-teacher distillation tasks. They demonstrate that models distilled using this data and approach achieve superior training efficiency. Both the OpenLVD200M dataset and the distilled models are publicly released to support further research and development in vision foundation modeling. <div>
arXiv:2512.20157v1 Announce Type: new 
Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2512.20174</link>
<guid>https://arxiv.org/abs/2512.20174</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Image Retrieval, Natural Language Query, Vision-Language Models, Dataset Benchmark, Visual Document Understanding<br /><br />Summary:<br /><br />1. The paper addresses Document Image Retrieval (DIR), which involves retrieving document images from a gallery based on a query, but shifts from conventional image-based queries to natural language-based queries with fine-grained semantics.<br /><br />2. It introduces a new benchmark called Natural Language-based Document Image Retrieval (NL-DIR) designed specifically to evaluate retrieval using rich natural language descriptions as queries.<br /><br />3. The NL-DIR dataset includes 41,000 authentic document images, each paired with five detailed semantic queries created and validated through a combination of large language models and manual verification ensuring high quality.<br /><br />4. The study evaluates existing contrastive vision-language models and OCR-free visual document understanding models on this dataset, both in zero-shot and fine-tuning scenarios.<br /><br />5. Additionally, a two-stage retrieval approach is proposed to improve performance while maintaining time and space efficiency.<br /><br />6. The authors aim for the NL-DIR benchmark to stimulate new research directions in the visual document understanding community, and they plan to make the datasets and code publicly accessible via Hugging Face. <div>
arXiv:2512.20174v1 Announce Type: new 
Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Latent Coding for Ultra-Low Bitrate Image Compression</title>
<link>https://arxiv.org/abs/2512.20194</link>
<guid>https://arxiv.org/abs/2512.20194</guid>
<content:encoded><![CDATA[
<div> Keywords: image compression, generative latent coding, VQ-VAE, semantic consistency, bit rate reduction<br /><br />Summary:  
This paper proposes a novel image compression method called Generative Latent Coding (GLC) that operates in the latent space of a vector-quantized variational auto-encoder (VQ-VAE) rather than directly in the pixel space. Unlike traditional transform coding in pixel space, GLC leverages the generative latent space, which is sparser, semantically richer, and better aligned with human visual perception, making it effective for achieving both high realism and high fidelity at very low bitrates. To further improve compression efficiency, the authors introduce a categorical hyper module that reduces the bit cost of hyper-information. They also design a code-prediction-based supervision mechanism to enhance semantic consistency in the compressed representations. Experimentally, GLC maintains high visual quality with bitrates less than 0.04 bits per pixel (bpp) on natural images and under 0.01 bpp on facial images. On the CLIC2020 test set, the method achieves the same Fréchet Inception Distance (FID) score as MS-ILLM while using 45% fewer bits. Beyond compression, the expressive generative latent space of GLC enables additional image manipulation applications such as image restoration and style transfer. The authors have made their code publicly available for research and development purposes at https://github.com/jzyustc/GLC. <div>
arXiv:2512.20194v1 Announce Type: new 
Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2512.20213</link>
<guid>https://arxiv.org/abs/2512.20213</guid>
<content:encoded><![CDATA[
<div> Underwater degradation, nonlinear coupling, joint degradation processing, feature mining, AquaBalanceLoss<br /><br />Summary:<br /><br />This paper addresses the challenge of underwater image degradation caused by the complex and variable nature of water as a medium. Unlike traditional treatments that consider degradations independently or through simple additive models, the authors emphasize the nonlinear coupling of multiple degradation types, which complicates effective restoration. To tackle this, they introduce JDPNet, a novel joint degradation processing network designed to mine and unify the information embedded within these coupled degradations under a single framework. The core innovation includes a joint feature-mining module that extracts relevant degradation characteristics, supported by a probabilistic bootstrap distribution strategy that facilitates unified adjustment of those features. Additionally, the study proposes AquaBalanceLoss, a specialized loss function crafted to balance critical image qualities like color fidelity, clarity, and contrast during training. Extensive experiments conducted on six publicly available underwater datasets and two new datasets created by the authors demonstrate that JDPNet achieves state-of-the-art restoration performance. Moreover, it offers an optimal balance between restoration quality, the number of model parameters, and computational efficiency, making it practical for real-world applications in underwater image enhancement. <div>
arXiv:2512.20213v1 Announce Type: new 
Abstract: Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title>
<link>https://arxiv.org/abs/2512.20217</link>
<guid>https://arxiv.org/abs/2512.20217</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D object detection, multi-modal fusion, LiDAR, quaternion embedding, deployment-friendly<br /><br />Summary:<br />3D object detection plays a crucial role in ensuring the safety and robustness of intelligent transportation systems. Existing multi-modal detectors rely heavily on LiDAR sensors and complex architectures, resulting in significant performance loss if LiDAR data is missing and difficulties in deployment on varied hardware (e.g., NPUs, FPGAs) due to 3D sparse convolution dependencies optimized for NVIDIA GPUs. To overcome these limitations, LiteFusion is proposed as a novel multi-modal 3D detection method that treats LiDAR data not as a separate modality but as a complementary geometric cue augmenting camera-based detection. This approach eliminates the need for a dedicated 3D LiDAR backbone, enhancing deployment flexibility. LiteFusion incorporates LiDAR features into image features within a quaternion space that preserves orthogonal constraints, effectively modeling domain-specific correlations and producing a compact cross-modal embedding. Experimental results on the nuScenes dataset demonstrate that LiteFusion boosts the baseline vision-only detector by +20.4% mAP and +19.7% NDS while increasing parameters by only 1.1%, without dedicated LiDAR encoders. Importantly, LiteFusion remains robust and effective even without LiDAR input, supporting diverse fusion paradigms and deployment scenarios. <div>
arXiv:2512.20217v1 Announce Type: new 
Abstract: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing</title>
<link>https://arxiv.org/abs/2512.20236</link>
<guid>https://arxiv.org/abs/2512.20236</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Layout Analysis, Indic Languages, Dataset, Multilingual, Pretraining<br /><br />Summary:<br /><br />1. Document layout analysis is crucial for tasks like information retrieval, extraction, OCR, and digitization, but existing large datasets such as PubLayNet and DocBank suffer from limited fine-grained labeling and multilingual scope.  
2. Human-annotated datasets like M6Doc and D4LA provide richer annotations and diverse domains but are small and lack sufficient multilingual representation, especially for Indic scripts.  
3. Indic documents with diverse scripts are underrepresented in current datasets, creating a significant gap that impedes progress in document understanding for these languages.  
4. To address these issues, the authors introduce IndicDLP, a large-scale dataset covering 11 Indic languages plus English, spanning 12 common document domains, with detailed annotations to support foundational document layout research.  
5. In addition, UED-mini, created from DocLayNet and M6Doc, supports pretraining efforts aimed at improving model robustness for Indic layouts.  
6. Experiments show that fine-tuning English-trained models on IndicDLP leads to significant performance gains, demonstrating the dataset's utility.  
7. Models trained on IndicDLP generalize well even beyond Indic documents, making the dataset a valuable resource for broad document digitization and layout tasks.  
8. This effort closes gaps in dataset scale, diversity, and annotation detail, fostering more inclusive and effective document understanding technologies. <div>
arXiv:2512.20236v1 Announce Type: new 
Abstract: Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</title>
<link>https://arxiv.org/abs/2512.20251</link>
<guid>https://arxiv.org/abs/2512.20251</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral image restoration, degradation-aware metric, mixture-of-experts, spatial-spectral adaptive module, unified model<br /><br />Summary: The paper addresses the challenge of unified hyperspectral image (HSI) restoration, which seeks to recover various degraded HSIs using a single model. Existing approaches rely on explicit degradation priors such as degradation labels, which are difficult to obtain due to the complexity and mixture of degradations in real-world scenarios. To overcome this limitation, the authors propose a Degradation-Aware Metric Prompting (DAMP) framework that uses spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations rather than relying on fixed degradation labels. These metrics serve as Degradation Prompts (DP), enabling the model to grasp cross-task similarities in degradation and improve shared feature learning. Furthermore, DAMP introduces a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. This module is incorporated into a Mixture-of-Experts (MoE) architecture, where DP act as gating routers, allowing adaptive, efficient, and robust restoration. Experiments on natural and remote sensing HSI datasets demonstrate that DAMP achieves state-of-the-art performance and exceptional generalization ability. Additionally, the authors have made their code publicly available to facilitate further research and applications. <div>
arXiv:2512.20251v1 Announce Type: new 
Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</title>
<link>https://arxiv.org/abs/2512.20255</link>
<guid>https://arxiv.org/abs/2512.20255</guid>
<content:encoded><![CDATA[
<div> High-resolution remote sensing, semantic segmentation, bidirectional co-refinement, heatmap supervision, Fisher discriminative loss<br /><br />Summary: This paper addresses the challenges in high-resolution remote sensing image semantic segmentation (HRSS), particularly high inter-class similarity and large intra-class variability, which cause blurred boundaries and class confusion. The authors propose the Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg) to improve segmentation performance and interpretability. Central to this framework is the Heatmap-driven Bidirectional Information Synergy Module (HBIS), which creates a two-way information flow between feature maps and class embeddings using class-level heatmaps. Leveraging HBIS, a hierarchical supervision strategy is introduced, using heatmaps from each HBIS module as low-resolution segmentation outputs for enhanced supervision of shallow features, boosting their discriminative power. Additionally, a novel cross-layer class embedding Fisher Discriminative Loss is developed to promote intra-class compactness and amplify inter-class separability among embedding representations. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves superior segmentation accuracy compared to existing methods while providing better interpretability of the model’s predictions. The authors have also made the code publicly available for reproducibility and further research. <div>
arXiv:2512.20255v1 Announce Type: new 
Abstract: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</title>
<link>https://arxiv.org/abs/2512.20257</link>
<guid>https://arxiv.org/abs/2512.20257</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal misinformation, limited annotation, model-soup, BLIP embeddings, DGM4 benchmark<br /><br />Summary:<br />1. The paper addresses the challenge of detecting misinformation conveyed through paired image-text content, a growing problem exacerbated by accessible media manipulation tools.<br />2. Existing detection methods often require large annotated datasets and computationally expensive architectures, limiting their practicality.<br />3. To overcome these issues, the authors introduce LADLE-MM, a multimodal misinformation detector designed to function effectively with limited annotated data and constrained training resources.<br />4. LADLE-MM is structured with two unimodal branches (separately processing image and text) and one multimodal branch, which enhances these representations by incorporating fixed multimodal embeddings derived from the BLIP model.<br />5. The model uses a model-soup initialization technique that combines multiple learned model states to boost robustness.<br />6. Despite having 60.3% fewer trainable parameters than previous state-of-the-art approaches, LADLE-MM achieves competitive or superior performance on both binary and multi-label classification tasks on the DGM4 benchmark.<br />7. Additionally, when tested on the VERITE dataset, LADLE-MM surpasses methods built on large vision-language models, evidencing strong generalization to open-set conditions and resilience against unimodal biases.<br />8. Overall, the study demonstrates that LADLE-MM provides an efficient, effective, and resource-conscious solution for multimodal misinformation detection. <div>
arXiv:2512.20257v1 Announce Type: new 
Abstract: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations</title>
<link>https://arxiv.org/abs/2512.20260</link>
<guid>https://arxiv.org/abs/2512.20260</guid>
<content:encoded><![CDATA[
<div> Camouflaged Object Detection, Weakly-Supervised Learning, Pseudo Labeling, Annotation Bias, Frequency-Aware Features<br /><br />Summary:<br /><br />Weakly-Supervised Camouflaged Object Detection (WSCOD) focuses on detecting and segmenting objects hidden in their backgrounds using limited annotations like scribbles. Existing methods struggle due to two main challenges: unreliable pseudo masks generated by generic segmentation models such as SAM that lack task-specific understanding, and ignoring the annotation bias present in scribble labels which undermines capturing the object's global structure. To address these issues, the paper introduces the ${D}^{3}$ETOR framework, which operates in two stages. The first involves Debate-Enhanced Pseudo Labeling, featuring an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to improve the precision and interpretability of pseudo masks tailored for camouflaged object detection. The second stage presents FADeNet, a Frequency-Aware Progressive Debiasing network that fuses multi-level frequency-aware features, balancing global semantic context with local details, while dynamically reweighting supervision to counteract scribble bias. By combining supervision from both pseudo masks and scribble semantics, ${D}^{3}$ETOR effectively bridges the performance gap between weakly supervised and fully supervised COD approaches, setting new state-of-the-art results across various benchmarks. <div>
arXiv:2512.20260v1 Announce Type: new 
Abstract: Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</title>
<link>https://arxiv.org/abs/2512.20288</link>
<guid>https://arxiv.org/abs/2512.20288</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, SHAP, uncertainty quantification, medical imaging, Dempster-Shafer theory<br /><br />Summary:<br /><br />1. Recent advancements in deep learning have expanded its use in medical imaging, leveraging complex architectures like ResNets, Vision Transformers, and Hybrid CNNs that improve performance but reduce model interpretability.<br />2. SHAP (SHapley Additive exPlanations) is widely used to generate interpretable visual explanations for model predictions but can produce unstable and unreliable results when uncertainty is present.<br />3. The study addresses SHAP's instability by incorporating Dirichlet posterior sampling and Dempster-Shafer theory to effectively quantify epistemic (model) and aleatoric (data) uncertainties in explanation outputs.<br />4. A novel framework is proposed utilizing belief, plausible, and fusion maps along with statistical analyses to provide robust uncertainty quantification in SHAP explanations for medical imaging contexts.<br />5. The framework is validated on three diverse medical imaging datasets spanning pathology, ophthalmology, and radiology, each with different class distributions, image resolutions, and modality-specific noise, demonstrating its capability to handle significant epistemic uncertainty and improve the reliability of interpretability methods. <div>
arXiv:2512.20288v1 Announce Type: new 
Abstract: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</title>
<link>https://arxiv.org/abs/2512.20296</link>
<guid>https://arxiv.org/abs/2512.20296</guid>
<content:encoded><![CDATA[
<div> Keywords: interactive videos, conversational speech, multimodal interaction, face-speech synchronization, TAVID<br /><br />Summary:<br /><br />This paper presents TAVID, a novel framework designed to jointly generate interactive videos and conversational speech from textual input combined with reference images. Unlike previous works that treated talking or listening head generation and conversational speech generation separately, TAVID aims to address the multimodal complexity of human conversation by integrating both audio and visual modalities in a synchronized fashion. The core innovation lies in the introduction of two cross-modal mappers, called the motion mapper and speaker mapper, which facilitate the bidirectional transfer of complementary information between speech and facial motion generation pipelines. This tight coupling enhances the naturalness and coherence of generated conversations by ensuring that facial expressions and speech cues are mutually informed and consistent. The system is rigorously evaluated on four key dimensions: the realism of talking faces, responsiveness of listening head motions, the fluency of dyadic interaction, and the overall speech quality. Experimental results demonstrate that TAVID achieves significant improvements across all these areas, highlighting the effectiveness of its integrated approach. Ultimately, this work advances the development of more human-like conversational AI systems that can seamlessly blend verbal and non-verbal communication cues. <div>
arXiv:2512.20296v1 Announce Type: new 
Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection</title>
<link>https://arxiv.org/abs/2512.20340</link>
<guid>https://arxiv.org/abs/2512.20340</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformer, Video Virtual Try-On, Keyframe Sampling, Garment Dynamics, Background Integrity<br /><br />Summary:<br /><br />The article addresses challenges in diffusion transformer (DiT)-based video virtual try-on (VVT) systems that currently struggle to accurately capture fine-grained garment dynamics and maintain consistent backgrounds throughout video frames. It recognizes existing solutions suffer from high computational costs due to added interaction modules and are limited by the scale and quality of public datasets, which impairs model training and generalization. To overcome these issues, the authors propose KeyTailor, a novel framework leveraging a keyframe-driven details injection strategy that capitalizes on keyframes containing crucial foreground and background information. KeyTailor employs an instruction-guided keyframe sampling method to select informative frames from input videos. It integrates two specialized modules: a garment details enhancement module that distills garment dynamics into garment-related latents, and a collaborative background optimization module that improves background latent consistency. These enriched details are injected into standard DiT blocks along with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis without altering the DiT architecture or adding complexity. The authors also introduce ViT-HD, a large-scale, high-definition dataset with over 15,000 samples at 810x1080 resolution, featuring diverse garments. Extensive experiments demonstrate KeyTailor's superior performance over state-of-the-art methods in garment fidelity and background integrity across dynamic and static scenarios. <div>
arXiv:2512.20340v1 Announce Type: new 
Abstract: Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.20362</link>
<guid>https://arxiv.org/abs/2512.20362</guid>
<content:encoded><![CDATA[
<div> Keywords: CRAFT, inference-time reasoning, multimodal image generation, prompt editing, vision-language model<br /><br />Summary:<br /><br />1. The paper introduces CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a novel framework designed to enhance text-to-image generation at inference time without retraining the underlying generative models.<br /><br />2. Unlike prior methods that rely on implicit or unconstrained prompt modifications, CRAFT employs explicit, structured reasoning by breaking down the prompt into dependency-structured visual questions and verifying the output using vision-language models.<br /><br />3. CRAFT iteratively applies targeted prompt edits using a large language model agent only when verification constraints fail, continuing the refinement loop until all constraints are met, which ensures interpretability, control, and an explicit stopping mechanism.<br /><br />4. Experiments across multiple model families and challenging benchmarks demonstrate that CRAFT consistently boosts compositional accuracy, text rendering fidelity, and user preference outcomes, with especially notable improvements for lightweight generative models.<br /><br />5. The approach adds minimal inference-time computational overhead and enables smaller or cheaper models to achieve quality closer to that of more expensive systems, highlighting the importance of structured, constraint-driven inference-time reasoning for reliable multimodal generation. <div>
arXiv:2512.20362v1 Announce Type: new 
Abstract: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge</title>
<link>https://arxiv.org/abs/2512.20376</link>
<guid>https://arxiv.org/abs/2512.20376</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.20376v1  
Keywords: face-voice association, multilingual environments, bilingual communication, FAME 2026 Challenge, ICASSP 2026  

<br /><br />Summary:  
1. The paper addresses the challenge of face-voice association in multilingual settings, where more than half of the global population is bilingual.  
2. The FAME 2026 Challenge, organized and held at ICASSP 2026, focuses on improving methods that associate faces with voices, particularly when the test language differs from the training language.  
3. This report offers a concise overview of the FAME 2026 Challenge, summarizing its main goals and scope.  
4. The emphasis is on developing models that can generalize across languages, enhancing robustness and reliability in real-world multilingual scenarios.  
5. The challenge encourages research advancements in the face-voice association task under the condition of language mismatch between training and testing phases. <div>
arXiv:2512.20376v1 Announce Type: new 
Abstract: Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</title>
<link>https://arxiv.org/abs/2512.20377</link>
<guid>https://arxiv.org/abs/2512.20377</guid>
<content:encoded><![CDATA[
arXiv:2512.20377v1 Announce Type: new 
Abstract: Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</title>
<link>https://arxiv.org/abs/2512.20409</link>
<guid>https://arxiv.org/abs/2512.20409</guid>
<content:encoded><![CDATA[
arXiv:2512.20409v1 Announce Type: new 
Abstract: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Anomaly Thoughts with Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.20417</link>
<guid>https://arxiv.org/abs/2512.20417</guid>
<content:encoded><![CDATA[
arXiv:2512.20417v1 Announce Type: new 
Abstract: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2512.20431</link>
<guid>https://arxiv.org/abs/2512.20431</guid>
<content:encoded><![CDATA[
arXiv:2512.20431v1 Announce Type: new 
Abstract: Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Dimensional Data Decomposition for Anomaly Detection of Textured Images</title>
<link>https://arxiv.org/abs/2512.20432</link>
<guid>https://arxiv.org/abs/2512.20432</guid>
<content:encoded><![CDATA[
arXiv:2512.20432v1 Announce Type: new 
Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</title>
<link>https://arxiv.org/abs/2512.20451</link>
<guid>https://arxiv.org/abs/2512.20451</guid>
<content:encoded><![CDATA[
arXiv:2512.20451v1 Announce Type: new 
Abstract: Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</title>
<link>https://arxiv.org/abs/2512.20479</link>
<guid>https://arxiv.org/abs/2512.20479</guid>
<content:encoded><![CDATA[
arXiv:2512.20479v1 Announce Type: new 
Abstract: AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems</title>
<link>https://arxiv.org/abs/2512.20487</link>
<guid>https://arxiv.org/abs/2512.20487</guid>
<content:encoded><![CDATA[
arXiv:2512.20487v1 Announce Type: new 
Abstract: Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</title>
<link>https://arxiv.org/abs/2512.20501</link>
<guid>https://arxiv.org/abs/2512.20501</guid>
<content:encoded><![CDATA[
arXiv:2512.20501v1 Announce Type: new 
Abstract: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</title>
<link>https://arxiv.org/abs/2512.20531</link>
<guid>https://arxiv.org/abs/2512.20531</guid>
<content:encoded><![CDATA[
arXiv:2512.20531v1 Announce Type: new 
Abstract: We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</title>
<link>https://arxiv.org/abs/2512.20538</link>
<guid>https://arxiv.org/abs/2512.20538</guid>
<content:encoded><![CDATA[
arXiv:2512.20538v1 Announce Type: new 
Abstract: Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</title>
<link>https://arxiv.org/abs/2512.20556</link>
<guid>https://arxiv.org/abs/2512.20556</guid>
<content:encoded><![CDATA[
arXiv:2512.20556v1 Announce Type: new 
Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title>
<link>https://arxiv.org/abs/2512.20557</link>
<guid>https://arxiv.org/abs/2512.20557</guid>
<content:encoded><![CDATA[
arXiv:2512.20557v1 Announce Type: new 
Abstract: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2512.20561</link>
<guid>https://arxiv.org/abs/2512.20561</guid>
<content:encoded><![CDATA[
arXiv:2512.20561v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</title>
<link>https://arxiv.org/abs/2512.20563</link>
<guid>https://arxiv.org/abs/2512.20563</guid>
<content:encoded><![CDATA[
arXiv:2512.20563v1 Announce Type: new 
Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repurposing Video Diffusion Transformers for Robust Point Tracking</title>
<link>https://arxiv.org/abs/2512.20606</link>
<guid>https://arxiv.org/abs/2512.20606</guid>
<content:encoded><![CDATA[
arXiv:2512.20606v1 Announce Type: new 
Abstract: Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPOD: the deployable units of training for federated learning</title>
<link>https://arxiv.org/abs/2512.20610</link>
<guid>https://arxiv.org/abs/2512.20610</guid>
<content:encoded><![CDATA[
arXiv:2512.20610v1 Announce Type: new 
Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Intelligence in Video Avatars via Closed-loop World Modeling</title>
<link>https://arxiv.org/abs/2512.20615</link>
<guid>https://arxiv.org/abs/2512.20615</guid>
<content:encoded><![CDATA[
arXiv:2512.20615v1 Announce Type: new 
Abstract: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialTree: How Spatial Abilities Branch Out in MLLMs</title>
<link>https://arxiv.org/abs/2512.20617</link>
<guid>https://arxiv.org/abs/2512.20617</guid>
<content:encoded><![CDATA[
arXiv:2512.20617v1 Announce Type: new 
Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemanticGen: Video Generation in Semantic Space</title>
<link>https://arxiv.org/abs/2512.20619</link>
<guid>https://arxiv.org/abs/2512.20619</guid>
<content:encoded><![CDATA[
arXiv:2512.20619v1 Announce Type: new 
Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM Audio: Segment Anything in Audio</title>
<link>https://arxiv.org/abs/2512.18099</link>
<guid>https://arxiv.org/abs/2512.18099</guid>
<content:encoded><![CDATA[
arXiv:2512.18099v1 Announce Type: cross 
Abstract: General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</title>
<link>https://arxiv.org/abs/2512.19731</link>
<guid>https://arxiv.org/abs/2512.19731</guid>
<content:encoded><![CDATA[
arXiv:2512.19731v1 Announce Type: cross 
Abstract: Thanks to the evolving network depth, convolutional neural networks (CNNs) have achieved remarkable success across various embedded scenarios, paving the way for ubiquitous embedded intelligence. Despite its promise, the evolving network depth comes at the cost of degraded hardware efficiency. In contrast to deep networks, shallow networks can deliver superior hardware efficiency but often suffer from inferior accuracy. To address this dilemma, we propose Double-Win NAS, a novel deep-to-shallow transformable neural architecture search (NAS) paradigm tailored for resource-constrained intelligent embedded systems. Specifically, Double-Win NAS strives to automatically explore deep networks to first win strong accuracy, which are then equivalently transformed into their shallow counterparts to further win strong hardware efficiency. In addition to search, we also propose two enhanced training techniques, including hybrid transformable training towards better training accuracy and arbitrary-resolution elastic training towards enabling natural network elasticity across arbitrary input resolutions. Extensive experimental results on two popular intelligent embedded systems (i.e., NVIDIA Jetson AGX Xavier and NVIDIA Jetson Nano) and two representative large-scale datasets (i.e., ImageNet and ImageNet-100) clearly demonstrate the superiority of Double-Win NAS over previous state-of-the-art NAS approaches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach</title>
<link>https://arxiv.org/abs/2512.20056</link>
<guid>https://arxiv.org/abs/2512.20056</guid>
<content:encoded><![CDATA[
arXiv:2512.20056v1 Announce Type: cross 
Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</title>
<link>https://arxiv.org/abs/2512.20129</link>
<guid>https://arxiv.org/abs/2512.20129</guid>
<content:encoded><![CDATA[
arXiv:2512.20129v1 Announce Type: cross 
Abstract: Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</title>
<link>https://arxiv.org/abs/2512.20145</link>
<guid>https://arxiv.org/abs/2512.20145</guid>
<content:encoded><![CDATA[
arXiv:2512.20145v1 Announce Type: cross 
Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.20233</link>
<guid>https://arxiv.org/abs/2512.20233</guid>
<content:encoded><![CDATA[
arXiv:2512.20233v1 Announce Type: cross 
Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</title>
<link>https://arxiv.org/abs/2512.20249</link>
<guid>https://arxiv.org/abs/2512.20249</guid>
<content:encoded><![CDATA[
arXiv:2512.20249v1 Announce Type: cross 
Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</title>
<link>https://arxiv.org/abs/2512.20299</link>
<guid>https://arxiv.org/abs/2512.20299</guid>
<content:encoded><![CDATA[
arXiv:2512.20299v1 Announce Type: cross 
Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Field-Space Attention for Structure-Preserving Earth System Transformers</title>
<link>https://arxiv.org/abs/2512.20350</link>
<guid>https://arxiv.org/abs/2512.20350</guid>
<content:encoded><![CDATA[
arXiv:2512.20350v1 Announce Type: cross 
Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</title>
<link>https://arxiv.org/abs/2512.20374</link>
<guid>https://arxiv.org/abs/2512.20374</guid>
<content:encoded><![CDATA[
arXiv:2512.20374v1 Announce Type: cross 
Abstract: Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</title>
<link>https://arxiv.org/abs/2512.20387</link>
<guid>https://arxiv.org/abs/2512.20387</guid>
<content:encoded><![CDATA[
arXiv:2512.20387v1 Announce Type: cross 
Abstract: We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simplifying Multi-Task Architectures Through Task-Specific Normalization</title>
<link>https://arxiv.org/abs/2512.20420</link>
<guid>https://arxiv.org/abs/2512.20420</guid>
<content:encoded><![CDATA[
arXiv:2512.20420v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$\sigma$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$\sigma$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI</title>
<link>https://arxiv.org/abs/2512.20436</link>
<guid>https://arxiv.org/abs/2512.20436</guid>
<content:encoded><![CDATA[
arXiv:2512.20436v1 Announce Type: cross 
Abstract: Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.
  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.
  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Snapshot 3D image projection using a diffractive decoder</title>
<link>https://arxiv.org/abs/2512.20464</link>
<guid>https://arxiv.org/abs/2512.20464</guid>
<content:encoded><![CDATA[
arXiv:2512.20464v1 Announce Type: cross 
Abstract: 3D image display is essential for next-generation volumetric imaging; however, dense depth multiplexing for 3D image projection remains challenging because diffraction-induced cross-talk rapidly increases as the axial image planes get closer. Here, we introduce a 3D display system comprising a digital encoder and a diffractive optical decoder, which simultaneously projects different images onto multiple target axial planes with high axial resolution. By leveraging multi-layer diffractive wavefront decoding and deep learning-based end-to-end optimization, the system achieves high-fidelity depth-resolved 3D image projection in a snapshot, enabling axial plane separations on the order of a wavelength. The digital encoder leverages a Fourier encoder network to capture multi-scale spatial and frequency-domain features from input images, integrates axial position encoding, and generates a unified phase representation that simultaneously encodes all images to be axially projected in a single snapshot through a jointly-optimized diffractive decoder. We characterized the impact of diffractive decoder depth, output diffraction efficiency, spatial light modulator resolution, and axial encoding density, revealing trade-offs that govern axial separation and 3D image projection quality. We further demonstrated the capability to display volumetric images containing 28 axial slices, as well as the ability to dynamically reconfigure the axial locations of the image planes, performed on demand. Finally, we experimentally validated the presented approach, demonstrating close agreement between the measured results and the target images. These results establish the diffractive 3D display system as a compact and scalable framework for depth-resolved snapshot 3D image projection, with potential applications in holographic displays, AR/VR interfaces, and volumetric optical computing.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2512.20595</link>
<guid>https://arxiv.org/abs/2512.20595</guid>
<content:encoded><![CDATA[
arXiv:2512.20595v1 Announce Type: cross 
Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVideoAgent: Multi-Agent Reasoning with Long Videos</title>
<link>https://arxiv.org/abs/2512.20618</link>
<guid>https://arxiv.org/abs/2512.20618</guid>
<content:encoded><![CDATA[
arXiv:2512.20618v1 Announce Type: cross 
Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TropNNC: Structured Neural Network Compression Using Tropical Geometry</title>
<link>https://arxiv.org/abs/2409.03945</link>
<guid>https://arxiv.org/abs/2409.03945</guid>
<content:encoded><![CDATA[
arXiv:2409.03945v3 Announce Type: replace 
Abstract: We present TropNNC, a framework for compressing neural networks with linear and convolutional layers and ReLU activations using tropical geometry. By representing a network's output as a tropical rational function, TropNNC enables structured compression via reduction of the corresponding tropical polynomials. Our method refines the geometric approximation of previous work by adaptively selecting the weights of retained neurons. Key contributions include the first application of tropical geometry to convolutional layers and the tightest known theoretical compression bound. TropNNC requires only access to network weights - no training data - and achieves competitive performance on MNIST, CIFAR, and ImageNet, matching strong baselines such as ThiNet and CUP.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Matching Filtering and Refinement by Planes and Beyond</title>
<link>https://arxiv.org/abs/2411.09484</link>
<guid>https://arxiv.org/abs/2411.09484</guid>
<content:encoded><![CDATA[
arXiv:2411.09484v4 Announce Type: replace 
Abstract: This paper introduces a modular, non-deep learning method for filtering and refining sparse correspondences in image matching. Assuming that motion flow within the scene can be approximated by local homography transformations, matches are aggregated into overlapping clusters corresponding to virtual planes using an iterative RANSAC-based approach discarding incompatible correspondences. Moreover, the underlying planar structural design provides an explicit map between local patches associated with the matches, by which optionally refine the keypoint positions through cross-correlation template matching after the patch reprojection. Finally, to enhance robustness and fault-tolerance against violations of the piece-wise planar approximation assumption, a further strategy is designed in order to minimize the relative patch distortion in the plane reprojection by introducing an intermediate homography that projects both patches into a common plane. The proposed method is extensively evaluated on standard datasets and image matching pipelines, and compared with state-of-the-art approaches. Unlike other current comparisons, the proposed benchmark also takes into account the more general, real, and practical cases where camera intrinsics are unavailable. Experimental results demonstrate that our proposed non-deep learning, geometry-based filter is effective in presence of outliers and the optional cross-correlation refinement step is valid in the case of corner-like keypoints. Finally, this study suggests that there is still significant development potential in practical image matching solutions in the considered research direction, which could be in the future incorporated in novel deep image matching architectures.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compression for Better: A General and Stable Lossless Compression Framework</title>
<link>https://arxiv.org/abs/2412.06868</link>
<guid>https://arxiv.org/abs/2412.06868</guid>
<content:encoded><![CDATA[
arXiv:2412.06868v2 Announce Type: replace 
Abstract: This work focus on how to stabilize and lossless model compression, aiming to reduce model complexity and enhance efficiency without sacrificing performance due to compression errors. A key challenge is effectively leveraging compression errors and defining the boundaries for lossless compression to minimize model loss. i.e., compression for better. Currently, there is no systematic approach to determining this error boundary or understanding its specific impact on model performance. We propose a general \textbf{L}oss\textbf{L}ess \textbf{C}ompression theoretical framework (\textbf{LLC}), which further delineates the compression neighborhood and higher-order analysis boundaries through the total differential, thereby specifying the error range within which a model can be compressed without loss. To verify the effectiveness of LLC, we apply various compression techniques, including quantization and decomposition. Specifically, for quantization, we reformulate the classic quantization search problem as a grouped knapsack problem within the lossless neighborhood, achieving lossless quantization while improving computational efficiency. For decomposition, LLC addresses the approximation problem under low-rank constraints, automatically determining the rank for each layer and producing lossless low-rank models. We conduct extensive experiments on multiple neural network architectures on different datasets. The results show that without fancy tricks, LLC can effectively achieve lossless model compression. Our code will be made publicly.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenVidBench: A 6-Million Benchmark for AI-Generated Video Detection</title>
<link>https://arxiv.org/abs/2501.11340</link>
<guid>https://arxiv.org/abs/2501.11340</guid>
<content:encoded><![CDATA[
arXiv:2501.11340v2 Announce Type: replace 
Abstract: The rapid advancement of video generation models has made it increasingly challenging to distinguish AI-generated videos from real ones. This issue underscores the urgent need for effective AI-generated video detectors to prevent the dissemination of false information via such videos. However, the development of high-performance AI-generated video detectors is currently impeded by the lack of large-scale, high-quality datasets specifically designed for generative video detection. To this end, we introduce GenVidBench, a challenging AI-generated video detection dataset with several key advantages: 1) Large-scale video collection: The dataset contains 6.78 million videos and is currently the largest dataset for AI-generated video detection. 2) Cross-Source and Cross-Generator: The cross-source generation reduces the interference of video content on the detection. The cross-generator ensures diversity in video attributes between the training and test sets, preventing them from being overly similar. 3) State-of-the-Art Video Generators: The dataset includes videos from 11 state-of-the-art AI video generators, ensuring that it covers the latest advancements in the field of video generation. These generators ensure that the datasets are not only large in scale but also diverse, aiding in the development of generalized and effective detection models. Additionally, we present extensive experimental results with advanced video classification models. With GenVidBench, researchers can efficiently develop and evaluate AI-generated video detection models.. Datasets and code are available at https://genvidbench.github.io.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regressor-Guided Generative Image Editing Balances User Emotions to Reduce Time Spent Online</title>
<link>https://arxiv.org/abs/2501.12289</link>
<guid>https://arxiv.org/abs/2501.12289</guid>
<content:encoded><![CDATA[
arXiv:2501.12289v2 Announce Type: replace 
Abstract: Internet overuse is a widespread phenomenon in today's digital society. Existing interventions, such as time limits or grayscaling, often rely on restrictive controls that provoke psychological reactance and are frequently circumvented. Building on prior work showing that emotional responses mediate the relationship between content consumption and online engagement, we investigate whether regulating the emotional impact of images can reduce online use in a non-coercive manner. We introduce and systematically analyze three regressor-guided image-editing approaches: (i) global optimization of emotion-related image attributes, (ii) optimization in a style latent space, and (iii) a diffusion-based method using classifier and classifier-free guidance. While the first two approaches modify low-level visual features (e.g., contrast, color), the diffusion-based method enables higher-level changes (e.g., adjusting clothing, facial features). Results from a controlled image-rating study and a social media experiment show that diffusion-based edits balance emotional responses and are associated with lower usage duration while preserving visual quality.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP</title>
<link>https://arxiv.org/abs/2501.16222</link>
<guid>https://arxiv.org/abs/2501.16222</guid>
<content:encoded><![CDATA[
arXiv:2501.16222v3 Announce Type: replace 
Abstract: Hyperspectral image (HSI) classification aims to categorize each pixel in an HSI into a specific land cover class, which is crucial for applications such as remote sensing, environmental monitoring, and agriculture. Although deep learning-based HSI classification methods have achieved significant advancements, existing methods still rely on manually labeled data for training, which is both time-consuming and labor-intensive. To address this limitation, we introduce a novel zero-shot hyperspectral image classification framework based on CLIP (SPECIAL), aiming to eliminate the need for manual annotations. The SPECIAL framework consists of two main stages: (1) CLIP-based pseudo-label generation, and (2) noisy label learning. In the first stage, HSI is spectrally interpolated to produce RGB bands. These bands are subsequently classified using CLIP, resulting in noisy pseudo-labels that are accompanied by confidence scores. To improve the quality of these labels, we propose a scaling strategy that fuses predictions from multiple spatial scales. In the second stage, spectral information and a label refinement technique are incorporated to mitigate label noise and further enhance classification accuracy. Experimental results on three benchmark datasets demonstrate that our SPECIAL outperforms existing methods in zero-shot HSI classification, showing its potential for more practical applications. The code is available at https://github.com/LiPang/SPECIAL.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</title>
<link>https://arxiv.org/abs/2503.07940</link>
<guid>https://arxiv.org/abs/2503.07940</guid>
<content:encoded><![CDATA[
arXiv:2503.07940v3 Announce Type: replace 
Abstract: Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors, and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Instruction-Guided Graph Reasoning for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2503.11006</link>
<guid>https://arxiv.org/abs/2503.11006</guid>
<content:encoded><![CDATA[
arXiv:2503.11006v2 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN) requires an embodied agent to traverse complex environments by following natural language instructions, demanding accurate alignment between visual observations and linguistic guidance. Despite recent progress, existing methods typically encode visual and directional cues in a coupled manner, and process instructions without explicitly extracting navigation-critical semantics, which often leads to imprecise spatial reasoning and suboptimal cross-modal alignment. To address these challenges, we propose a fine-grained instruction-guided graph reasoning framework (OIKG) that enhances both spatial representation and instruction understanding during navigation. Specifically, an observation-graph interaction mechanism is introduced to disentangle angular and visual cues while strengthening directed edge representations through geometric embedding, enabling more reliable spatial reasoning within the navigation graph. In addition, a fine-grained instruction guidance module is designed to explicitly extract and leverage location-specific and object-centric information from language instructions, facilitating more precise cross-modal alignment between linguistic semantics and navigable trajectories. By jointly integrating structured graph reasoning with instruction-critical semantic cues, the proposed approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR benchmarks demonstrate that our method consistently achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of fine-grained instruction-guided graph reasoning for vision-and-language navigation.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I Want It That Way! Specifying Nuanced Camera Motions in Video Editing</title>
<link>https://arxiv.org/abs/2504.09472</link>
<guid>https://arxiv.org/abs/2504.09472</guid>
<content:encoded><![CDATA[
arXiv:2504.09472v2 Announce Type: replace 
Abstract: Specifying nuanced and compelling camera motion remains a major hurdle for non-expert creators using generative tools, creating an ``expressive gap" where generic text prompts fail to capture cinematic vision. To address this, we present a novel zero-shot diffusion-based system that enables personalized camera motion transfer from a single reference video onto a user-provided static image. Our technical contribution introduces an intuitive interaction paradigm that bypasses the need for 3D data, predefined trajectories, or complex graphical interfaces. The core pipeline leverages a text-to-video diffusion model, employing a two-phase strategy: 1) a multi-concept learning method using LoRA layers and an orthogonality loss to distinctly capture spatial-temporal characteristics and scene features, and 2) a homography-based refinement strategy to enhance temporal and spatial alignment of the generated video. Extensive evaluation demonstrates the efficacy of our method. In a comparative study with 72 participants, our system was significantly preferred over prior work for both motion accuracy (90.45\%) and scene preservation (70.31\%). A second study confirmed our interface significantly improves usability and creative control for video direction. Our work contributes a robust technical solution and a novel human-centered design, significantly expanding cinematic video editing for diverse users.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VibrantLeaves: A principled parametric image generator for training deep restoration models</title>
<link>https://arxiv.org/abs/2504.10201</link>
<guid>https://arxiv.org/abs/2504.10201</guid>
<content:encoded><![CDATA[
arXiv:2504.10201v2 Announce Type: replace 
Abstract: Even though Deep Neural Networks are extremely powerful for image restoration tasks, they have several limitations. They are poorly understood and suffer from strong biases inherited from the training sets. One way to address these shortcomings is to have a better control over the training sets, in particular by using synthetic sets. In this paper, we propose a synthetic image generator relying on a few simple principles. In particular, we focus on geometric modeling, textures, and a simple modeling of image acquisition. These properties, integrated in a classical Dead Leaves model, enable the creation of efficient training sets. Standard image denoising and super-resolution networks can be trained on such datasets, reaching performance almost on par with training on natural image datasets. As a first step towards explainability, we provide a careful analysis of the considered principles, identifying which image properties are necessary to obtain good performances. Besides, such training also yields better robustness to various geometric and radiometric perturbations of the test sets.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FiGO: Fine-Grained Object Counting without Annotations</title>
<link>https://arxiv.org/abs/2504.11705</link>
<guid>https://arxiv.org/abs/2504.11705</guid>
<content:encoded><![CDATA[
arXiv:2504.11705v4 Announce Type: replace 
Abstract: Class-agnostic counting (CAC) methods reduce annotation costs by letting users define what to count at test-time through text or visual exemplars. However, current open-vocabulary approaches work well for broad categories but fail when fine-grained category distinctions are needed, such as telling apart waterfowl species or pepper cultivars. We present FiGO, a new annotation-free method that adapts existing counting models to fine-grained categories using only the category name. Our approach uses a text-to-image diffusion model to create synthetic examples and a joint positive/hard-negative loss to learn a compact concept embedding that conditions a specialization module to convert outputs from any frozen counter into accurate, fine-grained estimates. To evaluate fine-grained counting, we introduce LOOKALIKES, a dataset of 37 subcategories across 14 parent categories with many visually similar objects per image. Our method substantially outperforms strong open-vocabulary baselines, moving counting systems from "count all the peppers" to "count only the habaneros."
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</title>
<link>https://arxiv.org/abs/2504.21850</link>
<guid>https://arxiv.org/abs/2504.21850</guid>
<content:encoded><![CDATA[
arXiv:2504.21850v2 Announce Type: replace 
Abstract: Visual instruction tuning (VIT) datasets are constructed from randomly sampled image-question pairs, without regard to the informativeness of each pair. Recent dataset selection methods have shown that a small fraction of such datasets enriched with informative samples can lead to efficient finetuning of Multimodal Large Language Models. In this work, we explore the impact of sample complexity on informative data curation and introduce COMPACT (COMPositional Atomic-to-complex Visual Capability Tuning), a VIT data recipe that scales training sample complexity by combining multiple atomic visual capabilities in a single training example. Concretely, we synthesize rich and informative text questions for each image, allowing us to significantly reduce the number of training examples required for effective visual instruction tuning. COMPACT demonstrates superior data efficiency compared to existing data reduction methods. When applied to the LLAVA-665K VIT dataset, COMPACT reduces the data budget by 90% while still achieving 100.2% of the full VIT performance (compared to only 97.5% by the state-of-the-art method) across eight multimodal benchmarks. Further, training on the COMPACT data outperforms training on the full-scale data on particularly complex benchmarks such as MM-Vet (+8.6%) and MMStar (+2.9%). COMPACT offers a scalable and efficient synthetic data generation recipe to improve on visual language tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02824</link>
<guid>https://arxiv.org/abs/2505.02824</guid>
<content:encoded><![CDATA[
arXiv:2505.02824v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its promise, the robustness of DOV against copyright evasion attacks (CEA) remains unexplored. In this paper, we investigate how adversaries can circumvent these mechanisms, enabling models trained on watermarked datasets to bypass ownership verification. We begin by analyzing the limitations of potential attacks achieved by backdoor removal, including TPD and T2IShield. In practice, TPD suffers from inconsistent effectiveness due to randomness, while T2IShield fails when watermarks are embedded as local image patches. To this end, we introduce CEAT2I, the first CEA specifically targeting DOV in T2I diffusion models. CEAT2I consists of three stages: (1) motivated by the observation that T2I models converge faster on watermarked samples with respect to intermediate features rather than training loss, we reliably detect watermarked samples; (2) we iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens; and (3) we apply a closed-form concept erasure method to remove the injected watermarks. Extensive experiments demonstrate that CEAT2I effectively evades state-of-the-art DOV mechanisms while preserving model performance. The code is available at https://github.com/csyufei/CEAT2I.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Informative Attention Weights for Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.08961</link>
<guid>https://arxiv.org/abs/2505.08961</guid>
<content:encoded><![CDATA[
arXiv:2505.08961v2 Announce Type: replace 
Abstract: Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone</title>
<link>https://arxiv.org/abs/2507.08268</link>
<guid>https://arxiv.org/abs/2507.08268</guid>
<content:encoded><![CDATA[
arXiv:2507.08268v2 Announce Type: replace 
Abstract: Movement directly reflects neurological and musculoskeletal health, yet objective biomechanical assessment is rarely available in routine care. We introduce Portable Biomechanics Laboratory (PBL), a secure platform for fitting biomechanical models to video collected with a handheld, moving, smartphone. We validate this approach on over 15 hours of data synchronized to ground truth motion capture, finding mean joint-angle errors < 3$\deg$ and pelvis-translation errors of a few centimeters across patients with neurological-injury, lower-limb prosthesis users, pediatric in-patients, and controls. In > 5 hours of prospective deployments to neurosurgery and sports-medicine clinics, PBL was easy to setup, yielded highly reliable gait metrics (ICC > 0.9), and detected clinically relevant differences. For cervical-myelopathy patients, its measurement of gait quality correlated with modified Japanese Orthopedic Association (mJOA) scores and were responsive to clinical intervention. Handheld smartphone video can therefore deliver accurate, scalable, and low-burden biomechanical measurement, enabling greatly increased monitoring of movement impairments. We release the first clinically-validated method for measuring whole-body kinematics from handheld smartphone video at https://IntelligentSensingAndRehabilitation.github.io/MonocularBiomechanics/.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow</title>
<link>https://arxiv.org/abs/2507.19280</link>
<guid>https://arxiv.org/abs/2507.19280</guid>
<content:encoded><![CDATA[
arXiv:2507.19280v3 Announce Type: replace 
Abstract: Remote sensing imagery presents vast, inherently unstructured spatial data, necessitating sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should autonomously explore and construct its own inference paths, rather than being confined to predefined ground-truth sequences. Ideally, its architecture ought to be unified yet generalized, possessing capabilities to perform diverse reasoning tasks through one model without requiring additional fine-tuning. Existing remote sensing approaches rely on supervised fine-tuning paradigms and task-specific heads, limiting both autonomous reasoning and unified generalization. To this end, we propose RemoteReasoner, a unified workflow for geospatial reasoning. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task transformation strategies that enable multi-granularity tasks, including object-, region-, and pixel-level. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient reasoning autonomy. At the inference stage, our transformation strategies enable diverse task output formats without requiring task-specific decoders or further fine-tuning. Experiments demonstrated that RemoteReasoner achieves state-of-the-art (SOTA) performance across multi-granularity reasoning tasks. Furthermore, it retains the MLLM's inherent generalization capability, demonstrating robust performance on unseen tasks and out-of-distribution categories.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer</title>
<link>https://arxiv.org/abs/2508.00477</link>
<guid>https://arxiv.org/abs/2508.00477</guid>
<content:encoded><![CDATA[
arXiv:2508.00477v2 Announce Type: replace 
Abstract: In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.01741</link>
<guid>https://arxiv.org/abs/2508.01741</guid>
<content:encoded><![CDATA[
arXiv:2508.01741v2 Announce Type: replace 
Abstract: Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet underexplored attack surface: vulnerabilities in the base VLM could be retained in fine-tuned variants, rendering them susceptible to transferable jailbreak attacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack (SEA), a novel grey-box jailbreak method in which the adversary has full access to the base VLM but no knowledge of the fine-tuned target's weights or training configuration. To improve jailbreak transferability across fine-tuned VLMs, SEA combines two key techniques: Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG). FTS generates transferable adversarial images by simulating the vision encoder's parameter shifts, while TPG is a textual strategy that steers the language decoder toward adversarially optimized outputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA achieves high transfer attack success rates exceeding 86.5% and toxicity rates near 49.5% across diverse fine-tuned variants, even those specifically fine-tuned to improve safety behaviors. Notably, while direct PGD-based image jailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits inherited vulnerabilities from the base model, significantly enhancing transferability. These findings highlight an urgent need to safeguard fine-tuned proprietary VLMs against transferable vulnerabilities inherited from open-source foundations, motivating the development of holistic defenses across the entire model lifecycle.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Large Model: A Survey</title>
<link>https://arxiv.org/abs/2508.08189</link>
<guid>https://arxiv.org/abs/2508.08189</guid>
<content:encoded><![CDATA[
arXiv:2508.08189v3 Announce Type: replace 
Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.12711</link>
<guid>https://arxiv.org/abs/2508.12711</guid>
<content:encoded><![CDATA[
arXiv:2508.12711v4 Announce Type: replace 
Abstract: The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoCAST: Emotional Talking Portrait via Emotive Text Description</title>
<link>https://arxiv.org/abs/2508.20615</link>
<guid>https://arxiv.org/abs/2508.20615</guid>
<content:encoded><![CDATA[
arXiv:2508.20615v2 Announce Type: replace 
Abstract: Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are mainly collected in lab settings, further exacerbating these shortcomings and hindering real-world deployment. To address these challenges, we propose EmoCAST, a diffusion-based talking head framework for precise, text-driven emotional synthesis. Its contributions are threefold: (1) architectural modules that enable effective text control; (2) an emotional talking-head dataset that expands the framework's ability; and (3) training strategies that further improve performance. Specifically, for appearance modeling, emotional prompts are integrated through a text-guided emotive attention module, enhancing spatial knowledge to improve emotion understanding. To strengthen audio-emotion alignment, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide precise facial motion synthesis. Additionally, we construct a large-scale, in-the-wild emotional talking head dataset with emotive text descriptions to optimize the framework's performance. Based on this dataset, we propose an emotion-aware sampling strategy and a progressive functional training strategy that improve the model's ability to capture nuanced expressive features and achieve accurate lip-sync. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction</title>
<link>https://arxiv.org/abs/2509.19073</link>
<guid>https://arxiv.org/abs/2509.19073</guid>
<content:encoded><![CDATA[
arXiv:2509.19073v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</title>
<link>https://arxiv.org/abs/2510.07191</link>
<guid>https://arxiv.org/abs/2510.07191</guid>
<content:encoded><![CDATA[
arXiv:2510.07191v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</title>
<link>https://arxiv.org/abs/2511.13891</link>
<guid>https://arxiv.org/abs/2511.13891</guid>
<content:encoded><![CDATA[
arXiv:2511.13891v2 Announce Type: replace 
Abstract: Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Generation Models Are Good Latent Reward Models</title>
<link>https://arxiv.org/abs/2511.21541</link>
<guid>https://arxiv.org/abs/2511.21541</guid>
<content:encoded><![CDATA[
arXiv:2511.21541v2 Announce Type: replace 
Abstract: Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection</title>
<link>https://arxiv.org/abs/2512.10652</link>
<guid>https://arxiv.org/abs/2512.10652</guid>
<content:encoded><![CDATA[
arXiv:2512.10652v2 Announce Type: replace 
Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
<link>https://arxiv.org/abs/2512.13507</link>
<guid>https://arxiv.org/abs/2512.13507</guid>
<content:encoded><![CDATA[
arXiv:2512.13507v3 Announce Type: replace 
Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training</title>
<link>https://arxiv.org/abs/2407.04258</link>
<guid>https://arxiv.org/abs/2407.04258</guid>
<content:encoded><![CDATA[
arXiv:2407.04258v2 Announce Type: replace-cross 
Abstract: This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Corrections by Continuous Super-Resolution</title>
<link>https://arxiv.org/abs/2411.07576</link>
<guid>https://arxiv.org/abs/2411.07576</guid>
<content:encoded><![CDATA[
arXiv:2411.07576v2 Announce Type: replace-cross 
Abstract: Finite element methods typically require a high resolution to satisfactorily approximate micro and even macro patterns of an underlying physical model. This issue can be circumvented by appropriate multiscale strategies that are able to obtain reasonable approximations on under-resolved scales. In this paper, we study the implicit neural representation and propose a continuous super-resolution network as a correction strategy for multiscale effects. It can take coarse finite element data to learn both in-distribution and out-of-distribution high-resolution finite element predictions. Our highlight is the design of a local implicit transformer, which is able to learn multiscale features. We also propose Gabor wavelet-based coordinate encodings, which can overcome the bias of neural networks learning low-frequency features. Finally, perception is often preferred over distortion, so scientists can recognize the visual pattern for further investigation. However, implicit neural representation is known for its lack of local pattern supervision. We propose to use stochastic cosine similarities to compare the local feature differences between prediction and ground truth. It shows better performance on structural alignments. Our experiments show that our proposed strategy achieves superior performance as an in-distribution and out-of-distribution super-resolution strategy.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends</title>
<link>https://arxiv.org/abs/2412.16631</link>
<guid>https://arxiv.org/abs/2412.16631</guid>
<content:encoded><![CDATA[
arXiv:2412.16631v2 Announce Type: replace-cross 
Abstract: Land Surface Temperature (LST) plays a key role in climate monitoring, urban heat assessment, and land-atmosphere interactions. However, current thermal infrared satellite sensors cannot simultaneously achieve high spatial and temporal resolution. Spatio-temporal fusion (STF) techniques address this limitation by combining complementary satellite data, one with high spatial but low temporal resolution, and another with high temporal but low spatial resolution. Existing STF techniques, from classical models to modern deep learning (DL) architectures, were primarily developed for surface reflectance (SR). Their application to thermal data remains limited and often overlooks LST-specific spatial and temporal variability. This study provides a focused review of DL-based STF methods for LST. We present a formal mathematical definition of the thermal fusion task, propose a refined taxonomy of relevant DL methods, and analyze the modifications required when adapting SR-oriented models to LST. To support reproducibility and benchmarking, we introduce a new dataset comprising 51 Terra MODIS-Landsat LST pairs from 2013 to 2024, and evaluate representative models to explore their behavior on thermal data. The analysis highlights performance gaps, architecture sensitivities, and open research challenges. The dataset and accompanying resources are publicly available at https://github.com/Sofianebouaziz1/STF-LST.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.08528</link>
<guid>https://arxiv.org/abs/2505.08528</guid>
<content:encoded><![CDATA[
arXiv:2505.08528v2 Announce Type: replace-cross 
Abstract: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need</title>
<link>https://arxiv.org/abs/2509.09719</link>
<guid>https://arxiv.org/abs/2509.09719</guid>
<content:encoded><![CDATA[
arXiv:2509.09719v2 Announce Type: replace-cross 
Abstract: This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Structured State-Space Duality</title>
<link>https://arxiv.org/abs/2510.04944</link>
<guid>https://arxiv.org/abs/2510.04944</guid>
<content:encoded><![CDATA[
arXiv:2510.04944v2 Announce Type: replace-cross 
Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</title>
<link>https://arxiv.org/abs/2511.18417</link>
<guid>https://arxiv.org/abs/2511.18417</guid>
<content:encoded><![CDATA[
arXiv:2511.18417v2 Announce Type: replace-cross 
Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</title>
<link>https://arxiv.org/abs/2512.17939</link>
<guid>https://arxiv.org/abs/2512.17939</guid>
<content:encoded><![CDATA[
<div> anti-UAV, event-driven tracking, energy efficiency, neural processing unit, CMOS technology<br /><br />Summary:<br /><br />This paper presents an energy-efficient anti-UAV system designed to reliably detect small and fast-moving drones by integrating frame-based and event-driven object tracking methods. The system reconstructs binary event frames using run-length encoding and generates region proposals, which enable adaptive switching between frame mode and event mode depending on the object's size and velocity. A Fast Object Tracking Unit bolsters robustness for high-speed targets through adaptive thresholding and trajectory-based classification techniques. The neural processing unit supports dual-mode inference using both grayscale-patch and trajectory data, implemented with a custom instruction set and a zero-skipping multiply-accumulate (MAC) architecture that reduces redundant neural computations by over 97%. The entire system is implemented in a 40 nm CMOS technology chip measuring just 2 mm², achieving very low energy consumption of 96 picojoules per frame per pixel and 61 picojoules per event at 0.8 volts. Performance evaluation on public UAV datasets across detection ranges from 50 to 400 meters and speeds of 5 to 80 pixels per second demonstrated 98.2% recognition accuracy. These results establish the design as state-of-the-art in end-to-end energy efficiency for anti-UAV detection systems. <div>
arXiv:2512.17939v1 Announce Type: new 
Abstract: We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</title>
<link>https://arxiv.org/abs/2512.17943</link>
<guid>https://arxiv.org/abs/2512.17943</guid>
<content:encoded><![CDATA[
<div> Keywords: Nystagmus, photosensitivity, AI prediction, convolutional neural network, visual adaptation<br /><br />Summary: Nystagmus patients with photosensitivity encounter significant daily difficulties caused by involuntary eye movements amplified by varying environmental brightness. Current assistive approaches focus mainly on symptomatic relief without personalized prediction of risk environments. This paper introduces NystagmusNet, an AI-based system designed to predict high-risk visual settings and provide real-time recommendations for visual adaptations. The core model is a dual-branch convolutional neural network trained on synthetic and augmented datasets to estimate photosensitivity risk scores by analyzing environmental brightness and eye movement variance. The system achieves a validation accuracy of 75% on synthetic test data. Explainability methods such as SHAP and GradCAM are incorporated to identify environmental risk zones, thereby enhancing clinical trust and interpretability of model predictions. Additionally, NystagmusNet includes a rule-based recommendation engine that suggests adaptive visual filters tailored to predicted risk levels. Future work aims to deploy the system via smart glasses and use reinforcement learning to further personalize recommendations, ultimately improving the quality of life for patients. This multi-faceted approach merges predictive AI with practical assistive technology to address an unmet need in managing photosensitive nystagmus. <div>
arXiv:2512.17943v1 Announce Type: new 
Abstract: Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperFlow: Training Flow Matching Models with RL on the Fly</title>
<link>https://arxiv.org/abs/2512.17951</link>
<guid>https://arxiv.org/abs/2512.17951</guid>
<content:encoded><![CDATA[
<div> Keywords: flow-based generative models, reinforcement learning, variance-aware sampling, text-to-image generation, continuous-time flow dynamics<br /><br />Summary:<br />Recent advancements in flow-based generative models combined with reinforcement learning (RL) have enhanced text-image alignment and visual output quality. However, existing RL training methods for flow models possess two significant drawbacks: first, fixed per-prompt group sizes (as seen in GRPO-style methods) fail to consider sampling importance variation across prompts, resulting in inefficient sampling and slower training; second, the reuse of trajectory-level advantages as per-step estimates introduces bias in credit assignment along the flow. To address these issues, the paper introduces SuperFlow, an RL training framework that dynamically adjusts group sizes through variance-aware sampling to better allocate computational resources. Additionally, SuperFlow computes step-level advantages aligned with continuous-time flow dynamics to reduce bias and improve credit assignment accuracy. Empirical evaluations demonstrate that SuperFlow achieves competitive performance while requiring only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7%, all without modifying the underlying model architecture. On standard text-to-image tasks such as text rendering, compositional image generation, and human preference alignment, SuperFlow outperforms state-of-the-art baselines—improving over SD3.5-M by 4.6% to 47.2% and Flow-GRPO by 1.7% to 16.0%. <div>
arXiv:2512.17951v1 Announce Type: new 
Abstract: Recent progress in flow-based generative models and reinforcement learning (RL) has improved text-image alignment and visual quality. However, current RL training for flow models still has two main problems: (i) GRPO-style fixed per-prompt group sizes ignore variation in sampling importance across prompts, which leads to inefficient sampling and slower training; and (ii) trajectory-level advantages are reused as per-step estimates, which biases credit assignment along the flow. We propose SuperFlow, an RL training framework for flow-based models that adjusts group sizes with variance-aware sampling and computes step-level advantages in a way that is consistent with continuous-time flow dynamics. Empirically, SuperFlow reaches promising performance while using only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7% without any architectural changes. On standard text-to-image (T2I) tasks, including text rendering, compositional image generation, and human preference alignment, SuperFlow improves over SD3.5-M by 4.6% to 47.2%, and over Flow-GRPO by 1.7% to 16.0%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</title>
<link>https://arxiv.org/abs/2512.17953</link>
<guid>https://arxiv.org/abs/2512.17953</guid>
<content:encoded><![CDATA[
<div> background bias, human action recognition, Video Large Language Models, prompt tuning, segmentation<br /><br />Summary:<br /><br />This paper investigates the phenomenon of background bias in human action recognition models, where predictions are overly influenced by background cues rather than actual human movement and pose. The authors conduct a systematic analysis across various model types, including classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLMs), discovering that all these models prominently exhibit background bias. To address this issue, they propose mitigation strategies targeted at classification models, demonstrating that the incorporation of segmented human input reduces background bias by 3.78%. Additionally, the study explores both manual and automated prompt tuning techniques specifically applied to VLLMs. The results highlight that carefully designed prompts can guide the models to focus more on human-centered reasoning, thereby decreasing background reliance by 9.85%. Overall, this work provides a comprehensive examination of background bias and offers effective approaches to enhance the focus on human actions, which is critical for the reliability and interpretability of action recognition systems. <div>
arXiv:2512.17953v1 Announce Type: new 
Abstract: Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</title>
<link>https://arxiv.org/abs/2512.17954</link>
<guid>https://arxiv.org/abs/2512.17954</guid>
<content:encoded><![CDATA[
<div> Keywords: supervised contrastive learning, sigmoid-based loss, fine-grained recognition, adaptive decision boundaries, style-content disentanglement<br /><br />Summary:  
This paper addresses challenges in image classification related to subtle inter-class differences and significant intra-class variations, which limit the performance of existing contrastive learning methods. The authors identify drawbacks in current supervised contrastive learning approaches using the InfoNCE loss, such as negative-sample dilution and the lack of adaptive decision boundaries, which hinder discriminative capability in fine-grained tasks. To overcome these issues, they propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon), a novel framework that incorporates a sigmoid-based pairwise contrastive loss featuring learnable temperature and bias parameters, enabling adaptive decision boundaries. This approach emphasizes hard negative samples and reduces negative-sample dilution while better leveraging supervision. Additionally, an explicit style-distance constraint is introduced to disentangle style and content representations, fostering more robust feature learning. Comprehensive experiments across six benchmark datasets, including CUB200-2011 and Stanford Dogs, confirm SCS-SupCon achieves state-of-the-art results with both CNN and Transformer architectures. For example, it improves top-1 accuracy on CIFAR-100 with ResNet-50 by approximately 3.9 points over SupCon and 1.7 points over CS-SupCon under five-fold cross-validation. On fine-grained datasets, the method surpasses CS-SupCon by 0.4 to 3.0 points. Extensive ablation and statistical tests, including Friedman and Nemenyi evaluations, validate the robustness, generalization, and stability of the improvements introduced by SCS-SupCon. <div>
arXiv:2512.17954v1 Announce Type: new 
Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Modular Framework for Single-View 3D Reconstruction of Indoor Environments</title>
<link>https://arxiv.org/abs/2512.17955</link>
<guid>https://arxiv.org/abs/2512.17955</guid>
<content:encoded><![CDATA[
<div> Keywords: single-view 3D reconstruction, diffusion techniques, amodal completion, indoor scenes, depth estimation<br /><br />Summary:<br /> This paper introduces a modular framework for single-view 3D reconstruction of indoor scenes, leveraging diffusion-based techniques to handle the complexity of occluded and incomplete instance shapes common in indoor environments. Unlike traditional methods that attempt direct 3D shape prediction from incomplete 2D images, this approach divides the task into two steps: first, using diffusion models to predict complete views of the room’s background and occluded objects, and second, transforming these predictions into 3D reconstructions. The framework consists of four key components: an amodal completion module that restores fully occluded object views, a dedicated inpainting model trained specifically for room layout prediction, a hybrid depth estimation method that balances global geometric accuracy and fine-detail representation, and a view-space alignment technique that combines 2D and 3D cues to accurately place reconstructed objects within the scene. Evaluations conducted on the 3D-Front dataset demonstrate that this approach achieves superior visual quality and reconstruction accuracy compared to state-of-the-art methods. The framework is promising for practical applications such as interior design, real estate visualization, and augmented reality experiences, which require precise and high-quality 3D indoor scene reconstructions from single images. <div>
arXiv:2512.17955v1 Announce Type: new 
Abstract: We propose a modular framework for single-view indoor scene 3D reconstruction, where several core modules are powered by diffusion techniques. Traditional approaches for this task often struggle with the complex instance shapes and occlusions inherent in indoor environments. They frequently overshoot by attempting to predict 3D shapes directly from incomplete 2D images, which results in limited reconstruction quality. We aim to overcome this limitation by splitting the process into two steps: first, we employ diffusion-based techniques to predict the complete views of the room background and occluded indoor instances, then transform them into 3D. Our modular framework makes contributions to this field through the following components: an amodal completion module for restoring the full view of occluded instances, an inpainting model specifically trained to predict room layouts, a hybrid depth estimation technique that balances overall geometric accuracy with fine detail expressiveness, and a view-space alignment method that exploits both 2D and 3D cues to ensure precise placement of instances within the scene. This approach effectively reconstructs both foreground instances and the room background from a single image. Extensive experiments on the 3D-Front dataset demonstrate that our method outperforms current state-of-the-art (SOTA) approaches in terms of both visual quality and reconstruction accuracy. The framework holds promising potential for applications in interior design, real estate, and augmented reality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</title>
<link>https://arxiv.org/abs/2512.17987</link>
<guid>https://arxiv.org/abs/2512.17987</guid>
<content:encoded><![CDATA[
<div> Keywords: tea leaf disease, automated classification, pretrained models, attention modules, explainable AI<br /><br />Summary:<br /><br />Tea is one of the most widely consumed beverages worldwide, and tea production represents an important economic sector for many countries. A major challenge in tea farming is the occurrence of diseases affecting tea leaves, which if not detected and managed early, can cause significant financial losses. Traditional manual identification of tea leaf diseases is inefficient, time-consuming, and unreliable. To address this, the study aims to develop an automated system for classifying multiple types of tea leaf diseases, helping farmers take timely remedial actions. A new dataset comprising 5278 images classified into seven disease categories was created specifically for this research. The dataset underwent preprocessing before model training. The study deployed three pretrained convolutional neural network models: DenseNet, Inception, and EfficientNet (the latter used only within an ensemble approach). Additionally, two attention modules were applied to enhance feature learning and overall model accuracy. The ensemble model combining these networks achieved the highest classification accuracy of 85.68%. To further increase the transparency and usability of the AI system, explainable AI techniques were incorporated to interpret the model's decision-making process, making the system more accessible and trustworthy for end users such as farmers. <div>
arXiv:2512.17987v1 Announce Type: new 
Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Name That Part: 3D Part Segmentation and Naming</title>
<link>https://arxiv.org/abs/2512.18003</link>
<guid>https://arxiv.org/abs/2512.18003</guid>
<content:encoded><![CDATA[
<div> Semantic 3D segmentation, part naming, open-vocabulary, 3D partlets, multi-dataset ontology<br /><br />Summary:<br /><br />1. This paper addresses semantic 3D part segmentation, focusing on decomposing 3D objects into meaningfully named parts. 2. Existing datasets have inconsistent part definitions, which hinders effective and robust model training. 3. Previous work either produces unlabeled part decompositions or retrieves individual parts without full shape annotations, limiting practical usability. 4. The authors propose ALIGN-Parts, a novel framework formulating part naming as a set alignment task that decomposes shapes into implicit 3D part representations called partlets. 5. Partlets are matched to part descriptions via a bipartite assignment that integrates geometric information from 3D part fields, visual cues from multi-view images, and semantic knowledge from language-model-generated affordance descriptions. 6. A text-alignment loss enforces partlets and textual descriptions to share the same embedding space, enabling open-vocabulary part matching with sufficient training data. 7. ALIGN-Parts supports one-shot, zero-shot, and confidence-calibrated predictions for arbitrary part descriptions, making it useful for scalable annotation and human-verified ontology creation. 8. Using their method, the authors unify parts from multiple datasets (PartNet, 3DCoMPaT++, Find3D) into a consolidated ontology containing 1,794 unique 3D parts. 9. They also present the Tex-Parts dataset and introduce two novel evaluation metrics tailored to the task of named 3D part segmentation, demonstrating the effectiveness and applicability of their approach across various downstream tasks. <div>
arXiv:2512.18003v1 Announce Type: new 
Abstract: We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18004</link>
<guid>https://arxiv.org/abs/2512.18004</guid>
<content:encoded><![CDATA[
<div> Marathi, Handwritten Text Recognition, Machine Translation, Legal Documents, Low-Resource Languages<br /><br />Summary:<br /><br />This paper addresses the challenges posed by handwritten text recognition (HTR) and machine translation (MT) for low-resource languages, focusing on Marathi. Traditional solutions use a two-stage pipeline where OCR extracts text from handwriting, which is then translated separately. The study compares this conventional approach to newer Vision Large Language Models (VLLMs) that directly translate handwritten text images end-to-end, eliminating intermediate text extraction. The research is motivated by the pressing need to digitize legal documents such as FIRs, charge sheets, and witness statements from India's district and high courts, facilitating improved access and processing. A curated dataset of handwritten Marathi legal documents is used for evaluation to ensure relevance and applicability. Results provide valuable insights into the feasibility and performance trade-offs between OCR-MT pipelines and integrated VLLM approaches. The findings aim to foster development of scalable, accurate, and edge-deployable translation systems capable of operating effectively in low-resource environments. Ultimately, this work supports enhancing accessibility to legal information for non-native speakers and legal professionals, improving efficiency and inclusivity in the Indian legal system. <div>
arXiv:2512.18004v1 Announce Type: new 
Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2512.18038</link>
<guid>https://arxiv.org/abs/2512.18038</guid>
<content:encoded><![CDATA[
<div> Keywords: lung cancer screening, pulmonary nodules, CT synthesis, data augmentation, lesion detection<br /><br />Summary:  
The study addresses the underrepresentation and inconsistent curation of critical small pulmonary nodules in medical imaging datasets for lung cancer screening. The authors present NodMAISI, a novel CT synthesis and augmentation framework trained on a large multi-source cohort of 7,042 patients, incorporating 8,841 CT scans and 14,444 nodules. NodMAISI integrates a standardized curation and annotation pipeline linking CT images with organ masks and detailed nodule annotations. The framework uses a ControlNet-conditioned rectified-flow generator built upon MAISI-v2 to ensure anatomically and lesion-consistent image synthesis. Additionally, NodMAISI applies lesion-aware augmentation by selectively shrinking nodule masks while preserving surrounding anatomy to produce paired CT variants. Evaluation across six public test datasets demonstrates superior distributional fidelity compared to MAISI-v2, with improved real-to-synthetic FID scores ranging from 1.18 to 2.99 versus 1.69 to 5.21 for MAISI-v2. Lesion detectability analysis using a MONAI nodule detector shows marked sensitivity improvements, particularly for sub-centimeter nodules, where NodMAISI achieved sensitivity scores of 0.69 and 0.63 on IMD-CT and DLCS24 datasets, outperforming MAISI-v2. For downstream malignancy classification tasks trained on LUNA25 and tested on external datasets, NodMAISI-based augmentation increased AUC values by 0.07 to 0.21 under clinical data scarcity, effectively narrowing performance gaps in low-data regimes. <div>
arXiv:2512.18038v1 Announce Type: new 
Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs</title>
<link>https://arxiv.org/abs/2512.18046</link>
<guid>https://arxiv.org/abs/2512.18046</guid>
<content:encoded><![CDATA[
<div> Keywords: drone detection, Yolov5-CBi, small object detection, CBAM, knowledge distillation  

<br /><br />Summary:  
1. The article addresses the challenge of detecting small, fast-moving drones with low visual contrast in real-time, which is critical for civilian and defense applications.  
2. A modified architecture, Yolov5-CBi, is proposed by integrating the Convolutional Block Attention Module (CBAM) and Bidirectional Feature Pyramid Network (BiFPN) to enhance sensitivity toward small object detection.  
3. To train and evaluate the model, a curated dataset of 28,000 images featuring various flying objects was created, along with a local test set of 2,500 images containing very small drones.  
4. The Yolov5 baseline and the proposed Yolov5-CBi models were benchmarked against newer YOLO versions (Yolov8 and Yolov12), showing superior speed-accuracy trade-offs specifically for small drone detection.  
5. Four additional CBi variants, differing by CBAM and BiFPN placement, were developed and further improved using knowledge distillation, employing a Yolov5m-CBi teacher model and a Yolov5n-CBi student model for efficient edge deployment.  
6. The distilled student model achieved a mean average precision (mA@P0.5:0.9) of 0.6573, marking a 6.51% improvement over the teacher’s 0.6171 score, while running 82.9% faster than the baseline.  
7. These results demonstrate the effectiveness of the CBi architecture combined with knowledge distillation in enabling highly accurate and fast real-time detection of small UAVs suitable for practical use cases. <div>
arXiv:2512.18046v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles, commonly known as, drones pose increasing risks in civilian and defense settings, demanding accurate and real-time drone detection systems. However, detecting drones is challenging because of their small size, rapid movement, and low visual contrast. A modified architecture of YolovN called the YolovN-CBi is proposed that incorporates the Convolutional Block Attention Module (CBAM) and the Bidirectional Feature Pyramid Network (BiFPN) to improve sensitivity to small object detections. A curated training dataset consisting of 28K images is created with various flying objects and a local test dataset is collected with 2500 images consisting of very small drone objects. The proposed architecture is evaluated on four benchmark datasets, along with the local test dataset. The baseline Yolov5 and the proposed Yolov5-CBi architecture outperform newer Yolo versions, including Yolov8 and Yolov12, in the speed-accuracy trade-off for small object detection. Four other variants of the proposed CBi architecture are also proposed and evaluated, which vary in the placement and usage of CBAM and BiFPN. These variants are further distilled using knowledge distillation techniques for edge deployment, using a Yolov5m-CBi teacher and a Yolov5n-CBi student. The distilled model achieved a mA@P0.5:0.9 of 0.6573, representing a 6.51% improvement over the teacher's score of 0.6171, highlighting the effectiveness of the distillation process. The distilled model is 82.9% faster than the baseline model, making it more suitable for real-time drone detection. These findings highlight the effectiveness of the proposed CBi architecture, together with the distilled lightweight models in advancing efficient and accurate real-time detection of small UAVs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOODER: Real-time Facial Authentication and Expression Recognition</title>
<link>https://arxiv.org/abs/2512.18057</link>
<guid>https://arxiv.org/abs/2512.18057</guid>
<content:encoded><![CDATA[
<div> Keywords: Out-of-distribution detection, FMCW radar, facial authentication, facial expression recognition, MobileViT networks<br /><br />Summary:  
1. FOODER is a real-time, privacy-preserving framework that uses radar technology for facial authentication combined with facial expression recognition.  
2. It employs low-cost frequency-modulated continuous-wave (FMCW) radar operating at 60 GHz to capture range-Doppler and micro range-Doppler data, avoiding privacy concerns related to cameras.  
3. The authentication module features a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify enrolled individuals as in-distribution while identifying others as out-of-distribution (OOD).  
4. Upon successful authentication, expression recognition is performed using radar data processed through a ResNet block to differentiate between dynamic and static facial expressions.  
5. Two specialized MobileViT networks are employed to classify dynamic expressions (smile, shock) and static expressions (neutral, anger), enabling fine-grained expression recognition.  
6. Experimental results show FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%.  
7. FOODER outperforms existing state-of-the-art OOD detection methods and transformer-based architectures while maintaining real-time efficiency, presenting a robust and privacy-sensitive solution for secure facial authentication and expression analysis. <div>
arXiv:2512.18057v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</title>
<link>https://arxiv.org/abs/2512.18073</link>
<guid>https://arxiv.org/abs/2512.18073</guid>
<content:encoded><![CDATA[
<div> Fingerprint, Multimodal LLMs, Benchmark, Biometric Tasks, Explainability<br /><br />Summary:<br /><br />1. Multimodal large language models (MLLMs) have shown strong capabilities in complex data analysis and various visual and reasoning tasks, including biometric image analysis for iris and face recognition.  <br /><br />2. Despite these advances, the application of MLLMs to fingerprint understanding has not been explored prior to this work. <br /><br />3. The authors introduce \textsc{FPBench}, a comprehensive benchmark designed to evaluate the performance of 20 different MLLMs, including both open-source and proprietary models, on fingerprint-related tasks. <br /><br />4. The evaluation is conducted across 7 real and synthetic fingerprint datasets and covers 8 biometric and forensic tasks, utilizing zero-shot and chain-of-thought prompting methods to test model capabilities. <br /><br />5. The study discusses the performance results, the explainability of model outputs, and highlights challenges and limitations in current fingerprint understanding through MLLMs. <br /><br />6. \textsc{FPBench} is established as the first extensive benchmark specifically addressing fingerprint domain understanding, aiming to foster development of foundational models tailored for fingerprint analysis. <div>
arXiv:2512.18073v1 Announce Type: new 
Abstract: Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.18082</link>
<guid>https://arxiv.org/abs/2512.18082</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic segmentation, domain shift, uncertainty-gated retrieval, mean intersection-over-union, real-time performance<br /><br />Summary:<br /><br />1. The paper addresses semantic segmentation of outdoor street scenes, a critical task for autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians, where accurate distinction of roads, sidewalks, vehicles, and pedestrians is vital for safety.<br /><br />2. The method must be robust to varying environments, lighting and weather conditions, sensor noise, and meet real-time processing demands.<br /><br />3. The authors propose a novel region-level, uncertainty-gated retrieval mechanism that selectively retrieves relevant information to improve segmentation accuracy and calibration when faced with domain shifts.<br /><br />4. Their best performing model achieves an 11.3% improvement in mean intersection-over-union (mIoU), indicating a significant boost in segmentation quality.<br /><br />5. Additionally, the approach dramatically reduces retrieval cost by 87.5%, as it only performs retrieval on 12.5% of regions compared to a baseline that retrieves on all regions, leading to more efficient real-time operation under domain shift conditions. <div>
arXiv:2512.18082v1 Announce Type: new 
Abstract: Semantic segmentation of outdoor street scenes plays a key role in applications such as autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians. For these applications, accurately distinguishing between key surfaces and objects such as roads, sidewalks, vehicles, and pedestrians is essential for maintaining safety and minimizing risks. Semantic segmentation must be robust to different environments, lighting and weather conditions, and sensor noise, while being performed in real-time. We propose a region-level, uncertainty-gated retrieval mechanism that improves segmentation accuracy and calibration under domain shift. Our best method achieves an 11.3% increase in mean intersection-over-union while reducing retrieval cost by 87.5%, retrieving for only 12.5% of regions compared to 100% for always-on baseline.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</title>
<link>https://arxiv.org/abs/2512.18128</link>
<guid>https://arxiv.org/abs/2512.18128</guid>
<content:encoded><![CDATA[
<div> Canopy height mapping, deep learning, super-resolution, Sentinel-1/2, LiDAR<br /><br />Summary:<br /><br />1. The article addresses the challenge of creating high-resolution canopy height maps crucial for forest management and biodiversity monitoring. 2. It introduces SERA-H, an innovative end-to-end model that integrates a super-resolution module called EDSR with a temporal attention encoder named UTAE. 3. SERA-H is trained using high-density airborne LiDAR (ALS) data to generate 2.5 m spatial resolution height maps from freely accessible Sentinel-1 and Sentinel-2 satellite time series data, originally available at 10 m resolution. 4. Evaluations conducted on a publicly available benchmark dataset from France demonstrate that SERA-H outperforms traditional Sentinel-1/2 baseline models, achieving a mean absolute error (MAE) of 2.6 m and a determination coefficient (R²) of 0.82. 5. The model matches or surpasses the accuracy of methods relying on expensive commercial very high-resolution imagery such as SPOT-6/7, PlanetScope, and Maxar. 6. This result underscores how combining high-resolution supervisory data with the rich spatiotemporal information inherent in satellite time series allows reconstruction of fine details beyond the input sensor resolutions. 7. SERA-H enables cost-effective, freely accessible forest height mapping with high revisit frequency, facilitating improved forest monitoring and management practices. 8. The authors have released the source code publicly, promoting further research and application development. <div>
arXiv:2512.18128v1 Announce Type: new 
Abstract: High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors' native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery. The source code is available at https://github.com/ThomasBoudras/SERA-H#
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams</title>
<link>https://arxiv.org/abs/2512.18159</link>
<guid>https://arxiv.org/abs/2512.18159</guid>
<content:encoded><![CDATA[
<div> Keywords: EndoStreamDepth, monocular depth estimation, endoscopic video, temporal consistency, anatomical boundaries<br /><br />Summary:<br /><br />This paper introduces EndoStreamDepth, a novel monocular depth estimation framework designed specifically for endoscopic video streams, offering accurate per-frame depth maps with sharply defined anatomical boundaries. Unlike previous methods that rely on processing batches of frames, EndoStreamDepth handles individual frames while utilizing a temporal module to propagate information across frames, ensuring temporal consistency and real-time processing capability. The framework is composed of three core components: (1) a single-frame depth network enhanced by endoscopy-specific transformations to improve accuracy, (2) multi-level Mamba temporal modules that leverage inter-frame data to stabilize depth predictions and enhance precision, and (3) a hierarchical architecture incorporating multi-scale supervision with complementary loss functions to simultaneously refine local boundary sharpness and maintain global geometric consistency. Extensive evaluations were performed on two publicly available colonoscopy depth estimation datasets, demonstrating that EndoStreamDepth outperforms state-of-the-art monocular depth estimation approaches significantly. The model produces depth maps with anatomically aligned and sharp boundaries, which is critical for supporting downstream applications such as robotic surgery automation. Additionally, the authors have made the source code publicly accessible at the provided GitHub repository, promoting transparency and further research in this domain. <div>
arXiv:2512.18159v1 Announce Type: new 
Abstract: This work presents EndoStreamDepth, a monocular depth estimation framework for endoscopic video streams. It provides accurate depth maps with sharp anatomical boundaries for each frame, temporally consistent predictions across frames, and real-time throughput. Unlike prior work that uses batched inputs, EndoStreamDepth processes individual frames with a temporal module to propagate inter-frame information. The framework contains three main components: (1) a single-frame depth network with endoscopy-specific transformation to produce accurate depth maps, (2) multi-level Mamba temporal modules that leverage inter-frame information to improve accuracy and stabilize predictions, and (3) a hierarchical design with comprehensive multi-scale supervision, where complementary loss terms jointly improve local boundary sharpness and global geometric consistency. We conduct comprehensive evaluations on two publicly available colonoscopy depth estimation datasets. Compared to state-of-the-art monocular depth estimation methods, EndoStreamDepth substantially improves performance, and it produces depth maps with sharp, anatomically aligned boundaries, which are essential to support downstream tasks such as automation for robotic surgery. The code is publicly available at https://github.com/MedICL-VU/EndoStreamDepth
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction</title>
<link>https://arxiv.org/abs/2512.18161</link>
<guid>https://arxiv.org/abs/2512.18161</guid>
<content:encoded><![CDATA[
<div> Diffusion Models, 3D Image Reconstruction, Patch-based Learning, Computed Tomography, High-resolution Imaging  

<br /><br />Summary:  
This paper addresses the challenge of applying diffusion models to high-dimensional 3D medical imaging data, specifically for 3D Computed Tomography (CT) reconstruction. Existing approaches mainly rely on 2D diffusion priors, which limits their generative capability for 3D data. The authors propose a novel 3D patch-based diffusion model that learns a fully 3D diffusion prior from limited data, making it scalable for generating high-resolution 3D images. The key innovation is to model the joint distribution of position-aware 3D local patches alongside a downsampled global 3D volume, thereby coupling local detail with global structural context to ensure high image quality. This patch-based approach enables both efficiency and scalability without requiring excessive computational resources or large datasets. The method demonstrates superior performance and efficiency on multiple 3D CT reconstruction datasets, significantly outperforming state-of-the-art methods. Notably, their approach can reconstruct high-resolution 3D volumes sized 512 × 512 × 256 in approximately 20 minutes, offering a practical and accurate solution to high-resolution 3D inverse problems in medical imaging. <div>
arXiv:2512.18161v1 Announce Type: new 
Abstract: Diffusion models learn strong image priors that can be leveraged to solve inverse problems like medical image reconstruction. However, for real-world applications such as 3D Computed Tomography (CT) imaging, directly training diffusion models on 3D data presents significant challenges due to the high computational demands of extensive GPU resources and large-scale datasets. Existing works mostly reuse 2D diffusion priors to address 3D inverse problems, but fail to fully realize and leverage the generative capacity of diffusion models for high-dimensional data. In this study, we propose a novel 3D patch-based diffusion model that can learn a fully 3D diffusion prior from limited data, enabling scalable generation of high-resolution 3D images. Our core idea is to learn the prior of 3D patches to achieve scalable efficiency, while coupling local and global information to guarantee high-quality 3D image generation, by modeling the joint distribution of position-aware 3D local patches and downsampled 3D volume as global context. Our approach not only enables high-quality 3D generation, but also offers an unprecedentedly efficient and accurate solution to high-resolution 3D inverse problems. Experiments on 3D CT reconstruction across multiple datasets show that our method outperforms state-of-the-art methods in both performance and efficiency, notably achieving high-resolution 3D reconstruction of $512 \times 512 \times 256$ ($\sim$20 mins).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.18176</link>
<guid>https://arxiv.org/abs/2512.18176</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, foundation models, atlas-guided framework, one-shot customization, clinical workflows  

<br /><br />Summary:  
Accurate medical image segmentation is critical for clinical diagnosis and treatment planning. Although recent interactive foundation models, such as nnInteractive, leverage large-scale multimodal pretraining to improve generalization, they rely heavily on precise user prompts and often underperform in clinical contexts that are underrepresented in their training data. To address these challenges, the authors introduce AtlasSegFM, an atlas-guided framework designed to adapt existing foundation models to specific clinical contexts using just a single annotated example (one-shot customization). The framework innovates by implementing a pipeline that generates context-aware prompts through registration between a context atlas and query images, enhancing the foundation model's applicability to new contexts. Additionally, a test-time adapter is proposed to effectively fuse segmentation predictions from both atlas registration and the foundation model, thereby improving overall performance. The effectiveness of AtlasSegFM is validated via extensive experiments on multiple public and proprietary datasets, covering different imaging modalities and organs. Results demonstrate consistent improvements in segmentation accuracy, especially for small and delicate anatomical structures often challenging to segment. AtlasSegFM offers a lightweight and deployable approach that facilitates the practical integration of foundation models into real-world clinical workflows. The authors plan to release the code publicly, promoting accessibility and reproducibility. <div>
arXiv:2512.18176v1 Announce Type: new 
Abstract: Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</title>
<link>https://arxiv.org/abs/2512.18181</link>
<guid>https://arxiv.org/abs/2512.18181</guid>
<content:encoded><![CDATA[
<div> Keywords: music-driven dance generation, mixture-of-experts, diffusion model, BiMamba-Transformer, pose-driven image animation<br /><br />Summary:  
1. With the growth of online dance video platforms and AI-generated content, music-driven dance generation has become a significant research area.  
2. Existing methods in related fields like 3D dance generation, pose-driven animation, and talking-head synthesis cannot be easily adapted to generate high-quality dance videos combining realistic motion and visual appearance.  
3. The paper introduces MACE-Dance, a framework consisting of cascaded Mixture-of-Experts (MoE) modules: a Motion Expert and an Appearance Expert, specialized respectively in generating plausible and expressive 3D dance motions, and synthesizing videos that maintain identity and spatiotemporal coherence.  
4. The Motion Expert uses a diffusion model with a novel BiMamba-Transformer hybrid architecture and employs a Guidance-Free Training strategy, achieving state-of-the-art results in 3D dance generation.  
5. The Appearance Expert uses a decoupled kinematic-aesthetic fine-tuning strategy to excel at pose-driven image animation, also reaching state-of-the-art performance.  
6. To evaluate the task fairly, the authors present a large-scale, diverse dance dataset and propose a combined motion-appearance evaluation protocol.  
7. Under this evaluation, MACE-Dance outperforms existing methods, establishing new benchmarks for music-driven dance video generation.  
8. The project and further resources are available at https://macedance.github.io/. <div>
arXiv:2512.18181v1 Announce Type: new 
Abstract: With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</title>
<link>https://arxiv.org/abs/2512.18184</link>
<guid>https://arxiv.org/abs/2512.18184</guid>
<content:encoded><![CDATA[
<div> Flow matching, source distribution, Gaussian, norm alignment, sampling efficiency  

<br /><br />Summary:  
This paper investigates the impact of source distributions in flow matching generative models, focusing on alternatives to the commonly used Gaussian distribution for high-dimensional data generation. The authors introduce a novel 2D simulation framework that captures high-dimensional geometric properties, enabling interpretability in analyzing flow matching training dynamics. Through this analysis, they reveal four key insights: (1) attempts to improve density approximation can paradoxically worsen performance due to mode discrepancy issues, (2) directional alignment efforts suffer from path entanglement when the sampling source is too concentrated, (3) Gaussian distributions provide robust, omnidirectional coverage that enhances learning stability, and (4) norm misalignment between source and target data incurs significant learning costs. Leveraging these findings, the authors propose a new practical framework combining norm-aligned training with directionally-pruned sampling, which preserves the robust omnidirectional supervision property of Gaussian sources while avoiding initialization in data-scarce regions during inference. This pruning method is compatible with any flow matching model trained using Gaussian sources and yields immediate performance improvements without retraining. Empirical results demonstrate consistent gains in both sample quality and inference efficiency. The paper offers actionable insights for designing source distributions in flow matching and introduces a straightforward technique to enhance existing models. Code is publicly available for replication and further exploration. <div>
arXiv:2512.18184v1 Announce Type: new 
Abstract: Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.18187</link>
<guid>https://arxiv.org/abs/2512.18187</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D object detection, query initialization, occlusion-aware, LiDAR, image guidance  

<br /><br />Summary:  
This paper addresses inefficiencies in query initialization for query-based 3D object detection methods that utilize both camera and LiDAR inputs, highlighting problems especially with occluded or crowded objects. To overcome these challenges, the authors propose ALIGN, a novel framework emphasizing occlusion-robust and object-aware query initialization. The model includes three main components: (i) Occlusion-aware Center Estimation (OCE), which synergizes LiDAR geometric data and image semantic information to precisely estimate object centers; (ii) Adaptive Neighbor Sampling (ANS), which produces object candidates via LiDAR clustering and enriches them by sampling spatially and semantically aligned points around each candidate; and (iii) Dynamic Query Balancing (DQB), which adaptively manages the allocation of queries between foreground and background areas to optimize detection performance. Extensive experiments conducted on the nuScenes benchmark demonstrate that ALIGN improves multiple state-of-the-art 3D detection models, delivering up to +0.9 mAP and +1.2 NDS gains. These improvements are most pronounced in difficult scenarios featuring heavy occlusion or densely clustered objects. The authors also commit to releasing their code publicly upon publication to promote further research and development in this field. <div>
arXiv:2512.18187v1 Announce Type: new 
Abstract: Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Part Object Representations via Graph Structures and Co-Part Discovery</title>
<link>https://arxiv.org/abs/2512.18192</link>
<guid>https://arxiv.org/abs/2512.18192</guid>
<content:encoded><![CDATA[
<div> Keywords: object-centric representations, multi-part objects, graph representations, occlusion robustness, out-of-distribution recognition<br /><br />Summary:<br /><br />1. The paper addresses the challenge of discovering object-centric representations from images, which can improve vision models in robustness, sample efficiency, and generalizability. <br />2. Existing methods for images with multi-part objects typically rely on implicit representations that do not effectively recognize objects in occluded or out-of-distribution scenarios because they assume part-whole relationships are implicitly learned through indirect training objectives. <br />3. To overcome this limitation, the authors propose a novel approach that uses explicit graph representations of object parts combined with a co-part object discovery algorithm. <br />4. The study introduces three new benchmarks designed to evaluate the robustness of object-centric methods specifically for recognizing multi-part objects under occlusion and out-of-distribution conditions. <br />5. Experiments conducted on simulated, realistic, and real-world image datasets demonstrate significant improvement over state-of-the-art methods in object discovery quality and robust recognition in challenging contexts. <br />6. Furthermore, the discovered object-centric representations enable more accurate prediction of key object properties in downstream tasks, showcasing the practical advantages and advancement potential of the proposed method in the field of object-centric vision models. <div>
arXiv:2512.18192v1 Announce Type: new 
Abstract: Discovering object-centric representations from images can significantly enhance the robustness, sample efficiency and generalizability of vision models. Works on images with multi-part objects typically follow an implicit object representation approach, which fail to recognize these learned objects in occluded or out-of-distribution contexts. This is due to the assumption that object part-whole relations are implicitly encoded into the representations through indirect training objectives. We address this limitation by proposing a novel method that leverages on explicit graph representations for parts and present a co-part object discovery algorithm. We then introduce three benchmarks to evaluate the robustness of object-centric methods in recognizing multi-part objects within occluded and out-of-distribution settings. Experimental results on simulated, realistic, and real-world images show marked improvements in the quality of discovered objects compared to state-of-the-art methods, as well as the accurate recognition of multi-part objects in occluded and out-of-distribution contexts. We also show that the discovered object-centric representations can more accurately predict key object properties in a downstream task, highlighting the potential of our method to advance the field of object-centric representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</title>
<link>https://arxiv.org/abs/2512.18219</link>
<guid>https://arxiv.org/abs/2512.18219</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, student-teacher framework, ResNet-18, MVTech-AD dataset, feature pyramid  

<br /><br />Summary:  
1. The paper addresses the challenge of anomaly detection in unsupervised learning by proposing a student-teacher framework.  
2. The teacher network in this framework is enhanced to improve performance metrics significantly.  
3. The approach involves first pre-training the ResNet-18 model on the large-scale ImageNet dataset, followed by fine-tuning it using the MVTech-AD dataset, which is specific for anomaly detection tasks.  
4. Experiments conducted demonstrate that this method surpasses previous state-of-the-art anomaly detection techniques both at the image-level and pixel-level.  
5. The proposed model, named Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved high accuracy scores of 0.971 on image-level detection and 0.977 on pixel-level detection, indicating robust detection capabilities.  
6. This improvement highlights the importance of leveraging a pretrained teacher model and fine-tuning it carefully on relevant data to boost anomaly detection performance in complex visual tasks. <div>
arXiv:2512.18219v1 Announce Type: new 
Abstract: Anomaly detection or outlier is one of the challenging subjects in unsupervised learning . This paper is introduced a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics . For this purpose , we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset . Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods . Our model , Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo's 23 Wards</title>
<link>https://arxiv.org/abs/2512.18226</link>
<guid>https://arxiv.org/abs/2512.18226</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial openness, housing quality, visibility graph analysis, semantic segmentation, urban redevelopment<br /><br />Summary:<br /><br />This study develops a quantitative framework to assess spatial openness in residential housing by integrating two-dimensional (2D) and three-dimensional (3D) perspectives. Using data from 4,004 rental units across Tokyo's 23 wards, the research analyzes temporal and spatial variations in openness along with its associations with rent and housing attributes. The 2D openness is calculated through planar visibility leveraging visibility graph analysis (VGA) based on floor plans, while 3D openness is derived using Mask2Former, a semantic segmentation model that identifies interior elements such as walls, ceilings, floors, and windows from images. Findings reveal an increase in living room visibility over time, with a peak in overall openness observed around the 1990s. Spatial analysis indicates that openness, rent, and building characteristics exhibit partial correlations that reflect patterns of urban redevelopment. Although 2D and 3D openness measures do not directly correlate, both show a trend where higher openness aligns with increased rent values. The study also finds that existing models predicting impression scores only weakly relate to openness, suggesting interior design and furniture significantly influence perceived spatial quality. Ultimately, this multidimensional framework offers a novel data-driven approach to quantify residential spatial openness while linking it to urban and housing market dynamics. <div>
arXiv:2512.18226v1 Announce Type: new 
Abstract: Understanding spatial openness is vital for improving residential quality and design; however, studies often treat its influencing factors separately. This study developed a quantitative framework to evaluate the spatial openness in housing from two- (2D) and three- (3D) dimensional perspectives. Using data from 4,004 rental units in Tokyo's 23 wards, we examined the temporal and spatial variations in openness and its relationship with rent and housing attributes. 2D openness was computed via planar visibility using visibility graph analysis (VGA) from floor plans, whereas 3D openness was derived from interior images analysed using Mask2Former, a semantic segmentation model that identifies walls, ceilings, floors, and windows. The results showed an increase in living room visibility and a 1990s peak in overall openness. Spatial analyses revealed partial correlations among openness, rent, and building characteristics, reflecting urban redevelopment trends. Although the 2D and 3D openness indicators were not directly correlated, higher openness tended to correspond to higher rent. The impression scores predicted by the existing models were only weakly related to openness, suggesting that the interior design and furniture more strongly shape perceived space. This study offers a new multidimensional data-driven framework for quantifying residential spatial openness and linking it with urban and market dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Spatial Attention Bias in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18231</link>
<guid>https://arxiv.org/abs/2512.18231</guid>
<content:encoded><![CDATA[
<div> Bias, Vision-Language Models, Spatial Attention, Left-Right Order, Architectural Factors<br /><br />Summary:<br /><br />1. Vision-Language Models (VLMs) exhibit a consistent spatial attention bias, preferentially describing left-positioned content before right-positioned content when analyzing horizontally concatenated images.  
2. Controlled experiments conducted on both open-source and closed-source VLMs show that this left-first bias occurs in roughly 97% of the cases under neutral prompting, indicating its robustness across different architectures.  
3. The bias persists even in an Arabic-finetuned VLM, which uses right-to-left language training, suggesting that the observed effect is not driven by the direction of language reading or linguistic conventions.  
4. Analysis of annotation guidelines from major datasets like PixMo and Visual Genome indicates no explicit instructions for left-first ordering during training data labeling, which rules out training data bias as the root cause.  
5. The study concludes that this spatial prioritization bias stems primarily from inherent architectural aspects of current VLMs rather than from language patterns or training data instructions, highlighting a fundamental limitation in how these models process and attend to spatial information. <div>
arXiv:2512.18231v1 Announce Type: new 
Abstract: Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</title>
<link>https://arxiv.org/abs/2512.18237</link>
<guid>https://arxiv.org/abs/2512.18237</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular video, 3-D reconstruction, Vision-Transformer, bundle adjustment, local radiance field<br /><br />Summary:<br /><br />1. The paper addresses the challenge of photorealistic 3-D reconstruction from monocular video in large-scale scenes, where traditional approaches fail due to scale ambiguity in depth, pose drift, and limitations of a single global NeRF model.<br /><br />2. A novel joint learning framework is proposed that simultaneously solves depth, camera pose, and scene radiance, overcoming prior failure modes. Depth estimation uses a Vision-Transformer (ViT) trained with metric-scale supervision to achieve consistent global depth despite varying fields of view.<br /><br />3. Camera poses are refined through a multi-scale feature bundle-adjustment layer operating in learned feature space, which is more robust than traditional keypoint methods, effectively reducing drift over long trajectories.<br /><br />4. For scene representation, an incremental local radiance field hierarchy is introduced, where new hash-grid Neural Radiance Fields (NeRFs) are allocated and frozen dynamically based on view overlap, enabling efficient city-block-scale coverage on a single GPU.<br /><br />5. The method is evaluated on the Tanks and Temples benchmark, achieving significantly lower Absolute Trajectory Error (0.001-0.021 m) compared to state-of-the-art methods such as BARF and NoPe-NeRF while maintaining sub-pixel Relative Pose Error. This demonstrates accurate, drift-free metric-scale 3-D reconstruction and high-quality novel-view synthesis from a single uncalibrated RGB camera. <div>
arXiv:2512.18237v1 Announce Type: new 
Abstract: Photorealistic 3-D reconstruction from monocular video collapses in large-scale scenes when depth, pose, and radiance are solved in isolation: scale-ambiguous depth yields ghost geometry, long-horizon pose drift corrupts alignment, and a single global NeRF cannot model hundreds of metres of content. We introduce a joint learning framework that couples all three factors and demonstrably overcomes each failure case. Our system begins with a Vision-Transformer (ViT) depth network trained with metric-scale supervision, giving globally consistent depths despite wide field-of-view variations. A multi-scale feature bundle-adjustment (BA) layer refines camera poses directly in feature space--leveraging learned pyramidal descriptors instead of brittle keypoints--to suppress drift on unconstrained trajectories. For scene representation, we deploy an incremental local-radiance-field hierarchy: new hash-grid NeRFs are allocated and frozen on-the-fly when view overlap falls below a threshold, enabling city-block-scale coverage on a single GPU. Evaluated on the Tanks and Temples benchmark, our method reduces Absolute Trajectory Error to 0.001-0.021 m across eight indoor-outdoor sequences--up to 18x lower than BARF and 2x lower than NoPe-NeRF--while maintaining sub-pixel Relative Pose Error. These results demonstrate that metric-scale, drift-free 3-D reconstruction and high-fidelity novel-view synthesis are achievable from a single uncalibrated RGB camera.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</title>
<link>https://arxiv.org/abs/2512.18241</link>
<guid>https://arxiv.org/abs/2512.18241</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-time Video Frame Interpolation, flow-based methods, diffusion-based approaches, semantic priors, parameter-efficient fine-tuning<br /><br />Summary:<br /><br />This paper addresses the challenge of achieving both high perceptual quality and real-time performance in video frame interpolation (VFI). Traditional flow-based methods like RIFE provide fast processing but struggle with complex motion and occlusions, reducing visual quality. On the other hand, diffusion-based models such as Consec. BB offer superior perceptual results but suffer from high latency, making them unsuitable for real-time use. To bridge this gap, the authors introduce Semantic-Guided RIFE (SG-RIFE), which enhances a pre-trained RIFE backbone through parameter-efficient fine-tuning using semantic priors extracted from a frozen DINOv3 Vision Transformer. They develop two key modules: Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional semantic features, and Deformable Semantic Fusion (DSF) to align semantic information with pixel-level motion fields effectively. Experiments on the SNU-FILM dataset show that adding semantic guidance significantly boosts perceptual fidelity. SG-RIFE surpasses diffusion-based LDMVFI in FID and LPIPS metrics and reaches quality on par with Consec. BB on challenging benchmarks, while running much faster. This demonstrates that integrating semantic consistency allows flow-based VFI methods to achieve diffusion-level perceptual quality with real-time speed. <div>
arXiv:2512.18241v1 Announce Type: new 
Abstract: Real-time Video Frame Interpolation (VFI) has long been dominated by flow-based methods like RIFE, which offer high throughput but often fail in complicated scenarios involving large motion and occlusion. Conversely, recent diffusion-based approaches (e.g., Consec. BB) achieve state-of-the-art perceptual quality but suffer from prohibitive latency, rendering them impractical for real-time applications. To bridge this gap, we propose Semantic-Guided RIFE (SG-RIFE). Instead of training from scratch, we introduce a parameter-efficient fine-tuning strategy that augments a pre-trained RIFE backbone with semantic priors from a frozen DINOv3 Vision Transformer. We propose a Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional features, and a Deformable Semantic Fusion (DSF) module to align these semantic priors with pixel-level motion fields. Experiments on SNU-FILM demonstrate that semantic injection provides a decisive boost in perceptual fidelity. SG-RIFE outperforms diffusion-based LDMVFI in FID/LPIPS and achieves quality comparable to Consec. BB on complex benchmarks while running significantly faster, proving that semantic consistency enables flow-based methods to achieve diffusion-competitive perceptual quality in near real-time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</title>
<link>https://arxiv.org/abs/2512.18245</link>
<guid>https://arxiv.org/abs/2512.18245</guid>
<content:encoded><![CDATA[
<div> Hyperspectral imaging, Object detection, Spectral discrepancy, Semantic consistency learning, Spectral gated generator<br /><br />Summary: This paper addresses the challenges of object detection in hyperspectral images, which are complicated by intra- and inter-class similarities, spatial differences between inter-bands, and external interferences such as sensor noise and illumination variations. To tackle these issues, the authors propose a novel network named Spectral Discrepancy and Cross-Modal semantic consistency learning (SDCM). The SDCM network is designed to extract consistent information across hyperspectral bands by leveraging the spectral dimension to locate regions of interest effectively. The approach includes a Semantic Consistency Learning (SCL) module that reduces heterogeneity among band information by using inter-band contextual cues, resulting in coherent spectral representations. Additionally, a Spectral Gated Generator (SGG) is integrated into the framework to filter redundant hyperspectral data by weighting the importance of different bands. Furthermore, the Spectral Discrepancy Aware (SDA) module enriches semantic representation by extracting pixel-level spectral features at a high semantic level. The authors validate their method on two hyperspectral datasets, demonstrating that SDCM achieves state-of-the-art performance compared to existing methods, thus showing its effectiveness in improving hyperspectral object detection accuracy. <div>
arXiv:2512.18245v1 Announce Type: new 
Abstract: Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed \textbf{S}pectral \textbf{D}iscrepancy and \textbf{C}ross-\textbf{M}odal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</title>
<link>https://arxiv.org/abs/2512.18247</link>
<guid>https://arxiv.org/abs/2512.18247</guid>
<content:encoded><![CDATA[
<div> Keywords: archaeobotany, ancient plant seeds, image classification, APSNet, size perception<br /><br />Summary:<br /><br />1. This study focuses on understanding dietary preferences of ancient societies through the classification of ancient plant seeds, which are key archaeobotanical artifacts reflecting human-environment interactions.<br /><br />2. Traditional archaeobotanical classification heavily depends on expert knowledge, making it time-consuming and inefficient; thus, there is a need for intelligent automated methods.<br /><br />3. The authors construct the first Ancient Plant Seed Image Classification (APS) dataset, comprising 8,340 images from 17 genus- or species-level seed categories excavated from 18 archaeological sites across China.<br /><br />4. They propose a new framework called APSNet, which incorporates seed size (scale) information along with fine-grained visual features to improve classification accuracy.<br /><br />5. APSNet includes a Size Perception and Embedding (SPE) module in the encoder to explicitly extract size features and an Asynchronous Decoupled Decoding (ADD) architecture to learn discriminative features from channel and spatial perspectives.<br /><br />6. Experimental results demonstrate that APSNet outperforms state-of-the-art image classification methods, achieving an accuracy of 90.5%.<br /><br />7. This work presents an effective large-scale tool for systematic archaeological research, bridging a gap in archaeobotanical data and methods. <div>
arXiv:2512.18247v1 Announce Type: new 
Abstract: Understanding the dietary preferences of ancient societies and their evolution across periods and regions is crucial for revealing human-environment interactions. Seeds, as important archaeological artifacts, represent a fundamental subject of archaeobotanical research. However, traditional studies rely heavily on expert knowledge, which is often time-consuming and inefficient. Intelligent analysis methods have made progress in various fields of archaeology, but there remains a research gap in data and methods in archaeobotany, especially in the classification task of ancient plant seeds. To address this, we construct the first Ancient Plant Seed Image Classification (APS) dataset. It contains 8,340 images from 17 genus- or species-level seed categories excavated from 18 archaeological sites across China. In addition, we design a framework specifically for the ancient plant seed classification task (APSNet), which introduces the scale feature (size) of seeds based on learning fine-grained information to guide the network in discovering key "evidence" for sufficient classification. Specifically, we design a Size Perception and Embedding (SPE) module in the encoder part to explicitly extract size information for the purpose of complementing fine-grained information. We propose an Asynchronous Decoupled Decoding (ADD) architecture based on traditional progressive learning to decode features from both channel and spatial perspectives, enabling efficient learning of discriminative features. In both quantitative and qualitative analyses, our approach surpasses existing state-of-the-art image classification methods, achieving an accuracy of 90.5%. This demonstrates that our work provides an effective tool for large-scale, systematic archaeological research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loom: Diffusion-Transformer for Interleaved Generation</title>
<link>https://arxiv.org/abs/2512.18254</link>
<guid>https://arxiv.org/abs/2512.18254</guid>
<content:encoded><![CDATA[
<div> Keywords: interleaved text-image generation, diffusion-transformer, Loom model, temporal coherence, compositional synthesis

<br /><br />Summary:  
Interleaved text-image generation focuses on creating sequences of coherent visual frames alongside aligned textual descriptions, supporting complex tasks such as style transfer, compositional synthesis, and procedural tutorials. The paper introduces Loom, a unified diffusion-transformer framework that builds upon the Bagel model through full-parameter fine-tuning and an interleaved architecture alternating between textual and visual embeddings, enabling multi-condition reasoning and sequential planning. Loom employs a language planning strategy that breaks down user instructions into stepwise prompts and frame embeddings to guide temporally consistent synthesis. Instead of using all historical frames, Loom conditions on a small set of sampled prior frames plus the global textual context for each frame, which enhances both control and efficiency in long-horizon generation. Evaluations across style transfer, compositional generation, and tutorial-like procedures show Loom's superior performance in compositionality, temporal coherence, and text-image alignment compared to the open-source baseline Anole, with a 2.6-point improvement on a 5-point scale across relevant metrics. Additionally, the authors curated a 50K interleaved tutorial dataset and demonstrated Loom's strong advantages over unified and diffusion editing baselines. <div>
arXiv:2512.18254v1 Announce Type: new 
Abstract: Interleaved text-image generation aims to jointly produce coherent visual frames and aligned textual descriptions within a single sequence, enabling tasks such as style transfer, compositional synthesis, and procedural tutorials. We present Loom, a unified diffusion-transformer framework for interleaved text-image generation. Loom extends the Bagel unified model via full-parameter fine-tuning and an interleaved architecture that alternates textual and visual embeddings for multi-condition reasoning and sequential planning. A language planning strategy first decomposes a user instruction into stepwise prompts and frame embeddings, which guide temporally consistent synthesis. For each frame, Loom conditions on a small set of sampled prior frames together with the global textual context, rather than concatenating all history, yielding controllable and efficient long-horizon generation. Across style transfer, compositional generation, and tutorial-like procedures, Loom delivers superior compositionality, temporal coherence, and text-image alignment. Experiments demonstrate that Loom substantially outperforms the open-source baseline Anole, achieving an average gain of 2.6 points (on a 5-point scale) across temporal and semantic metrics in text-to-interleaved tasks. We also curate a 50K interleaved tutorial dataset and demonstrate strong improvements over unified and diffusion editing baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</title>
<link>https://arxiv.org/abs/2512.18264</link>
<guid>https://arxiv.org/abs/2512.18264</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, privacy protection, attribute inference attacks, VPI-COCO benchmark, visual consistency

<br /><br />Summary:  
The paper addresses the growing privacy risks posed by vision-language models (VLMs), which can be exploited through attribute inference attacks to reveal private information from images shared on social media. To counter this, the authors propose a novel protection method that simultaneously suppresses private attributes while preserving the utility of images, all under a constraint that maintains visual consistency to ensure user experience is not degraded. Recognizing that fair evaluation of such protection methods has been limited by the lack of suitable datasets, they introduce VPI-COCO, a new publicly available benchmark with 522 images accompanied by hierarchically structured privacy-related questions and corresponding non-private versions. This enables comprehensive assessment of both privacy preservation and image utility. Experimental results on several state-of-the-art VLMs using VPI-COCO demonstrate that the proposed method significantly reduces the privacy attack rate (PAR) to below 25%, retains a non-privacy attack rate (NPAR) above 88%, and maintains high visual fidelity. Furthermore, the method shows strong generalization to unseen and paraphrased privacy queries, underscoring its robustness and practical relevance for protecting user privacy in real-world VLM applications. <div>
arXiv:2512.18264v1 Announce Type: new 
Abstract: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</title>
<link>https://arxiv.org/abs/2512.18269</link>
<guid>https://arxiv.org/abs/2512.18269</guid>
<content:encoded><![CDATA[
<div> Dark patterns, visual detection, dataset, YOLOv12x, real-time performance<br /><br />Summary:<br /><br />1. The paper addresses the growing concern of dark patterns—UI designs that manipulate users—amid advancing digital transformation and online platform sophistication.<br /><br />2. To improve proactive and real-time detection beyond current reactive regulatory methods, the authors propose a visual dark pattern detection framework.<br /><br />3. They manually collected and annotated a dataset of 4,066 UI/UX screenshots from 194 websites across six major sectors in South Korea and internationally, focusing on five UI components linked to dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code.<br /><br />4. This dataset has been publicly released to encourage further research and development in the area.<br /><br />5. The study employs the YOLOv12x object detection model with transfer learning to optimize dark pattern recognition, achieving a high accuracy of 92.8% mAP@50 and a real-time inference speed of 40.5 FPS.<br /><br />6. These results demonstrate the framework’s effectiveness for practical, real-time deployment in online environments.<br /><br />7. The paper also provides open access to the dataset via a GitHub repository to support ongoing and future technological advancements. <div>
arXiv:2512.18269v1 Announce Type: new 
Abstract: With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users' ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations</title>
<link>https://arxiv.org/abs/2512.18279</link>
<guid>https://arxiv.org/abs/2512.18279</guid>
<content:encoded><![CDATA[
<div> Keywords: place recognition, multimodal, unified framework, polar BEV, autonomous vehicles<br /><br />Summary:<br /><br />Place recognition is vital for autonomous vehicles and robotics, especially in GPS-denied environments, allowing for global localization. The paper identifies three key challenges in multimodal place recognition (MPR): dynamic adaptation to any input modality in a single framework, robustness when some modalities are missing or degraded, and generalization across different sensor setups. To address these, the authors propose UniMPR, a unified framework that uses a single trained model capable of handling any combination of common perception sensors like cameras, LiDAR, and radar. UniMPR converts all sensor data into a unified polar Bird’s-Eye-View (BEV) feature space, which is then processed by a multi-branch network designed to extract intra-modal and inter-modal features effectively regardless of the modality combinations. To enhance robustness and generalization, the authors aggregate a large-scale training dataset from multiple sources and introduce an adaptive label assignment strategy during extensive pre-training. Extensive experiments on seven diverse datasets demonstrate that UniMPR outperforms existing methods across varying sensor configurations, modality inputs, and environmental conditions. The framework’s code is publicly available, promoting reproducibility and further research in robust multimodal place recognition. <div>
arXiv:2512.18279v1 Announce Type: new 
Abstract: Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pyramidal Adaptive Cross-Gating for Multimodal Detection</title>
<link>https://arxiv.org/abs/2512.18291</link>
<guid>https://arxiv.org/abs/2512.18291</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, aerial imagery, feature fusion, cross-gating, pyramidal network<br /><br />Summary: This paper addresses the challenge of object detection in aerial imagery, which is crucial for UAV reconnaissance and other applications. Existing multimodal fusion methods often use simple aggregation strategies that introduce cross-modal noise and disrupt the hierarchical feature pyramid, reducing performance on small object detection. To overcome these issues, the authors propose PACGNet, a Pyramidal Adaptive Cross-Gating Network that performs deep fusion within the backbone network. The architecture incorporates two key components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module uses a bidirectional and symmetrical gating mechanism to selectively integrate complementary information while suppressing noise and maintaining the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy through progressive hierarchical gating, where features from higher-resolution levels guide fusion at lower-resolution levels, preserving fine-grained details throughout propagation. Experimental evaluations on the DroneVehicle and VEDAI datasets demonstrate that PACGNet achieves state-of-the-art performance, with mAP50 scores of 81.7% and 82.1%, respectively, highlighting the effectiveness of the proposed fusion strategy for fine-grained object detection in aerial imagery. <div>
arXiv:2512.18291v1 Announce Type: new 
Abstract: Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical "horizontal" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 81.7% and 82.1% respectively.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatE: Material Extraction from Single-Image via Geometric Prior</title>
<link>https://arxiv.org/abs/2512.18312</link>
<guid>https://arxiv.org/abs/2512.18312</guid>
<content:encoded><![CDATA[
<div> PBR materials, diffusion model, depth map, material maps, image rectification<br /><br />Summary:<br /><br />This paper introduces MatE, a novel method designed to generate tileable physically-based rendering (PBR) materials from a single image captured under real-world, unconstrained conditions. The process begins with the user providing an image and a mask, after which MatE performs a coarse rectification using an estimated depth map as a geometric prior. Subsequently, a dual-branch diffusion model refines the initial rectification by addressing residual distortions. This model is trained on rotation-aligned and scale-aligned data, enabling it to maintain consistency and improve accuracy. The output consists of a comprehensive set of material maps—including albedo, normal, roughness, and height maps—that are essential for realistic 3D rendering. MatE’s framework is robust against unknown illumination and perspective variations, which allows for extraction of intrinsic material properties from casual photographs without specialized equipment. Extensive experiments on both synthetic datasets and real-world images validate the effectiveness and robustness of the approach. This solution democratizes the creation of high-fidelity PBR materials, reducing dependency on expert knowledge and specialized apparatus, thereby facilitating easier and more accessible material generation for graphic artists and developers. <div>
arXiv:2512.18312v1 Announce Type: new 
Abstract: The creation of high-fidelity, physically-based rendering (PBR) materials remains a bottleneck in many graphics pipelines, typically requiring specialized equipment and expert-driven post-processing. To democratize this process, we present MatE, a novel method for generating tileable PBR materials from a single image taken under unconstrained, real-world conditions. Given an image and a user-provided mask, MatE first performs coarse rectification using an estimated depth map as a geometric prior, and then employs a dual-branch diffusion model. Leveraging a learned consistency from rotation-aligned and scale-aligned training data, this model further rectify residual distortions from the coarse result and translate it into a complete set of material maps, including albedo, normal, roughness and height. Our framework achieves invariance to the unknown illumination and perspective of the input image, allowing for the recovery of intrinsic material properties from casual captures. Through comprehensive experiments on both synthetic and real-world data, we demonstrate the efficacy and robustness of our approach, enabling users to create realistic materials from real-world image.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</title>
<link>https://arxiv.org/abs/2512.18314</link>
<guid>https://arxiv.org/abs/2512.18314</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, material parameters, diffusion models, Gaussian Splatting, neural refinement  

<br /><br />Summary:  
This paper addresses the challenge of accurately transferring 2D material maps onto 3D reconstructed geometry, a crucial task for creating realistic and relightable assets in gaming and film production. The authors propose a novel framework that fuses 2D material data such as albedo, roughness, and metallicity into 3D geometry reconstructed via Gaussian Splatting. Initially, diffusion models generate 2D physically based rendering (PBR) material maps from input images, which are then integrated into the 3D scene either by optimizing an image-based loss or projecting material parameters using Gaussian ray tracing. To improve fine-scale detail and ensure multi-view consistency, the framework incorporates a lightweight neural refinement component called Neural Merger, which refines ray-traced material features for enhanced accuracy. Experimental results demonstrate that this approach outperforms existing methods both quantitatively and visually, producing more photorealistic and relightable renderings. Consequently, this method significantly improves asset creation workflows by reducing manual effort and enhancing the quality of reconstructed scenes, making it a valuable contribution to content production pipelines in computer graphics. <div>
arXiv:2512.18314v1 Announce Type: new 
Abstract: Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A two-stream network with global-local feature fusion for bone age assessment</title>
<link>https://arxiv.org/abs/2512.18331</link>
<guid>https://arxiv.org/abs/2512.18331</guid>
<content:encoded><![CDATA[
<div> Keywords: Bone Age Assessment, Deep Learning, Transformer, RFAConv, Inception-V3<br /><br />Summary:<br /><br />This study addresses the challenge of accurately assessing bone age (BAA), an important clinical technique for evaluating growth and development. The authors propose an automated system called BoNet+, which employs a two-stream deep learning architecture combining both global and local feature extraction to improve assessment accuracy. The global feature extraction channel integrates a Transformer module utilizing a multi-head self-attention mechanism for robust global representation. For local feature extraction, a novel RFAConv module generates adaptive attention maps across multiscale receptive fields to capture detailed skeletal features. These global and local features are concatenated along the channel dimension and further optimized using an Inception-V3 network. The effectiveness of BoNet+ is validated on two datasets: RSNA and RHPE, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively, which are competitive with the current state-of-the-art methods. Overall, the BoNet+ model offers a high-precision, automated, and objective bone age assessment solution, which could significantly reduce clinical workload and enhance diagnostic consistency. <div>
arXiv:2512.18331v1 Announce Type: new 
Abstract: Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</title>
<link>https://arxiv.org/abs/2512.18344</link>
<guid>https://arxiv.org/abs/2512.18344</guid>
<content:encoded><![CDATA[
<div> Vegetation index saturation, semi-supervised learning, remote sensing, LAI estimation, MCVI-SANet  

<br /><br />Summary:  
This study addresses the challenges in accurately estimating Leaf Area Index (LAI) and SPAD (chlorophyll content indicator) for winter wheat, which are hindered by vegetation index (VI) saturation during dense canopy stages and limited ground-truth data. Existing VI-based and texture-driven machine learning models have limited feature expressiveness, while deep learning approaches face domain gaps and require large labeled datasets, restricting their generalization ability. To overcome these issues, the authors propose the Multi-Channel Vegetation Indices Saturation Aware Net (MCVI-SANet), a lightweight semi-supervised vision model designed for enhanced feature extraction and robustness. MCVI-SANet incorporates a novel Vegetation Index Saturation-Aware Block (VI-SABlock) that adaptively enhances channel and spatial features, improving sensitivity to VI saturation effects. Additionally, the model employs a VICReg-based semi-supervised learning framework to boost generalization with fewer labeled samples. The dataset partitioning strategy is informed by vegetation height to ensure representative samples across different growth stages. Experimental results from 10 repeated runs showed MCVI-SANet achieved state-of-the-art accuracy, with an average R² of 0.8123 and RMSE of 0.4796 for LAI estimation, and an average R² of 0.6846 and RMSE of 2.4222 for SPAD estimation. These results surpass baseline methods by approximately 9% and 8% improvements in R² for LAI and SPAD, respectively. The model remains computationally efficient with only 0.10 million parameters, making it suitable for precision agriculture applications. Overall, integrating semi-supervised learning and agronomic priors through MCVI-SANet offers a promising solution to remote sensing challenges in crop monitoring. <div>
arXiv:2512.18344v1 Announce Type: new 
Abstract: Vegetation index (VI) saturation during the dense canopy stage and limited ground-truth annotations of winter wheat constrain accurate estimation of LAI and SPAD. Existing VI-based and texture-driven machine learning methods exhibit limited feature expressiveness. In addition, deep learning baselines suffer from domain gaps and high data demands, which restrict their generalization. Therefore, this study proposes the Multi-Channel Vegetation Indices Saturation Aware Net (MCVI-SANet), a lightweight semi-supervised vision model. The model incorporates a newly designed Vegetation Index Saturation-Aware Block (VI-SABlock) for adaptive channel-spatial feature enhancement. It also integrates a VICReg-based semi-supervised strategy to further improve generalization. Datasets were partitioned using a vegetation height-informed strategy to maintain representativeness across growth stages. Experiments over 10 repeated runs demonstrate that MCVI-SANet achieves state-of-the-art accuracy. The model attains an average R2 of 0.8123 and RMSE of 0.4796 for LAI, and an average R2 of 0.6846 and RMSE of 2.4222 for SPAD. This performance surpasses the best-performing baselines, with improvements of 8.95% in average LAI R2 and 8.17% in average SPAD R2. Moreover, MCVI-SANet maintains high inference speed with only 0.10M parameters. Overall, the integration of semi-supervised learning with agronomic priors provides a promising approach for enhancing remote sensing-based precision agriculture.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing 3D Semantic Scene Completion with a Refinement Module</title>
<link>https://arxiv.org/abs/2512.18363</link>
<guid>https://arxiv.org/abs/2512.18363</guid>
<content:encoded><![CDATA[
<div> Semantic Scene Completion, Refinement Module, 3D U-Net, Prediction Noise-Aware Module, Voxel-level Local Geometry Module  

<br /><br />Summary:  
The paper introduces ESSC-RM, a modular Enhancing framework designed for Semantic Scene Completion (SSC) that can be integrated with existing SSC models without extensive modification. ESSC-RM functions in two phases: it starts with a baseline SSC network producing an initial coarse voxel-wise semantic prediction. This coarse output is then refined through two specialized modules—a 3D U-Net-based Prediction Noise-Aware Module (PNAM) that addresses prediction noise and a Voxel-level Local Geometry Module (VLGM) that enhances local geometric understanding. Both modules operate under multiscale supervision to ensure improved refinement quality. The framework was validated using the SemanticKITTI dataset, where it demonstrated consistent improvements in semantic segmentation accuracy. For example, when integrated into state-of-the-art models CGFormer and MonoScene, ESSC-RM enhanced the mean Intersection over Union (mIoU) scores from 16.87% to 17.27% and 11.08% to 11.51%, respectively. These results highlight ESSC-RM’s capability as a versatile and generalizable refinement tool that improves semantic prediction performance across various SSC architectures without the need to redesign the baseline models from scratch. <div>
arXiv:2512.18363v1 Announce Type: new 
Abstract: We propose ESSC-RM, a plug-and-play Enhancing framework for Semantic Scene Completion with a Refinement Module, which can be seamlessly integrated into existing SSC models. ESSC-RM operates in two phases: a baseline SSC network first produces a coarse voxel prediction, which is subsequently refined by a 3D U-Net-based Prediction Noise-Aware Module (PNAM) and Voxel-level Local Geometry Module (VLGM) under multiscale supervision. Experiments on SemanticKITTI show that ESSC-RM consistently improves semantic prediction performance. When integrated into CGFormer and MonoScene, the mean IoU increases from 16.87% to 17.27% and from 11.08% to 11.51%, respectively. These results demonstrate that ESSC-RM serves as a general refinement framework applicable to a wide range of SSC models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</title>
<link>https://arxiv.org/abs/2512.18365</link>
<guid>https://arxiv.org/abs/2512.18365</guid>
<content:encoded><![CDATA[
<div> Diffusion models, image editing, zero-shot, likelihood surrogate, Gaussian posterior transitions<br /><br />Summary:<br /><br />1. Diffusion models have become powerful tools for image editing tasks such as inpainting and local modifications, aiming to generate realistic content consistent with observed image regions.<br />2. Zero-shot methods leveraging pretrained diffusion models without any retraining achieve effective reconstruction but generally rely on complex surrogate likelihood functions as score proxies.<br />3. Existing approaches require vector-Jacobian products through the denoiser network at every reverse diffusion step, incurring significant memory usage and runtime overhead.<br />4. The authors propose a novel likelihood surrogate that enables simple and efficient sampling of Gaussian posterior transitions, eliminating the need for backpropagation through the denoiser during inference.<br />5. Experiments demonstrate that their method achieves superior observation consistency compared to fine-tuned baselines, produces coherent and high-quality image reconstructions, and substantially reduces inference costs.<br />6. The implementation code is publicly available on GitHub for further research and application. <div>
arXiv:2512.18365v1 Announce Type: new 
Abstract: Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost. Code is available at https://github.com/YazidJanati/ding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</title>
<link>https://arxiv.org/abs/2512.18386</link>
<guid>https://arxiv.org/abs/2512.18386</guid>
<content:encoded><![CDATA[
<div> Gaussian scene fusion, recurrent updates, object-level manipulation, SE(3) refinement, photorealistic synthesis  

<br /><br />Summary:  
The paper introduces RecurGS, a novel recurrent fusion framework designed to incrementally integrate discrete Gaussian scene states into a unified and evolving 3D scene representation that supports interaction. Unlike existing methods that either update only a single scene or process one state at a time without fusing multiple observations, RecurGS can handle discrete changes across multiple states efficiently. It detects object-level changes between consecutive scene states and aligns their geometric motion using semantic correspondence combined with Lie-algebra based SE(3) refinement for precise transformation estimation. The framework utilizes a voxelized, visibility-aware fusion module that selectively incorporates newly observed regions while preserving stable areas, mitigating catastrophic forgetting, and enabling long-horizon scene updates. Through recurrent updates and replay supervision, RecurGS maintains historical structures and photorealistic fidelity in the evolving environment. Additionally, it supports object-level manipulation and novel-state synthesis without requiring additional scans, making it applicable for interactive 3D environments. Extensive experiments on synthetic and real-world datasets show that RecurGS achieves high-quality reconstructions, greatly improves update efficiency, and provides a scalable solution for continuously interactive Gaussian scene representations in robotics and vision applications. <div>
arXiv:2512.18386v1 Announce Type: new 
Abstract: Recent advances in 3D scene representations have enabled high-fidelity novel view synthesis, yet adapting to discrete scene changes and constructing interactive 3D environments remain open challenges in vision and robotics. Existing approaches focus solely on updating a single scene without supporting novel-state synthesis. Others rely on diffusion-based object-background decoupling that works on one state at a time and cannot fuse information across multiple observations. To address these limitations, we introduce RecurGS, a recurrent fusion framework that incrementally integrates discrete Gaussian scene states into a single evolving representation capable of interaction. RecurGS detects object-level changes across consecutive states, aligns their geometric motion using semantic correspondence and Lie-algebra based SE(3) refinement, and performs recurrent updates that preserve historical structures through replay supervision. A voxelized, visibility-aware fusion module selectively incorporates newly observed regions while keeping stable areas fixed, mitigating catastrophic forgetting and enabling efficient long-horizon updates. RecurGS supports object-level manipulation, synthesizes novel scene states without requiring additional scans, and maintains photorealistic fidelity across evolving environments. Extensive experiments across synthetic and real-world datasets demonstrate that our framework delivers high-quality reconstructions with substantially improved update efficiency, providing a scalable step toward continuously interactive Gaussian worlds.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2512.18406</link>
<guid>https://arxiv.org/abs/2512.18406</guid>
<content:encoded><![CDATA[
<div> Keywords: Mosaic segmentation, Segment Anything Model 2, Digital preservation, Tesserae annotation, Image segmentation<br /><br />Summary:<br /><br />This paper focuses on the digital preservation of ancient mosaics by addressing the challenge of segmenting tesserae from their background using image segmentation techniques. It introduces a novel approach leveraging Meta AI’s foundation model, Segment Anything Model 2 (SAM 2), known for its superior performance compared to conventional segmentation models. Due to the scarcity of publicly available datasets for mosaic segmentation, the authors created a new annotated mosaic image dataset, which was used both to fine-tune and evaluate their model. Quantitative results on their testing set demonstrate significant performance improvements: Intersection over Union (IoU) increased from 89.00% to 91.02% and Recall from 92.12% to 95.89%, compared to the baseline SAM 2 model. Moreover, when assessed on a benchmark established by prior methods, their fine-tuned model achieved a 3% higher F-measure and notably reduced the absolute error in tesserae count difference from 0.20 to 0.02. These advancements highlight the potential of the fine-tuned SAM 2 model combined with the newly developed dataset to enable real-time and accurate segmentation of mosaic images, contributing substantially to the field of cultural heritage digitization. <div>
arXiv:2512.18406v1 Announce Type: new 
Abstract: Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</title>
<link>https://arxiv.org/abs/2512.18407</link>
<guid>https://arxiv.org/abs/2512.18407</guid>
<content:encoded><![CDATA[
<div> Keywords: image retrieval, semantic graphs, importance prediction, graph neural network, relational reasoning<br /><br />Summary:<br />1. The paper addresses the challenge of accurately retrieving semantically similar images, highlighting the limitations of traditional methods that neglect relational and contextual nuances.<br />2. It introduces PRISm, a multimodal image-to-image retrieval framework that leverages semantic graphs and importance prediction to improve performance.<br />3. PRISm comprises two novel components: an Importance Prediction Module that prunes irrelevant objects and relational triplets to retain crucial elements, and an Edge-Aware Graph Neural Network that encodes relational structures alongside global visual features to generate semantically rich embeddings.<br />4. By explicitly modeling the importance of objects and their interactions within images, PRISm aligns retrieval results more closely with human perception, addressing gaps in previous approaches.<br />5. Extensive experiments on benchmark and real-world datasets demonstrate PRISm’s superior top-ranked retrieval accuracy, while qualitative analyses confirm its ability to identify key objects and interactions, yielding interpretable and meaningful retrieval outcomes. <div>
arXiv:2512.18407v1 Announce Type: new 
Abstract: Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</title>
<link>https://arxiv.org/abs/2512.18411</link>
<guid>https://arxiv.org/abs/2512.18411</guid>
<content:encoded><![CDATA[
<div> Multi-prompt learning, vision-language models, model-prompt matching bias, ensemble learning, adaptive debiasing  

<br /><br />Summary:  
Multi-prompt learning has proven effective for quickly adapting vision-language models to new tasks with limited data by leveraging multiple prompts within a single model. However, existing methods overlook model-prompt matching bias, where the same prompt conveys different semantics across various vision-language models (e.g., CLIP-ViT-B/16 vs. CLIP-ViT-B/32), causing inconsistent predictions. To address this, the paper proposes an ensemble learning approach that aggregates diverse model predictions to counteract this bias. Further, the authors identify sample-prompt matching bias, arising from prompt-irrelevant semantics in input samples, which can degrade the quality of ensemble weight calculations if not properly managed. To overcome this, they extract prompt-relevant semantics guided by information theory and adaptively compute debiased ensemble weights. This leads to the development of Adaptive-Debiased Ensemble MultiPrompt Learning (AmPLe), which simultaneously mitigates both types of bias. Extensive experiments on three tasks—generalization to novel classes, adapting to new target datasets, and handling unseen domain shifts—demonstrate that AmPLe consistently outperforms existing multi-prompt methods. Theoretical analysis from a causal inference perspective further validates AmPLe’s effectiveness in improving vision-language model adaptation and robustness. <div>
arXiv:2512.18411v1 Announce Type: new 
Abstract: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble MultiPrompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-RGB-D: Real-Time Event-Based Perception with Structured Light</title>
<link>https://arxiv.org/abs/2512.18429</link>
<guid>https://arxiv.org/abs/2512.18429</guid>
<content:encoded><![CDATA[
<div> Event-based cameras, RGB-D sensing, Digital Light Processing, Active Structured Light, color detection<br /><br />Summary:  
This paper introduces a novel method to overcome the limitations of traditional monochrome event-based cameras (ECs), which struggle with static or slow-moving objects and lack color information. The authors integrate a Digital Light Processing (DLP) projector to create an Active Structured Light (ASL) system for RGB-D sensing, enabling separate detection of color and depth at each pixel. Their approach combines the advantages of event-based sensing—such as high dynamic range, temporal resolution, and low power consumption—with projection-based techniques to achieve frameless RGB-D sensing. Dynamic projection adjustments optimize bandwidth, allowing selective color data capture without compromising spatial resolution, resulting in colorful point clouds. Implemented using a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, the system achieves impressive performance: color detection speed up to 1400 frames per second and pixel depth detection at 4 kHz. This significantly pushes forward computer vision capabilities across various domains, including robotics and 3D reconstruction. The code accompanying this research is made publicly available, promoting further development and practical applications. <div>
arXiv:2512.18429v1 Announce Type: new 
Abstract: Event-based cameras (ECs) have emerged as bio-inspired sensors that report pixel brightness changes asynchronously, offering unmatched speed and efficiency in vision sensing. Despite their high dynamic range, temporal resolution, low power consumption, and computational simplicity, traditional monochrome ECs face limitations in detecting static or slowly moving objects and lack color information essential for certain applications. To address these challenges, we present a novel approach that integrates a Digital Light Processing (DLP) projector, forming Active Structured Light (ASL) for RGB-D sensing. By combining the benefits of ECs and projection-based techniques, our method enables the detection of color and the depth of each pixel separately. Dynamic projection adjustments optimize bandwidth, ensuring selective color data acquisition and yielding colorful point clouds without sacrificing spatial resolution. This integration, facilitated by a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, not only enables frameless RGB-D sensing applications but also achieves remarkable performance milestones. With our approach, we achieved a color detection speed equivalent to 1400 fps and 4 kHz of pixel depth detection, significantly advancing the realm of computer vision across diverse fields from robotics to 3D reconstruction methods. Our code is publicly available: https://github.com/MISTLab/event_based_rgbd_ros
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</title>
<link>https://arxiv.org/abs/2512.18437</link>
<guid>https://arxiv.org/abs/2512.18437</guid>
<content:encoded><![CDATA[
<div> Meniscal tear, MRI, multi-view dataset, severity grading, deep learning<br /><br />Summary:<br /><br />1. Precise grading of meniscal horn tears is essential for accurate knee injury diagnosis but has been underexplored in automated MRI analysis.<br />2. Existing methods typically use coarse study-level labels or simple binary classification, failing to provide detailed localization or grading of injury severity.<br />3. This paper introduces MeniMV, a new multi-view benchmark dataset tailored for meniscal horn injury grading.<br />4. MeniMV includes 3,000 annotated knee MRI exams from 750 patients collected at three medical centers, with each exam containing 6,000 co-registered sagittal and coronal images.<br />5. Each exam is labeled with a four-tier severity grading system (grade 0 to 3) for both anterior and posterior meniscal horns, with annotations verified by chief orthopedic physicians.<br />6. The dataset offers more than twice the volume of pathology-labeled data compared to previous datasets and uniquely incorporates the dual-view diagnostic perspective crucial in clinical practice.<br />7. Multiple state-of-the-art convolutional neural network (CNN) and Transformer-based models are benchmarked on MeniMV.<br />8. Experimental results set strong baselines while also exposing challenges related to automatic severity grading.<br />9. Overall, MeniMV provides a valuable resource and foundation to advance future research in automated musculoskeletal imaging and diagnosis. <div>
arXiv:2512.18437v1 Announce Type: new 
Abstract: Precise grading of meniscal horn tears is critical in knee injury diagnosis but remains underexplored in automated MRI analysis. Existing methods often rely on coarse study-level labels or binary classification, lacking localization and severity information. In this paper, we introduce MeniMV, a multi-view benchmark dataset specifically designed for horn-specific meniscus injury grading. MeniMV comprises 3,000 annotated knee MRI exams from 750 patients across three medical centers, providing 6,000 co-registered sagittal and coronal images. Each exam is meticulously annotated with four-tier (grade 0-3) severity labels for both anterior and posterior meniscal horns, verified by chief orthopedic physicians. Notably, MeniMV offers more than double the pathology-labeled data volume of prior datasets while uniquely capturing the dual-view diagnostic context essential in clinical practice. To demonstrate the utility of MeniMV, we benchmark multiple state-of-the-art CNN and Transformer-based models. Our extensive experiments establish strong baselines and highlight challenges in severity grading, providing a valuable foundation for future research in automated musculoskeletal imaging.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-Centric Framework for Video Moment Retrieval</title>
<link>https://arxiv.org/abs/2512.18448</link>
<guid>https://arxiv.org/abs/2512.18448</guid>
<content:encoded><![CDATA[
<div> object-centric, scene graph, moment retrieval, relational tracklet transformer, object-level dynamics<br /><br />Summary:<br /><br />1. The paper addresses limitations in existing video moment retrieval methods, which typically rely on temporal sequences of global frame- or clip-level features, making them insufficient for capturing fine-grained object semantics and appearance. <br /><br />2. To overcome this, the authors propose an object-centric framework that focuses on localizing video moments described by object-oriented queries involving specific entities and their interactions. <br /><br />3. The method first extracts query-relevant objects using a scene graph parser and generates scene graphs from video frames to represent these objects and their relationships. <br /><br />4. Object-level feature sequences are constructed from the scene graphs, encoding rich visual and semantic information that capture object dynamics over time. <br /><br />5. These sequences are processed by a relational tracklet transformer that models spatio-temporal correlations among objects, explicitly capturing object state changes for more accurate moment localization.<br /><br />6. The framework was evaluated on three benchmarks—Charades-STA, QVHighlights, and TACoS—with experimental results showing it outperforms state-of-the-art methods across all datasets. <div>
arXiv:2512.18448v1 Announce Type: new 
Abstract: Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In particular, temporal dynamics at the object level have been largely overlooked, limiting the effectiveness of existing approaches in scenarios requiring detailed object-level reasoning. To address this limitation, we propose a novel object-centric framework for moment retrieval. Our method first extracts query-relevant objects using a scene graph parser and then generates scene graphs from video frames to represent these objects and their relationships. Based on the scene graphs, we construct object-level feature sequences that encode rich visual and semantic information. These sequences are processed by a relational tracklet transformer, which models spatio-temporal correlations among objects over time. By explicitly capturing object-level state changes, our framework enables more accurate localization of moments aligned with object-oriented queries. We evaluated our method on three benchmarks: Charades-STA, QVHighlights, and TACoS. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across all benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plasticine: A Traceable Diffusion Model for Medical Image Translation</title>
<link>https://arxiv.org/abs/2512.18455</link>
<guid>https://arxiv.org/abs/2512.18455</guid>
<content:encoded><![CDATA[
<div> Keywords: domain gaps, image-to-image translation, traceability, denoising diffusion, spatial correspondence

<br /><br />Summary:  
The paper addresses domain gaps caused by differences in imaging devices and population distributions, which challenge medical image analysis using machine learning. Existing image-to-image translation methods focus on generating synthetic images between domains but often ignore maintaining spatial correspondence, leading to a lack of pixel-level alignment between original and translated images. For clinical applications, such pixel-wise traceability is crucial as it enhances interpretability and reliability within clinical workflows. To fill this gap, the authors propose Plasticine, a novel end-to-end image-to-image translation framework explicitly designed to ensure traceability as a fundamental goal. Plasticine integrates both intensity translation and spatial transformations within a denoising diffusion framework. This integration allows for synthetic image generation with interpretable intensity changes and consistent spatial deformations, preserving precise pixel-level correspondence throughout the translation process. By doing so, Plasticine supports clinical interpretability, offering a trustworthy approach for generating synthetic medical images that maintain anatomical fidelity and spatial alignment. The method is, to the authors’ knowledge, the first to explicitly incorporate traceability in image-to-image translation for medical imaging, marking a significant advance for domain adaptation in this field. <div>
arXiv:2512.18455v1 Announce Type: new 
Abstract: Domain gaps arising from variations in imaging devices and population distributions pose significant challenges for machine learning in medical image analysis. Existing image-to-image translation methods primarily aim to learn mappings between domains, often generating diverse synthetic data with variations in anatomical scale and shape, but they usually overlook spatial correspondence during the translation process. For clinical applications, traceability, defined as the ability to provide pixel-level correspondences between original and translated images, is equally important. This property enhances clinical interpretability but has been largely overlooked in previous approaches. To address this gap, we propose Plasticine, which is, to the best of our knowledge, the first end-to-end image-to-image translation framework explicitly designed with traceability as a core objective. Our method combines intensity translation and spatial transformation within a denoising diffusion framework. This design enables the generation of synthetic images with interpretable intensity transitions and spatially coherent deformations, supporting pixel-wise traceability throughout the translation process.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18496</link>
<guid>https://arxiv.org/abs/2512.18496</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, visual compression, adaptive compression, multimodal tasks, computational efficiency<br /><br />Summary:<br /><br />1. Large-scale vision-language models (VLMs) achieve strong performance in multimodal understanding but face challenges due to the high computational and memory costs of processing high-dimensional visual features.<br /><br />2. VoCo-LLaMA addresses this by compressing visual patch tokens into fewer VoCo tokens, lowering overhead while maintaining effective cross-modal alignment.<br /><br />3. Traditional methods use a fixed compression rate, which can be suboptimal because images vary in visual complexity and require different levels of compression.<br /><br />4. This paper introduces Adaptive-VoCo, a framework that enhances VoCo-LLaMA by incorporating a lightweight predictor that dynamically selects the compression rate based on the image’s complexity.<br /><br />5. Visual complexity is assessed through statistical cues derived from the vision encoder, such as patch token entropy and attention map variance.<br /><br />6. A joint loss function combining rate regularization and complexity alignment helps the model balance between inference efficiency and representational quality, especially for difficult inputs.<br /><br />7. Experimental results demonstrate that Adaptive-VoCo consistently outperforms fixed-rate baselines across multiple multimodal tasks, proving the benefits of adaptive visual compression for more efficient and robust VLMs. <div>
arXiv:2512.18496v1 Announce Type: new 
Abstract: In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</title>
<link>https://arxiv.org/abs/2512.18500</link>
<guid>https://arxiv.org/abs/2512.18500</guid>
<content:encoded><![CDATA[
<div> Keywords: Plant disease detection, ResNet50, fine-tuning, deep learning, agricultural diagnostics<br /><br />Summary:<br /><br />1. The paper addresses the significant impact of plant diseases, which cause 70-80% of global crop losses and pose a major threat to food security.<br /><br />2. Traditional methods for detecting these diseases are inefficient, relying mainly on expert visual inspection that is time-consuming and unsuitable for large-scale farming.<br /><br />3. The authors introduce PlantDiseaseNet-RT50, a novel deep learning model based on the ResNet50 architecture, fine-tuned specifically for automated plant disease detection.<br /><br />4. Key innovations include selectively unfreezing terminal layers of the pretrained model, integrating a custom classification head enhanced with batch normalization and dropout regularization, and applying a dynamic cosine decay learning rate schedule.<br /><br />5. Tested on a comprehensive dataset featuring multiple crop species and distinct disease categories, PlantDiseaseNet-RT50 achieved around 98% accuracy, precision, and recall, showcasing the effectiveness of the model.<br /><br />6. The study highlights how careful architectural adjustments and optimization techniques can transform standard pretrained networks into specialized tools for agricultural diagnostics.<br /><br />7. PlantDiseaseNet-RT50 offers a computationally efficient and accurate solution that can be readily deployed in practical farming environments to enable rapid disease identification, support timely treatment, and ultimately reduce crop losses. <div>
arXiv:2512.18500v1 Announce Type: new 
Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NASTaR: NovaSAR Automated Ship Target Recognition Dataset</title>
<link>https://arxiv.org/abs/2512.18503</link>
<guid>https://arxiv.org/abs/2512.18503</guid>
<content:encoded><![CDATA[
arXiv:2512.18503v1 Announce Type: new 
Abstract: Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18504</link>
<guid>https://arxiv.org/abs/2512.18504</guid>
<content:encoded><![CDATA[
arXiv:2512.18504v1 Announce Type: new 
Abstract: Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.
  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.
  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</title>
<link>https://arxiv.org/abs/2512.18527</link>
<guid>https://arxiv.org/abs/2512.18527</guid>
<content:encoded><![CDATA[
arXiv:2512.18527v1 Announce Type: new 
Abstract: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring</title>
<link>https://arxiv.org/abs/2512.18528</link>
<guid>https://arxiv.org/abs/2512.18528</guid>
<content:encoded><![CDATA[
arXiv:2512.18528v1 Announce Type: new 
Abstract: Chronic wounds, including diabetic foot ulcers which affect up to one-third of people with diabetes, impose a substantial clinical and economic burden, with U.S. healthcare costs exceeding 25 billion dollars annually. Current wound assessment remains predominantly subjective, leading to inconsistent classification and delayed interventions. We present WoundNet-Ensemble, an Internet of Medical Things system leveraging a novel ensemble of three complementary deep learning architectures: ResNet-50, the self-supervised Vision Transformer DINOv2, and Swin Transformer, for automated classification of six clinically distinct wound types. Our system achieves 99.90 percent ensemble accuracy on a comprehensive dataset of 5,175 wound images spanning diabetic foot ulcers, pressure ulcers, venous ulcers, thermal burns, pilonidal sinus wounds, and fungating malignant tumors. The weighted fusion strategy demonstrates a 3.7 percent improvement over previous state-of-the-art methods. Furthermore, we implement a longitudinal wound healing tracker that computes healing rates, severity scores, and generates clinical alerts. This work demonstrates a robust, accurate, and clinically deployable tool for modernizing wound care through artificial intelligence, addressing critical needs in telemedicine and remote patient monitoring. The implementation and trained models will be made publicly available to support reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Bayesian Framework for Multisource Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.18553</link>
<guid>https://arxiv.org/abs/2512.18553</guid>
<content:encoded><![CDATA[
arXiv:2512.18553v1 Announce Type: new 
Abstract: Multisource domain adaptation (MDA) aims to use multiple source datasets with available labels to infer labels on a target dataset without available labels for target supervision. Prior works on MDA in the literature is ad-hoc as the pretraining of source models is either based on weight sharing or uses independently trained models. This work proposes a Bayesian framework for pretraining in MDA by considering that the distributions of different source domains are typically similar. The Hierarchical Bayesian Framework uses similarity between the different source data distributions to optimize the pretraining for MDA. Experiments using the proposed Bayesian framework for MDA show that our framework improves accuracy on recognition tasks for a large benchmark dataset. Performance comparison with state-of-the-art MDA methods on the challenging problem of human action recognition in multi-domain benchmark Daily-DA RGB video shows the proposed Bayesian Framework offers a 17.29% improvement in accuracy when compared to the state-of-the-art methods in the literature.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Medical Large Vision-Language Models via Alignment Distillation</title>
<link>https://arxiv.org/abs/2512.18554</link>
<guid>https://arxiv.org/abs/2512.18554</guid>
<content:encoded><![CDATA[
arXiv:2512.18554v1 Announce Type: new 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenView: Empowering MLLMs with Out-of-view VQA</title>
<link>https://arxiv.org/abs/2512.18563</link>
<guid>https://arxiv.org/abs/2512.18563</guid>
<content:encoded><![CDATA[
arXiv:2512.18563v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</title>
<link>https://arxiv.org/abs/2512.18573</link>
<guid>https://arxiv.org/abs/2512.18573</guid>
<content:encoded><![CDATA[
arXiv:2512.18573v1 Announce Type: new 
Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach</title>
<link>https://arxiv.org/abs/2512.18597</link>
<guid>https://arxiv.org/abs/2512.18597</guid>
<content:encoded><![CDATA[
arXiv:2512.18597v1 Announce Type: new 
Abstract: A vision-based trajectory analysis solution is proposed to address the "zero-speed braking" issue caused by inaccurate Controller Area Network (CAN) signals in commercial vehicle Automatic Emergency Braking (AEB) systems during low-speed operation. The algorithm utilizes the NVIDIA Jetson AGX Xavier platform to process sequential video frames from a blind spot camera, employing self-adaptive Contrast Limited Adaptive Histogram Equalization (CLAHE)-enhanced Scale-Invariant Feature Transform (SIFT) feature extraction and K-Nearest Neighbors (KNN)-Random Sample Consensus (RANSAC) matching. This allows for precise classification of the vehicle's motion state (static, vibration, moving). Key innovations include 1) multiframe trajectory displacement statistics (5-frame sliding window), 2) a dual-threshold state decision matrix, and 3) OBD-II driven dynamic Region of Interest (ROI) configuration. The system effectively suppresses environmental interference and false detection of dynamic objects, directly addressing the challenge of low-speed false activation in commercial vehicle safety systems. Evaluation in a real-world dataset (32,454 video segments from 1,852 vehicles) demonstrates an F1-score of 99.96% for static detection, 97.78% for moving state recognition, and a processing delay of 14.2 milliseconds (resolution 704x576). The deployment on-site shows an 89% reduction in false braking events, a 100% success rate in emergency braking, and a fault rate below 5%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</title>
<link>https://arxiv.org/abs/2512.18599</link>
<guid>https://arxiv.org/abs/2512.18599</guid>
<content:encoded><![CDATA[
arXiv:2512.18599v1 Announce Type: new 
Abstract: Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</title>
<link>https://arxiv.org/abs/2512.18613</link>
<guid>https://arxiv.org/abs/2512.18613</guid>
<content:encoded><![CDATA[
arXiv:2512.18613v1 Announce Type: new 
Abstract: Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</title>
<link>https://arxiv.org/abs/2512.18614</link>
<guid>https://arxiv.org/abs/2512.18614</guid>
<content:encoded><![CDATA[
arXiv:2512.18614v1 Announce Type: new 
Abstract: Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.
  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.18635</link>
<guid>https://arxiv.org/abs/2512.18635</guid>
<content:encoded><![CDATA[
arXiv:2512.18635v1 Announce Type: new 
Abstract: Generating or editing images directly from Neural signals has immense potential at the intersection of neuroscience, vision, and Brain-computer interaction. In this paper, We present Uni-Neur2Img, a unified framework for neural signal-driven image generation and editing. The framework introduces a parameter-efficient LoRA-based neural signal injection module that independently processes each conditioning signal as a pluggable component, facilitating flexible multi-modal conditioning without altering base model parameters. Additionally, we employ a causal attention mechanism accommodate the long-sequence modeling demands of conditional generation tasks. Existing neural-driven generation research predominantly focuses on textual modalities as conditions or intermediate representations, resulting in limited exploration of visual modalities as direct conditioning signals. To bridge this research gap, we introduce the EEG-Style dataset. We conduct comprehensive evaluations across public benchmarks and self-collected neural signal datasets: (1) EEG-driven image generation on the public CVPR40 dataset; (2) neural signal-guided image editing on the public Loongx dataset for semantic-aware local modifications; and (3) EEG-driven style transfer on our self-collected EEG-Style dataset. Extensive experimental results demonstrate significant improvements in generation fidelity, editing consistency, and style transfer quality while maintaining low computational overhead and strong scalability to additional modalities. Thus, Uni-Neur2Img offers a unified, efficient, and extensible solution for bridging neural signals and visual content generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric-Photometric Event-based 3D Gaussian Ray Tracing</title>
<link>https://arxiv.org/abs/2512.18640</link>
<guid>https://arxiv.org/abs/2512.18640</guid>
<content:encoded><![CDATA[
arXiv:2512.18640v1 Announce Type: new 
Abstract: Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness in Zero-Shot Learning:An Empirical Study on Class and Concept-Level Vulnerabilities</title>
<link>https://arxiv.org/abs/2512.18651</link>
<guid>https://arxiv.org/abs/2512.18651</guid>
<content:encoded><![CDATA[
arXiv:2512.18651v1 Announce Type: new 
Abstract: Zero-shot Learning (ZSL) aims to enable image classifiers to recognize images from unseen classes that were not included during training. Unlike traditional supervised classification, ZSL typically relies on learning a mapping from visual features to predefined, human-understandable class concepts. While ZSL models promise to improve generalization and interpretability, their robustness under systematic input perturbations remain unclear. In this study, we present an empirical analysis about the robustness of existing ZSL methods at both classlevel and concept-level. Specifically, we successfully disrupted their class prediction by the well-known non-target class attack (clsA). However, in the Generalized Zero-shot Learning (GZSL) setting, we observe that the success of clsA is only at the original best-calibrated point. After the attack, the optimal bestcalibration point shifts, and ZSL models maintain relatively strong performance at other calibration points, indicating that clsA results in a spurious attack success in the GZSL. To address this, we propose the Class-Bias Enhanced Attack (CBEA), which completely eliminates GZSL accuracy across all calibrated points by enhancing the gap between seen and unseen class probabilities.Next, at concept-level attack, we introduce two novel attack modes: Class-Preserving Concept Attack (CPconA) and NonClass-Preserving Concept Attack (NCPconA). Our extensive experiments evaluate three typical ZSL models across various architectures from the past three years and reveal that ZSL models are vulnerable not only to the traditional class attack but also to concept-based attacks. These attacks allow malicious actors to easily manipulate class predictions by erasing or introducing concepts. Our findings highlight a significant performance gap between existing approaches, emphasizing the need for improved adversarial robustness in current ZSL models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement</title>
<link>https://arxiv.org/abs/2512.18655</link>
<guid>https://arxiv.org/abs/2512.18655</guid>
<content:encoded><![CDATA[
arXiv:2512.18655v1 Announce Type: new 
Abstract: Low-light 3D reconstruction from sparse views remains challenging due to exposure imbalance and degraded color fidelity. While existing methods struggle with view inconsistency and require per-scene training, we propose SplatBright, which is, to our knowledge, the first generalizable 3D Gaussian framework for joint low-light enhancement and reconstruction from sparse sRGB inputs. Our key idea is to integrate physically guided illumination modeling with geometry-appearance decoupling for consistent low-light reconstruction. Specifically, we adopt a dual-branch predictor that provides stable geometric initialization of 3D Gaussian parameters. On the appearance side, illumination consistency leverages frequency priors to enable controllable and cross-view coherent lighting, while an appearance refinement module further separates illumination, material, and view-dependent cues to recover fine texture. To tackle the lack of large-scale geometrically consistent paired data, we synthesize dark views via a physics-based camera model for training. Extensive experiments on public and self-collected datasets demonstrate that SplatBright achieves superior novel view synthesis, cross-view consistency, and better generalization to unseen low-light scenes compared with both 2D and 3D methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2512.18660</link>
<guid>https://arxiv.org/abs/2512.18660</guid>
<content:encoded><![CDATA[
arXiv:2512.18660v1 Announce Type: new 
Abstract: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</title>
<link>https://arxiv.org/abs/2512.18671</link>
<guid>https://arxiv.org/abs/2512.18671</guid>
<content:encoded><![CDATA[
arXiv:2512.18671v1 Announce Type: new 
Abstract: Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference</title>
<link>https://arxiv.org/abs/2512.18675</link>
<guid>https://arxiv.org/abs/2512.18675</guid>
<content:encoded><![CDATA[
arXiv:2512.18675v1 Announce Type: new 
Abstract: Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight timestep prediction module (TPM), trained with Group Relative Policy Optimization (GRPO), selects a more feasible conditioning timestep based on the current state, effectively choosing a desired noise level to control image detail and textural richness. At deployment, a scaling hyper-parameter can be used to interpolate between the original and de-synchronized timesteps, enabling conservative or aggressive adjustments. To keep the study computationally affordable, we cap the inference at 15 steps for SD3.5 and 10 steps for Flux. Evaluated on Stable Diffusion 3.5 Medium and Flux.1-dev across MS-COCO 2014 and T2I-CompBench datasets, our method optimizes a composite reward that averages Image Reward, HPSv2, CLIP Score and Pick Score, and shows consistent improvement.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>brat: Aligned Multi-View Embeddings for Brain MRI Analysis</title>
<link>https://arxiv.org/abs/2512.18679</link>
<guid>https://arxiv.org/abs/2512.18679</guid>
<content:encoded><![CDATA[
arXiv:2512.18679v1 Announce Type: new 
Abstract: We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title>
<link>https://arxiv.org/abs/2512.18684</link>
<guid>https://arxiv.org/abs/2512.18684</guid>
<content:encoded><![CDATA[
arXiv:2512.18684v1 Announce Type: new 
Abstract: This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</title>
<link>https://arxiv.org/abs/2512.18692</link>
<guid>https://arxiv.org/abs/2512.18692</guid>
<content:encoded><![CDATA[
arXiv:2512.18692v1 Announce Type: new 
Abstract: Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization. However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians. To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time. EcoSplat adopts a two-stage optimization process. The first stage is Pixel-aligned Gaussian Training (PGT) where our model learns initial primitive prediction. The second stage is Importance-aware Gaussian Finetuning (IGF) stage where our model learns rank primitives and adaptively adjust their parameters based on the target primitive count. Extensive experiments across multiple dense-view settings show that EcoSplat is robust and outperforms state-of-the-art methods under strict primitive-count constraints, making it well-suited for flexible downstream rendering tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts</title>
<link>https://arxiv.org/abs/2512.18718</link>
<guid>https://arxiv.org/abs/2512.18718</guid>
<content:encoded><![CDATA[
arXiv:2512.18718v1 Announce Type: new 
Abstract: Image correction and rectangling are valuable tasks in practical photography systems such as smartphones. Recent remarkable advancements in deep learning have undeniably brought about substantial performance improvements in these fields. Nevertheless, existing methods mainly rely on task-specific architectures. This significantly restricts their generalization ability and effective application across a wide range of different tasks. In this paper, we introduce the Unified Rectification Framework (UniRect), a comprehensive approach that addresses these practical tasks from a consistent distortion rectification perspective. Our approach incorporates various task-specific inverse problems into a general distortion model by simulating different types of lenses. To handle diverse distortions, UniRect adopts one task-agnostic rectification framework with a dual-component structure: a {Deformation Module}, which utilizes a novel Residual Progressive Thin-Plate Spline (RP-TPS) model to address complex geometric deformations, and a subsequent Restoration Module, which employs Residual Mamba Blocks (RMBs) to counteract the degradation caused by the deformation process and enhance the fidelity of the output image. Moreover, a Sparse Mixture-of-Experts (SMoEs) structure is designed to circumvent heavy task competition in multi-task learning due to varying distortions. Extensive experiments demonstrate that our models have achieved state-of-the-art performance compared with other up-to-date methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2512.18734</link>
<guid>https://arxiv.org/abs/2512.18734</guid>
<content:encoded><![CDATA[
arXiv:2512.18734v1 Announce Type: new 
Abstract: Predicting breast cancer recurrence risk is a critical clinical challenge. This study investigates the potential of computational pathology to stratify patients using deep learning on routine Hematoxylin and Eosin (H&amp;E) stained whole-slide images (WSIs). We developed and compared three Multiple Instance Learning (MIL) frameworks -- CLAM-SB, ABMIL, and ConvNeXt-MIL-XGBoost -- on an in-house dataset of 210 patient cases. The models were trained to predict 5-year recurrence risk, categorized into three tiers (low, medium, high), with ground truth labels established by the 21-gene Recurrence Score. Features were extracted using the UNI and CONCH pre-trained models. In a 5-fold cross-validation, the modified CLAM-SB model demonstrated the strongest performance, achieving a mean Area Under the Curve (AUC) of 0.836 and a classification accuracy of 76.2%. Our findings demonstrate the feasibility of using deep learning on standard histology slides for automated, genomics-correlated risk stratification, highlighting a promising pathway toward rapid and cost-effective clinical decision support.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2512.18735</link>
<guid>https://arxiv.org/abs/2512.18735</guid>
<content:encoded><![CDATA[
arXiv:2512.18735v1 Announce Type: new 
Abstract: Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection</title>
<link>https://arxiv.org/abs/2512.18738</link>
<guid>https://arxiv.org/abs/2512.18738</guid>
<content:encoded><![CDATA[
arXiv:2512.18738v1 Announce Type: new 
Abstract: Landmines remain a persistent humanitarian threat, with an estimated 110 million mines deployed across 60 countries, claiming approximately 26,000 casualties annually. Current detection methods are hazardous, inefficient, and prohibitively expensive. We present the Adaptive Multispectral Landmine Identification Dataset (AMLID), the first open-source dataset combining Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) imagery for Unmanned Aerial Systems (UAS)-based landmine detection. AMLID comprises of 12,078 labeled images featuring 21 globally deployed landmine types across anti-personnel and anti-tank categories in both metal and plastic compositions. The dataset spans 11 RGB-LWIR fusion levels, four sensor altitudes, two seasonal periods, and three daily illumination conditions. By providing comprehensive multispectral coverage across diverse environmental variables, AMLID enables researchers to develop and benchmark adaptive detection algorithms without requiring access to live ordnance or expensive data collection infrastructure, thereby democratizing humanitarian demining research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation</title>
<link>https://arxiv.org/abs/2512.18741</link>
<guid>https://arxiv.org/abs/2512.18741</guid>
<content:encoded><![CDATA[
arXiv:2512.18741v1 Announce Type: new 
Abstract: Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</title>
<link>https://arxiv.org/abs/2512.18745</link>
<guid>https://arxiv.org/abs/2512.18745</guid>
<content:encoded><![CDATA[
arXiv:2512.18745v1 Announce Type: new 
Abstract: The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPCV: Information-Preserving Compression for MLLM Visual Encoders</title>
<link>https://arxiv.org/abs/2512.18747</link>
<guid>https://arxiv.org/abs/2512.18747</guid>
<content:encoded><![CDATA[
arXiv:2512.18747v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</title>
<link>https://arxiv.org/abs/2512.18750</link>
<guid>https://arxiv.org/abs/2512.18750</guid>
<content:encoded><![CDATA[
arXiv:2512.18750v1 Announce Type: new 
Abstract: Action recognition is a critical task in video understanding, requiring the comprehensive capture of spatio-temporal cues across various scales. However, existing methods often overlook the multi-granularity nature of actions. To address this limitation, we introduce the Context-Aware Network (CAN). CAN consists of two core modules: the Multi-scale Temporal Cue Module (MTCM) and the Group Spatial Cue Module (GSCM). MTCM effectively extracts temporal cues at multiple scales, capturing both fast-changing motion details and overall action flow. GSCM, on the other hand, extracts spatial cues at different scales by grouping feature maps and applying specialized extraction methods to each group. Experiments conducted on five benchmark datasets (Something-Something V1 and V2, Diving48, Kinetics-400, and UCF101) demonstrate the effectiveness of CAN. Our approach achieves competitive performance, outperforming most mainstream methods, with accuracies of 50.4% on Something-Something V1, 63.9% on Something-Something V2, 88.4% on Diving48, 74.9% on Kinetics-400, and 86.9% on UCF101. These results highlight the importance of capturing multi-scale spatio-temporal cues for robust action recognition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</title>
<link>https://arxiv.org/abs/2512.18766</link>
<guid>https://arxiv.org/abs/2512.18766</guid>
<content:encoded><![CDATA[
arXiv:2512.18766v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated significant potential for post-training language models and autoregressive visual generative models, but adapting RL to masked generative models remains challenging. The core factor is that policy optimization requires accounting for the probability likelihood of each step due to its multi-step and iterative refinement process. This reliance on entire sampling trajectories introduces high computational cost, whereas natively optimizing random steps often yields suboptimal results. In this paper, we present MaskFocus, a novel RL framework that achieves effective policy optimization for masked generative models by focusing on critical steps. Specifically, we determine the step-level information gain by measuring the similarity between the intermediate images at each sampling step and the final generated image. Crucially, we leverage this to identify the most critical and valuable steps and execute focused policy optimization on them. Furthermore, we design a dynamic routing sampling mechanism based on entropy to encourage the model to explore more valuable masking strategies for samples with low entropy. Extensive experiments on multiple Text-to-Image benchmarks validate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Audio Control of Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.18772</link>
<guid>https://arxiv.org/abs/2512.18772</guid>
<content:encoded><![CDATA[
arXiv:2512.18772v1 Announce Type: new 
Abstract: Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</title>
<link>https://arxiv.org/abs/2512.18784</link>
<guid>https://arxiv.org/abs/2512.18784</guid>
<content:encoded><![CDATA[
arXiv:2512.18784v1 Announce Type: new 
Abstract: We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation</title>
<link>https://arxiv.org/abs/2512.18804</link>
<guid>https://arxiv.org/abs/2512.18804</guid>
<content:encoded><![CDATA[
arXiv:2512.18804v1 Announce Type: new 
Abstract: Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</title>
<link>https://arxiv.org/abs/2512.18809</link>
<guid>https://arxiv.org/abs/2512.18809</guid>
<content:encoded><![CDATA[
arXiv:2512.18809v1 Announce Type: new 
Abstract: The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</title>
<link>https://arxiv.org/abs/2512.18813</link>
<guid>https://arxiv.org/abs/2512.18813</guid>
<content:encoded><![CDATA[
arXiv:2512.18813v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.18814</link>
<guid>https://arxiv.org/abs/2512.18814</guid>
<content:encoded><![CDATA[
arXiv:2512.18814v1 Announce Type: new 
Abstract: Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2512.18843</link>
<guid>https://arxiv.org/abs/2512.18843</guid>
<content:encoded><![CDATA[
arXiv:2512.18843v1 Announce Type: new 
Abstract: Advances in neuroscience and artificial intelligence have enabled preliminary decoding of brain activity. However, despite the progress, the interpretability of neural representations remains limited. A significant challenge arises from the intrinsic properties of electroencephalography (EEG) signals, including high noise levels, spatial diffusion, and pronounced temporal variability. To interpret the neural mechanism underlying thoughts, we propose a transformers-based framework to extract spatial-temporal representations associated with observed visual stimuli from EEG recordings. These features are subsequently incorporated into the attention mechanisms of Latent Diffusion Models (LDMs) to facilitate the reconstruction of visual stimuli from brain activity. The quantitative evaluations on publicly available benchmark datasets demonstrate that the proposed method excels at modeling the semantic structures from EEG signals; achieving up to 6.5% increase in latent space clustering accuracy and 11.8% increase in zero shot generalization across unseen classes while having comparable Inception Score and Fr\'echet Inception Distance with existing baselines. Our work marks a significant step towards generalizable semantic interpretation of the EEG signals.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</title>
<link>https://arxiv.org/abs/2512.18853</link>
<guid>https://arxiv.org/abs/2512.18853</guid>
<content:encoded><![CDATA[
arXiv:2512.18853v1 Announce Type: new 
Abstract: The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification</title>
<link>https://arxiv.org/abs/2512.18864</link>
<guid>https://arxiv.org/abs/2512.18864</guid>
<content:encoded><![CDATA[
arXiv:2512.18864v1 Announce Type: new 
Abstract: Concept-driven counterfactuals explain decisions of classifiers by altering the model predictions through semantic changes. In this paper, we present a novel approach that leverages cross-modal decompositionality and image-specific concepts to create counterfactual scenarios expressed in natural language. We apply the proposed interpretability framework, termed Decompose and Explain (DeX), to the challenging domain of image privacy decisions, which are contextual and subjective. This application enables the quantification of the differential contributions of key scene elements to the model prediction. We identify relevant decision factors via a multi-criterion selection mechanism that considers both image similarity for minimal perturbations and decision confidence to prioritize impactful changes. This approach evaluates and compares diverse explanations, and assesses the interdependency and mutual influence among explanatory properties. By leveraging image-specific concepts, DeX generates image-grounded, sparse explanations, yielding significant improvements over the state of the art. Importantly, DeX operates as a training-free framework, offering high flexibility. Results show that DeX not only uncovers the principal contributing factors influencing subjective decisions, but also identifies underlying dataset biases allowing for targeted mitigation strategies to improve fairness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of deep learning approaches for medieval historical documents transcription</title>
<link>https://arxiv.org/abs/2512.18865</link>
<guid>https://arxiv.org/abs/2512.18865</guid>
<content:encoded><![CDATA[
arXiv:2512.18865v1 Announce Type: new 
Abstract: Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</title>
<link>https://arxiv.org/abs/2512.18878</link>
<guid>https://arxiv.org/abs/2512.18878</guid>
<content:encoded><![CDATA[
arXiv:2512.18878v1 Announce Type: new 
Abstract: Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\% improvement in crash localization, and a 40\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)</title>
<link>https://arxiv.org/abs/2512.18888</link>
<guid>https://arxiv.org/abs/2512.18888</guid>
<content:encoded><![CDATA[
arXiv:2512.18888v1 Announce Type: new 
Abstract: Deep neural networks often exploit shortcuts. These are spurious cues which are associated with output labels in the training data but are unrelated to task semantics. When the shortcut features are associated with sensitive attributes, shortcut learning can lead to biased model performance. Existing methods for localising and understanding shortcut learning are mostly based upon qualitative, image-level inspection and assume cues are human-visible, limiting their use in domains such as medical imaging. We introduce OSCAR (Ordinal Scoring Correlations for Attribution Representations), a model-agnostic framework for quantifying shortcut learning and localising shortcut features. OSCAR converts image-level task attribution maps into dataset-level rank profiles of image regions and compares them across three models: a balanced baseline model (BA), a test model (TS), and a sensitive attribute predictor (SA). By computing pairwise, partial, and deviation-based correlations on these rank profiles, we produce a set of quantitative metrics that characterise the degree of shortcut reliance for TS, together with a ranking of image-level regions that contribute most to it. Experiments on CelebA, CheXpert, and ADNI show that our correlations are (i) stable across seeds and partitions, (ii) sensitive to the level of association between shortcut features and output labels in the training data, and (iii) able to distinguish localised from diffuse shortcut features. As an illustration of the utility of our method, we show how worst-group performance disparities can be reduced using a simple test-time attenuation approach based on the identified shortcut regions. OSCAR provides a lightweight, pixel-space audit that yields statistical decision rules and spatial maps, enabling users to test, localise, and mitigate shortcut reliance. The code is available at https://github.com/acharaakshit/oscar
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</title>
<link>https://arxiv.org/abs/2512.18897</link>
<guid>https://arxiv.org/abs/2512.18897</guid>
<content:encoded><![CDATA[
arXiv:2512.18897v1 Announce Type: new 
Abstract: Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18910</link>
<guid>https://arxiv.org/abs/2512.18910</guid>
<content:encoded><![CDATA[
arXiv:2512.18910v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</title>
<link>https://arxiv.org/abs/2512.18930</link>
<guid>https://arxiv.org/abs/2512.18930</guid>
<content:encoded><![CDATA[
arXiv:2512.18930v1 Announce Type: new 
Abstract: Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point What You Mean: Visually Grounded Instruction Policy</title>
<link>https://arxiv.org/abs/2512.18933</link>
<guid>https://arxiv.org/abs/2512.18933</guid>
<content:encoded><![CDATA[
arXiv:2512.18933v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetrization of 3D Generative Models</title>
<link>https://arxiv.org/abs/2512.18953</link>
<guid>https://arxiv.org/abs/2512.18953</guid>
<content:encoded><![CDATA[
arXiv:2512.18953v1 Announce Type: new 
Abstract: We propose a novel data-centric approach to promote symmetry in 3D generative models by modifying the training data rather than the model architecture. Our method begins with an analysis of reflectional symmetry in both real-world 3D shapes and samples generated by state-of-the-art models. We hypothesize that training a generative model exclusively on half-objects, obtained by reflecting one half of the shapes along the x=0 plane, enables the model to learn a rich distribution of partial geometries which, when reflected during generation, yield complete shapes that are both visually plausible and geometrically symmetric. To test this, we construct a new dataset of half-objects from three ShapeNet classes (Airplane, Car, and Chair) and train two generative models. Experiments demonstrate that the generated shapes are symmetrical and consistent, compared with the generated objects from the original model and the original dataset objects.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2512.18954</link>
<guid>https://arxiv.org/abs/2512.18954</guid>
<content:encoded><![CDATA[
arXiv:2512.18954v1 Announce Type: new 
Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</title>
<link>https://arxiv.org/abs/2512.18964</link>
<guid>https://arxiv.org/abs/2512.18964</guid>
<content:encoded><![CDATA[
arXiv:2512.18964v1 Announce Type: new 
Abstract: Recent tuning-free identity customization methods achieve high facial fidelity but often overlook visual context, such as lighting, skin texture, and environmental tone. This limitation leads to ``Semantic-Visual Dissonance,'' where accurate facial geometry clashes with the input's unique atmosphere, causing an unnatural ``sticker-like'' effect. We propose **DVI (Disentangled Visual-Identity)**, a zero-shot framework that orthogonally disentangles identity into fine-grained semantic and coarse-grained visual streams. Unlike methods relying solely on semantic vectors, DVI exploits the inherent statistical properties of the VAE latent space, utilizing mean and variance as lightweight descriptors for global visual atmosphere. We introduce a **Parameter-Free Feature Modulation** mechanism that adaptively modulates semantic embeddings with these visual statistics, effectively injecting the reference's ``visual soul'' without training. Furthermore, a **Dynamic Temporal Granularity Scheduler** aligns with the diffusion process, prioritizing visual atmosphere in early denoising stages while refining semantic details later. Extensive experiments demonstrate that DVI significantly enhances visual consistency and atmospheric fidelity without parameter fine-tuning, maintaining robust identity preservation and outperforming state-of-the-art methods in IBench evaluations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Total Curvature Regularization and its_Minimization for Surface and Image Smoothing</title>
<link>https://arxiv.org/abs/2512.18968</link>
<guid>https://arxiv.org/abs/2512.18968</guid>
<content:encoded><![CDATA[
arXiv:2512.18968v1 Announce Type: new 
Abstract: We introduce a novel formulation for curvature regularization by penalizing normal curvatures from multiple directions. This total normal curvature regularization is capable of producing solutions with sharp edges and precise isotropic properties. To tackle the resulting high-order nonlinear optimization problem, we reformulate it as the task of finding the steady-state solution of a time-dependent partial differential equation (PDE) system. Time discretization is achieved through operator splitting, where each subproblem at the fractional steps either has a closed-form solution or can be efficiently solved using advanced algorithms. Our method circumvents the need for complex parameter tuning and demonstrates robustness to parameter choices. The efficiency and effectiveness of our approach have been rigorously validated in the context of surface and image smoothing problems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</title>
<link>https://arxiv.org/abs/2512.18969</link>
<guid>https://arxiv.org/abs/2512.18969</guid>
<content:encoded><![CDATA[
arXiv:2512.18969v1 Announce Type: new 
Abstract: Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2512.18991</link>
<guid>https://arxiv.org/abs/2512.18991</guid>
<content:encoded><![CDATA[
arXiv:2512.18991v1 Announce Type: new 
Abstract: Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards AI-Guided Open-World Ecological Taxonomic Classification</title>
<link>https://arxiv.org/abs/2512.18994</link>
<guid>https://arxiv.org/abs/2512.18994</guid>
<content:encoded><![CDATA[
arXiv:2512.18994v1 Announce Type: new 
Abstract: AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</title>
<link>https://arxiv.org/abs/2512.19020</link>
<guid>https://arxiv.org/abs/2512.19020</guid>
<content:encoded><![CDATA[
arXiv:2512.19020v1 Announce Type: new 
Abstract: Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</title>
<link>https://arxiv.org/abs/2512.19021</link>
<guid>https://arxiv.org/abs/2512.19021</guid>
<content:encoded><![CDATA[
arXiv:2512.19021v1 Announce Type: new 
Abstract: Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting "ghost" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2512.19022</link>
<guid>https://arxiv.org/abs/2512.19022</guid>
<content:encoded><![CDATA[
arXiv:2512.19022v1 Announce Type: new 
Abstract: Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \textit{Multi-Aspect Prompting} (MAP) and \textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</title>
<link>https://arxiv.org/abs/2512.19026</link>
<guid>https://arxiv.org/abs/2512.19026</guid>
<content:encoded><![CDATA[
arXiv:2512.19026v1 Announce Type: new 
Abstract: The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</title>
<link>https://arxiv.org/abs/2512.19032</link>
<guid>https://arxiv.org/abs/2512.19032</guid>
<content:encoded><![CDATA[
arXiv:2512.19032v1 Announce Type: new 
Abstract: Fluorescence Microcopy Calcium Imaging is a fundamental tool to in-vivo record and analyze large scale neuronal activities simultaneously at a single cell resolution. Automatic and precise detection of behaviorally relevant neuron activity from the recordings is critical to study the mapping of brain activity in organisms. However a perpetual bottleneck to this problem is the manual segmentation which is time and labor intensive and lacks generalizability. To this end, we present a Bayesian Deep Learning Framework to detect neuronal activities in 4D spatio-temporal data obtained by light sheet microscopy. Our approach accounts for the use of temporal information by calculating pixel wise correlation maps and combines it with spatial information given by the mean summary image. The Bayesian framework not only produces probability segmentation maps but also models the uncertainty pertaining to active neuron detection. To evaluate the accuracy of our framework we implemented the test of reproducibility to assert the generalization of the network to detect neuron activity. The network achieved a mean Dice Score of 0.81 relative to the synthetic Ground Truth obtained by Otsu's method and a mean Dice Score of 0.79 between the first and second run for test of reproducibility. Our method successfully deployed can be used for rapid detection of active neuronal activities for behavioural studies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.19036</link>
<guid>https://arxiv.org/abs/2512.19036</guid>
<content:encoded><![CDATA[
arXiv:2512.19036v1 Announce Type: new 
Abstract: Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</title>
<link>https://arxiv.org/abs/2512.19048</link>
<guid>https://arxiv.org/abs/2512.19048</guid>
<content:encoded><![CDATA[
arXiv:2512.19048v1 Announce Type: new 
Abstract: Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Generative Modeling for Human-Object Interaction Synthesis</title>
<link>https://arxiv.org/abs/2512.19049</link>
<guid>https://arxiv.org/abs/2512.19049</guid>
<content:encoded><![CDATA[
arXiv:2512.19049v1 Announce Type: new 
Abstract: Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</title>
<link>https://arxiv.org/abs/2512.19058</link>
<guid>https://arxiv.org/abs/2512.19058</guid>
<content:encoded><![CDATA[
arXiv:2512.19058v1 Announce Type: new 
Abstract: Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</title>
<link>https://arxiv.org/abs/2512.19070</link>
<guid>https://arxiv.org/abs/2512.19070</guid>
<content:encoded><![CDATA[
arXiv:2512.19070v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.19088</link>
<guid>https://arxiv.org/abs/2512.19088</guid>
<content:encoded><![CDATA[
arXiv:2512.19088v1 Announce Type: new 
Abstract: Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</title>
<link>https://arxiv.org/abs/2512.19091</link>
<guid>https://arxiv.org/abs/2512.19091</guid>
<content:encoded><![CDATA[
arXiv:2512.19091v1 Announce Type: new 
Abstract: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</title>
<link>https://arxiv.org/abs/2512.19095</link>
<guid>https://arxiv.org/abs/2512.19095</guid>
<content:encoded><![CDATA[
arXiv:2512.19095v1 Announce Type: new 
Abstract: Magnetic resonance imaging (MRI) is a cornerstone of modern clinical diagnosis, offering unparalleled soft-tissue contrast without ionizing radiation. However, prolonged scan times remain a major barrier to patient throughput and comfort. Existing accelerated MRI techniques often struggle with two key challenges: (1) failure to effectively utilize inherent K-space prior information, leading to persistent aliasing artifacts from zero-filled inputs; and (2) contamination of target reconstruction quality by irrelevant information when employing multi-contrast fusion strategies. To overcome these challenges, we present MambaMDN, a dual-domain framework for multi-contrast MRI reconstruction. Our approach first employs fully-sampled reference K-space data to complete the undersampled target data, generating structurally aligned but modality-mixed inputs. Subsequently, we develop a Mamba-based modality disentanglement network to extract and remove reference-specific features from the mixed representation. Furthermore, we introduce an iterative refinement mechanism to progressively enhance reconstruction accuracy through repeated feature purification. Extensive experiments demonstrate that MambaMDN can significantly outperform existing multi-contrast reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.19108</link>
<guid>https://arxiv.org/abs/2512.19108</guid>
<content:encoded><![CDATA[
arXiv:2512.19108v1 Announce Type: new 
Abstract: Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</title>
<link>https://arxiv.org/abs/2512.19110</link>
<guid>https://arxiv.org/abs/2512.19110</guid>
<content:encoded><![CDATA[
arXiv:2512.19110v1 Announce Type: new 
Abstract: This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gr\"obner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</title>
<link>https://arxiv.org/abs/2512.19115</link>
<guid>https://arxiv.org/abs/2512.19115</guid>
<content:encoded><![CDATA[
arXiv:2512.19115v1 Announce Type: new 
Abstract: Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</title>
<link>https://arxiv.org/abs/2512.19150</link>
<guid>https://arxiv.org/abs/2512.19150</guid>
<content:encoded><![CDATA[
arXiv:2512.19150v1 Announce Type: new 
Abstract: Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking." These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</title>
<link>https://arxiv.org/abs/2512.19159</link>
<guid>https://arxiv.org/abs/2512.19159</guid>
<content:encoded><![CDATA[
arXiv:2512.19159v1 Announce Type: new 
Abstract: Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</title>
<link>https://arxiv.org/abs/2512.19190</link>
<guid>https://arxiv.org/abs/2512.19190</guid>
<content:encoded><![CDATA[
arXiv:2512.19190v1 Announce Type: new 
Abstract: Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</title>
<link>https://arxiv.org/abs/2512.19213</link>
<guid>https://arxiv.org/abs/2512.19213</guid>
<content:encoded><![CDATA[
arXiv:2512.19213v1 Announce Type: new 
Abstract: Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry</title>
<link>https://arxiv.org/abs/2512.19214</link>
<guid>https://arxiv.org/abs/2512.19214</guid>
<content:encoded><![CDATA[
arXiv:2512.19214v1 Announce Type: new 
Abstract: Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Minimal Fine-Tuning of VLMs</title>
<link>https://arxiv.org/abs/2512.19219</link>
<guid>https://arxiv.org/abs/2512.19219</guid>
<content:encoded><![CDATA[
arXiv:2512.19219v1 Announce Type: new 
Abstract: We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Predicates Structuring urban perception with scene graphs</title>
<link>https://arxiv.org/abs/2512.19221</link>
<guid>https://arxiv.org/abs/2512.19221</guid>
<content:encoded><![CDATA[
arXiv:2512.19221v1 Announce Type: new 
Abstract: Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</title>
<link>https://arxiv.org/abs/2512.19243</link>
<guid>https://arxiv.org/abs/2512.19243</guid>
<content:encoded><![CDATA[
arXiv:2512.19243v1 Announce Type: new 
Abstract: Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory</title>
<link>https://arxiv.org/abs/2512.19271</link>
<guid>https://arxiv.org/abs/2512.19271</guid>
<content:encoded><![CDATA[
arXiv:2512.19271v1 Announce Type: new 
Abstract: Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</title>
<link>https://arxiv.org/abs/2512.19275</link>
<guid>https://arxiv.org/abs/2512.19275</guid>
<content:encoded><![CDATA[
arXiv:2512.19275v1 Announce Type: new 
Abstract: Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</title>
<link>https://arxiv.org/abs/2512.19283</link>
<guid>https://arxiv.org/abs/2512.19283</guid>
<content:encoded><![CDATA[
arXiv:2512.19283v1 Announce Type: new 
Abstract: Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</title>
<link>https://arxiv.org/abs/2512.19300</link>
<guid>https://arxiv.org/abs/2512.19300</guid>
<content:encoded><![CDATA[
arXiv:2512.19300v1 Announce Type: new 
Abstract: Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</title>
<link>https://arxiv.org/abs/2512.19302</link>
<guid>https://arxiv.org/abs/2512.19302</guid>
<content:encoded><![CDATA[
arXiv:2512.19302v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</title>
<link>https://arxiv.org/abs/2512.19311</link>
<guid>https://arxiv.org/abs/2512.19311</guid>
<content:encoded><![CDATA[
arXiv:2512.19311v1 Announce Type: new 
Abstract: This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations</title>
<link>https://arxiv.org/abs/2512.19316</link>
<guid>https://arxiv.org/abs/2512.19316</guid>
<content:encoded><![CDATA[
arXiv:2512.19316v1 Announce Type: new 
Abstract: Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</title>
<link>https://arxiv.org/abs/2512.19327</link>
<guid>https://arxiv.org/abs/2512.19327</guid>
<content:encoded><![CDATA[
arXiv:2512.19327v1 Announce Type: new 
Abstract: Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either "bounce", "net", or "empty_event" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2512.19331</link>
<guid>https://arxiv.org/abs/2512.19331</guid>
<content:encoded><![CDATA[
arXiv:2512.19331v1 Announce Type: new 
Abstract: Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\% using ResNet-50 features and 2.36\% using UNI features. For slide-level classification, it increases accuracy by 3.09\% with ResNet-50 features and 3.75\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis</title>
<link>https://arxiv.org/abs/2512.19336</link>
<guid>https://arxiv.org/abs/2512.19336</guid>
<content:encoded><![CDATA[
arXiv:2512.19336v1 Announce Type: new 
Abstract: The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\times10^{-4}$ and $1\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\times160\times192$ for MRI-to-CT and $32\times128\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</title>
<link>https://arxiv.org/abs/2512.19354</link>
<guid>https://arxiv.org/abs/2512.19354</guid>
<content:encoded><![CDATA[
arXiv:2512.19354v1 Announce Type: new 
Abstract: Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</title>
<link>https://arxiv.org/abs/2512.19365</link>
<guid>https://arxiv.org/abs/2512.19365</guid>
<content:encoded><![CDATA[
arXiv:2512.19365v1 Announce Type: new 
Abstract: Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</title>
<link>https://arxiv.org/abs/2512.19387</link>
<guid>https://arxiv.org/abs/2512.19387</guid>
<content:encoded><![CDATA[
arXiv:2512.19387v1 Announce Type: new 
Abstract: Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.
  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.
  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.
  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis</title>
<link>https://arxiv.org/abs/2512.19415</link>
<guid>https://arxiv.org/abs/2512.19415</guid>
<content:encoded><![CDATA[
arXiv:2512.19415v1 Announce Type: new 
Abstract: Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus =G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2512.19433</link>
<guid>https://arxiv.org/abs/2512.19433</guid>
<content:encoded><![CDATA[
arXiv:2512.19433v1 Announce Type: new 
Abstract: Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</title>
<link>https://arxiv.org/abs/2512.19438</link>
<guid>https://arxiv.org/abs/2512.19438</guid>
<content:encoded><![CDATA[
arXiv:2512.19438v1 Announce Type: new 
Abstract: Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</title>
<link>https://arxiv.org/abs/2512.19443</link>
<guid>https://arxiv.org/abs/2512.19443</guid>
<content:encoded><![CDATA[
arXiv:2512.19443v1 Announce Type: new 
Abstract: Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\% while retaining 99.2\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\% performance at a 90\% token reduction rate, marking a significant advancement with up to 63. 53\% improvement over existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sign Language Recognition using Parallel Bidirectional Reservoir Computing</title>
<link>https://arxiv.org/abs/2512.19451</link>
<guid>https://arxiv.org/abs/2512.19451</guid>
<content:encoded><![CDATA[
arXiv:2512.19451v1 Announce Type: new 
Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation</title>
<link>https://arxiv.org/abs/2512.19479</link>
<guid>https://arxiv.org/abs/2512.19479</guid>
<content:encoded><![CDATA[
arXiv:2512.19479v1 Announce Type: new 
Abstract: Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</title>
<link>https://arxiv.org/abs/2512.19486</link>
<guid>https://arxiv.org/abs/2512.19486</guid>
<content:encoded><![CDATA[
arXiv:2512.19486v1 Announce Type: new 
Abstract: Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</title>
<link>https://arxiv.org/abs/2512.19504</link>
<guid>https://arxiv.org/abs/2512.19504</guid>
<content:encoded><![CDATA[
arXiv:2512.19504v1 Announce Type: new 
Abstract: Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.
  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.
  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.
  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</title>
<link>https://arxiv.org/abs/2512.19512</link>
<guid>https://arxiv.org/abs/2512.19512</guid>
<content:encoded><![CDATA[
arXiv:2512.19512v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Convolutional Neural Deferred Shader for Physics Based Rendering</title>
<link>https://arxiv.org/abs/2512.19522</link>
<guid>https://arxiv.org/abs/2512.19522</guid>
<content:encoded><![CDATA[
arXiv:2512.19522v1 Announce Type: new 
Abstract: Recent advances in neural rendering have achieved impressive results on photorealistic shading and relighting, by using a multilayer perceptron (MLP) as a regression model to learn the rendering equation from a real-world dataset. Such methods show promise for photorealistically relighting real-world objects, which is difficult to classical rendering, as there is no easy-obtained material ground truth. However, significant challenges still remain the dense connections in MLPs result in a large number of parameters, which requires high computation resources, complicating the training, and reducing performance during rendering. Data driven approaches require large amounts of training data for generalization; unbalanced data might bias the model to ignore the unusual illumination conditions, e.g. dark scenes. This paper introduces pbnds+: a novel physics-based neural deferred shading pipeline utilizing convolution neural networks to decrease the parameters and improve the performance in shading and relighting tasks; Energy regularization is also proposed to restrict the model reflection during dark illumination. Extensive experiments demonstrate that our approach outperforms classical baselines, a state-of-the-art neural shading model, and a diffusion-based method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Soccer Scene Analysis with Masked Pre-Training</title>
<link>https://arxiv.org/abs/2512.19528</link>
<guid>https://arxiv.org/abs/2512.19528</guid>
<content:encoded><![CDATA[
arXiv:2512.19528v1 Announce Type: new 
Abstract: In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</title>
<link>https://arxiv.org/abs/2512.19534</link>
<guid>https://arxiv.org/abs/2512.19534</guid>
<content:encoded><![CDATA[
arXiv:2512.19534v1 Announce Type: new 
Abstract: Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</title>
<link>https://arxiv.org/abs/2512.19535</link>
<guid>https://arxiv.org/abs/2512.19535</guid>
<content:encoded><![CDATA[
arXiv:2512.19535v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StoryMem: Multi-shot Long Video Storytelling with Memory</title>
<link>https://arxiv.org/abs/2512.19539</link>
<guid>https://arxiv.org/abs/2512.19539</guid>
<content:encoded><![CDATA[
arXiv:2512.19539v1 Announce Type: new 
Abstract: Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</title>
<link>https://arxiv.org/abs/2512.19546</link>
<guid>https://arxiv.org/abs/2512.19546</guid>
<content:encoded><![CDATA[
arXiv:2512.19546v1 Announce Type: new 
Abstract: Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabyFlow: 3D modeling of realistic and expressive infant faces</title>
<link>https://arxiv.org/abs/2512.19560</link>
<guid>https://arxiv.org/abs/2512.19560</guid>
<content:encoded><![CDATA[
arXiv:2512.19560v1 Announce Type: new 
Abstract: Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</title>
<link>https://arxiv.org/abs/2512.19602</link>
<guid>https://arxiv.org/abs/2512.19602</guid>
<content:encoded><![CDATA[
arXiv:2512.19602v1 Announce Type: new 
Abstract: Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapTrace: Scalable Data Generation for Route Tracing on Maps</title>
<link>https://arxiv.org/abs/2512.19609</link>
<guid>https://arxiv.org/abs/2512.19609</guid>
<content:encoded><![CDATA[
arXiv:2512.19609v1 Announce Type: new 
Abstract: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment</title>
<link>https://arxiv.org/abs/2512.19632</link>
<guid>https://arxiv.org/abs/2512.19632</guid>
<content:encoded><![CDATA[
arXiv:2512.19632v1 Announce Type: new 
Abstract: The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D Gaussian Splatting as a Learned Dynamical System</title>
<link>https://arxiv.org/abs/2512.19648</link>
<guid>https://arxiv.org/abs/2512.19648</guid>
<content:encoded><![CDATA[
arXiv:2512.19648v1 Announce Type: new 
Abstract: We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over++: Generative Video Compositing for Layer Interaction Effects</title>
<link>https://arxiv.org/abs/2512.19661</link>
<guid>https://arxiv.org/abs/2512.19661</guid>
<content:encoded><![CDATA[
arXiv:2512.19661v1 Announce Type: new 
Abstract: In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</title>
<link>https://arxiv.org/abs/2512.19663</link>
<guid>https://arxiv.org/abs/2512.19663</guid>
<content:encoded><![CDATA[
arXiv:2512.19663v1 Announce Type: new 
Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</title>
<link>https://arxiv.org/abs/2512.19676</link>
<guid>https://arxiv.org/abs/2512.19676</guid>
<content:encoded><![CDATA[
arXiv:2512.19676v1 Announce Type: new 
Abstract: Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</title>
<link>https://arxiv.org/abs/2512.19678</link>
<guid>https://arxiv.org/abs/2512.19678</guid>
<content:encoded><![CDATA[
arXiv:2512.19678v1 Announce Type: new 
Abstract: Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VA-$\pi$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.19680</link>
<guid>https://arxiv.org/abs/2512.19680</guid>
<content:encoded><![CDATA[
arXiv:2512.19680v1 Announce Type: new 
Abstract: Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$\pi$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$\pi$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$\pi$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$\pi$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</title>
<link>https://arxiv.org/abs/2512.19683</link>
<guid>https://arxiv.org/abs/2512.19683</guid>
<content:encoded><![CDATA[
arXiv:2512.19683v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Reconstruction of In-Scene Object Manipulation from Video</title>
<link>https://arxiv.org/abs/2512.19684</link>
<guid>https://arxiv.org/abs/2512.19684</guid>
<content:encoded><![CDATA[
arXiv:2512.19684v1 Announce Type: new 
Abstract: We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</title>
<link>https://arxiv.org/abs/2512.19686</link>
<guid>https://arxiv.org/abs/2512.19686</guid>
<content:encoded><![CDATA[
arXiv:2512.19686v1 Announce Type: new 
Abstract: Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</title>
<link>https://arxiv.org/abs/2512.19692</link>
<guid>https://arxiv.org/abs/2512.19692</guid>
<content:encoded><![CDATA[
arXiv:2512.19692v1 Announce Type: new 
Abstract: Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</title>
<link>https://arxiv.org/abs/2512.19693</link>
<guid>https://arxiv.org/abs/2512.19693</guid>
<content:encoded><![CDATA[
arXiv:2512.19693v1 Announce Type: new 
Abstract: Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception</title>
<link>https://arxiv.org/abs/2512.16265</link>
<guid>https://arxiv.org/abs/2512.16265</guid>
<content:encoded><![CDATA[
arXiv:2512.16265v1 Announce Type: cross 
Abstract: Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled representations via score-based variational autoencoders</title>
<link>https://arxiv.org/abs/2512.17127</link>
<guid>https://arxiv.org/abs/2512.17127</guid>
<content:encoded><![CDATA[
arXiv:2512.17127v1 Announce Type: cross 
Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A curated UK rain radar data set for training and benchmarking nowcasting models</title>
<link>https://arxiv.org/abs/2512.17924</link>
<guid>https://arxiv.org/abs/2512.17924</guid>
<content:encoded><![CDATA[
arXiv:2512.17924v1 Announce Type: cross 
Abstract: This paper documents a data set of UK rain radar image sequences for use in statistical modeling and machine learning methods for nowcasting. The main dataset contains 1,000 randomly sampled sequences of length 20 steps (15-minute increments) of 2D radar intensity fields of dimension 40x40 (at 5km spatial resolution). Spatially stratified sampling ensures spatial homogeneity despite removal of clear-sky cases by threshold-based truncation. For each radar sequence, additional atmospheric and geographic features are made available, including date, location, mean elevation, mean wind direction and speed and prevailing storm type. New R functions to extract data from the binary "Nimrod" radar data format are provided. A case study is presented to train and evaluate a simple convolutional neural network for radar nowcasting, including self-contained R code.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology</title>
<link>https://arxiv.org/abs/2512.17930</link>
<guid>https://arxiv.org/abs/2512.17930</guid>
<content:encoded><![CDATA[
arXiv:2512.17930v1 Announce Type: cross 
Abstract: Bone marrow cell cytomorphology analysis is critical for the diagnosis of hematological malignancies but remains a labor-intensive process subject to significant inter-observer variability. While recent foundation models have shown promise in computational pathology, they often require extensive computational resources and fail to account for the asymmetric risks associated with clinical misdiagnosis. We introduce CytoDINO, a framework that achieves state-of-the-art performance on the Munich Leukemia Laboratory (MLL) dataset by fine-tuning DINOv3 using Low-Rank Adaptation (LoRA). Our primary contribution is a novel Hierarchical Focal Loss with Critical Penalties, which encodes biological relationships between cell lineages and explicitly penalizes clinically dangerous misclassifications (e.g., classifying blasts as normal cells). CytoDINO achieves an 88.2% weighted F1 score and 76.5% macro F1 on a held-out test set of 21 cell classes. By utilizing parameter-efficient fine-tuning with only 8% trainable parameters on a single NVIDIA RTX 5080, we demonstrate that consumer-grade hardware can match specialized infrastructure. Furthermore, confidence-based selective prediction yields 99.5% accuracy on 67% of samples, suggesting a viable pathway for clinical deployment where high-uncertainty cases are flagged for expert review
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</title>
<link>https://arxiv.org/abs/2512.18007</link>
<guid>https://arxiv.org/abs/2512.18007</guid>
<content:encoded><![CDATA[
arXiv:2512.18007v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2512.18028</link>
<guid>https://arxiv.org/abs/2512.18028</guid>
<content:encoded><![CDATA[
arXiv:2512.18028v1 Announce Type: cross 
Abstract: Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown</title>
<link>https://arxiv.org/abs/2512.18115</link>
<guid>https://arxiv.org/abs/2512.18115</guid>
<content:encoded><![CDATA[
arXiv:2512.18115v1 Announce Type: cross 
Abstract: Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</title>
<link>https://arxiv.org/abs/2512.18177</link>
<guid>https://arxiv.org/abs/2512.18177</guid>
<content:encoded><![CDATA[
arXiv:2512.18177v1 Announce Type: cross 
Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results</title>
<link>https://arxiv.org/abs/2512.18197</link>
<guid>https://arxiv.org/abs/2512.18197</guid>
<content:encoded><![CDATA[
arXiv:2512.18197v1 Announce Type: cross 
Abstract: Perivascular spaces (PVS), when abnormally enlarged and visible in magnetic resonance imaging (MRI) structural sequences, are important imaging markers of cerebral small vessel disease and potential indicators of neurodegenerative conditions. Despite their clinical significance, automatic enlarged PVS (EPVS) segmentation remains challenging due to their small size, variable morphology, similarity with other pathological features, and limited annotated datasets. This paper presents the EPVS Challenge organized at MICCAI 2024, which aims to advance the development of automated algorithms for EPVS segmentation across multi-site data. We provided a diverse dataset comprising 100 training, 50 validation, and 50 testing scans collected from multiple international sites (UK, Singapore, and China) with varying MRI protocols and demographics. All annotations followed the STRIVE protocol to ensure standardized ground truth and covered the full brain parenchyma. Seven teams completed the full challenge, implementing various deep learning approaches primarily based on U-Net architectures with innovations in multi-modal processing, ensemble strategies, and transformer-based components. Performance was evaluated using dice similarity coefficient, absolute volume difference, recall, and precision metrics. The winning method employed MedNeXt architecture with a dual 2D/3D strategy for handling varying slice thicknesses. The top solutions showed relatively good performance on test data from seen datasets, but significant degradation of performance was observed on the previously unseen Shanghai cohort, highlighting cross-site generalization challenges due to domain shift. This challenge establishes an important benchmark for EPVS segmentation methods and underscores the need for the continued development of robust algorithms that can generalize in diverse clinical settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion</title>
<link>https://arxiv.org/abs/2512.18200</link>
<guid>https://arxiv.org/abs/2512.18200</guid>
<content:encoded><![CDATA[
arXiv:2512.18200v1 Announce Type: cross 
Abstract: In recent years, the demand of image compression models for machine vision has increased dramatically. However, the training frameworks of image compression still focus on the vision of human, maintaining the excessive perceptual details, thus have limitations in optimally reducing the bits per pixel in the case of performing machine vision tasks. In this paper, we propose Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion, termed SLIM. This is a new effective training framework of image compression for machine vision, using a pretrained latent diffusion model.The compressor model of our method focuses only on the Region-of-Interest (RoI) areas for machine vision in the image latent, to compress it compactly. Then the pretrained Unet model enhances the decompressed latent, utilizing a RoI-focused text caption which containing semantic information of the image. Therefore, SLIM is able to focus on RoI areas of the image without any guide mask at the inference stage, achieving low bitrate when compressing. And SLIM is also able to enhance a decompressed latent by denoising steps, so the final reconstructed image from the enhanced latent can be optimized for the machine vision task while still containing perceptual details for human vision. Experimental results show that SLIM achieves a higher classification accuracy in the same bits per pixel condition, compared to conventional image compression models for machines.Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable and Efficient Single-Rollout RL for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.18215</link>
<guid>https://arxiv.org/abs/2512.18215</guid>
<content:encoded><![CDATA[
arXiv:2512.18215v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</title>
<link>https://arxiv.org/abs/2512.18318</link>
<guid>https://arxiv.org/abs/2512.18318</guid>
<content:encoded><![CDATA[
arXiv:2512.18318v1 Announce Type: cross 
Abstract: This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</title>
<link>https://arxiv.org/abs/2512.18450</link>
<guid>https://arxiv.org/abs/2512.18450</guid>
<content:encoded><![CDATA[
arXiv:2512.18450v1 Announce Type: cross 
Abstract: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</title>
<link>https://arxiv.org/abs/2512.18453</link>
<guid>https://arxiv.org/abs/2512.18453</guid>
<content:encoded><![CDATA[
arXiv:2512.18453v1 Announce Type: cross 
Abstract: Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the condition numbers of standard integer based transforms explode, reaching kappa = 2 x 10^5 for F(8,3), rendering them unusable in FP16 or Int8. We introduce NOVA (Numerical Optimization of Vandermonde Arithmetic), a discovery framework that breaks the decades old convention of integer interpolation. Treating Winograd point selection as a continuous optimization problem, NOVA searches the manifold R^n-1 via Evolution Strategy, snaps candidates to simple rationals, and guarantees correctness via symbolic verification. This process uncovers a hidden landscape of stable, fractional configurations such as {+-5/6, +-7/6, +-3/5} that defy traditional vocabulary constraints. The impact is transformative: NOVA improves the conditioning of F(8,3) by 415x in 1D, which squares to a 172,484x improvement for 2D convolution. In real world FP16 ImageNet inference, where standard transforms collapse to random chance (e.g., 4.7 percent accuracy on VGG16), NOVA's points restore full accuracy (75 to 78 percent), recovering over 70 percentage points without retraining, calibration, or learned parameters. These discovered transforms act as drop in replacements, effectively unlocking the efficiency of large tile Winograd convolution for next generation hardware.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STORM: Search-Guided Generative World Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.18477</link>
<guid>https://arxiv.org/abs/2512.18477</guid>
<content:encoded><![CDATA[
arXiv:2512.18477v1 Announce Type: cross 
Abstract: We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.18571</link>
<guid>https://arxiv.org/abs/2512.18571</guid>
<content:encoded><![CDATA[
arXiv:2512.18571v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Reinforcement Learning for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.18662</link>
<guid>https://arxiv.org/abs/2512.18662</guid>
<content:encoded><![CDATA[
arXiv:2512.18662v1 Announce Type: cross 
Abstract: End-to-end (E2E) autonomous driving models that take only camera images as input and directly predict a future trajectory are appealing for their computational efficiency and potential for improved generalization via unified optimization; however, persistent failure modes remain due to reliance on imitation learning (IL). While online reinforcement learning (RL) could mitigate IL-induced issues, the computational burden of neural rendering-based simulation and large E2E networks renders iterative reward and hyperparameter tuning costly. We introduce a camera-only E2E offline RL framework that performs no additional exploration and trains solely on a fixed simulator dataset. Offline RL offers strong data efficiency and rapid experimental iteration, yet is susceptible to instability from overestimation on out-of-distribution (OOD) actions. To address this, we construct pseudo ground-truth trajectories from expert driving logs and use them as a behavior regularization signal, suppressing imitation of unsafe or suboptimal behavior while stabilizing value learning. Training and closed-loop evaluation are conducted in a neural rendering environment learned from the public nuScenes dataset. Empirically, the proposed method achieves substantial improvements in collision rate and route completion compared with IL baselines. Our code will be available at [URL].
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</title>
<link>https://arxiv.org/abs/2512.18987</link>
<guid>https://arxiv.org/abs/2512.18987</guid>
<content:encoded><![CDATA[
arXiv:2512.18987v1 Announce Type: cross 
Abstract: In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.19133</link>
<guid>https://arxiv.org/abs/2512.19133</guid>
<content:encoded><![CDATA[
arXiv:2512.19133v1 Announce Type: cross 
Abstract: Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.19173</link>
<guid>https://arxiv.org/abs/2512.19173</guid>
<content:encoded><![CDATA[
arXiv:2512.19173v1 Announce Type: cross 
Abstract: Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI</title>
<link>https://arxiv.org/abs/2512.19225</link>
<guid>https://arxiv.org/abs/2512.19225</guid>
<content:encoded><![CDATA[
arXiv:2512.19225v1 Announce Type: cross 
Abstract: Breast cancer remains the most common cancer among women and is a leading cause of female mortality. Dynamic contrast-enhanced MRI (DCE-MRI) is a powerful imaging tool for evaluating breast tumors, yet the field lacks a standardized benchmark for analyzing treatment responses and guiding personalized care. We participated in the MAMA-MIA Challenge's Primary Tumor Segmentation task and this work presents a proposed selective, phase-aware training framework for the nnU-Net architecture, emphasizing quality-focused data selection to strengthen model robustness and generalization. We employed the No New Net (nnU-Net) framework with a selective training strategy that systematically analyzed the impact of image quality and center-specific variability on segmentation performance. Controlled experiments on the DUKE, NACT, ISPY1, and ISPY2 datasets revealed that including ISPY scans with motion artifacts and reduced contrast impaired segmentation performance, even with advanced preprocessing, such as contrast-limited adaptive histogram equalization (CLAHE). In contrast, training on DUKE and NACT data, which exhibited clearer contrast and fewer motion artifacts despite varying resolutions, with early phase images (0000-0002) provided more stable training conditions. Our results demonstrate the importance of phase-sensitive and quality-aware training strategies in achieving reliable segmentation performance in heterogeneous clinical datasets, highlighting the limitations of the expansion of naive datasets and motivating the need for future automation of quality-based data selection strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</title>
<link>https://arxiv.org/abs/2512.19253</link>
<guid>https://arxiv.org/abs/2512.19253</guid>
<content:encoded><![CDATA[
arXiv:2512.19253v1 Announce Type: cross 
Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Achieving Superior Model Merging via Magnitude Calibration</title>
<link>https://arxiv.org/abs/2512.19320</link>
<guid>https://arxiv.org/abs/2512.19320</guid>
<content:encoded><![CDATA[
arXiv:2512.19320v1 Announce Type: cross 
Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.19390</link>
<guid>https://arxiv.org/abs/2512.19390</guid>
<content:encoded><![CDATA[
arXiv:2512.19390v1 Announce Type: cross 
Abstract: The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</title>
<link>https://arxiv.org/abs/2512.19402</link>
<guid>https://arxiv.org/abs/2512.19402</guid>
<content:encoded><![CDATA[
arXiv:2512.19402v1 Announce Type: cross 
Abstract: Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability</title>
<link>https://arxiv.org/abs/2512.19489</link>
<guid>https://arxiv.org/abs/2512.19489</guid>
<content:encoded><![CDATA[
arXiv:2512.19489v1 Announce Type: cross 
Abstract: This work revisits the hyperspectral super-resolution (HSR) problem, i.e., fusing a pair of spatially co-registered hyperspectral (HSI) and multispectral (MSI) images to recover a super-resolution image (SRI) that enhances the spatial resolution of the HSI. Coupled tensor decomposition (CTD)-based methods have gained traction in this domain, offering recoverability guarantees under various assumptions. Existing models such as canonical polyadic decomposition (CPD) and Tucker decomposition provide strong expressive power but lack physical interpretability. The block-term decomposition model with rank-$(L_r, L_r, 1)$ terms (the LL1 model) yields interpretable factors under the linear mixture model (LMM) of spectral images, but LMM assumptions are often violated in practice -- primarily due to nonlinear effects such as endmember variability (EV). To address this, we propose modeling spectral images using a more flexible block-term tensor decomposition with rank-$(L_r, M_r, N_r)$ terms (the LMN model). This modeling choice retains interpretability, subsumes CPD, Tucker, and LL1 as special cases, and robustly accounts for non-ideal effects such as EV, offering a balanced tradeoff between expressiveness and interpretability for HSR. Importantly, under the LMN model for HSI and MSI, recoverability of the SRI can still be established under proper conditions -- providing strong theoretical support. Extensive experiments on synthetic and real datasets further validate the effectiveness and robustness of the proposed method compared with existing CTD-based approaches.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Primordial $B$-mode Extraction</title>
<link>https://arxiv.org/abs/2512.19577</link>
<guid>https://arxiv.org/abs/2512.19577</guid>
<content:encoded><![CDATA[
arXiv:2512.19577v1 Announce Type: cross 
Abstract: The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</title>
<link>https://arxiv.org/abs/2512.19584</link>
<guid>https://arxiv.org/abs/2512.19584</guid>
<content:encoded><![CDATA[
arXiv:2512.19584v1 Announce Type: cross 
Abstract: Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.19605</link>
<guid>https://arxiv.org/abs/2512.19605</guid>
<content:encoded><![CDATA[
arXiv:2512.19605v1 Announce Type: cross 
Abstract: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</title>
<link>https://arxiv.org/abs/2512.19629</link>
<guid>https://arxiv.org/abs/2512.19629</guid>
<content:encoded><![CDATA[
arXiv:2512.19629v1 Announce Type: cross 
Abstract: Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \href{https://steinate.github.io/logoplanner.github.io/}{project page}.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</title>
<link>https://arxiv.org/abs/2512.19675</link>
<guid>https://arxiv.org/abs/2512.19675</guid>
<content:encoded><![CDATA[
arXiv:2512.19675v1 Announce Type: cross 
Abstract: We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</title>
<link>https://arxiv.org/abs/2512.19687</link>
<guid>https://arxiv.org/abs/2512.19687</guid>
<content:encoded><![CDATA[
arXiv:2512.19687v1 Announce Type: cross 
Abstract: We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Open-Set Object Detection: Issues, a New Formulation, and Taxonomy</title>
<link>https://arxiv.org/abs/2207.09775</link>
<guid>https://arxiv.org/abs/2207.09775</guid>
<content:encoded><![CDATA[
arXiv:2207.09775v3 Announce Type: replace 
Abstract: Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing "what to detect," which contradicts the idea of identifying "unknown" objects. This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods' performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. The results show that existing methods fail to accurately detect unknown objects due to misclassification of known and unknown classes rather than incorrect bounding box prediction. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</title>
<link>https://arxiv.org/abs/2402.06118</link>
<guid>https://arxiv.org/abs/2402.06118</guid>
<content:encoded><![CDATA[
arXiv:2402.06118v4 Announce Type: replace 
Abstract: By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models</title>
<link>https://arxiv.org/abs/2406.05491</link>
<guid>https://arxiv.org/abs/2406.05491</guid>
<content:encoded><![CDATA[
arXiv:2406.05491v4 Announce Type: replace 
Abstract: Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OW-Rep: Open World Object Detection with Instance Representation Learning</title>
<link>https://arxiv.org/abs/2409.16073</link>
<guid>https://arxiv.org/abs/2409.16073</guid>
<content:encoded><![CDATA[
arXiv:2409.16073v3 Announce Type: replace 
Abstract: Open World Object Detection(OWOD) addresses realistic scenarios where unseen object classes emerge, enabling detectors trained on known classes to detect unknown objects and incrementally incorporate the knowledge they provide. While existing OWOD methods primarily focus on detecting unknown objects, they often overlook the rich semantic relationships between detected objects, which are essential for scene understanding and applications in open-world environments (e.g., open-world tracking and novel class discovery). In this paper, we extend the OWOD framework to jointly detect unknown objects and learn semantically rich instance embeddings, enabling the detector to capture fine-grained semantic relationships between instances. To this end, we propose two modules that leverage the rich and generalizable knowledge of Vision Foundation Models(VFMs) and can be integrated into open-world object detectors. First, the Unknown Box Refine Module uses instance masks from the Segment Anything Model to accurately localize unknown objects. The Embedding Transfer Module then distills instance-wise semantic similarities from VFM features to the detector's embeddings via a relaxed contrastive loss, enabling the detector to learn a semantically meaningful and generalizable instance feature. Extensive experiments show that our method significantly improves both unknown object detection and instance embedding quality, while also enhancing performance in downstream tasks such as open-world tracking.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</title>
<link>https://arxiv.org/abs/2410.24116</link>
<guid>https://arxiv.org/abs/2410.24116</guid>
<content:encoded><![CDATA[
arXiv:2410.24116v2 Announce Type: replace 
Abstract: Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Episodic Memory Visual Query Localization with Egocentric Streaming Object Memory</title>
<link>https://arxiv.org/abs/2411.16934</link>
<guid>https://arxiv.org/abs/2411.16934</guid>
<content:encoded><![CDATA[
arXiv:2411.16934v3 Announce Type: replace 
Abstract: Episodic memory retrieval enables wearable cameras to recall objects or events previously observed in video. However, existing formulations assume an "offline" setting with full video access at query time, limiting their applicability in real-world scenarios with power and storage-constrained wearable devices. Towards more application-ready episodic memory systems, we introduce Online Visual Query 2D (OVQ2D), a task where models process video streams online, observing each frame only once, and retrieve object localizations using a compact memory instead of full video history. We address OVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework integrating an object discovery module, an object tracking module, and a memory module that find, track, and store spatio-temporal object information for efficient querying. Experiments on Ego4D demonstrate ESOM's superiority over other online approaches, though OVQ2D remains challenging, with top performance at only ~4% success. ESOM's accuracy increases markedly with perfect object tracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need of applied research on these components.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learnable Sparsity for Vision Generative Models</title>
<link>https://arxiv.org/abs/2412.02852</link>
<guid>https://arxiv.org/abs/2412.02852</guid>
<content:encoded><![CDATA[
arXiv:2412.02852v2 Announce Type: replace 
Abstract: Diffusion models have achieved impressive advancements in various vision tasks. However, these gains often rely on increasing model size, which escalates computational complexity and memory demands, complicating deployment, raising inference costs, and causing environmental impact. While some studies have explored pruning techniques to improve the memory efficiency of diffusion models, most existing methods require extensive retraining to retain the model performance. Retraining a modern large diffusion model is extremely costly and resource-intensive, which limits the practicality of these methods. In this work, we achieve low-cost diffusion pruning without retraining by proposing a model-agnostic structural pruning framework for diffusion models that learns a differentiable mask to sparsify the model. To ensure effective pruning that preserves the quality of the final denoised latent, we design a novel end-to-end pruning objective that spans the entire diffusion process. As end-to-end pruning is memory-intensive, we further propose time step gradient checkpointing, a technique that significantly reduces memory usage during optimization, enabling end-to-end pruning within a limited memory budget. Results on state-of-the-art U-Net diffusion models SDXL and diffusion transformers (FLUX) demonstrate that our method can effectively prune up to 20% parameters with minimal perceptible performance degradation, and notably, without the need for model retraining. We also showcase that our method can still prune on top of time step distilled diffusion models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2412.04939</link>
<guid>https://arxiv.org/abs/2412.04939</guid>
<content:encoded><![CDATA[
arXiv:2412.04939v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have garnered significant attention recently and demonstrate outstanding capabilities in various tasks such as OCR, VQA, captioning, $\textit{etc}$. However, hallucination remains a persistent issue. While numerous methods have been proposed to mitigate hallucinations, achieving notable improvements, these methods primarily focus on mitigating hallucinations about $\textbf{object/noun-related}$ concepts. Verb concepts, crucial for understanding human actions, have been largely overlooked. In this paper, to the best of our knowledge, we are the $\textbf{first}$ to investigate the $\textbf{verb hallucination}$ phenomenon of MLLMs from various perspectives. Our findings reveal that most state-of-the-art MLLMs suffer from severe verb hallucination. To assess the effectiveness of existing mitigation methods for object concept hallucination on verb hallucination, we evaluated these methods and found that they do not effectively address verb hallucination. To address this issue, we propose a novel rich verb knowledge-based tuning method to mitigate verb hallucination. The experiment results demonstrate that our method significantly reduces hallucinations related to verbs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text to Blind Motion</title>
<link>https://arxiv.org/abs/2412.05277</link>
<guid>https://arxiv.org/abs/2412.05277</guid>
<content:encoded><![CDATA[
arXiv:2412.05277v2 Announce Type: replace 
Abstract: People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaceShield: Defending Facial Image against Deepfake Threats</title>
<link>https://arxiv.org/abs/2412.09921</link>
<guid>https://arxiv.org/abs/2412.09921</guid>
<content:encoded><![CDATA[
arXiv:2412.09921v3 Announce Type: replace 
Abstract: The rising use of deepfakes in criminal activities presents a significant issue, inciting widespread controversy. While numerous studies have tackled this problem, most primarily focus on deepfake detection. These reactive solutions are insufficient as a fundamental approach for crimes where authenticity is disregarded. Existing proactive defenses also have limitations, as they are effective only for deepfake models based on specific Generative Adversarial Networks (GANs), making them less applicable in light of recent advancements in diffusion-based models. In this paper, we propose a proactive defense method named FaceShield, which introduces novel defense strategies targeting deepfakes generated by Diffusion Models (DMs) and facilitates defenses on various existing GAN-based deepfake models through facial feature extractor manipulations. Our approach consists of three main components: (i) manipulating the attention mechanism of DMs to exclude protected facial features during the denoising process, (ii) targeting prominent facial feature extraction models to enhance the robustness of our adversarial perturbation, and (iii) employing Gaussian blur and low-pass filtering techniques to improve imperceptibility while enhancing robustness against JPEG compression. Experimental results on the CelebA-HQ and VGGFace2-HQ datasets demonstrate that our method achieves state-of-the-art performance against the latest deepfake models based on DMs, while also exhibiting transferability to GANs and showcasing greater imperceptibility of noise along with enhanced robustness. Code is available here: https://github.com/kuai-lab/iccv25_faceshield
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Easy to Hard: Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision</title>
<link>https://arxiv.org/abs/2412.11154</link>
<guid>https://arxiv.org/abs/2412.11154</guid>
<content:encoded><![CDATA[
arXiv:2412.11154v3 Announce Type: replace 
Abstract: Recently, single-frame infrared small target (SIRST) detection with single point supervision has drawn wide-spread attention. However, the latest label evolution with single point supervision (LESPS) framework suffers from instability, excessive label evolution, and difficulty in exerting embedded network performance. Inspired by organisms gradually adapting to their environment and continuously accumulating knowledge, we construct an innovative Progressive Active Learning (PAL) framework, which drives the existing SIRST detection networks progressively and actively recognizes and learns harder samples. Specifically, to avoid the early low-performance model leading to the wrong selection of hard samples, we propose a model pre-start concept, which focuses on automatically selecting a portion of easy samples and helping the model have basic task-specific learning capabilities. Meanwhile, we propose a refined dual-update strategy, which can promote reasonable learning of harder samples and continuous refinement of pseudo-labels. In addition, to alleviate the risk of excessive label evolution, a decay factor is reasonably introduced, which helps to achieve a dynamic balance between the expansion and contraction of target annotations. Extensive experiments show that existing SIRST detection networks equipped with our PAL framework have achieved state-of-the-art (SOTA) results on multiple public datasets. Furthermore, our PAL framework can build an efficient and stable bridge between full supervision and single point supervision tasks. Our code is available at https://github.com/YuChuang1205/PAL
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2412.14579</link>
<guid>https://arxiv.org/abs/2412.14579</guid>
<content:encoded><![CDATA[
arXiv:2412.14579v2 Announce Type: replace 
Abstract: Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary Event-Driven Spiking Transformer</title>
<link>https://arxiv.org/abs/2501.05904</link>
<guid>https://arxiv.org/abs/2501.05904</guid>
<content:encoded><![CDATA[
arXiv:2501.05904v2 Announce Type: replace 
Abstract: Transformer-based Spiking Neural Networks (SNNs) introduce a novel event-driven self-attention paradigm that combines the high performance of Transformers with the energy efficiency of SNNs. However, the larger model size and increased computational demands of the Transformer structure limit their practicality in resource-constrained scenarios. In this paper, we integrate binarization techniques into Transformer-based SNNs and propose the Binary Event-Driven Spiking Transformer, i.e. BESTformer. The proposed BESTformer can significantly reduce storage and computational demands by representing weights and attention maps with a mere 1-bit. However, BESTformer suffers from a severe performance drop from its full-precision counterpart due to the limited representation capability of binarization. To address this issue, we propose a Coupled Information Enhancement (CIE) method, which consists of a reversible framework and information enhancement distillation. By maximizing the mutual information between the binary model and its full-precision counterpart, the CIE method effectively mitigates the performance degradation of the BESTformer. Extensive experiments on static and neuromorphic datasets demonstrate that our method achieves superior performance to other binary SNNs, showcasing its potential as a compact yet high-performance model for resource-limited edge devices. The repository of this paper is available at https://github.com/CaoHLin/BESTFormer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Redundancy Reduction for Open-Vocabulary Semantic Segmentation</title>
<link>https://arxiv.org/abs/2501.17642</link>
<guid>https://arxiv.org/abs/2501.17642</guid>
<content:encoded><![CDATA[
arXiv:2501.17642v2 Announce Type: replace 
Abstract: Open-vocabulary semantic segmentation (OVSS) is an open-world task that aims to assign each pixel within an image to a specific class defined by arbitrary text descriptions. While large-scale vision-language models have shown remarkable open-vocabulary capabilities, their image-level pretraining limits effectiveness on pixel-wise dense prediction tasks like OVSS. Recent cost-based methods narrow this granularity gap by constructing pixel-text cost maps and refining them via cost aggregation mechanisms. Despite achieving promising performance, these approaches suffer from high computational costs and long inference latency. In this paper, we identify two major sources of redundancy in the cost-based OVSS framework: redundant information introduced during cost maps construction and inefficient sequence modeling in cost aggregation. To address these issues, we propose ERR-Seg, an efficient architecture that incorporates Redundancy-Reduced Hierarchical Cost maps (RRHC) and Redundancy-Reduced Cost Aggregation (RRCA). Specifically, RRHC reduces redundant class channels by customizing a compact class vocabulary for each image and integrates hierarchical cost maps to enrich semantic representation. RRCA alleviates computational burden by performing both spatial-level and class-level sequence reduction before aggregation. Overall, ERR-Seg results in a lightweight structure for OVSS, characterized by substantial memory and computational savings without compromising accuracy. Compared to previous state-of-the-art methods on the ADE20K-847 benchmark, ERR-Seg improves performance by $5.6\%$ while achieving a 3.1$\times$ speedup.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</title>
<link>https://arxiv.org/abs/2502.04226</link>
<guid>https://arxiv.org/abs/2502.04226</guid>
<content:encoded><![CDATA[
arXiv:2502.04226v2 Announce Type: replace 
Abstract: In the era of pre-trained models, effective classification can often be achieved using simple linear probing or lightweight readout layers. In contrast, many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets, including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SNN-Driven Multimodal Human Action Recognition via Sparse Spatial-Temporal Data Fusion</title>
<link>https://arxiv.org/abs/2502.13385</link>
<guid>https://arxiv.org/abs/2502.13385</guid>
<content:encoded><![CDATA[
arXiv:2502.13385v2 Announce Type: replace 
Abstract: Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models</title>
<link>https://arxiv.org/abs/2503.01298</link>
<guid>https://arxiv.org/abs/2503.01298</guid>
<content:encoded><![CDATA[
arXiv:2503.01298v3 Announce Type: replace 
Abstract: Unified generative models have shown remarkable performance in text and image generation. For image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, direct T2I generation limits the models in handling complex compositional instructions, which frequently occur in real-world scenarios. Although this issue is vital, existing works mainly focus on improving the basic image generation capability of the models. While such improvements help to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems step by step, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first propose Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts by function. FoXperts disentangles potential conflicts in mainstream modality-oriented designs and provides a solid foundation for CoT. When introducing CoT, the first question is how to design it for complex image generation. To this end, we emulate a human-like artistic workflow--planning, acting, reflection, and correction--and propose the Multimodal Chain of Thought (MCoT) approach, as the data involves both text and image. To address the subsequent challenge of designing an effective MCoT training paradigm, we develop a multi-task joint training scheme that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm avoids the difficulty of collecting consistent multi-step data tuples. Extensive experiments show that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable improvements in complex image generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning</title>
<link>https://arxiv.org/abs/2503.02341</link>
<guid>https://arxiv.org/abs/2503.02341</guid>
<content:encoded><![CDATA[
arXiv:2503.02341v2 Announce Type: replace 
Abstract: Recent great advances in video generation models have demonstrated their potential to produce high-quality videos, bringing challenges to effective evaluation. Unlike human evaluation, existing automated evaluation metrics lack highlevel semantic understanding and reasoning capabilities for video, thus making them infeasible and unexplainable. To fill this gap, we curate GRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset, including 3.3k videos from over 10 existing video generation models and multi-step reasoning assessments converted by 16k human annotations. We then introduce GRADEO, one of the first specifically designed video evaluation models, which grades AI-generated videos for explainable scores and assessments through multi-step reasoning. Experiments show that our method aligns better with human evaluations than existing methods. Furthermore, our benchmarking reveals that current video generation models struggle to produce content that aligns with human reasoning and complex real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: Your Diffusion Model is Secretly an Instance Edge Detector</title>
<link>https://arxiv.org/abs/2503.07982</link>
<guid>https://arxiv.org/abs/2503.07982</guid>
<content:encoded><![CDATA[
arXiv:2503.07982v3 Announce Type: replace 
Abstract: High-quality instance and panoptic segmentation has traditionally relied on dense instance-level annotations such as masks, boxes, or points, which are costly, inconsistent, and difficult to scale. Unsupervised and weakly-supervised approaches reduce this burden but remain constrained by semantic backbone constraints and human bias, often producing merged or fragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance Edges), showing that text-to-image diffusion models secretly function as instance edge annotators. TRACE identifies the Instance Emergence Point (IEP) where object boundaries first appear in self-attention maps, extracts boundaries through Attention Boundary Divergence (ABDiv), and distills them into a lightweight one-step edge decoder. This design removes the need for per-image diffusion inversion, achieving 81x faster inference while producing sharper and more connected boundaries. On the COCO benchmark, TRACE improves unsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic segmentation it outperforms point-supervised baselines by +1.7 PQ without using any instance-level labels. These results reveal that diffusion models encode hidden instance boundary priors, and that decoding these signals offers a practical and scalable alternative to costly manual annotation. Code is available at https://github.com/shjo-april/DiffEGG.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents</title>
<link>https://arxiv.org/abs/2503.10200</link>
<guid>https://arxiv.org/abs/2503.10200</guid>
<content:encoded><![CDATA[
arXiv:2503.10200v5 Announce Type: replace 
Abstract: Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agent's performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80\% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3\% on LongVideoBench. Code is available at https://github.com/64327069/LVAgent.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comp-Attn: Present-and-Align Attention for Compositional Video Generation</title>
<link>https://arxiv.org/abs/2503.14428</link>
<guid>https://arxiv.org/abs/2503.14428</guid>
<content:encoded><![CDATA[
arXiv:2503.14428v2 Announce Type: replace 
Abstract: In the domain of text-to-video (T2V) generation, reliably synthesizing compositional content involving multiple subjects with intricate relations is still underexplored. The main challenges are twofold: 1) Subject presence, where not all subjects can be presented in the video; 2) Inter-subject relations, where the interaction and spatial relationship between subjects are misaligned. Existing methods adopt techniques, such as inference-time latent optimization or layout control, which fail to address both issues simultaneously. To tackle these problems, we propose Comp-Attn, a composition-aware cross-attention variant that follows a Present-and-Align paradigm: it decouples the two challenges by enforcing subject presence at the condition level and achieving relational alignment at the attention-distribution level. Specifically, 1) We introduce Subject-aware Condition Interpolation (SCI) to reinforce subject-specific conditions and ensure each subject's presence; 2) We propose Layout-forcing Attention Modulation (LAM), which dynamically enforces the attention distribution to align with the relational layout of multiple subjects. Comp-Attn can be seamlessly integrated into various T2V baselines in a training-free manner, boosting T2V-CompBench scores by 15.7\% and 11.7\% on Wan2.1-T2V-14B and Wan2.2-T2V-A14B with only a 5\% increase in inference time. Meanwhile, it also achieves strong performance on VBench and T2I-CompBench, demonstrating its scalability in general video generation and compositional text-to-image (T2I) tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HandSCS: Structural Coordinate Space for Animatable Hand Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.14736</link>
<guid>https://arxiv.org/abs/2503.14736</guid>
<content:encoded><![CDATA[
arXiv:2503.14736v2 Announce Type: replace 
Abstract: Creating animatable hand avatars from multi-view images requires modeling complex articulations and maintaining structural consistency across poses in real time. We present HandSCS, a structure-guided 3D Gaussian Splatting framework for high-fidelity hand animation. Unlike existing approaches that condition all Gaussians on the same global pose parameters, which are inadequate for highly articulated hands, HandSCS equips each Gaussian with explicit structural guidance from both intra-pose and inter-pose perspectives. To establish intra-pose structural guidance, we introduce a Structural Coordinate Space (SCS), which bridges the gap between sparse bones and dense Gaussians through hybrid static-dynamic coordinate basis and angular-radial descriptors. To improve cross-pose coherence, we further introduce an Inter-pose Consistency Loss that promotes consistent Gaussian attributes under similar articulations. Together, these components achieve high-fidelity results with consistent fine details, even in challenging high-deformation and self-contact regions. Experiments on the InterHand2.6M dataset demonstrate that HandSCS achieves state-of-the-art performance in hand avatar animation, confirming the effectiveness of explicit structural modeling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UCS: A Universal Model for Curvilinear Structure Segmentation</title>
<link>https://arxiv.org/abs/2504.04034</link>
<guid>https://arxiv.org/abs/2504.04034</guid>
<content:encoded><![CDATA[
arXiv:2504.04034v2 Announce Type: replace 
Abstract: Curvilinear structure segmentation (CSS) is essential in various domains, including medical imaging, landscape analysis, industrial surface inspection, and plant analysis. While existing methods achieve high performance within specific domains, their generalizability is limited. On the other hand, large-scale models such as Segment Anything Model (SAM) exhibit strong generalization but are not optimized for curvilinear structures. Existing adaptations of SAM primarily focus on general object segmentation and lack specialized design for CSS tasks. To bridge this gap, we propose the Universal Curvilinear structure Segmentation (UCS) model, which adapts SAM to CSS tasks while further enhancing its cross-domain generalization. UCS features a novel encoder architecture integrating a pretrained SAM encoder with two innovations: a Sparse Adapter, strategically inserted to inherit the pre-trained SAM encoder's generalization capability while minimizing the number of fine-tuning parameters, and a Prompt Generation module, which leverages Fast Fourier Transform with a high-pass filter to generate curve-specific prompts. Furthermore, the UCS incorporates a mask decoder that eliminates reliance on manual interaction through a dual-compression module: a Hierarchical Feature Compression module, which aggregates the outputs of the sampled encoder to enhance detail preservation, and a Guidance Feature Compression module, which extracts and compresses image-driven guidance features. Evaluated on a comprehensive multi-domain dataset, including an in-house dataset covering eight natural curvilinear structures, UCS demonstrates state-of-the-art generalization and open-set segmentation performance across medical, engineering, natural, and plant imagery, establishing a new benchmark for universal CSS. The source code is available at https://github.com/kylechuuuuu/UCS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient as Conditions: Rethinking HOG for All-in-one Image Restoration</title>
<link>https://arxiv.org/abs/2504.09377</link>
<guid>https://arxiv.org/abs/2504.09377</guid>
<content:encoded><![CDATA[
arXiv:2504.09377v3 Announce Type: replace 
Abstract: All-in-one image restoration (AIR) aims to address diverse degradations within a unified model by leveraging informative degradation conditions to guide the restoration process. However, existing methods often rely on implicitly learned priors, which may entangle feature representations and hinder performance in complex or unseen scenarios. Histogram of Oriented Gradients (HOG) as a classical gradient representation, we observe that it has strong discriminative capability across diverse degradations, making it a powerful and interpretable prior for AIR. Based on this insight, we propose HOGformer, a Transformer-based model that integrates learnable HOG features for degradation-aware restoration. The core of HOGformer is a Dynamic HOG-aware Self-Attention (DHOGSA) mechanism, which adaptively models long-range spatial dependencies conditioned on degradation-specific cues encoded by HOG descriptors. To further adapt the heterogeneity of degradations in AIR, we propose a Dynamic Interaction Feed-Forward (DIFF) module that facilitates channel-spatial interactions, enabling robust feature transformation under diverse degradations. Besides, we propose a HOG loss to explicitly enhance structural fidelity and edge sharpness. Extensive experiments on a variety of benchmarks, including adverse weather and natural degradations, demonstrate that HOGformer achieves state-of-the-art performance and generalizes well to complex real-world scenarios.Code is available at https://github.com/Fire-friend/HOGformer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightFormer: A lightweight and efficient decoder for remote sensing image segmentation</title>
<link>https://arxiv.org/abs/2504.10834</link>
<guid>https://arxiv.org/abs/2504.10834</guid>
<content:encoded><![CDATA[
arXiv:2504.10834v2 Announce Type: replace 
Abstract: Deep learning techniques have achieved remarkable success in the semantic segmentation of remote sensing images and in land-use change detection. Nevertheless, their real-time deployment on edge platforms remains constrained by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder for time-critical tasks that involve unstructured targets, such as disaster assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage monitoring. LightFormer employs a feature-fusion and refinement module built on channel processing and a learnable gating mechanism to aggregate multi-scale, multi-range information efficiently, which drastically curtails model complexity. Furthermore, we propose a spatial information selection module (SISM) that integrates long-range attention with a detail preservation branch to capture spatial dependencies across multiple scales, thereby substantially improving the recognition of unstructured targets in complex scenes. On the ISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9% vs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters, thus achieving an excellent accuracy-efficiency trade-off. Consistent results on LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its robustness and superior perception of unstructured objects. These findings highlight LightFormer as a practical solution for remote sensing applications where both computational economy and high-precision segmentation are imperative.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach</title>
<link>https://arxiv.org/abs/2504.11922</link>
<guid>https://arxiv.org/abs/2504.11922</guid>
<content:encoded><![CDATA[
arXiv:2504.11922v3 Announce Type: replace 
Abstract: The rise of AI-generated image tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated ``Perception-Creation-Evaluation'' pipeline to ensure semantic coherence and visual realism. In addition, we further propose \textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying subtle forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.04262</link>
<guid>https://arxiv.org/abs/2505.04262</guid>
<content:encoded><![CDATA[
arXiv:2505.04262v2 Announce Type: replace 
Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
arXiv:2505.08438v3 Announce Type: replace 
Abstract: Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning</title>
<link>https://arxiv.org/abs/2505.10999</link>
<guid>https://arxiv.org/abs/2505.10999</guid>
<content:encoded><![CDATA[
arXiv:2505.10999v3 Announce Type: replace 
Abstract: While diffusion models excel at image synthesis, useful representations have been shown to emerge from generative pre-training, suggesting a path towards unified generative and discriminative learning. However, suboptimal semantic flow within current architectures can hinder this potential: features encoding the richest high-level semantics are underutilized and diluted when propagating through decoding layers, impeding the formation of an explicit semantic bottleneck layer. To address this, we introduce self-conditioning, a lightweight mechanism that reshapes the model's layer-wise semantic hierarchy without external guidance. By aggregating and rerouting intermediate features to guide subsequent decoding layers, our method concentrates more high-level semantics, concurrently strengthening global generative guidance and forming more discriminative representations. This simple approach yields a dual-improvement trend across pixel-space UNet, UViT and latent-space DiT models with minimal overhead. Crucially, it creates an architectural semantic bridge that propagates discriminative improvements into generation and accommodates further techniques such as contrastive self-distillation. Experiments show that our enhanced models, especially self-conditioned DiT, are powerful dual learners that yield strong and transferable representations on image and dense classification tasks, surpassing various generative self-supervised models in linear probing while also improving or maintaining high generation quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms</title>
<link>https://arxiv.org/abs/2505.17020</link>
<guid>https://arxiv.org/abs/2505.17020</guid>
<content:encoded><![CDATA[
arXiv:2505.17020v2 Announce Type: replace 
Abstract: The advent of Large Multimodal Models (LMMs) has significantly enhanced Large Language Models (LLMs) to process and interpret diverse data modalities (e.g., image and video). However, as input complexity increases, particularly with long video sequences, the number of required tokens has grown significantly, leading to quadratically computational costs. This has made the efficient compression of video tokens in LMMs, while maintaining performance integrity, a pressing research challenge. In this paper, we introduce CrossLMM, decoupling long video sequences from LMMs via a dual cross-attention mechanism, which substantially reduces visual token quantity with minimal performance degradation. Specifically, we first implement a significant token reduction from pretrained visual encoders through a pooling methodology. Then, within LLM layers, we employ a visual-to-visual cross-attention mechanism, wherein the pooled visual tokens function as queries against the original visual token set. This module enables more efficient token utilization while retaining fine-grained informational fidelity. In addition, we introduce a text-to-visual cross-attention mechanism, for which the text tokens are enhanced through interaction with the original visual tokens, enriching the visual comprehension of the text tokens. Comprehensive empirical evaluation demonstrates that our approach achieves comparable or superior performance across diverse video-based LMM benchmarks, despite utilizing substantially fewer computational resources.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2505.18947</link>
<guid>https://arxiv.org/abs/2505.18947</guid>
<content:encoded><![CDATA[
arXiv:2505.18947v2 Announce Type: replace 
Abstract: Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., "Find a water bottle and take a sip") into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI's superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions. Our project page at \href{https://openhoi.github.io}
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding</title>
<link>https://arxiv.org/abs/2506.07576</link>
<guid>https://arxiv.org/abs/2506.07576</guid>
<content:encoded><![CDATA[
arXiv:2506.07576v2 Announce Type: replace 
Abstract: Video understanding has been considered as one critical step towards world modeling, which is an important long-term problem in AI research. Recently, multimodal foundation models have shown such potential via large-scale pretraining. These models effectively align encoders of different modalities via contrastive learning. To further enhance performance on complex target movements and diversified video scenes, we propose to augment this alignment with deeper multimodal interactions, which are critical for understanding complex target movements with diversified video scenes. To fill this gap, we propose a unified Super Encoding Network (SEN) for video understanding, which builds up such distinct interactions through the recursive association of multimodal encoders in the foundation models. Specifically, we creatively treat those well-trained encoders as ``super neurons" in our SEN. Via designing a Recursive Association (RA) block, we progressively fuse multi-modalities with the input video, based on knowledge integrating, distributing, and prompting of super neurons in a recursive manner. In this way, our SEN can effectively encode deeper multimodal interactions for prompting various video understanding tasks in the downstream. Extensive experiments show that our SEN can remarkably boost the four most representative video tasks, including tracking, recognition, chatting, and editing, e.g., for pixel-level tracking, the average jaccard index improves 2.7%, and temporal coherence(TC) drops by 8.8% compared to the popular CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%, and frame consistency increases by 4.1% compared to the Tune-A-Video approach.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling</title>
<link>https://arxiv.org/abs/2506.10609</link>
<guid>https://arxiv.org/abs/2506.10609</guid>
<content:encoded><![CDATA[
arXiv:2506.10609v2 Announce Type: replace 
Abstract: Scene text retrieval has made significant progress with the assistance of accurate text localization. However, existing approaches typically require costly bounding box annotations for training. Besides, they mostly adopt a customized retrieval strategy but struggle to unify various types of queries to meet diverse retrieval needs. To address these issues, we introduce Muti-query Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for scene text retrieval. It incorporates progressive vision embedding to dynamically capture the multi-grained representation of texts and harmonizes free-style text queries with style-aware instructions. Additionally, a multi-instance matching module is integrated to enhance vision-language alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset, the first benchmark designed to evaluate the multi-query scene text retrieval capability of models, comprising four query types and 16k images. Extensive experiments demonstrate the superiority of our method across seven public datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly outperforms the previous models by an average of 8.5%. The code and datasets are available at https://github.com/yingift/MSTAR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts</title>
<link>https://arxiv.org/abs/2506.12520</link>
<guid>https://arxiv.org/abs/2506.12520</guid>
<content:encoded><![CDATA[
arXiv:2506.12520v2 Announce Type: replace 
Abstract: We propose VINO, the first zero-shot, training-free video editing method conditioned on both image and text. Our approach introduces $\rho$-start sampling and dilated dual masking to construct structured noise maps that enable coherent and accurate edits. To further enhance visual fidelity, we present zero image guidance, a controllable negative prompt strategy. Extensive experiments demonstrate that VINO faithfully incorporates the reference image into video edits, achieving strong performance compared to state-of-the-art baselines, all without any test-time or instance-specific training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Shape Generation: A Survey</title>
<link>https://arxiv.org/abs/2506.22678</link>
<guid>https://arxiv.org/abs/2506.22678</guid>
<content:encoded><![CDATA[
arXiv:2506.22678v2 Announce Type: replace 
Abstract: Recent advances in deep learning have significantly transformed the field of 3D shape generation, enabling the synthesis of complex, diverse, and semantically meaningful 3D objects. This survey provides a comprehensive overview of the current state-of-the-art in 3D shape generation, organizing the discussion around three core components: shape representations, generative modeling approaches, and evaluation protocols. We begin by categorizing 3D representations into explicit, implicit, and hybrid setups, highlighting their structural properties, advantages, and limitations. Next, we review a wide range of generation methods, focusing on feedforward architectures. We further summarize commonly used datasets and evaluation metrics that assess fidelity, diversity, and realism of generated shapes. Finally, we identify open challenges and outline future research directions that could drive progress in controllable, efficient, and high-quality 3D shape generation. This survey aims to serve as a valuable reference for researchers and practitioners seeking a structured and in-depth understanding of this rapidly evolving field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention</title>
<link>https://arxiv.org/abs/2507.09885</link>
<guid>https://arxiv.org/abs/2507.09885</guid>
<content:encoded><![CDATA[
arXiv:2507.09885v3 Announce Type: replace 
Abstract: Reconstructing hyperspectral images (HSIs) from RGB inputs provides a cost-effective alternative to hyperspectral cameras, but reconstructing high-dimensional spectra from three channels is inherently ill-posed. Existing methods typically directly regress RGB-to-HSI mappings using large attention networks, which are computationally expensive and handle ill-posedness only implicitly. We propose MCGA, a Mixture-of-Codebooks with Grayscale-aware Attention framework that explicitly addresses these challenges using spectral priors and photometric consistency. MCGA first learns transferable spectral priors via a mixture-of-codebooks (MoC) from heterogeneous HSI datasets, then aligns RGB features with these priors through grayscale-aware photometric attention (GANet). Efficiency and robustness are further improved via top-K attention design and test-time adaptation (TTA). Experiments on multiple real-world benchmarks demonstrate the state-of-the-art accuracy, strong cross-dataset generalization, and 4-5x faster inference. Codes will be available once acceptance at https://github.com/Fibonaccirabbit/MCGA.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation</title>
<link>https://arxiv.org/abs/2507.13292</link>
<guid>https://arxiv.org/abs/2507.13292</guid>
<content:encoded><![CDATA[
arXiv:2507.13292v2 Announce Type: replace 
Abstract: Accurate age verification can protect underage users from unauthorized access to online platforms and e-commerce sites that provide age-restricted services. However, accurate age estimation can be confounded by several factors, including facial makeup that can induce changes to alter perceived identity and age to fool both humans and machines. In this work, we propose \textsc{DiffClean} which erases makeup traces using a text-guided diffusion model to defend against makeup attacks without requiring any reference image unlike prior work. \textsc{DiffClean} improves age estimation (minor vs. adult accuracy by 5.8\%) and face verification (TMR by 5.1\% at FMR=0.01\%) compared to images with makeup. Our method is: (1) robust across digitally simulated and real-world makeup styles with high visual fidelity, (2) can be easily integrated as a pre-processing module in existing age and identity verification frameworks, and (3) advances the state-of-the art in terms of biometric and perceptual utility. Our codes are available at https://github.com/Ektagavas/DiffClean
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction</title>
<link>https://arxiv.org/abs/2507.13387</link>
<guid>https://arxiv.org/abs/2507.13387</guid>
<content:encoded><![CDATA[
arXiv:2507.13387v2 Announce Type: replace 
Abstract: Accurate perception of the surrounding environment is essential for safe autonomous driving. 3D occupancy prediction, which estimates detailed 3D structures of roads, buildings, and other objects, is particularly important for vision-centric autonomous driving systems that do not rely on LiDAR sensors. However, in 3D semantic occupancy prediction -- where each voxel is assigned a semantic label -- annotated LiDAR point clouds are required, making data acquisition costly. In contrast, large-scale binary occupancy data, which only indicate occupied or free space without semantic labels, can be collected at a lower cost. Despite their availability, the potential of leveraging such data remains unexplored. In this study, we investigate the utilization of large-scale binary occupancy data from two perspectives: (1) pre-training and (2) learning-based auto-labeling. We propose a novel binary occupancy-based framework that decomposes the prediction process into binary and semantic occupancy modules, enabling effective use of binary occupancy data. Our experimental results demonstrate that the proposed framework outperforms existing methods in both pre-training and auto-labeling tasks, highlighting its effectiveness in enhancing 3D semantic occupancy prediction. The code is available at https://github.com/ToyotaInfoTech/b2s-occupancy
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</title>
<link>https://arxiv.org/abs/2507.14501</link>
<guid>https://arxiv.org/abs/2507.14501</guid>
<content:encoded><![CDATA[
arXiv:2507.14501v5 Announce Type: replace 
Abstract: 3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Topological and Geometric Embeddings for Point Cloud Recovery</title>
<link>https://arxiv.org/abs/2507.19121</link>
<guid>https://arxiv.org/abs/2507.19121</guid>
<content:encoded><![CDATA[
arXiv:2507.19121v3 Announce Type: replace 
Abstract: Recovering point clouds involves the sequential process of sampling and restoration, yet existing methods struggle to effectively leverage both topological and geometric attributes. To address this, we propose an end-to-end architecture named \textbf{TopGeoFormer}, which maintains these critical properties throughout the sampling and restoration phases. First, we revisit traditional feature extraction techniques to yield topological embedding using a continuous mapping of relative relationships between neighboring points, and integrate it in both phases for preserving the structure of the original space. Second, we propose the \textbf{InterTwining Attention} to fully merge topological and geometric embeddings, which queries shape with local awareness in both phases to form a learnable 3D shape context facilitated with point-wise, point-shape-wise, and intra-shape features. Third, we introduce a full geometry loss and a topological constraint loss to optimize the embeddings in both Euclidean and topological spaces. The geometry loss uses inconsistent matching between coarse-to-fine generations and targets for reconstructing better geometric details, and the constraint loss limits embedding variances for better approximation of the topological space. In experiments, we comprehensively analyze the circumstances using the conventional and learning-based sampling/upsampling/recovery algorithms. The quantitative and qualitative results demonstrate that our method significantly outperforms existing sampling and recovery methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2508.00493</link>
<guid>https://arxiv.org/abs/2508.00493</guid>
<content:encoded><![CDATA[
arXiv:2508.00493v2 Announce Type: replace 
Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral medical imaging that introduces spectral angle prompting to guide the Segment Anything Model (SAM) using spectral similarity alongside spatial cues. This early fusion of spectral information enables more accurate and robust segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0 achieves up to +3.8% higher Dice scores compared to RGB-only models and up to +3.1% over prior spectral fusion methods. Our approach enhances few-shot and zero-shot performance, demonstrating strong generalization in challenging low-data and noisy scenarios common in clinical imaging.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</title>
<link>https://arxiv.org/abs/2508.01171</link>
<guid>https://arxiv.org/abs/2508.01171</guid>
<content:encoded><![CDATA[
arXiv:2508.01171v2 Announce Type: replace 
Abstract: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification</title>
<link>https://arxiv.org/abs/2508.06831</link>
<guid>https://arxiv.org/abs/2508.06831</guid>
<content:encoded><![CDATA[
arXiv:2508.06831v2 Announce Type: replace 
Abstract: Adapting person re-identification (reID) models to new target environments remains a challenging problem that is typically addressed using unsupervised domain adaptation (UDA) methods. Recent works show that when labeled data originates from several distinct sources (e.g., datasets and cameras), considering each source separately and applying multi-source domain adaptation (MSDA) typically yields higher accuracy and robustness compared to blending the sources and performing conventional UDA. However, state-of-the-art MSDA methods learn domain-specific backbone models or require access to source domain data during adaptation, resulting in significant growth in training parameters and computational cost. In this paper, a Source-free Adaptive Gated Experts (SAGE-reID) method is introduced for person reID. Our SAGE-reID is a cost-effective, source-free MSDA method that first trains individual source-specific low-rank adapters (LoRA) through source-free UDA. Next, a lightweight gating network is introduced and trained to dynamically assign optimal merging weights for fusion of LoRA experts, enabling effective cross-domain knowledge transfer. While the number of backbone parameters remains constant across source domains, LoRA experts scale linearly but remain negligible in size (<= 2% of the backbone), reducing both the memory consumption and risk of overfitting. Extensive experiments conducted on three challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that SAGE-reID outperforms state-of-the-art methods while being computationally efficient.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method</title>
<link>https://arxiv.org/abs/2508.17054</link>
<guid>https://arxiv.org/abs/2508.17054</guid>
<content:encoded><![CDATA[
arXiv:2508.17054v3 Announce Type: replace 
Abstract: Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($\Delta$Flow), a lightweight 3D framework that captures motion cues via a $\Delta$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $\Delta$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework</title>
<link>https://arxiv.org/abs/2508.17061</link>
<guid>https://arxiv.org/abs/2508.17061</guid>
<content:encoded><![CDATA[
arXiv:2508.17061v2 Announce Type: replace 
Abstract: Photorealism is an important aspect of modern video games since it can shape player experience and impact immersion, narrative engagement, and visual fidelity. To achieve photorealism, beyond traditional rendering pipelines, generative models have been increasingly adopted as an effective approach for bridging the gap between the visual realism of synthetic and real worlds. However, under real-time constraints of video games, existing generative approaches continue to face a tradeoff between visual quality and runtime efficiency. In this work, we present a framework for enhancing the photorealism of rendered game frames using generative networks. We propose REGEN, which first employs a robust unpaired image-to-image translation model to generate semantically consistent photorealistic frames. These generated frames are then used to create a paired dataset, which transforms the problem to a simpler unpaired image-to-image translation. This enables training with a lightweight method, achieving real-time inference without compromising visual quality. We evaluate REGEN on Unreal Engine, showing, by employing the CMMD metric, that it achieves comparable or slightly improved visual quality compared to the robust method, while improving the frame rate by 12x. Additional experiments also validate that REGEN adheres to the semantic preservation of the initial robust image-to-image translation method and maintains temporal consistency. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title>
<link>https://arxiv.org/abs/2508.20072</link>
<guid>https://arxiv.org/abs/2508.20072</guid>
<content:encoded><![CDATA[
arXiv:2508.20072v3 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</title>
<link>https://arxiv.org/abs/2509.00767</link>
<guid>https://arxiv.org/abs/2509.00767</guid>
<content:encoded><![CDATA[
arXiv:2509.00767v2 Announce Type: replace 
Abstract: Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light</title>
<link>https://arxiv.org/abs/2509.06741</link>
<guid>https://arxiv.org/abs/2509.06741</guid>
<content:encoded><![CDATA[
arXiv:2509.06741v2 Announce Type: replace 
Abstract: Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest environments for tasks such as environmental monitoring and search and rescue, which require safe navigation through dense foliage and precise data collection. Traditional sensing approaches, including passive multispectral and RGB imaging, suffer from latency, poor depth resolution, and strong dependence on ambient light - especially under forest canopies. In this work, we present a novel event spectroscopy system that simultaneously enables high-resolution, low-latency depth reconstruction with integrated multispectral imaging using a single sensor. Depth is reconstructed using structured light, and by modulating the wavelength of the projected structured light, our system captures spectral information in controlled bands between 650 nm and 850 nm. We demonstrate up to $60\%$ improvement in RMSE over commercial depth sensors and validate the spectral accuracy against a reference spectrometer and commercial multispectral cameras, demonstrating comparable performance. A portable version limited to RGB (3 wavelengths) is used to collect real-world depth and spectral data from a Masoala Rainforest. We demonstrate the use of this prototype for color image reconstruction and material differentiation between leaves and branches using spectral and depth data. Our results show that adding depth (available at no extra effort with our setup) to material differentiation improves the accuracy by over $30\%$ compared to color-only method. Our system, tested in both lab and real-world rainforest environments, shows strong performance in depth estimation, RGB reconstruction, and material differentiation - paving the way for lightweight, integrated, and robust UAV perception and data collection in complex natural environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12715</link>
<guid>https://arxiv.org/abs/2509.12715</guid>
<content:encoded><![CDATA[
arXiv:2509.12715v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Encoding-Decoding Direction Pairs to Unveil Concepts of Influence in Deep Vision Networks</title>
<link>https://arxiv.org/abs/2509.23926</link>
<guid>https://arxiv.org/abs/2509.23926</guid>
<content:encoded><![CDATA[
arXiv:2509.23926v2 Announce Type: replace 
Abstract: Empirical evidence shows that deep vision networks often represent concepts as directions in latent space with concept information written along directional components in the vector representation of the input. However, the mechanism to encode (write) and decode (read) concept information to and from vector representations is not directly accessible as it constitutes a latent mechanism that naturally emerges from the training process of the network. Recovering this mechanism unlocks significant potential to open the black-box nature of deep networks, enabling understanding, debugging, and improving deep learning models. In this work, we propose an unsupervised method to recover this mechanism. For each concept, we explain that under the hypothesis of linear concept representations, this mechanism can be implemented with the help of two directions: the first facilitating encoding of concept information and the second facilitating decoding. Unlike prior matrix decomposition, autoencoder, or dictionary learning methods that rely on feature reconstruction, we propose a new perspective: decoding directions are identified via directional clustering of activations, and encoding directions are estimated with signal vectors under a probabilistic view. We further leverage network weights through a novel technique, Uncertainty Region Alignment, which reveals interpretable directions affecting predictions. Our analysis shows that (a) on synthetic data, our method recovers ground-truth direction pairs; (b) on real data, decoding directions map to monosemantic, interpretable concepts and outperform unsupervised baselines; and (c) signal vectors faithfully estimate encoding directions, validated via activation maximization. Finally, we demonstrate applications in understanding global model behavior, explaining individual predictions, and intervening to produce counterfactuals or correct errors.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Learning of Canonical Parameterizations of $2D$-curves</title>
<link>https://arxiv.org/abs/2509.26070</link>
<guid>https://arxiv.org/abs/2509.26070</guid>
<content:encoded><![CDATA[
arXiv:2509.26070v2 Announce Type: replace 
Abstract: Most datasets encountered in computer vision and medical applications present symmetries that should be taken into account in classification tasks. A typical example is the symmetry by rotation and/or scaling in object detection. A common way to build neural networks that learn the symmetries is to use data augmentation. In order to avoid data augmentation and build more sustainable algorithms, we present an alternative method to mod out symmetries based on the notion of section of a principal fiber bundle. This framework allows the use of simple metrics on the space of objects in order to measure dissimilarities between orbits of objects under the symmetry group. Moreover, the section used can be optimized to maximize separation of classes. We illustrate this methodology on a dataset of contours of objects for the groups of translations, rotations, scalings and reparameterizations. In particular, we present a $2$-parameter family of canonical parameterizations of curves, containing the constant-speed parameterization as a special case, which we believe is interesting in its own right. We hope that this simple application will serve to convey the geometric concepts underlying this method, which have a wide range of possible applications. The code is available at the following link: $\href{https://github.com/GiLonga/Geometric-Learning}{https://github.com/GiLonga/Geometric-Learning}$. A tutorial notebook showcasing an application of the code to a specific dataset is available at the following link: $\href{https://github.com/ioanaciuclea/geometric-learning-notebook}{https://github.com/ioanaciuclea/geometric-learning-notebook}$
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints</title>
<link>https://arxiv.org/abs/2510.04840</link>
<guid>https://arxiv.org/abs/2510.04840</guid>
<content:encoded><![CDATA[
arXiv:2510.04840v3 Announce Type: replace 
Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is essential for its optimal operation and maintenance. However, such a model may not be easily available. This work introduces a novel approach for PV power plant mapping based on aerial overview images. It enables the automation of the mapping process while removing the reliance on third-party data. The presented mapping method takes advantage of the structural layout of the power plants to achieve detailed modeling down to the level of individual PV modules. The approach relies on visual segmentation of PV modules in overview images and the inference of structural information in each image, assigning modules to individual benches, rows, and columns. We identify visual keypoints related to the layout and use these to merge detections from multiple images while maintaining their structural integrity. The presented method was experimentally verified and evaluated on two different power plants. The final fusion of 3D positions and semantic structures results in a compact georeferenced model suitable for power plant maintenance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2510.11496</link>
<guid>https://arxiv.org/abs/2510.11496</guid>
<content:encoded><![CDATA[
arXiv:2510.11496v3 Announce Type: replace 
Abstract: In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2510.14836</link>
<guid>https://arxiv.org/abs/2510.14836</guid>
<content:encoded><![CDATA[
arXiv:2510.14836v2 Announce Type: replace 
Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</title>
<link>https://arxiv.org/abs/2510.16416</link>
<guid>https://arxiv.org/abs/2510.16416</guid>
<content:encoded><![CDATA[
arXiv:2510.16416v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</title>
<link>https://arxiv.org/abs/2510.20155</link>
<guid>https://arxiv.org/abs/2510.20155</guid>
<content:encoded><![CDATA[
arXiv:2510.20155v2 Announce Type: replace 
Abstract: Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2510.20162</link>
<guid>https://arxiv.org/abs/2510.20162</guid>
<content:encoded><![CDATA[
arXiv:2510.20162v2 Announce Type: replace 
Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at https://github.com/xud-yan/TOMCAT .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EIRES:Training-free AI-Generated Image Detection via Edit-Induced Reconstruction Error Shift</title>
<link>https://arxiv.org/abs/2510.25141</link>
<guid>https://arxiv.org/abs/2510.25141</guid>
<content:encoded><![CDATA[
arXiv:2510.25141v2 Announce Type: replace 
Abstract: Diffusion models have recently achieved remarkable photorealism, making it increasingly difficult to distinguish real images from generated ones, raising significant privacy and security concerns. In response, we present a key finding: structural edits enhance the reconstruction of real images while degrading that of generated images, creating a distinctive edit-induced reconstruction error shift. This asymmetric shift enhances the separability between real and generated images. Building on this insight, we propose EIRES, a training-free method that leverages structural edits to reveal inherent differences between real and generated images. To explain the discriminative power of this shift, we derive the reconstruction error lower bound under edit perturbations. Since EIRES requires no training, thresholding depends solely on the natural separability of the signal, where a larger margin yields more reliable detection. Extensive experiments show that EIRES is effective across diverse generative models and remains robust on the unbiased subset, even under post-processing operations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionRAG: Region-level Retrieval-Augmented Generation for Visual Document Understanding</title>
<link>https://arxiv.org/abs/2510.27261</link>
<guid>https://arxiv.org/abs/2510.27261</guid>
<content:encoded><![CDATA[
arXiv:2510.27261v3 Announce Type: replace 
Abstract: Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method for empowering LLMs by leveraging candidate visual documents. However, current methods consider the entire document as the basic retrieval unit, introducing substantial irrelevant visual content in two ways: 1) Relevant documents often contain large regions unrelated to the query, diluting the focus on salient information; 2) Retrieving multiple documents to increase recall further introduces redundant and irrelevant documents. These redundant contexts distract the model's attention and further degrade the performance. To address this challenge, we propose RegionRAG, a novel framework that shifts the retrieval paradigm from the document level to the region level. During training, we design a hybrid supervision strategy from both labeled data and unlabeled data to pinpoint relevant patches. During inference, we propose a dynamic pipeline that intelligently groups salient patches into complete semantic regions. By delegating the task of identifying relevant regions to the retriever, RegionRAG enables the generator to focus solely on concise, query-relevant visual content, improving both efficiency and accuracy. Experiments on six benchmarks demonstrate that RegionRAG achieves state-of-the-art performance. It improves retrieval accuracy by 10.02% in R@1 on average, and boosts question answering accuracy by 3.56% while using only 71.42% visual tokens compared with prior methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.00908</link>
<guid>https://arxiv.org/abs/2511.00908</guid>
<content:encoded><![CDATA[
arXiv:2511.00908v2 Announce Type: replace 
Abstract: Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Guidance through Calibration and Regularization</title>
<link>https://arxiv.org/abs/2511.05844</link>
<guid>https://arxiv.org/abs/2511.05844</guid>
<content:encoded><![CDATA[
arXiv:2511.05844v3 Announce Type: replace 
Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound</title>
<link>https://arxiv.org/abs/2511.10412</link>
<guid>https://arxiv.org/abs/2511.10412</guid>
<content:encoded><![CDATA[
arXiv:2511.10412v2 Announce Type: replace 
Abstract: The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging.
  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.
  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\pm$ 1.98mm and a mean rotation error of 5.31 $\pm$ 3.945$^\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2511.13031</link>
<guid>https://arxiv.org/abs/2511.13031</guid>
<content:encoded><![CDATA[
arXiv:2511.13031v3 Announce Type: replace 
Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</title>
<link>https://arxiv.org/abs/2511.13704</link>
<guid>https://arxiv.org/abs/2511.13704</guid>
<content:encoded><![CDATA[
arXiv:2511.13704v2 Announce Type: replace 
Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Language Models are Confused Tourists</title>
<link>https://arxiv.org/abs/2511.17004</link>
<guid>https://arxiv.org/abs/2511.17004</guid>
<content:encoded><![CDATA[
arXiv:2511.17004v2 Announce Type: replace 
Abstract: Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRIC: Bridging Kinematic Plans and Physical Control at Test Time</title>
<link>https://arxiv.org/abs/2511.20431</link>
<guid>https://arxiv.org/abs/2511.20431</guid>
<content:encoded><![CDATA[
arXiv:2511.20431v3 Announce Type: replace 
Abstract: We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs</title>
<link>https://arxiv.org/abs/2512.00086</link>
<guid>https://arxiv.org/abs/2512.00086</guid>
<content:encoded><![CDATA[
arXiv:2512.00086v2 Announce Type: replace 
Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $\mu$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</title>
<link>https://arxiv.org/abs/2512.01701</link>
<guid>https://arxiv.org/abs/2512.01701</guid>
<content:encoded><![CDATA[
arXiv:2512.01701v2 Announce Type: replace 
Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking</title>
<link>https://arxiv.org/abs/2512.02789</link>
<guid>https://arxiv.org/abs/2512.02789</guid>
<content:encoded><![CDATA[
arXiv:2512.02789v2 Announce Type: replace 
Abstract: The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</title>
<link>https://arxiv.org/abs/2512.07276</link>
<guid>https://arxiv.org/abs/2512.07276</guid>
<content:encoded><![CDATA[
arXiv:2512.07276v2 Announce Type: replace 
Abstract: Three-dimensional geospatial analysis is critical for applications in urban planning, climate adaptation, and environmental assessment. However, current methodologies depend on costly, specialized sensors, such as LiDAR and multispectral sensors, which restrict global accessibility. Additionally, existing sensor-based and rule-driven methods struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We present Geo3DVQA, a comprehensive benchmark that evaluates vision-language models (VLMs) in height-aware 3D geospatial reasoning from RGB imagery alone. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios integrating elevation, sky view factors, and land cover patterns. The benchmark comprises 110k curated question-answer pairs across 16 task categories, including single-feature inference, multi-feature reasoning, and application-level analysis. Through a systematic evaluation of ten state-of-the-art VLMs, we reveal fundamental limitations in RGB-to-3D spatial reasoning. Our results further show that domain-specific instruction tuning consistently enhances model performance across all task categories, including height-aware and open-ended, application-oriented reasoning. Geo3DVQA provides a unified, interpretable framework for evaluating RGB-based 3D geospatial reasoning and identifies key challenges and opportunities for scalable 3D spatial analysis. The code and data are available at https://github.com/mm1129/Geo3DVQA.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method</title>
<link>https://arxiv.org/abs/2512.07651</link>
<guid>https://arxiv.org/abs/2512.07651</guid>
<content:encoded><![CDATA[
arXiv:2512.07651v2 Announce Type: replace 
Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment</title>
<link>https://arxiv.org/abs/2512.08930</link>
<guid>https://arxiv.org/abs/2512.08930</guid>
<content:encoded><![CDATA[
arXiv:2512.08930v2 Announce Type: replace 
Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors</title>
<link>https://arxiv.org/abs/2512.09056</link>
<guid>https://arxiv.org/abs/2512.09056</guid>
<content:encoded><![CDATA[
arXiv:2512.09056v2 Announce Type: replace 
Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent Density Estimation</title>
<link>https://arxiv.org/abs/2512.10067</link>
<guid>https://arxiv.org/abs/2512.10067</guid>
<content:encoded><![CDATA[
arXiv:2512.10067v2 Announce Type: replace 
Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Frequency Matters: Uncertainty Guided Image Compression with Wavelet Diffusion</title>
<link>https://arxiv.org/abs/2407.12538</link>
<guid>https://arxiv.org/abs/2407.12538</guid>
<content:encoded><![CDATA[
arXiv:2407.12538v3 Announce Type: replace-cross 
Abstract: Diffusion probabilistic models have recently achieved remarkable success in generating high-quality images. However, balancing high perceptual quality and low distortion remains challenging in application of diffusion models in image compression. To address this issue, we propose a novel Uncertainty-Guided image compression approach with wavelet Diffusion (UGDiff). Our approach focuses on high frequency compression via the wavelet transform, since high frequency components are crucial for reconstructing image details. We introduce a wavelet conditional diffusion model for high frequency prediction, followed by a residual codec that compresses and transmits prediction residuals to the decoder. This diffusion prediction-then-residual compression paradigm effectively addresses the low fidelity issue common in direct reconstructions by existing diffusion models. Considering the uncertainty from the random sampling of the diffusion model, we further design an uncertainty-weighted rate-distortion (R-D) loss tailored for residual compression, providing a more rational trade-off between rate and distortion. Comprehensive experiments on two benchmark datasets validate the effectiveness of UGDiff, surpassing state-of-the-art image compression methods in R-D performance, perceptual quality, subjective quality, and inference time. Our code is available at: https://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implementation of neural network operators with applications to remote sensing data</title>
<link>https://arxiv.org/abs/2412.00375</link>
<guid>https://arxiv.org/abs/2412.00375</guid>
<content:encoded><![CDATA[
arXiv:2412.00375v2 Announce Type: replace-cross 
Abstract: In this paper, we provide two algorithms based on the theory of multidimensional neural network (NN) operators activated by hyperbolic tangent sigmoidal functions. Theoretical results are recalled to justify the performance of the here implemented algorithms. Specifically, the first algorithm models multidimensional signals (such as digital images), while the second one addresses the problem of rescaling and enhancement of the considered data. We discuss several applications of the NN-based algorithms for modeling and rescaling/enhancement remote sensing data (represented as images), with numerical experiments conducted on a selection of remote sensing (RS) images from the (open access) RETINA dataset. A comparison with classical interpolation methods, such as bilinear and bicubic interpolation, shows that the proposed algorithms outperform the others, particularly in terms of the Structural Similarity Index (SSIM).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveOrder: A differentiable wave-optical framework for scalable biological microscopy with diverse modalities</title>
<link>https://arxiv.org/abs/2412.09775</link>
<guid>https://arxiv.org/abs/2412.09775</guid>
<content:encoded><![CDATA[
arXiv:2412.09775v3 Announce Type: replace-cross 
Abstract: Correlative computational microscopy can accelerate imaging and modeling of cellular dynamics by relaxing trade-offs inherent to dynamic imaging. Existing computational microscopy frameworks are either specialized or overly generic, limiting use to fixed configurations or domain experts. We introduce WaveOrder, a generalist wave-optical framework for imaging the architectural order of biomolecules. WaveOrder reconstructs diverse specimen properties from multi-channel acquisitions, with or without fluorescence. It provides a unified representation of linear optical properties and differentiable physics-based image formation models spanning widefield, confocal, light-sheet, and oblique label-free geometries. WaveOrder uses physics-informed ML to auto-tune model parameters and solve blind shift-variant restoration problems. This open-source, PyTorch-based framework enables scalable quantitative imaging across scales from organelles to adult zebrafish, and improves restoration of cellular structures in high-throughput experiments. We validate WaveOrder on diverse imaging applications, demonstrating its ability to recover biomolecular structure beyond the limits of existing approaches.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIGUIQA: A Physical Imaging Guided Perceptual Framework for Underwater Image Quality Assessment</title>
<link>https://arxiv.org/abs/2412.15527</link>
<guid>https://arxiv.org/abs/2412.15527</guid>
<content:encoded><![CDATA[
arXiv:2412.15527v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a Physical Imaging Guided perceptual framework for Underwater Image Quality Assessment (UIQA), termed PIGUIQA. First, we formulate UIQA as a comprehensive problem that considers the combined effects of direct transmission attenuation and backward scattering on image perception. By leveraging underwater radiative transfer theory, we systematically integrate physics-based imaging estimations to establish quantitative metrics for these distortions. Second, recognizing spatial variations in image content significance and human perceptual sensitivity to distortions, we design a module built upon a neighborhood attention mechanism for local perception of images. This module effectively captures subtle features in images, thereby enhancing the adaptive perception of distortions on the basis of local information. Third, by employing a global perceptual aggregator that further integrates holistic image scene with underwater distortion information, the proposed model accurately predicts image quality scores. Extensive experiments across multiple benchmarks demonstrate that PIGUIQA achieves state-of-the-art performance while maintaining robust cross-dataset generalizability. The implementation is publicly available at https://github.com/WeizhiXian/PIGUIQA
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Advances in 3D Object and Scene Generation: A Survey</title>
<link>https://arxiv.org/abs/2504.11734</link>
<guid>https://arxiv.org/abs/2504.11734</guid>
<content:encoded><![CDATA[
arXiv:2504.11734v2 Announce Type: replace-cross 
Abstract: In recent years, the demand for 3D content has grown exponentially with the intelligent upgrade of interactive media, extended reality (XR), and Metaverse industries. In order to overcome the limitations of traditional manual modeling approaches, such as labor-intensive workflows and prolonged production cycles, revolutionary advances have been achieved through the convergence of novel 3D representation paradigms and artificial intelligence generative technologies. In this survey, we conduct a systematic review of the cutting-edge achievements in static 3D object and scene generation, as well as establish a comprehensive technical framework through systematic categorization. We start our analysis with mainstream 3D object representations. Subsequently, we delve into the technical pathways of 3D object generation based on four mainstream deep generative models: Variational Autoencoders, Generative Adversarial Networks, Autoregressive Models, and Diffusion Models. Regarding scene generation, we focus on three dominant paradigms: layout-guided generation, lifting based on 2D priors, and rule-driven modeling. Finally, we critically examine persistent challenges in 3D generation and propose potential research directions for future investigation. This survey aims to provide readers with a structured understanding of state-of-the-art 3D generation technologies while inspiring researchers to undertake more exploration in this domain.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v2 Announce Type: replace-cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERDI: VLM-Embedded Reasoning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15925</link>
<guid>https://arxiv.org/abs/2505.15925</guid>
<content:encoded><![CDATA[
arXiv:2505.15925v3 Announce Type: replace-cross 
Abstract: While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps</title>
<link>https://arxiv.org/abs/2506.15849</link>
<guid>https://arxiv.org/abs/2506.15849</guid>
<content:encoded><![CDATA[
arXiv:2506.15849v2 Announce Type: replace-cross 
Abstract: We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subspace-Boosted Model Merging</title>
<link>https://arxiv.org/abs/2506.16506</link>
<guid>https://arxiv.org/abs/2506.16506</guid>
<content:encoded><![CDATA[
arXiv:2506.16506v3 Announce Type: replace-cross 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we empirically and theoretically analyze this limitation, proving that for Task Arithmetic-based methods, as more experts are merged, the common information dominates the task-specific information, leading to inevitable rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging. Code and models are available at https://github.com/ronskoro/Subspace-Boosting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity</title>
<link>https://arxiv.org/abs/2506.23484</link>
<guid>https://arxiv.org/abs/2506.23484</guid>
<content:encoded><![CDATA[
arXiv:2506.23484v4 Announce Type: replace-cross 
Abstract: AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v4 Announce Type: replace-cross 
Abstract: Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for "one prior, many embodiments": strong, inexpensive video priors together with minimal on-robot alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models</title>
<link>https://arxiv.org/abs/2507.13993</link>
<guid>https://arxiv.org/abs/2507.13993</guid>
<content:encoded><![CDATA[
arXiv:2507.13993v3 Announce Type: replace-cross 
Abstract: The growing volume of medical imaging data has increased the need for automated diagnostic tools, especially for musculoskeletal injuries like rib fractures, commonly detected via CT scans. Manual interpretation is time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep learning framework for rib fracture diagnosis and report generation. It integrates a YOLOv9 model for fracture detection, a medical knowledge graph for retrieving clinical context, and a fine-tuned LLaVA language model for generating diagnostic reports. OrthoInsight combines visual features from CT images with expert textual data to deliver clinically useful outputs. Evaluated on 28,675 annotated CT images and expert reports, it achieves high performance across Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value, with an average score of 4.28, outperforming models like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal learning in transforming medical image analysis and providing effective support for radiologists.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising</title>
<link>https://arxiv.org/abs/2508.15553</link>
<guid>https://arxiv.org/abs/2508.15553</guid>
<content:encoded><![CDATA[
arXiv:2508.15553v2 Announce Type: replace-cross 
Abstract: Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Benefits and Pitfalls of Current Methods for the Segmentation of Undersampled MRI Data</title>
<link>https://arxiv.org/abs/2508.18975</link>
<guid>https://arxiv.org/abs/2508.18975</guid>
<content:encoded><![CDATA[
arXiv:2508.18975v2 Announce Type: replace-cross 
Abstract: MR imaging is a valuable diagnostic tool allowing to non-invasively visualize patient anatomy and pathology with high soft-tissue contrast. However, MRI acquisition is typically time-consuming, leading to patient discomfort and increased costs to the healthcare system. Recent years have seen substantial research effort into the development of methods that allow for accelerated MRI acquisition while still obtaining a reconstruction that appears similar to the fully-sampled MR image. However, for many applications a perfectly reconstructed MR image may not be necessary, particularly, when the primary goal is a downstream task such as segmentation. This has led to growing interest in methods that aim to perform segmentation directly on accelerated MRI data. Despite recent advances, existing methods have largely been developed in isolation, without direct comparison to one another, often using separate or private datasets, and lacking unified evaluation standards. To date, no high-quality, comprehensive comparison of these methods exists, and the optimal strategy for segmenting accelerated MR data remains unknown. This paper provides the first unified benchmark for the segmentation of undersampled MRI data comparing 7 approaches. A particular focus is placed on comparing \textit{one-stage approaches}, that combine reconstruction and segmentation into a unified model, with \textit{two-stage approaches}, that utilize established MRI reconstruction methods followed by a segmentation network. We test these methods on two MRI datasets that include multi-coil k-space data as well as a human-annotated segmentation ground-truth. We find that simple two-stage methods that consider data-consistency lead to the best segmentation scores, surpassing complex specialized methods that are developed specifically for this task.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling spectral filtering effects on color-matching functions: Implications for observer variability</title>
<link>https://arxiv.org/abs/2508.19291</link>
<guid>https://arxiv.org/abs/2508.19291</guid>
<content:encoded><![CDATA[
arXiv:2508.19291v2 Announce Type: replace-cross 
Abstract: This study investigates the impact of spectral filtering on color-matching functions (CMFs) and its implications for observer variability modeling. We conducted color matching experiments with two observers, both with and without a spectral filter in front of a bipartite field. Using a novel computational approach, we estimated the filter transmittance and transformation matrix necessary to convert unfiltered CMFs to filtered CMFs. Statistical analysis revealed good agreement between estimated and measured filter characteristics, particularly in central wavelength regions. Applying this methodology to compare between Stiles and Burch 1955 (SB1955) mean observer CMFs and our previously published "ICVIO" mean observer CMFs, we identified a "yellow" (short-wavelength suppressing) filter that effectively transforms between these datasets. This finding aligns with our hypothesis that observed differences between the CMF sets are attributable to age-related lens yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955). Our approach enables efficient representation of observer variability through a single filter rather than three separate functions, offering potentially reduced experimental overhead while maintaining accuracy in characterizing individual color vision differences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Resolution UDF Meshing via Iterative Networks</title>
<link>https://arxiv.org/abs/2509.17212</link>
<guid>https://arxiv.org/abs/2509.17212</guid>
<content:encoded><![CDATA[
arXiv:2509.17212v3 Announce Type: replace-cross 
Abstract: Unsigned Distance Fields (UDFs) are a natural implicit representation for open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to triangulate into explicit meshes. This is especially true at high resolutions where neural UDFs exhibit higher noise levels, which makes it hard to capture fine details. Most current techniques perform within single voxels without reference to their neighborhood, resulting in missing surface and holes where the UDF is ambiguous or noisy. We show that this can be remedied by performing several passes and by reasoning on previously extracted surface elements to incorporate neighborhood information. Our key contribution is an iterative neural network that does this and progressively improves surface recovery within each voxel by spatially propagating information from increasingly distant neighbors. Unlike single-pass methods, our approach integrates newly detected surfaces, distance values, and gradients across multiple iterations, effectively correcting errors and stabilizing extraction in challenging regions. Experiments on diverse 3D models demonstrate that our method produces significantly more accurate and complete meshes than existing approaches, particularly for complex geometries, enabling UDF surface extraction at higher resolutions where traditional methods fail.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows</title>
<link>https://arxiv.org/abs/2509.20490</link>
<guid>https://arxiv.org/abs/2509.20490</guid>
<content:encoded><![CDATA[
arXiv:2509.20490v3 Announce Type: replace-cross 
Abstract: Agentic systems offer a potential path to solve complex clinical tasks through collaboration among specialized agents, augmented by tool use and external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation, prevailing methods remain limited: (i) reasoning is frequently neither clinically interpretable nor aligned with guidelines, reflecting mere aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused, yielding text-only rationales that are not visually grounded; and (iii) systems rarely detect or resolve cross-tool inconsistencies and provide no principled verification mechanisms. To bridge the above gaps, we present RadAgents, a multi-agent framework that couples clinical priors with task-aware multimodal reasoning and encodes a radiologist-style workflow into a modular, auditable pipeline. In addition, we integrate grounding and multimodal retrieval-augmentation to verify and resolve context conflicts, resulting in outputs that are more reliable, transparent, and consistent with clinical practice.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury</title>
<link>https://arxiv.org/abs/2510.03248</link>
<guid>https://arxiv.org/abs/2510.03248</guid>
<content:encoded><![CDATA[
arXiv:2510.03248v2 Announce Type: replace-cross 
Abstract: Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title>
<link>https://arxiv.org/abs/2511.07329</link>
<guid>https://arxiv.org/abs/2511.07329</guid>
<content:encoded><![CDATA[
arXiv:2511.07329v2 Announce Type: replace-cross 
Abstract: It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</title>
<link>https://arxiv.org/abs/2511.11512</link>
<guid>https://arxiv.org/abs/2511.11512</guid>
<content:encoded><![CDATA[
arXiv:2511.11512v2 Announce Type: replace-cross 
Abstract: Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v4 Announce Type: replace-cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</title>
<link>https://arxiv.org/abs/2511.16268</link>
<guid>https://arxiv.org/abs/2511.16268</guid>
<content:encoded><![CDATA[
arXiv:2511.16268v2 Announce Type: replace-cross 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.20732</link>
<guid>https://arxiv.org/abs/2511.20732</guid>
<content:encoded><![CDATA[
arXiv:2511.20732v2 Announce Type: replace-cross 
Abstract: Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v2 Announce Type: replace-cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v2 Announce Type: replace-cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
<div> Keywords: seismic processing, deep learning, ContextSeisNet, spatial consistency, demultiple  

<br /><br />Summary:  
Seismic processing converts raw seismic data into images of subsurface structures, which are critical for geophysical applications. Traditional seismic demultiple methods often struggle with noisy data and require manual parameter tuning, limiting their effectiveness. Deep learning approaches have been proposed to address some of these issues but typically suffer from spatial inconsistency across neighboring seismic gathers and lack mechanisms for user control. To overcome these challenges, the authors present ContextSeisNet, a novel in-context learning model tailored for seismic demultiple processing. ContextSeisNet conditions its predictions on a support set composed of spatially related example pairs—neighboring common-depth point gathers along the same seismic line and their labels—enabling it to adapt processing behavior dynamically at inference time without retraining. This framework enhances lateral spatial consistency and offers flexibility through user-defined examples. Evaluations on synthetic data show that ContextSeisNet outperforms a U-Net baseline both quantitatively and in spatial coherence. Field data experiments demonstrate superior lateral consistency compared to traditional Radon demultiple methods and the U-Net model, with improved near-offset performance and more complete removal of multiples. Remarkably, ContextSeisNet achieves comparable field results while being trained on 90% less data, highlighting its data efficiency. These findings position ContextSeisNet as a promising, practical solution for spatially consistent seismic demultiple, with potential applications across broader seismic processing tasks. <div>
arXiv:2512.11575v2 Announce Type: replace 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
<div> Fine-tuning, Large Language Models, Safety Alignment, LookAhead Tuning, Model Adaptation<br /><br />Summary:<br /><br />Fine-tuning large language models (LLMs) allows them to adapt to specific domains, but it frequently causes a loss of previously established safety alignment. To address this, the authors propose LookAhead Tuning, a lightweight and data-driven method designed to preserve model safety during the fine-tuning process. This approach incorporates two straightforward strategies that modify training data by previewing partial answer prefixes, which helps minimize disruption to the model’s initial token probability distributions. By doing so, the method effectively maintains the model's inherent safety mechanisms without introducing significant modifications. Comprehensive experiments validate that LookAhead Tuning successfully retains safety standards while supporting robust performance on a variety of downstream tasks. The findings highlight that this technique offers a reliable and efficient solution for safely adapting LLMs to new domains, balancing both safety retention and task effectiveness. Overall, LookAhead Tuning presents a practical advancement for maintaining safety in LLM fine-tuning workflows without compromising their downstream capabilities. <div>
arXiv:2503.19041v4 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Agent: An Interactive Video Search System Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16925</link>
<guid>https://arxiv.org/abs/2512.16925</guid>
<content:encoded><![CDATA[
<div> Keywords: V-Agent, vision-language model, video search, multimodal retrieval, MultiVENT 2.0  

<br /><br />Summary:  
This paper presents V-Agent, a novel multi-agent system designed to enhance advanced video search and interactive user conversations. The core innovation lies in fine-tuning a vision-language model (VLM) using a small video preference dataset, which, combined with a retrieval vector from an image-text retrieval model, surpasses traditional text-based retrieval methods in handling multimodal data. V-Agent’s retrieval model independently encodes video frames and audio transcriptions generated by an automatic speech recognition (ASR) module into a unified multimodal representation space, enabling the system to understand both visual and spoken content for more context-aware video searching. The framework is composed of three collaborative agents: a routing agent that directs user intents, a search agent leveraging the enhanced VLM-based retrieval along with a re-ranking module to improve the search output quality, and a chat agent that interacts with users to refine results and address queries. Experimental results demonstrate that V-Agent achieves state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, showcasing its effectiveness and potential applicability in both research and practical multimedia retrieval scenarios. <div>
arXiv:2512.16925v1 Announce Type: new 
Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content</title>
<link>https://arxiv.org/abs/2512.16947</link>
<guid>https://arxiv.org/abs/2512.16947</guid>
<content:encoded><![CDATA[
<div> Keywords: pornographic content detection, CNN, VGG-16, deep learning, Indonesia website blocking<br /><br />Summary:<br /><br />1. In 2020, the Indonesian government blocked 59,741 websites for containing negative content, including 14,266 identified as pornographic, but these sites remained accessible via VPNs.<br /><br />2. This accessibility issue motivated the development of a fast and accurate system to identify pornographic website content automatically.<br /><br />3. The study employed deep learning techniques, specifically comparing a custom convolutional neural network (CNN) model with the Visual Geometry Group 16 (VGG-16) pre-trained model, to detect pornographic images.<br /><br />4. Both models were comprehensively evaluated to determine which was more effective for rapid and accurate content identification.<br /><br />5. Experimental results demonstrated that the CNN model outperformed VGG-16, achieving the highest accuracy of 94.87% at 50 epochs and a learning rate of 0.001, indicating CNN’s superior capability for quick and precise pornographic content detection.<br /><br />6. The findings suggest that implementing CNN-based detectors could significantly aid in controlling access to prohibited content on the internet, enhancing content filtering efforts in Indonesia and similar contexts. <div>
arXiv:2512.16947v1 Announce Type: new 
Abstract: In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals</title>
<link>https://arxiv.org/abs/2512.16948</link>
<guid>https://arxiv.org/abs/2512.16948</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive Visual Model, Vision Transformer, neural response, cross-dataset adaptation, cortical modeling<br /><br />Summary:<br /><br />1. Deep learning models for neural response simulation often struggle to differentiate between stable visual encoding and condition-specific adaptation, limiting their generalization across different stimuli and subjects.<br /><br />2. The Adaptive Visual Model (AVM) is introduced as a framework that preserves core visual representations while enabling condition-aware adaptation through modular subnetworks, maintaining a frozen Vision Transformer encoder.<br /><br />3. AVM's modular design includes independently trained modulation paths that capture neural response variability due to stimulus content and subject identity without altering the primary encoder.<br /><br />4. The model was evaluated under three experimental paradigms: stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all reflecting structured changes in inputs and individuals.<br /><br />5. Tested on two large-scale mouse V1 datasets, AVM outperformed the state-of-the-art V1T model by roughly 2% in predictive correlation, achieved a 9.1% gain in explained variance (FEVE) during cross-dataset adaptation, demonstrating robust generalization, interpretable condition-specific modulation, and architectural efficiency.<br /><br />6. The AVM framework offers a unified and scalable approach for adaptive neural modeling across biological and experimental conditions, potentially guiding future cortical modeling in neuroscience and biologically inspired AI systems. <div>
arXiv:2512.16948v1 Announce Type: new 
Abstract: While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</title>
<link>https://arxiv.org/abs/2512.16950</link>
<guid>https://arxiv.org/abs/2512.16950</guid>
<content:encoded><![CDATA[
<div> Tree species classification, Terrestrial Laser Scanning (TLS), YOLOv8, Finer-CAM, deep learning<br /><br />Summary:<br /><br />1. The study addresses the challenge of classifying tree species using Terrestrial Laser Scanning (TLS) data combined with deep learning techniques, specifically focusing on the interpretability of classification models. <br /><br />2. Researchers trained five YOLOv8 models on TLS data from 2,445 trees representing seven European species, achieving a high mean classification accuracy of 96% with low variance. <br /><br />3. They proposed a novel method linking Finer-CAM (Class Activation Mapping) explanations to TLS projection segments, allowing systematic identification of which tree structural features drive species discrimination. <br /><br />4. Analysis of 630 saliency maps revealed that model decisions predominantly rely on crown features for most species such as Silver Birch, European Beech, English Oak, and Norway Spruce, while stem and finer branch features contribute more to classification of European Ash, Scots Pine, and Douglas Fir. <br /><br />5. The model’s notion of species similarity aligned well with expert human classification, highlighting the importance of model interpretability to better understand data biases, model limitations, and to increase confidence in automated tree species identification. <div>
arXiv:2512.16950v1 Announce Type: new 
Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories</title>
<link>https://arxiv.org/abs/2512.16954</link>
<guid>https://arxiv.org/abs/2512.16954</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation, character consistency, large language model, text-to-image, cultural bias

<br /><br />Summary: This paper addresses the challenge of generating long and cohesive video stories with consistent characters using text-to-video AI. The authors propose a novel multi-stage pipeline that mimics the filmmaking process. First, a large language model generates a detailed production script that guides the overall narrative. Next, a text-to-image model uses this script to create consistent visual representations of each character, which act as visual anchors. Finally, these visuals guide a video generation model to synthesize individual scenes, enhancing character identity preservation. Comparative experiments demonstrate the critical importance of the visual anchoring mechanism; eliminating this step causes a drastic drop in character consistency scores, confirming visual priors are essential. Additionally, the study investigates cultural differences in current video generation models, identifying biases related to subject consistency and dynamic expression when comparing Indian-themed versus Western-themed video outputs. The findings highlight the need for culturally aware and multi-step approaches to improve identity consistency and reliability in long video story generation. <div>
arXiv:2512.16954v1 Announce Type: new 
Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</title>
<link>https://arxiv.org/abs/2512.16975</link>
<guid>https://arxiv.org/abs/2512.16975</guid>
<content:encoded><![CDATA[
<div> Keywords: video tokenization, adaptive compression, information theory, transformer, ELBO algorithm<br /><br />Summary:<br /><br />1. The paper addresses the challenge of discrete video tokenization for long video sequences, emphasizing the need for accurate and efficient compression methods due to varying information density in video content. 2. Current video tokenizers compress all video content at a fixed rate, which causes redundancy or loss of important information. 3. Inspired by Shannon's information theory, the authors introduce InfoTok, a new framework for adaptive video tokenization that dynamically adjusts token allocation based on informational richness. 4. The paper proves that existing data-agnostic training methods result in suboptimal representation lengths and proposes an evidence lower bound (ELBO)-based algorithm that approaches optimal compression. 5. Leveraging this theoretical framework, the authors develop a transformer-based adaptive compressor that significantly improves compression efficiency. Empirical results show InfoTok saves 20% of tokens without degrading performance and achieves a 2.3x compression rate while outperforming heuristic adaptive approaches. 6. InfoTok’s approach optimally allocates tokens according to the information content of video segments, enabling more compact yet accurate video representations and offering valuable insights for future video understanding and compression research. <div>
arXiv:2512.16975v1 Announce Type: new 
Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video</title>
<link>https://arxiv.org/abs/2512.16977</link>
<guid>https://arxiv.org/abs/2512.16977</guid>
<content:encoded><![CDATA[
<div> Endoscopic video segmentation, Semi-supervised learning, Pseudo-labeling, Mutual learning, Spatiotemporal correction<br /><br />Summary:<br /><br />This paper introduces Endo-SemiS, a semi-supervised framework designed for accurate segmentation of endoscopic video frames using limited annotated data. The method incorporates four core strategies to leverage both labeled and unlabeled data effectively: (1) Cross-supervision between two individual networks, where each network supervises the other to enhance learning; (2) Generation of uncertainty-guided pseudo-labels by selecting high-confidence regions in unlabeled data to ensure label quality; (3) Joint pseudo-label supervision, which combines reliable pixel information from both networks' pseudo-labels, providing precise supervision for unlabeled samples; and (4) Mutual learning, enabling both networks to learn collaboratively at feature and image levels, minimizing variance and encouraging consistency. Furthermore, a separate corrective network utilizes spatiotemporal information from the video context to further refine segmentation results. The approach is validated on two clinical tasks: kidney stone laser lithotomy segmentation via ureteroscopy and polyp detection through colonoscopy. Comparative experiments demonstrate that Endo-SemiS outperforms current state-of-the-art segmentation methods, particularly when labeled data are scarce. The paper also provides publicly accessible code, promoting transparency and facilitating future research. <div>
arXiv:2512.16977v1 Announce Type: new 
Abstract: In this paper, we present Endo-SemiS, a semi-supervised segmentation framework for providing reliable segmentation of endoscopic video frames with limited annotation. EndoSemiS uses 4 strategies to improve performance by effectively utilizing all available data, particularly unlabeled data: (1) Cross-supervision between two individual networks that supervise each other; (2) Uncertainty-guided pseudo-labels from unlabeled data, which are generated by selecting high-confidence regions to improve their quality; (3) Joint pseudolabel supervision, which aggregates reliable pixels from the pseudo-labels of both networks to provide accurate supervision for unlabeled data; and (4) Mutual learning, where both networks learn from each other at the feature and image levels, reducing variance and guiding them toward a consistent solution. Additionally, a separate corrective network that utilizes spatiotemporal information from endoscopy video to improve segmentation performance. Endo-SemiS is evaluated on two clinical applications: kidney stone laser lithotomy from ureteroscopy and polyp screening from colonoscopy. Compared to state-of-the-art segmentation methods, Endo-SemiS substantially achieves superior results on both datasets with limited labeled data. The code is publicly available at https://github.com/MedICL-VU/Endo-SemiS
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</title>
<link>https://arxiv.org/abs/2512.16978</link>
<guid>https://arxiv.org/abs/2512.16978</guid>
<content:encoded><![CDATA[
<div> Long-form video understanding, Multimodal reasoning, Benchmark, Agentic system, Evaluation rubric<br /><br />Summary:<br /><br />1. The paper introduces LongShOTBench, a new diagnostic benchmark designed for long-form multimodal video understanding that integrates vision, speech, and ambient audio while requiring coherent long-range reasoning. <br /><br />2. LongShOTBench addresses limitations of existing benchmarks by combining temporal length and multimodal richness with open-ended, intent-driven questions, single- and multi-turn dialogues, and tasks demanding multimodal reasoning and agentic tool use across video, audio, and speech. <br /><br />3. Every sample in LongShOTBench includes a reference answer and a graded rubric for interpretable and traceable evaluation, ensuring coverage and reproducibility through a scalable, human-validated pipeline with human verification and corrections. <br /><br />4. The authors also present LongShOTAgent, an agentic system designed to analyze long videos using preprocessing, search, and iterative refinement methods. <br /><br />5. Evaluation results on LongShOTBench reveal significant gaps in the performance of state-of-the-art multimodal large language models (MLLMs): Gemini-2.5-Flash achieves 52.95% accuracy, open-source models fall below 30%, and LongShOTAgent attains 44.66%, highlighting the challenge of real-world long-form video understanding. <br /><br />6. LongShOTBench offers a practical and reproducible foundation for the evaluation and improvement of MLLMs, with all related resources made publicly available on GitHub. <div>
arXiv:2512.16978v1 Announce Type: new 
Abstract: Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title>
<link>https://arxiv.org/abs/2512.17012</link>
<guid>https://arxiv.org/abs/2512.17012</guid>
<content:encoded><![CDATA[
<div> 4D-RGPT, Multimodal LLM, 4D perception, Video Question Answering, region-level prompting<br /><br />Summary:<br /><br />This paper addresses limitations in current Multimodal Large Language Models (MLLMs) regarding their reasoning over 3D structures and temporal dynamics, citing weak 4D perception and temporal understanding as key challenges. To overcome these issues, the authors introduce 4D-RGPT, a specialized MLLM architecture designed to capture and process 4D (spatial plus temporal) representations from video inputs, thereby enhancing temporal perception capabilities. Alongside this model, they propose Perceptual 4D Distillation (P4D), a novel training framework that transfers rich 4D representations from a frozen expert model into 4D-RGPT, aimed at fostering comprehensive 4D perceptual learning. Additionally, to evaluate and benchmark performance in this domain more effectively, the authors develop R4D-Bench, a new dataset featuring depth-aware dynamic scenes with region-level prompting. This benchmark is constructed through a hybrid automated and human-verified process, addressing the shortcomings of previous VQA datasets that focused largely on static scenes and lacked fine-grained regional queries. Experimental results demonstrate that 4D-RGPT significantly outperforms existing methods on both established 4D VQA benchmarks and the newly introduced R4D-Bench, validating the benefits of their approach in advancing 4D video understanding and question answering. <div>
arXiv:2512.17012v1 Announce Type: new 
Abstract: Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring</title>
<link>https://arxiv.org/abs/2512.17021</link>
<guid>https://arxiv.org/abs/2512.17021</guid>
<content:encoded><![CDATA[
<div> Keywords: forest monitoring, canopy height, SPOT satellites, disturbance detection, climate change  

<br /><br />Summary:  
1. The study addresses the decline of the European forest carbon sink by developing advanced forest monitoring tools with high spatial resolution.  
2. Existing satellite disturbance products have limitations in detecting changes below 100 m², typically missing individual tree-level disturbances.  
3. FORMSpoT, introduced in this work, is a nationwide forest canopy height mapping over France from 2014 to 2024, using 1.5 m resolution data from SPOT-6/7 satellite composites.  
4. Canopy heights were estimated via a hierarchical transformer model (PVTv2) trained on airborne laser scanning (ALS) data, ensuring high accuracy.  
5. The authors developed a post-processing pipeline incorporating co-registration and spatio-temporal total variation denoising to robustly detect changes across heterogeneous satellite acquisitions.  
6. Validation against ALS revisits and National Forest Inventory plots demonstrates that FORMSpoT-Δ outperforms existing disturbance products, achieving notably higher F1-scores, especially in fragmented mountainous forests.  
7. This tool enables tree-level monitoring across a national scale, allowing analysis of forest management, early detection of decline, and improved carbon loss quantification from subtle disturbances like thinning or selective logging.  
8. The study highlights the importance of sustaining very high-resolution satellite missions such as SPOT and open-data initiatives like DINAMIS for effective forest monitoring under climate change pressures. <div>
arXiv:2512.17021v1 Announce Type: new 
Abstract: The recent decline of the European forest carbon sink highlights the need for spatially explicit and frequently updated forest monitoring tools. Yet, existing satellite-based disturbance products remain too coarse to detect changes at the scale of individual trees, typically below 100 m$^{2}$. Here, we introduce FORMSpoT (Forest Mapping with SPOT Time series), a decade-long (2014-2024) nationwide mapping of forest canopy height at 1.5 m resolution, together with annual disturbance polygons (FORMSpoT-$\Delta$) covering mainland France. Canopy heights were derived from annual SPOT-6/7 composites using a hierarchical transformer model (PVTv2) trained on high-resolution airborne laser scanning (ALS) data. To enable robust change detection across heterogeneous acquisitions, we developed a dedicated post-processing pipeline combining co-registration and spatio-temporal total variation denoising. Validation against ALS revisits across 19 sites and 5,087 National Forest Inventory plots shows that FORMSpoT-$\Delta$ substantially outperforms existing disturbance products. In mountainous forests, where disturbances are small and spatially fragmented, FORMSpoT-$\Delta$ achieves an F1-score of 0.44, representing an order of magnitude higher than existing benchmarks. By enabling tree-level monitoring of forest dynamics at national scale, FORMSpoT-$\Delta$ provides a unique tool to analyze management practices, detect early signals of forest decline, and better quantify carbon losses from subtle disturbances such as thinning or selective logging. These results underscore the critical importance of sustaining very high-resolution satellite missions like SPOT and open-data initiatives such as DINAMIS for monitoring forests under climate change.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</title>
<link>https://arxiv.org/abs/2512.17040</link>
<guid>https://arxiv.org/abs/2512.17040</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, camera-controlled video generation, pose fidelity, infinite homography warping, data augmentation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating camera-controlled novel-view videos of dynamic scenes that maintain high fidelity to specified camera poses and ensure view consistency despite limited input observations. Existing approaches either rely on trajectory-conditioned video generation models trained on limited trajectory-video datasets or use depth estimation to reproject video frames along new trajectories. However, depth-based reprojection is prone to errors due to inaccurate depth maps, and existing datasets lack sufficient diversity in camera trajectories, limiting model generalization. To overcome these issues, the authors propose InfCam, a depth-free video-to-video generation framework that achieves high camera-pose accuracy. InfCam introduces infinite homography warping, which encodes 3D camera rotations directly into the 2D latent space of a video diffusion model, enabling noise-free rotational conditioning. The residual parallax is predicted via end-to-end training, enhancing pose fidelity without requiring explicit depth information. Furthermore, a novel data augmentation pipeline expands synthetic multiview datasets by simulating diverse trajectories and focal lengths, improving model robustness. Experimental evaluations show that InfCam surpasses baseline methods in both camera-pose accuracy and visual quality, and generalizes effectively from synthetic data to real-world videos, advancing the state-of-the-art in camera-controlled novel-view video generation. <div>
arXiv:2512.17040v1 Announce Type: new 
Abstract: Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Similarity of Synthetic Image Utility</title>
<link>https://arxiv.org/abs/2512.17080</link>
<guid>https://arxiv.org/abs/2512.17080</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic medical images, deep learning, clinical decision support, interpretable utility similarity, image dataset evaluation<br /><br />Summary: This paper addresses the challenge of quantitatively assessing the similarity between synthetic and real medical image datasets in the context of deep learning-based clinical decision support (CDS) systems. The authors propose a new metric called Interpretable Utility Similarity (IUS), which is inspired by generalized neural additive models and designed to be interpretable, unlike common inception-based measures. IUS evaluates how useful a synthetic dataset is compared to real data for training CDS systems, focusing on clinically relevant image features. Experiments were conducted on public datasets spanning various color medical imaging modalities, including endoscopic, dermoscopic, and fundus images, demonstrating that selecting synthetic images with high IUS scores can lead to classification performance improvements of up to 54.6%. Furthermore, the authors show that IUS generalizes well to grayscale imaging modalities such as X-rays and ultrasound, supporting its broad applicability. The study thereby provides a novel, interpretable, and effective approach to benchmark synthetic medical image datasets for training DL models in clinical applications. The implementation of IUS is made publicly available on GitHub, encouraging further research and practical use. <div>
arXiv:2512.17080v1 Announce Type: new 
Abstract: Synthetic medical image data can unlock the potential of deep learning (DL)-based clinical decision support (CDS) systems through the creation of large scale, privacy-preserving, training sets. Despite the significant progress in this field, there is still a largely unanswered research question: "How can we quantitatively assess the similarity of a synthetically generated set of images with a set of real images in a given application domain?". Today, answers to this question are mainly provided via user evaluation studies, inception-based measures, and the classification performance achieved on synthetic images. This paper proposes a novel measure to assess the similarity between synthetically generated and real sets of images, in terms of their utility for the development of DL-based CDS systems. Inspired by generalized neural additive models, and unlike inception-based measures, the proposed measure is interpretable (Interpretable Utility Similarity, IUS), explaining why a synthetic dataset could be more useful than another one in the context of a CDS system based on clinically relevant image features. The experimental results on publicly available datasets from various color medical imaging modalities including endoscopic, dermoscopic and fundus imaging, indicate that selecting synthetic images of high utility similarity using IUS can result in relative improvements of up to 54.6% in terms of classification performance. The generality of IUS for synthetic data assessment is demonstrated also for greyscale X-ray and ultrasound imaging modalities. IUS implementation is available at https://github.com/innoisys/ius
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGH: Dynamic Gaussian Hair</title>
<link>https://arxiv.org/abs/2512.17094</link>
<guid>https://arxiv.org/abs/2512.17094</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Hair, Gaussian Representation, Hair Dynamics, Differentiable Rendering, 3D Avatar  

<br /><br />Summary:  
This paper introduces Dynamic Gaussian Hair (DGH), a novel approach for modeling photorealistic dynamic hair that addresses challenges such as complex motions, occlusions, and light scattering. (1) DGH employs a coarse-to-fine model designed to learn temporally coherent hair motion dynamics, effectively accommodating a wide variety of hairstyles. (2) The framework includes a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance, supporting differentiable rendering which facilitates gradient-based learning for consistent view-dependent appearance during motion. Unlike traditional physics-based simulations that require extensive manual tuning and heavy computation, DGH is fully data-driven, scalable with training data, and generalizes well across diverse hair styles and head motions. Furthermore, DGH integrates seamlessly into existing 3D Gaussian avatar frameworks, enabling realistic and animatable hair for high-fidelity avatar representations. Experimental results show that DGH achieves competitive geometry and appearance quality, presenting an efficient and scalable alternative to simulation-based hair modeling and rendering techniques. This makes DGH a promising solution for the creation of dynamic, photorealistic hair in digital human modeling and virtual avatar applications. <div>
arXiv:2512.17094v1 Announce Type: new 
Abstract: The creation of photorealistic dynamic hair remains a major challenge in digital human modeling because of the complex motions, occlusions, and light scattering. Existing methods often resort to static capture and physics-based models that do not scale as they require manual parameter fine-tuning to handle the diversity of hairstyles and motions, and heavy computation to obtain high-quality appearance. In this paper, we present Dynamic Gaussian Hair (DGH), a novel framework that efficiently learns hair dynamics and appearance. We propose: (1) a coarse-to-fine model that learns temporally coherent hair motion dynamics across diverse hairstyles; (2) a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance with support for differentiable rendering, enabling gradient-based learning of view-consistent appearance under motion. Unlike prior simulation-based pipelines, our approach is fully data-driven, scales with training data, and generalizes across various hairstyles and head motion sequences. Additionally, DGH can be seamlessly integrated into a 3D Gaussian avatar framework, enabling realistic, animatable hair for high-fidelity avatar representation. DGH achieves promising geometry and appearance results, providing a scalable, data-driven alternative to physics-based simulation and rendering.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of Maritime Radar Data Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.17098</link>
<guid>https://arxiv.org/abs/2512.17098</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime autonomous systems, transformer architectures, radar frame prediction, spatiotemporal forecasting, trajectory prediction  

<br /><br />Summary:  
1. Maritime autonomous systems need robust predictive models to forecast vessel movements and environmental dynamics for safer and more efficient navigation.  
2. Transformer architectures have made significant advancements in AIS-based trajectory prediction and sonar frame forecasting, showing great potential in maritime applications.  
3. Despite these successes, transformer-based prediction models have not yet been applied to maritime radar frame forecasting, which is crucial due to radar's reliability under varying weather conditions.  
4. This survey presents a systematic review of existing predictive modeling techniques applied to maritime radar data, focusing especially on transformer architectures used for spatiotemporal sequence forecasting.  
5. The analysis categorizes existing methods based on the type of data, model architecture, and prediction horizons, highlighting that no current studies address transformer-based radar frame prediction.  
6. The identified research gap underlines the urgent need to explore transformer models for maritime radar data, suggesting a promising future direction for improving autonomous maritime navigation. <div>
arXiv:2512.17098v1 Announce Type: new 
Abstract: Maritime autonomous systems require robust predictive capabilities to anticipate vessel motion and environmental dynamics. While transformer architectures have revolutionized AIS-based trajectory prediction and demonstrated feasibility for sonar frame forecasting, their application to maritime radar frame prediction remains unexplored, creating a critical gap given radar's all-weather reliability for navigation. This survey systematically reviews predictive modeling approaches relevant to maritime radar, with emphasis on transformer architectures for spatiotemporal sequence forecasting, where existing representative methods are analyzed according to data type, architecture, and prediction horizon. Our review shows that, while the literature has demonstrated transformer-based frame prediction for sonar sensing, no prior work addresses transformer-based maritime radar frame prediction, thereby defining a clear research gap and motivating a concrete research direction for future work in this area.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</title>
<link>https://arxiv.org/abs/2512.17137</link>
<guid>https://arxiv.org/abs/2512.17137</guid>
<content:encoded><![CDATA[
<div> Clinical MRI, deep learning reconstruction, scalable model, coil sensitivity estimation, data consistency<br /><br />Summary: This work introduces the Scalable Deep Unrolled Model (SDUM), a universal deep learning framework designed to address the diversity of clinical MRI protocols, including various anatomical targets, contrast types, sampling patterns, and acceleration factors. Unlike existing methods that are protocol-specific, SDUM integrates several key components: a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME) applied per cascade, sampling-aware weighted data consistency (SWDC), universal conditioning (UC) using cascade index and protocol metadata, and a progressive cascade expansion training approach. The model shows foundation-model-like scaling behavior where reconstruction quality measured by PSNR improves predictably logarithmically with the number of parameters and cascades, achieving high correlation (r=0.986) up to 18 cascades. A single SDUM model trained on heterogeneous datasets achieves state-of-the-art results across multiple challenging MRI reconstruction benchmarks, including the 2025 multi-center, multi-disease, 5T, and pediatric tracks, outperforming task-specific baselines by up to +1.0 dB. It also surpasses top entries from other major challenges like CMRxRecon2024 and fastMRI brain by substantial margins. Ablation studies confirm the performance gains contributed by each component: SWDC (+0.43 dB), per-cascade CSME (+0.51 dB), and UC (+0.38 dB). Overall, SDUM paves the way for a versatile, scalable, and practical universal MRI reconstruction solution. <div>
arXiv:2512.17137v1 Announce Type: new 
Abstract: Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps</title>
<link>https://arxiv.org/abs/2512.17143</link>
<guid>https://arxiv.org/abs/2512.17143</guid>
<content:encoded><![CDATA[
<div> Keywords: professional photography, canonical UV space, reposing, multi image finetuning, novel view synthesis<br /><br />Summary:  
1) This paper addresses the challenge of transforming everyday photos of people ("in the wild") into professional-quality portraits characterized by good lighting, chosen poses, and standard clothing, while preserving the unique identity, face, and body features of the person.  
2) Since large paired datasets of the same individuals photographed under casual and professional conditions do not exist, the authors propose novel techniques that do not rely on such data.  
3) The first key insight is to transform the input photo and the person’s face into a canonical UV space, which facilitates modeling occlusions and enables novel view synthesis through reposing methodologies, leveraging existing unpaired datasets effectively.  
4) The second insight involves personalizing the generated output by performing multi-image finetuning, allowing the model to better capture individual-specific details for higher fidelity in the reposed portraits.  
5) Experimental results demonstrate that this approach generates high-quality, reposed professional-style portraits and achieves strong qualitative and quantitative performance on real-world imagery, outperforming previous methods. <div>
arXiv:2512.17143v1 Announce Type: new 
Abstract: Photographs of people taken by professional photographers typically present the person in beautiful lighting, with an interesting pose, and flattering quality. This is unlike common photos people can take of themselves. In this paper, we explore how to create a ``professional'' version of a person's photograph, i.e., in a chosen pose, in a simple environment, with good lighting, and standard black top/bottom clothing. A key challenge is to preserve the person's unique identity, face and body features while transforming the photo. If there would exist a large paired dataset of the same person photographed both ``in the wild'' and by a professional photographer, the problem would potentially be easier to solve. However, such data does not exist, especially for a large variety of identities. To that end, we propose two key insights: 1) Our method transforms the input photo and person's face to a canonical UV space, which is further coupled with reposing methodology to model occlusions and novel view synthesis. Operating in UV space allows us to leverage existing unpaired datasets. 2) We personalize the output photo via multi image finetuning. Our approach yields high-quality, reposed portraits and achieves strong qualitative and quantitative performance on real-world imagery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Conditioned Background Generation for Editable Multi-Layer Documents</title>
<link>https://arxiv.org/abs/2512.17151</link>
<guid>https://arxiv.org/abs/2512.17151</guid>
<content:encoded><![CDATA[
<div> Keywords: document-centric background generation, latent masking, Automated Readability Optimization (ARO), multi-page consistency, multi-layer editing<br /><br />Summary:  
The paper introduces a novel framework focused on document-centric background generation that supports multi-page editing with thematic continuity. It employs a latent masking technique in the diffusion space inspired by smooth barrier functions to softly limit updates, ensuring text regions remain readable. Automated Readability Optimization (ARO) is presented as a method to place semi-transparent, rounded shapes behind text, automatically adjusting opacity to meet WCAG 2.2 perceptual contrast standards for optimal readability without manual effort. To maintain multi-page consistency, the framework summarizes each page into compact representations that guide the generation of subsequent pages, mimicking human context retention for coherent visual motif evolution. The framework treats documents as structured compositions with separate layers for text, figures, and backgrounds, enabling targeted background editing without degrading text readability. Additionally, user prompts facilitate stylistic customization in color and texture, balancing automated consistency with flexibility. Notably, the entire system is training-free, producing visually coherent, text-preserving, and thematically aligned documents that integrate generative modeling with practical design workflows. <div>
arXiv:2512.17151v1 Announce Type: new 
Abstract: We present a framework for document-centric background generation with multi-page editing and thematic continuity. To ensure text regions remain readable, we employ a \emph{latent masking} formulation that softly attenuates updates in the diffusion space, inspired by smooth barrier functions in physics and numerical optimization. In addition, we introduce \emph{Automated Readability Optimization (ARO)}, which automatically places semi-transparent, rounded backing shapes behind text regions. ARO determines the minimal opacity needed to satisfy perceptual contrast standards (WCAG 2.2) relative to the underlying background, ensuring readability while maintaining aesthetic harmony without human intervention. Multi-page consistency is maintained through a summarization-and-instruction process, where each page is distilled into a compact representation that recursively guides subsequent generations. This design reflects how humans build continuity by retaining prior context, ensuring that visual motifs evolve coherently across an entire document. Our method further treats a document as a structured composition in which text, figures, and backgrounds are preserved or regenerated as separate layers, allowing targeted background editing without compromising readability. Finally, user-provided prompts allow stylistic adjustments in color and texture, balancing automated consistency with flexible customization. Our training-free framework produces visually coherent, text-preserving, and thematically aligned documents, bridging generative modeling with natural design workflows.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</title>
<link>https://arxiv.org/abs/2512.17152</link>
<guid>https://arxiv.org/abs/2512.17152</guid>
<content:encoded><![CDATA[
<div> Keywords: fire prediction, physics-informed model, world model, cross-task training, fire spread dynamics<br /><br />Summary:<br /><br />1. Fine-grained fire prediction is critical for effective emergency response, but current methods mainly rely on binary fire masks that lack sufficient detail to capture complex fire dynamics. 2. Infrared images and fire masks provide complementary thermal and spatial boundary information, but existing approaches fail to fully utilize this multi-modal data. 3. PhysFire-WM is introduced as a novel physics-informed world model designed to emulate fire spread by integrating combustion dynamics through structured physical priors derived from a physical simulator, addressing inconsistencies in purely data-driven video generation models. 4. The approach employs a Cross-task Collaborative Training (CC-Train) strategy that shares parameters and coordinates gradients between tasks of thermal radiation prediction and boundary delineation, overcoming the limitations of sparse mask information and enhancing both physical realism and geometric accuracy. 5. Extensive experiments on a fine-grained multimodal fire dataset demonstrate that PhysFire-WM achieves superior accuracy in predicting fire spread, validating the importance of incorporating physical priors and collaborative training, and offering valuable insights for advancing physics-informed modeling in disaster prediction applications. <div>
arXiv:2512.17152v1 Announce Type: new 
Abstract: Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</title>
<link>https://arxiv.org/abs/2512.17160</link>
<guid>https://arxiv.org/abs/2512.17160</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Large Language Model, Diffusion Model, Zero-shot Classification, Prompt Generation  

<br /><br />Summary:  
The paper addresses limitations in current Vision-Language Models (VLMs) like CLIP, which depend heavily on annotated text-image pairs and require dual-tower encoders, increasing cost and complexity. To overcome these challenges, the authors propose LGCLIP, a novel framework that integrates a Large Language Model (LLM) to generate class-specific prompts. These prompts guide a diffusion model to synthesize reference images serving as visual prototypes. Real images are then compared with these generated prototypes via extracted visual features to perform classification. The LGCLIP framework simplifies the architecture by using only a visual encoder, making it lightweight and efficient. Importantly, it eliminates the requirement for manually annotated text-image pairs during training, requiring only class labels as input. Experimental results demonstrate that LGCLIP achieves strong zero-shot classification performance while reducing dataset preparation costs and computational complexity. This approach establishes a new paradigm for classification by combining prompt generation, image synthesis, and contrastive learning without the need for extensive paired data or dual encoders. <div>
arXiv:2512.17160v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)" framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</title>
<link>https://arxiv.org/abs/2512.17178</link>
<guid>https://arxiv.org/abs/2512.17178</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, attribute binding, compositional image-text matching, semantic refinement, local token-patch alignment  

<br /><br />Summary:  
This paper addresses the challenge of compositional image-text matching in CLIP-like models, focusing on the difficulty of correctly associating attributes with their corresponding objects. Traditional CLIP models rely on global representations that tend to overlook fine-grained semantic details, leading to attribute confusion. Existing solutions often require additional training or hard negative sampling but struggle to generalize well to novel compositional concepts. To overcome these issues, the authors introduce ABE-CLIP, a training-free Attribute Binding Enhancement method that strengthens the link between objects and attributes without extra training overhead. ABE-CLIP incorporates a Semantic Refinement Mechanism that refines token embeddings for both objects and attributes in the text, which enhances semantic precision and reduces confusion. Furthermore, the method employs a Local Token-Patch Alignment strategy, calculating localized similarity scores between refined textual tokens and their corresponding image patches rather than relying on global features. By aggregating these local similarities, ABE-CLIP produces a more accurate image-text similarity score. Experimental results on several datasets demonstrate that ABE-CLIP significantly improves the ability to bind attributes to objects, outperforming prior methods that depend on extensive training, thus offering an efficient and effective solution to compositional image-text understanding. <div>
arXiv:2512.17178v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities</title>
<link>https://arxiv.org/abs/2512.17186</link>
<guid>https://arxiv.org/abs/2512.17186</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban greenery, perception, Green View Index, spatial variation, cultural influence  

<br /><br />Summary:  
This study focuses on quantifying and assessing urban greenery by comparing objective measures such as the Green View Index (GVI) with subjective perceptions derived from surveys. Researchers collected data through street view imagery and an extensive urban visual perception survey from 1,000 participants across five countries, including detailed demographic and personality profiles. The investigation highlights discrepancies between objective greenery measurements and how green spaces are perceived, revealing that these differences are consistent worldwide. Notably, demographics and personality traits have limited influence on how greenery is perceived. The study finds a strong correlation between perceived and measured greenery influenced by geographic factors, particularly where participants live. Location emerged as one of the most significant factors affecting perception, suggesting cultural, environmental, and experiential backgrounds play crucial roles in shaping individual views of urban greenery. The findings underscore the importance of considering spatial and human factors when planning green spaces to better align objective greenery with community perceptions and well-being outcomes. This work advances the understanding of urban greenery perception by integrating human, geographic, and spatial dimensions across diverse global contexts. <div>
arXiv:2512.17186v1 Announce Type: new 
Abstract: Quantifying and assessing urban greenery is consequential for planning and development, reflecting the everlasting importance of green spaces for multiple climate and well-being dimensions of cities. Evaluation can be broadly grouped into objective (e.g., measuring the amount of greenery) and subjective (e.g., polling the perception of people) approaches, which may differ -- what people see and feel about how green a place is might not match the measurements of the actual amount of vegetation. In this work, we advance the state of the art by measuring such differences and explaining them through human, geographic, and spatial dimensions. The experiments rely on contextual information extracted from street view imagery and a comprehensive urban visual perception survey collected from 1,000 people across five countries with their extensive demographic and personality information. We analyze the discrepancies between objective measures (e.g., Green View Index (GVI)) and subjective scores (e.g., pairwise ratings), examining whether they can be explained by a variety of human and visual factors such as age group and spatial variation of greenery in the scene. The findings reveal that such discrepancies are comparable around the world and that demographics and personality do not play a significant role in perception. Further, while perceived and measured greenery correlate consistently across geographies (both where people and where imagery are from), where people live plays a significant role in explaining perceptual differences, with these two, as the top among seven, features that influences perceived greenery the most. This location influence suggests that cultural, environmental, and experiential factors substantially shape how individuals observe greenery in cities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences</title>
<link>https://arxiv.org/abs/2512.17188</link>
<guid>https://arxiv.org/abs/2512.17188</guid>
<content:encoded><![CDATA[
<div> affine correspondences, multi-camera systems, relative pose estimation, global optimization, inertial measurement unit (IMU)  

<br /><br />Summary:  
This paper addresses the problem of relative pose estimation in multi-camera systems aided by inertial measurement units (IMUs), which is crucial for applications like self-driving cars. The authors propose a globally optimal solver that utilizes affine correspondences and assumes a known vertical direction to estimate the generalized relative pose. Their method first decouples the rotation matrix and translation vector, establishing a cost function based on the relative rotation angle that minimizes algebraic errors resulting from geometric constraints given by affine correspondences. They then convert the global optimization problem into solving two polynomials with two unknowns, derived from the characteristic equation and its first derivative set to zero. The relative rotation angle is computed using a polynomial eigenvalue solver, and the translation vector is subsequently obtained from the corresponding eigenvector. Additionally, the paper introduces a novel linear solution tailored for scenarios where the relative rotation is small. The effectiveness of the proposed solver is demonstrated through extensive experiments on both synthetic and real-world datasets, showing superior accuracy compared to existing state-of-the-art methods in relative pose estimation for multi-camera setups. <div>
arXiv:2512.17188v1 Announce Type: new 
Abstract: Mobile devices equipped with a multi-camera system and an inertial measurement unit (IMU) are widely used nowadays, such as self-driving cars. The task of relative pose estimation using visual and inertial information has important applications in various fields. To improve the accuracy of relative pose estimation of multi-camera systems, we propose a globally optimal solver using affine correspondences to estimate the generalized relative pose with a known vertical direction. First, a cost function about the relative rotation angle is established after decoupling the rotation matrix and translation vector, which minimizes the algebraic error of geometric constraints from affine correspondences. Then, the global optimization problem is converted into two polynomials with two unknowns based on the characteristic equation and its first derivative is zero. Finally, the relative rotation angle can be solved using the polynomial eigenvalue solver, and the translation vector can be obtained from the eigenvector. Besides, a new linear solution is proposed when the relative rotation is small. The proposed solver is evaluated on synthetic data and real-world datasets. The experiment results demonstrate that our method outperforms comparable state-of-the-art methods in accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</title>
<link>https://arxiv.org/abs/2512.17189</link>
<guid>https://arxiv.org/abs/2512.17189</guid>
<content:encoded><![CDATA[
<div> Medical Vision-Language Models, hallucinations, contrastive decoding, anatomical mask, diagnostic accuracy

<br /><br />Summary: Medical Vision-Language Models (MedVLMs) have significant potential for use in clinical settings but are often plagued by hallucinations, where the models incorrectly generate answers not based on visual evidence but on textual biases. Current solutions face challenges: training-based methods require expensive expert annotations, limiting their scalability, whereas training-free approaches like contrastive decoding apply broad, untargeted corrections that may be unreliable in complex medical contexts. To overcome these limitations, the authors propose Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play technique that specifically targets hallucinations by using anatomical masks to guide the model's focus. ARCD performs a novel three-tier contrastive decoding process that dynamically adjusts weights at the token, attention, and logits levels, enhancing the model’s attention on relevant anatomical regions while suppressing unsupported outputs. Extensive experiments were conducted on diverse medical imaging datasets, including chest X-rays, CT scans, brain MRIs, and ocular ultrasounds. Results show that ARCD effectively improves regional anatomical understanding, significantly reduces hallucinations, and boosts overall diagnostic accuracy, demonstrating its promise as a reliable strategy for enhancing MedVLMs in real-world clinical applications. <div>
arXiv:2512.17189v1 Announce Type: new 
Abstract: Medical Vision-Language Models (MedVLMs) show immense promise in clinical applicability. However, their reliability is hindered by hallucinations, where models often fail to derive answers from visual evidence, instead relying on learned textual priors. Existing mitigation strategies for MedVLMs have distinct limitations: training-based methods rely on costly expert annotations, limiting scalability, while training-free interventions like contrastive decoding, though data-efficient, apply a global, untargeted correction whose effects in complex real-world clinical settings can be unreliable. To address these challenges, we introduce Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play strategy that mitigates hallucinations by providing targeted, region-specific guidance. Our module leverages an anatomical mask to direct a three-tiered contrastive decoding process. By dynamically re-weighting at the token, attention, and logits levels, it verifiably steers the model's focus onto specified regions, reinforcing anatomical understanding and suppressing factually incorrect outputs. Extensive experiments across diverse datasets, including chest X-ray, CT, brain MRI, and ocular ultrasound, demonstrate our method's effectiveness in improving regional understanding, reducing hallucinations, and enhancing overall diagnostic accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening</title>
<link>https://arxiv.org/abs/2512.17202</link>
<guid>https://arxiv.org/abs/2512.17202</guid>
<content:encoded><![CDATA[
<div> Fose, pansharpening, diffusion model, end-to-end model, image fusion<br /><br />Summary:  
Pansharpening is an important image fusion technique that merges low-resolution multispectral images (LRMSI) with high-resolution panchromatic images (PAN) to produce high-resolution multispectral images (HRMSI). Recent advancements in diffusion models (DM) and end-to-end (E2E) models have advanced the state-of-the-art in pansharpening. Diffusion models rely on a multi-step process to accurately estimate the residual between LRMSI and HRMSI, but this approach demands substantial computational resources and is time-intensive. Conversely, E2E models offer simplicity but are constrained by limited prior information and structural complexity, which limits their performance. To address these challenges, the paper proposes a novel four-stage training strategy that combines a one-step diffusion model with an end-to-end model into a lightweight network called Fose. This strategy involves performing one-step distillation on an enhanced state-of-the-art diffusion model, effectively reducing the inference steps from 50 to just 1. Subsequently, the one-step diffusion model and the E2E model are fused using lightweight ensemble blocks. Extensive experiments on three widely-used benchmarks demonstrate that Fose significantly outperforms existing methods. Additionally, it achieves a 7.42 times speedup in inference compared to the baseline diffusion model while delivering superior pansharpening results. The authors have released the code and model publicly for community use. <div>
arXiv:2512.17202v1 Announce Type: new 
Abstract: Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</title>
<link>https://arxiv.org/abs/2512.17206</link>
<guid>https://arxiv.org/abs/2512.17206</guid>
<content:encoded><![CDATA[
<div> Exploration capacity, latent modulation, variational autoencoder, reinforcement learning, reasoning strategies<br /><br />Summary:<br /><br />This paper introduces Reasoning Palette, a novel approach to improve exploration and strategic reasoning in large (vision-) language models. First, the method addresses the issue of redundant reasoning paths produced by stochastic sampling during inference, which limits diversity and exploration efficiency. Second, Reasoning Palette employs a stochastic latent variable inferred from the mean-pooled embedding of question-answer pairs via a variational autoencoder (VAE). Third, during inference, this latent variable is decoded into learnable token prefixes, which are prepended to the model input to modulate its internal reasoning trajectory, thus shaping the style and structure of the model’s generated responses. Fourth, a brief supervised fine-tuning phase helps the model adapt to this latent conditioning, enabling more effective integration of diverse reasoning contexts. Finally, integrating this framework within reinforcement learning leads to enhanced structured exploration by injecting diverse reasoning modes on demand, which improves sustained learning ability and exploration efficiency. Experimental results on multiple reasoning benchmarks demonstrate that Reasoning Palette provides interpretable and controllable strategic behavior in language models and achieves consistent performance gains compared to standard reinforcement learning methods. <div>
arXiv:2512.17206v1 Announce Type: new 
Abstract: Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title>
<link>https://arxiv.org/abs/2512.17213</link>
<guid>https://arxiv.org/abs/2512.17213</guid>
<content:encoded><![CDATA[
<div> Medical Vision-Language Models, hallucinations, Knowledge Graph Consistency Reward, process supervision, MIMIC-CXR-VQA<br /><br />Summary:<br /><br />1. Medical Vision-Language Models (VLMs) often suffer from hallucinations, which undermine their clinical reliability by producing inaccurate or unverifiable outputs.  
2. Existing reinforcement learning approaches, such as Group Relative Policy Optimization (GRPO), use sparse, outcome-based rewards that lead models to "overthink" — generating verbose and convoluted Chain-of-Thought reasoning that hides factual errors and poses safety risks.  
3. To tackle these issues, the authors propose CheXPO-v2, an alignment framework that replaces outcome supervision with process supervision to better guide model reasoning.  
4. The core innovation is the Knowledge Graph Consistency Reward mechanism, which uses Entity-Relation Matching to parse reasoning into structured triplets ("Disease, Relation, Anatomy") allowing fine-grained supervision that penalizes incoherent logic and hallucinations at an atomic level.  
5. Integrating this reward with a hard-example mining strategy, CheXPO-v2 significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA, achieving a new state-of-the-art accuracy with only 5,000 samples, showcasing exceptional data efficiency and clinically sound, verifiable reasoning.  
6. The authors have made the source code publicly available to foster further development and transparency. <div>
arXiv:2512.17213v1 Announce Type: new 
Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</title>
<link>https://arxiv.org/abs/2512.17221</link>
<guid>https://arxiv.org/abs/2512.17221</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, DAVE, document understanding, web agents, self-supervised pretraining  

<br /><br />Summary: This paper addresses a critical limitation in vision-language models (VLMs), specifically their vision encoders, which often lack robust structural and spatial features necessary for document understanding and web-based agent tasks. To overcome this, the authors introduce DAVE, a specialized vision encoder designed to enhance VLM performance on these domains. The training process of DAVE begins with self-supervised pretraining on abundant unlabeled images, reducing reliance on expensive large-scale annotated datasets. Following this, a supervised autoregressive pretraining phase enables the model to learn parsing and localization tasks using a smaller set of high-quality labeled data. During supervised training, the authors propose two innovations: (i) a novel model-merging scheme that combines encoders paired with different text decoders, promoting compatibility with multiple web agent architectures, and (ii) ensemble training that integrates features from both pretrained generalist encoders like SigLIP2 and the specialized document/web representations developed by DAVE. The effectiveness of DAVE is demonstrated through extensive experiments spanning document understanding tasks, visual question answering (VQA), web localization, and agent-based benchmarks. Results confirm that DAVE significantly improves the vision encoding capabilities of VLMs in document and web contexts, establishing it as a powerful and versatile vision encoder for these applications. <div>
arXiv:2512.17221v1 Announce Type: new 
Abstract: While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder's alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</title>
<link>https://arxiv.org/abs/2512.17224</link>
<guid>https://arxiv.org/abs/2512.17224</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing Foundation Models, optical satellites, multi-scale adaptive patch embedding, spectral-spatial relationships, cross sensor fusion<br /><br />Summary:  
This paper addresses the challenges posed by varying band compositions and spatial resolutions in optical satellite imagery, which hinder the generalization of existing Remote Sensing Foundation Models (RSFMs). Traditional RSFMs are limited due to their pretraining on fixed band layouts and resolutions, making them less effective in real-world scenarios involving missing bands, cross-sensor fusion, and unseen spatial scales. To overcome these issues, the authors propose the Any Optical Model (AOM), a universal RSFM capable of handling arbitrary band configurations, sensor types, and resolution scales. AOM introduces a spectrum-independent tokenizer that assigns a dedicated band embedding to each channel to explicitly encode spectral identity, preserving distinctive spectral characteristics even when bands are missing or newly introduced. The model incorporates a multi-scale adaptive patch embedding mechanism to dynamically adjust the receptive field, enabling effective capture of texture and contextual patterns from sub-meter to hundred-meter scale imagery. Furthermore, a multi-scale semantic alignment mechanism ensures global semantic consistency across varying resolutions. The pretraining strategy includes a channel-wise self-supervised masking and reconstruction approach that jointly models spectral-spatial relationships. Extensive evaluation on more than 10 public datasets from sensors like Sentinel-2, Landsat, and HLS confirms that AOM achieves state-of-the-art performance under challenging conditions such as missing bands, cross sensor usage, and cross resolution settings. <div>
arXiv:2512.17224v1 Announce Type: new 
Abstract: Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors</title>
<link>https://arxiv.org/abs/2512.17226</link>
<guid>https://arxiv.org/abs/2512.17226</guid>
<content:encoded><![CDATA[
<div> Keywords: visual localization, global descriptors, geometric cues, contrastive loss, learning-based  

<br /><br />Summary:  
Recent learning-based visual localization techniques commonly rely on global descriptors derived primarily from geometric information such as covisibility graphs. However, this approach limits descriptor discriminative power and robustness, especially when geometric constraints are noisy or unreliable. To address this, the authors propose a novel aggregator module that learns global descriptors consistent with both the underlying geometric structure and visual similarity. This ensures that only images that are visually similar and spatially connected are close in descriptor space, thereby correcting erroneous associations stemming from unreliable overlap scores. The training method employs a batch-mining strategy based solely on overlap scores combined with a modified contrastive loss, allowing the model to train without requiring manual place labels. This strategy enhances generalization across diverse and large-scale environments. Experimental validation on challenging benchmarks demonstrates that the proposed method substantially improves localization accuracy while maintaining computational and memory efficiency. The approach integrates visual and geometric cues robustly, overcoming limitations of previous methods that depended exclusively on geometric constraints. The authors also provide code to facilitate reproducibility and further research. <div>
arXiv:2512.17226v1 Announce Type: new 
Abstract: Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints. We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected. This corrects erroneous associations caused by unreliable overlap scores. Using a batch-mining strategy based solely on the overlap scores and a modified contrastive loss, our method trains without manual place labels and generalizes across diverse environments. Experiments on challenging benchmarks show substantial localization gains in large-scale environments while preserving computational and memory efficiency. Code is available at \href{https://github.com/sontung/robust\_scr}{github.com/sontung/robust\_scr}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.17227</link>
<guid>https://arxiv.org/abs/2512.17227</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual forgetting, disentangled reasoning, Perception-Grounded Chain-of-Thought, reinforcement learning  

<br /><br />Summary: Multimodal Large Language Models (MLLMs) face significant challenges in complex visual reasoning tasks due to a phenomenon termed "visual forgetting," where the model progressively loses visual grounding during extended reasoning. This issue arises because current training paradigms prematurely combine two distinct cognitive skills: abstract logical reasoning ("how-to-think") and strategic visual perception ("when-to-look"). This premature entanglement leads to a foundational cold-start deficiency that weakens abstract reasoning and a strategic perception deficit as the model lacks a policy for deciding when to attend to visual inputs. To address these deficiencies, the paper proposes a novel two-stage curriculum-based framework. First, a disentangled Supervised Fine-Tuning (SFT) curriculum is introduced to build a robust abstract reasoning backbone using text-only data. This is then connected to visual inputs using a novel Perception-Grounded Chain-of-Thought (PG-CoT) method. Second, the strategic perception deficit is tackled by formulating the timing of visual attention as a reinforcement learning problem. The authors design a Pivotal Perception Reward that teaches the model when to look by linking perceptual actions with linguistic indicators of cognitive uncertainty such as "wait" or "verify," allowing the model to autonomously learn a grounded perception policy. This approach transforms the MLLM from a heuristic-driven observer into a strategic, grounded reasoner. Code is publicly available. <div>
arXiv:2512.17227v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is "visual forgetting", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as "think longer, see less". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning "how-to-think") and (2) strategic visual perception ("when-to-look"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., "wait", "verify"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \textbf{Code}: \url{https://github.com/gaozilve-max/learning-when-to-look}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</title>
<link>https://arxiv.org/abs/2512.17229</link>
<guid>https://arxiv.org/abs/2512.17229</guid>
<content:encoded><![CDATA[
<div> Keywords: Long Video Question-Answering, Multi-modal Large Language Models, Question-aware Memory, VideoDetective, GLVC Dataset<br /><br />Summary:<br /><br />Long Video Question-Answering (LVQA) poses challenges for Multi-modal Large Language Models (MLLMs) due to the large amount of contextual and visual information, often leading to high memory consumption and computational cost. Existing approaches attempt to mitigate this by either reducing the number of visual tokens or extending the model’s context length, but these methods risk losing important information or require extensive computation. The proposed method, VideoDetective, introduces an efficient question-aware memory mechanism that enables MLLMs to focus on critical video clues through iterative processing of video sub-segments. For each sub-segment, a question-aware compression approach utilizes special memory tokens to purposefully compress information, allowing the model to maintain essential clues while minimizing token usage. These memory tokens are recurrently aggregated to update the historical context, which influences the understanding of subsequent sub-segments. To better evaluate long video understanding, the authors introduce GLVC (Grounding Long Video Clues), a dataset designed with critical clues scattered across entire videos. Experiments show that VideoDetective can effectively process extremely long videos (up to 100K tokens, equivalent to an hour at 1fps) within practical resource limits (2 minutes runtime and 37GB GPU memory). The method outperforms other baselines in seeking relevant clues across multiple long video QA benchmarks. <div>
arXiv:2512.17229v1 Announce Type: new 
Abstract: Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitty: Diffusion-based Human-to-Robot Video Generation</title>
<link>https://arxiv.org/abs/2512.17253</link>
<guid>https://arxiv.org/abs/2512.17253</guid>
<content:encoded><![CDATA[
<div> Keywords: human demonstration videos, diffusion transformer, video In-Context Learning, robot-execution videos, paired-data synthesis  

<br /><br />Summary: Learning from human demonstration videos is critical for advancing scalable and generalizable robot learning, yet existing approaches suffer from reliance on intermediate representations like keypoints or trajectories, causing information loss and inconsistencies. The paper introduces Mitty, a Diffusion Transformer designed for video In-Context Learning that generates robot-execution videos directly from human demonstrations in an end-to-end manner without needing action labels or intermediate abstractions. Mitty utilizes a pretrained video diffusion model to leverage strong visual-temporal priors, compressing human demonstration videos into condition tokens and integrating these with robot denoising tokens via bidirectional attention during diffusion. To address the challenge of limited paired data, the authors develop an automatic synthesis pipeline that creates high-quality human-robot video pairs derived from large egocentric video datasets. Experiments on Human2Robot and EPIC-Kitchens datasets demonstrate that Mitty achieves state-of-the-art performance, exhibits strong generalization capabilities to previously unseen environments, and provides valuable insights for scalable robot learning directly from human visual observations. This work marks a significant step towards more practical and efficient robot learning frameworks grounded in raw human demonstration videos. <div>
arXiv:2512.17253v1 Announce Type: new 
Abstract: Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning</title>
<link>https://arxiv.org/abs/2512.17263</link>
<guid>https://arxiv.org/abs/2512.17263</guid>
<content:encoded><![CDATA[
<div> Chest X-ray, Anatomical segmentation, Synthetic supervision, Multi-organ segmentation, Domain randomization  

<br /><br />Summary:  
1. The paper addresses the challenge of robust anatomical segmentation in chest X-rays (CXRs), focusing on the limitations posed by scarce annotations and variability in real-world imaging conditions.  
2. The authors introduce AnyCXR, a unified framework designed for generalizable multi-organ segmentation across varied CXR projection angles using entirely synthetic supervision without relying on real annotated data.  
3. AnyCXR integrates a Multi-stage Domain Randomization (MSDR) engine that synthesizes over 100,000 anatomically accurate and diverse radiographs from 3D CT volumes to simulate a wide range of imaging scenarios.  
4. A Conditional Joint Annotation Regularization (CAR) learning method is employed to handle partial and imperfect labels by enforcing anatomical consistency in a latent space, improving model robustness.  
5. Trained solely on synthetic data, AnyCXR demonstrates strong zero-shot generalization to multiple real-world datasets, accurately segmenting 54 anatomical structures in posterior-anterior (PA), lateral, and oblique views.  
6. The produced segmentation maps facilitate downstream clinical applications, including automated cardiothoracic ratio measurement, spine curvature analysis, and disease classification, with enhanced performance through anatomical prior integration.  
7. Overall, AnyCXR offers a scalable, practical solution that reduces annotation burden while enhancing robustness in diverse CXR imaging conditions, laying a reliable foundation for anatomy-aware CXR analysis. <div>
arXiv:2512.17263v1 Announce Type: new 
Abstract: Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.17278</link>
<guid>https://arxiv.org/abs/2512.17278</guid>
<content:encoded><![CDATA[
<div> Breast ultrasound segmentation, wavelet enhancement, dual-attention fusion, tumor detection, WDFFU-Mamba  

<br /><br />Summary:  
This work addresses the challenges in segmenting breast tumors in ultrasound (BUS) images, such as speckle noise, imaging artifacts, irregular lesion shapes, and blurred boundaries that hinder accurate detection. To overcome these issues, the authors propose WDFFU-Mamba, a novel U-shaped segmentation network incorporating wavelet-guided enhancement and dual-attention feature fusion. The network includes a Wavelet-denoised High-Frequency-guided Feature (WHF) module that enhances low-level features by suppressing noise and emphasizing high-frequency information. In addition, a Dual Attention Feature Fusion (DAFF) module merges skip-connected and semantic features to improve contextual consistency across the image. Extensive experiments conducted on two public BUS datasets demonstrate that WDFFU-Mamba achieves significantly higher segmentation accuracy compared to existing methods, measured by Dice coefficient and 95th percentile Hausdorff Distance (HD95). The integration of wavelet domain enhancement and attention mechanisms allows the model to be both accurate and robust while maintaining computational efficiency. Furthermore, the model shows strong generalization ability across different datasets, indicating its potential applicability in real-world clinical settings for automated breast tumor ultrasound analysis. Overall, WDFFU-Mamba provides a promising solution for improving BUS image segmentation performance in clinical diagnosis and early tumor screening. <div>
arXiv:2512.17278v1 Announce Type: new 
Abstract: Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</title>
<link>https://arxiv.org/abs/2512.17279</link>
<guid>https://arxiv.org/abs/2512.17279</guid>
<content:encoded><![CDATA[
<div> ultrasound AI, deep learning, multi-organ segmentation, diagnostic accuracy, domain generalization  

<br /><br />Summary:  
This study addresses the limitations of current ultrasound AI tools, which are typically designed for single tasks and lack versatility compared to modern ultrasound systems. The objective was to evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models capable of performing multi-organ classification and segmentation. The Universal UltraSound Image Challenge 2025 (UUSIC25) provided a large dataset of 11,644 images, supplemented with an independent multi-center test set of 2,479 images including data from an entirely unseen center to test the models’ generalization abilities. Key outcome measures were diagnostic performance using Dice Similarity Coefficient (DSC) and Area Under the ROC Curve (AUC), as well as computational efficiency measured by inference time and GPU memory usage. Among 15 submitted algorithms, the top model named SMART achieved a macro-averaged DSC of 0.854 across five segmentation tasks and an AUC of 0.766 for binary classification. While segmentation tasks, such as fetal head identification, showed high accuracy (DSC of 0.942), performance varied on more complex tasks with domain shifts, as evident in breast cancer molecular subtyping where AUC dropped from 0.571 internally to 0.508 on the unseen external dataset. The study concludes that general-purpose AI models can efficiently handle multiple ultrasound tasks with high accuracy, but domain generalization remains a key challenge for clinical application. <div>
arXiv:2512.17279v1 Announce Type: new 
Abstract: IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems.
  OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges.
  CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Model Guided Image Restoration</title>
<link>https://arxiv.org/abs/2512.17292</link>
<guid>https://arxiv.org/abs/2512.17292</guid>
<content:encoded><![CDATA[
<div> Vision-language models, image restoration, diffusion model, semantic coherence, CLIP<br /><br />Summary:<br /><br /> This paper addresses the challenge of combining pixel-level fidelity and high-level semantic understanding in image restoration (IR) tasks. Traditional IR approaches have difficulties in leveraging both visual and linguistic knowledge effectively. To overcome this, the authors propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which integrates vision-language priors from models like CLIP to enhance IR quality. The framework operates in two key stages: first, it performs feature extraction by capturing complementary visual and linguistic embeddings from input images, using techniques such as LoRA fine-tuning and cosine similarity loss to align low-quality and high-quality image captions. Additionally, a degradation predictor is used to separate degradations from clean image content in the embedding space. Second, these rich embeddings are incorporated into a diffusion-based restoration model through cross-attention, enabling improved visual perception and semantic coherence during restoration. Extensive experiments and ablation studies validate VLMIR’s superior performance in both universal and degradation-specific IR tasks, highlighting how integrating visual and linguistic knowledge from vision-language models advances the state-of-the-art in image restoration. <div>
arXiv:2512.17292v1 Announce Type: new 
Abstract: Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, into universal IR. Nevertheless, these methods fail to utilize the linguistic priors to ensure semantic coherence during the restoration process. To address this issue, in this paper, we propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which leverages the rich vision-language priors of VLMs, such as CLIP, to enhance IR performance through improved visual perception and semantic understanding. Our approach consists of two stages: VLM-based feature extraction and diffusion-based image restoration. In the first stage, we extract complementary visual and linguistic representations of input images by condensing the visual perception and high-level semantic priors through VLMs. Specifically, we align the embeddings of captions from low-quality and high-quality images using a cosine similarity loss with LoRA fine-tuning, and employ a degradation predictor to decompose degradation and clean image content embeddings. These complementary visual and textual embeddings are then integrated into a diffusion-based model via cross-attention mechanisms for enhanced restoration. Extensive experiments and ablation studies demonstrate that VLMIR achieves superior performance across both universal and degradation-specific IR tasks, underscoring the critical role of integrated visual and linguistic knowledge from VLMs in advancing image restoration capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\ via Self-Supervised Image Reconstruction</title>
<link>https://arxiv.org/abs/2512.17296</link>
<guid>https://arxiv.org/abs/2512.17296</guid>
<content:encoded><![CDATA[
<div> Keywords: PCBA defect inspection, self-supervised learning, high-resolution images, anomaly localization, SIPCBA-500 dataset<br /><br />Summary:<br /><br />1. The paper addresses the challenge of automated defect inspection on Printed Circuit Board Assemblies (PCBA), focusing on difficulties posed by limited labeled data and micro-defects in high-resolution, visually complex images.  
2. The authors propose HiSIR-Net, a High-resolution, Self-supervised Reconstruction framework designed for pixel-wise localization of defects in PCBAs.  
3. HiSIR-Net incorporates two key modules: (i) the Selective Input-Reconstruction Gate (SIR-Gate), which dynamically determines whether to rely on the reconstructed image or the original input to reduce false positive anomalies and artifacts, and (ii) the Region-level Optimized Patch Selection (ROPS) scheme, which uses positional cues to coherently stitch overlapping patch reconstructions at arbitrary image resolutions.  
4. This design yields clean, high-resolution anomaly maps with a low false positive rate, making defect detection more precise and reliable in practical scenarios, including 4K resolution images.  
5. To support research in high-resolution PCBA defect localization, the authors contribute a new dataset called SIPCBA-500, consisting of 500 self-collected images, and showcase the superior localization performance and practical speed of HiSIR-Net on both the new dataset and existing public benchmarks. Full code and data will be released after acceptance. <div>
arXiv:2512.17296v1 Announce Type: new 
Abstract: Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2512.17298</link>
<guid>https://arxiv.org/abs/2512.17298</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, feature caching, ProCache, dynamic caching, generative modeling<br /><br />Summary:<br /><br />1. Diffusion Transformers (DiTs) excel in generative modeling but are computationally expensive, limiting their real-time use.<br /><br />2. Existing feature caching techniques aim to accelerate DiTs by reusing features across time steps but suffer from uniform caching intervals that do not match the non-uniform feature evolution and lead to error accumulation over large intervals.<br /><br />3. This paper studies how DiT features change during denoising and finds that both feature dynamics and error propagation vary significantly depending on the time step and network depth.<br /><br />4. To address these issues, the authors propose ProCache, a training-free dynamic caching framework with two main components: (i) a constraint-aware caching pattern search module that creates non-uniform caching schedules aligned with the model's temporal feature changes through offline constrained sampling, and (ii) a selective computation module that selectively recomputes key activations within deep blocks and for high-importance tokens to reduce errors while keeping overhead low.<br /><br />5. Experiments on PixArt-alpha and DiT models show that ProCache achieves up to 1.96x and 2.90x speedup respectively, with negligible impact on output quality, outperforming existing caching methods significantly. <div>
arXiv:2512.17298v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatLat: Material Latent Space for PBR Texture Generation</title>
<link>https://arxiv.org/abs/2512.17302</link>
<guid>https://arxiv.org/abs/2512.17302</guid>
<content:encoded><![CDATA[
<div> PBR textures, generative framework, latent space, VAE fine-tuning, cross-view consistency  

<br /><br />Summary:  
This paper proposes a novel generative framework designed to produce high-quality Physically Based Rendering (PBR) textures for 3D meshes, addressing the challenge of scarce large-scale PBR texture datasets. The method leverages pretrained latent image generative models by learning a specialized material latent space (MatLat) through targeted fine-tuning. Unlike prior approaches that freeze the embedding network—leading to distribution shifts when encoding additional PBR channels and complicating diffusion training—this work fine-tunes the pretrained Variational Autoencoder (VAE) to incorporate new material channels with minimal latent distribution deviation. Furthermore, the authors identify that correspondence-aware attention alone does not ensure cross-view consistency unless the latent-to-image mapping preserves spatial locality. To tackle this, they introduce a regularization technique during VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions, thereby maintaining strong pixel-latent spatial correspondence. Ablation studies and comparisons with previous methods show that each component of the proposed framework is critical and collectively contributes to achieving state-of-the-art results in PBR texture fidelity. This work advances the generation of realistic and consistent materials for 3D assets by improving both texture quality and multi-view coherence. <div>
arXiv:2512.17302v1 Announce Type: new 
Abstract: We propose a generative framework for producing high-quality PBR textures on a given 3D mesh. As large-scale PBR texture datasets are scarce, our approach focuses on effectively leveraging the embedding space and diffusion priors of pretrained latent image generative models while learning a material latent space, MatLat, through targeted fine-tuning. Unlike prior methods that freeze the embedding network and thus lead to distribution shifts when encoding additional PBR channels and hinder subsequent diffusion training, we fine-tune the pretrained VAE so that new material channels can be incorporated with minimal latent distribution deviation. We further show that correspondence-aware attention alone is insufficient for cross-view consistency unless the latent-to-image mapping preserves locality. To enforce this locality, we introduce a regularization in the VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions to maintain strong pixel-latent spatial correspondence. Ablation studies and comparison with previous baselines demonstrate that our framework improves PBR texture fidelity and that each component is critical for achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</title>
<link>https://arxiv.org/abs/2512.17303</link>
<guid>https://arxiv.org/abs/2512.17303</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, classifier-free guidance, attention modification, generative quality, adaptive layer selection<br /><br />Summary:  
The paper addresses challenges in diffusion and flow-matching generative models by focusing on guidance techniques that enhance sample quality and consistency. It discusses classifier-free guidance (CFG) as the current standard, which improves performance by contrasting conditional and unconditional samples. However, recent advancements rely on contrasting negative samples at inference time using weaker auxiliary models or attention perturbations but fall short in controlling the granularity or difficulty of these negative samples, with fixed target-layer selection limiting flexibility. To overcome these issues, the authors introduce Exponential Moving Average Guidance (EMAG), a novel, training-free approach that adaptively modifies attention mechanisms within diffusion transformers during inference. This method employs a statistics-based adaptive layer selection that yields more challenging, semantically faithful negative samples, revealing subtle failure modes. EMAG enables the denoiser to correct fine-grained artifacts, resulting in a significant quality boost and an improvement of +0.46 in the human preference score (HPS) compared to CFG. Furthermore, EMAG is compatible with other advanced guidance methods like APG and CADS, allowing further enhancement in sample quality and preference metrics. This work demonstrates a practical and effective strategy for improving generative model outputs without additional training overhead. <div>
arXiv:2512.17303v1 Announce Type: new 
Abstract: In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</title>
<link>https://arxiv.org/abs/2512.17306</link>
<guid>https://arxiv.org/abs/2512.17306</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Chain-of-Thought, multi-turn reasoning, self-reflection, reinforcement learning<br /><br />Summary: Recent developments in large Vision-Language Models (VLMs) show strong reasoning abilities on complex visual tasks by utilizing Chain-of-Thought (CoT) processes that involve active tool use to analyze images rather than passive perception. However, current models lack effective self-reflection and often fail to correct incorrect reasoning paths. To overcome this, the paper introduces DRIM, a model designed to enable deep, reliable multi-turn reasoning within multimodal CoT frameworks. The proposed approach involves a three-stage pipeline: data construction, cold-start supervised fine-tuning (SFT), and reinforcement learning (RL). Initially, a high-resolution image dataset with challenging, verifiable visual question-answer pairs requiring multiple tool calls is created. In the SFT phase, tool operation trajectories are gathered to guide the model towards structured multi-turn reasoning. The RL phase employs redundancy-penalized policy optimization, encouraging self-reflective reasoning by penalizing insufficient exploration and incorrect answer generation. Experiments demonstrate that DRIM significantly improves performance on visual understanding benchmarks by fostering multi-scale exploration and self-correction, addressing key limitations in prior VLM reasoning approaches. <div>
arXiv:2512.17306v1 Announce Type: new 
Abstract: Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning</title>
<link>https://arxiv.org/abs/2512.17312</link>
<guid>https://arxiv.org/abs/2512.17312</guid>
<content:encoded><![CDATA[
<div> Keywords: CodeDance, visual reasoning, executable code, tool orchestration, reinforcement learning  

<br /><br />Summary: This paper presents CodeDance, an innovative approach for visual reasoning that leverages executable code to orchestrate multiple tools, allowing stepwise and transparent reasoning through the creation and execution of code snippets. Unlike traditional methods relying on fixed visual schemas or text-only reasoning chains, CodeDance integrates flexible tool calls to compute intermediate results and generate visual artifacts such as bounding boxes, lines, and plots to facilitate interpretability and self-verifiability. A key contribution is the introduction of a reward mechanism balancing exploration and efficiency during reinforcement learning, preventing overuse of any single tool. Remarkably, CodeDance exhibits emergent behaviors beyond its initial training, including novel tool invocations, unseen compositions of tools, and cross-task transfer capabilities, all without task-specific fine-tuning. This suggests the approach has strong generalization potential for scalable executable visual reasoning. Extensive evaluation on various benchmarks—such as visual search, mathematical problem-solving, and chart question answering—demonstrates that CodeDance consistently outperforms schema-driven and text-only baselines. It also surpasses performance of advanced closed models, including GPT-4o, and larger open-source models, highlighting its effectiveness and versatility in complex reasoning tasks that require integrated visual and computational understanding. <div>
arXiv:2512.17312v1 Announce Type: new 
Abstract: Recent releases such as o3 highlight human-like "thinking with images" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.17313</link>
<guid>https://arxiv.org/abs/2512.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Few-Shot Adaptation, Parameter-Efficient Fine-Tuning, Auxiliary Descriptive Knowledge, Large Language Model<br /><br />Summary: The paper addresses the challenge that Vision-Language Models (VLMs) face in downstream tasks when there is a distribution shift from their pre-training data, particularly focusing on Few-Shot Adaptation (FSA) using Parameter-Efficient Fine-Tuning (PEFT). Existing PEFT approaches rely heavily on fixed, handcrafted prompts, which limit their semantic understanding of classes and reduce effectiveness. To overcome this, the authors introduce Auxiliary Descriptive Knowledge (ADK), a novel framework designed to enrich text representations efficiently without increasing inference costs. ADK utilizes a Large Language Model offline to generate a comprehensive set of descriptive prompts for each class. These descriptions are used in two forms: Compositional Knowledge, which averages prompts to offer rich semantics useful for ambiguous or unfamiliar class names, and Instance-Specific Knowledge, which applies a lightweight, non-parametric attention mechanism to dynamically select relevant descriptions for each image. This method supplements the existing handcrafted prompts, improving category discrimination across domains. ADK is a parameter-free, plug-and-play module that enhances various PEFT methods. Extensive experiments demonstrate that ADK consistently improves multiple PEFT baselines and establishes a new state-of-the-art performance in diverse scenarios. <div>
arXiv:2512.17313v1 Announce Type: new 
Abstract: Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</title>
<link>https://arxiv.org/abs/2512.17319</link>
<guid>https://arxiv.org/abs/2512.17319</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, Remote Sensing, Super-High-Resolution Benchmark, Visual Question Answering, Adversarial Filtering<br /><br />Summary: The paper addresses limitations in current remote sensing (RS) benchmarks, which mostly use low-resolution images or have poorly designed reasoning tasks. It is observed that text-only large language models (LLMs) can perform comparably to multimodal vision-language models (VLMs) on RS reasoning tasks without visual input, exposing a mismatch between current benchmarks and evaluation goals focused on visual understanding. To tackle this, the authors propose RSHR-Bench, a novel super-high-resolution benchmark comprising 5,329 full-scene images with a minimum dimension of 4,000 pixels and up to approximately 300 million pixels per image, sourced from popular RS and UAV datasets. The benchmark features four main task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks encompass nine perception categories and four reasoning types, supporting multi-turn and multi-image dialogue scenarios. To minimize reliance on language biases, they introduce adversarial filtering using strong LLMs, followed by rigorous human verification. The dataset includes 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation pairs. Evaluation of various open-source, closed-source, and RS-specific VLMs reveals significant performance gaps in handling super-high-resolution data, highlighting challenges for future research. <div>
arXiv:2512.17319v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</title>
<link>https://arxiv.org/abs/2512.17320</link>
<guid>https://arxiv.org/abs/2512.17320</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, concept erasure, EMMA benchmark, bias evaluation, model robustness<br /><br />Summary:<br />The paper addresses key challenges in text-to-image (T2I) generation, particularly privacy, bias, and copyright issues arising from widespread adoption. It focuses on concept erasure techniques that selectively remove unwanted concepts from pre-trained T2I models without full retraining. Existing concept erasure methods face limitations as they are often evaluated on a narrow set of concepts using simplistic prompts. To overcome this, the authors introduce EMMA, a comprehensive benchmark designed to evaluate concept erasure methods across five critical dimensions using 12 diverse metrics. EMMA extends evaluation beyond traditional criteria like image quality and computational efficiency by incorporating tests under challenging conditions such as indirect concept references, visually similar but non-target concepts, and potential amplification of gender and ethnicity biases. Using EMMA, the study systematically analyzes five concept erasure methods across five domains: objects, celebrities, art styles, NSFW (not safe for work), and copyright-sensitive content. Results reveal that current methods struggle to fully erase concepts when indirectly prompted and frequently fail to avoid generating visually similar unintended concepts. Additionally, some methods exacerbate gender and ethnicity biases present in the original models. The findings highlight substantial gaps and call for improved, socially aware concept erasure strategies in T2I generation. <div>
arXiv:2512.17320v1 Announce Type: new 
Abstract: The widespread adoption of text-to-image (T2I) generation has raised concerns about privacy, bias, and copyright violations. Concept erasure techniques offer a promising solution by selectively removing undesired concepts from pre-trained models without requiring full retraining. However, these methods are often evaluated on a limited set of concepts, relying on overly simplistic and direct prompts. To test the boundaries of concept erasure techniques, and assess whether they truly remove targeted concepts from model representations, we introduce EMMA, a benchmark that evaluates five key dimensions of concept erasure over 12 metrics. EMMA goes beyond standard metrics like image quality and time efficiency, testing robustness under challenging conditions, including indirect descriptions, visually similar non-target concepts, and potential gender and ethnicity bias, providing a socially aware analysis of method behavior. Using EMMA, we analyze five concept erasure methods across five domains (objects, celebrities, art styles, NSFW, and copyright). Our results show that existing methods struggle with implicit prompts (i.e., generating the erased concept when it is indirectly referenced) and visually similar non-target concepts (i.e., failing to generate non-targeted concepts resembling the erased one), while some amplify gender and ethnicity bias compared to the original model.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rotterdam artery-vein segmentation (RAV) dataset</title>
<link>https://arxiv.org/abs/2512.17322</link>
<guid>https://arxiv.org/abs/2512.17322</guid>
<content:encoded><![CDATA[
<div> artery-vein segmentation, fundus images, retinal vascular analysis, machine learning, image quality<br /><br />Summary:<br /><br />Purpose: The article presents a diverse and high-quality dataset of color fundus images (CFIs) annotated with detailed artery-vein (A/V) segmentation to advance machine learning techniques in ophthalmic vascular analysis.<br /><br />Methods: Images were obtained from the Rotterdam Study, covering a broad spectrum of ages, imaging devices, and conditions. Annotation was performed via a custom interface enabling graders to label arteries, veins, and unknown vessels on separate layers, starting from initial vessel segmentations. The connectivity of vessel annotations was explicitly checked and corrected through connected component visualization.<br /><br />Results: The dataset comprises 1024x1024 pixel PNG images available in three forms: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. It contains a wide range of image qualities, including challenging samples typically excluded in automated quality assessments but valuable for vascular information.<br /><br />Conclusion: This dataset provides an extensive and heterogeneous resource of CFIs with validated segmentations, supporting robust training and benchmarking of machine learning models capable of handling real-world imaging variability.<br /><br />Translational Relevance: By offering connectivity-validated A/V masks alongside diverse imaging conditions, the dataset supports the creation of clinically relevant and generalizable machine learning tools, potentially enhancing automated screening and diagnosis of ocular and systemic diseases. <div>
arXiv:2512.17322v1 Announce Type: new 
Abstract: Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology.
  Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools.
  Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information.
  Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings.
  Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training</title>
<link>https://arxiv.org/abs/2512.17323</link>
<guid>https://arxiv.org/abs/2512.17323</guid>
<content:encoded><![CDATA[
<div> Keywords: Video frame prediction, event cameras, diffusion model, residual training, temporal consistency<br /><br />Summary:<br />1. This paper addresses the challenge of video frame prediction in dynamic scenes, where traditional methods struggle due to a lack of future frame information and suffer from errors like holes and blurring when using optical flow-based reconstruction.<br />2. The authors leverage event cameras, which capture asynchronous per-pixel brightness changes with high temporal resolution, providing richer motion information for better prediction.<br />3. They propose DESSERT, a novel framework combining diffusion models with residual training to synthesize video frames from event data, improving temporal consistency and sharpness.<br />4. The training occurs in two stages: first, an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) aligns event frames with residual differences between anchor and target frames; second, a diffusion model denoises residual latents conditioned on event inputs.<br />5. Additionally, a Diverse-Length Temporal (DLT) augmentation strategy is introduced to enhance robustness by training on frame segments with varying temporal lengths.<br />6. Experimental results demonstrate that DESSERT outperforms prior event-based reconstruction, image-based video prediction, event-based video prediction, and event-based interpolation methods, delivering superior frame quality and temporal coherence. <div>
arXiv:2512.17323v1 Announce Type: new 
Abstract: Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling</title>
<link>https://arxiv.org/abs/2512.17326</link>
<guid>https://arxiv.org/abs/2512.17326</guid>
<content:encoded><![CDATA[
<div> Polysome, HISTAI, Vision-Language Models, Whole-Slide Images, Visual-Question Answering<br /><br />Summary:<br /><br />1. The paper addresses limitations in current vision-language models (VLMs) for pathology, which often focus only on small regions within whole-slide images (WSIs), provide static outputs, or depend on non-public data, thus hindering reproducibility and generalizability.<br /><br />2. To overcome these challenges, the authors introduce Polysome, a standardized tool designed for synthetic instruction generation that can create detailed instructional data for model training.<br /><br />3. Using Polysome, they generate HISTAI-Instruct from the publicly available HISTAI dataset, resulting in a large instruction tuning dataset comprising 24,259 WSIs and over 1.1 million instruction-response pairs.<br /><br />4. The authors train a new vision-language model named ANTONI-α on HISTAI-Instruct, equipping it with the capability to perform visual-question answering (VQA) tasks at the whole-slide image level.<br /><br />5. Experimental results demonstrate that ANTONI-α surpasses the existing MedGemma model in critical pathology-related VQA tasks such as tissue identification, neoplasm detection, and differential diagnosis.<br /><br />6. Furthermore, the study explores how varying training data amounts impact ANTONI-α’s performance, offering insights into model scalability.<br /><br />7. Importantly, all developed methods, data, and code are made publicly available, promoting transparency and reproducibility in future research. <div>
arXiv:2512.17326v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have the potential to become co-pilots for pathologists. However, most VLMs either focus on small regions of interest within whole-slide images, provide only static slide-level outputs, or rely on data that is not publicly available, limiting reproducibility. Furthermore, training data containing WSIs paired with detailed clinical reports is scarce, restricting progress toward transparent and generalisable VLMs. We address these limitations with three main contributions. First, we introduce Polysome, a standardised tool for synthetic instruction generation. Second, we apply Polysome to the public HISTAI dataset, generating HISTAI-Instruct, a large whole-slide instruction tuning dataset spanning 24,259 slides and over 1.1 million instruction-response pairs. Finally, we use HISTAI-Instruct to train ANTONI-{\alpha}, a VLM capable of visual-question answering (VQA). We show that ANTONI-{\alpha} outperforms MedGemma on WSI-level VQA tasks of tissue identification, neoplasm detection, and differential diagnosis. We also compare the performance of multiple incarnations of ANTONI-{\alpha} trained with different amounts of data. All methods, data, and code are publicly available.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation</title>
<link>https://arxiv.org/abs/2512.17331</link>
<guid>https://arxiv.org/abs/2512.17331</guid>
<content:encoded><![CDATA[
<div> Keywords: neural portrait animation, warping framework, 3D dense optical flow, cross-attention, confidence-guided fusion<br /><br />Summary:<br /><br />This paper introduces SynergyWarpNet, a novel attention-guided cooperative warping framework aimed at enhancing high-fidelity talking head synthesis. The authors address limitations of previous methods such as explicit warping's difficulty with accurate motion transfer and missing regions, and attention-based warping's high complexity and poor geometric grounding. SynergyWarpNet operates in three progressive stages: First, it employs an explicit warping module that uses 3D dense optical flow to achieve coarse spatial alignment between a source portrait and a driving image. Second, a reference-augmented correction module utilizes cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions, improving detail restoration. Third, a confidence-guided fusion module integrates the warped outputs using spatially-adaptive fusion governed by a learned confidence map, balancing structural alignment and visual consistency in the final output. Extensive experiments on benchmark datasets demonstrate that SynergyWarpNet achieves state-of-the-art results in neural portrait animation, making it a promising solution for applications such as virtual avatars, telepresence, and digital content creation. <div>
arXiv:2512.17331v1 Announce Type: new 
Abstract: Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-level distortion-aware deformable network for omnidirectional image super-resolution</title>
<link>https://arxiv.org/abs/2512.17343</link>
<guid>https://arxiv.org/abs/2512.17343</guid>
<content:encoded><![CDATA[
<div> OmniDirectional Image Super-Resolution, EquiRectangular Projection, geometric distortion, deformable convolutions, multi-level feature fusion<br /><br />Summary:<br /><br />This paper addresses the challenge of enhancing visual quality in OmniDirectional Images (ODIs) through super-resolution techniques, focusing on the geometric distortions introduced by EquiRectangular Projection (ERP). ERP projection causes latitude-dependent distortions, with minimal distortion near the equator but significant stretching near the poles, complicating feature extraction in ODIs. Existing super-resolution methods struggle with limited sampling ranges and insufficient capacity to capture these wide-ranging distortions. To overcome these limitations, the authors propose a novel Multi-level Distortion-aware Deformable Network (MDDN) tailored for OmniDirectional Image Super-Resolution (ODISR). MDDN’s feature extractor includes three parallel branches: a deformable attention mechanism with dilation rate 1 and two dilated deformable convolutions with dilation rates 2 and 3, which together broaden the receptive field and sampling range to better capture distorted patterns. The network integrates a multi-level feature fusion module to adaptively combine extracted features for richer representation of geometric distortions. Additionally, a low-rank decomposition strategy is employed on the dilated deformable convolutions to reduce computational costs. Extensive experiments on public datasets demonstrate that MDDN outperforms current state-of-the-art methods, validating its effectiveness and superiority in handling distortion-aware ODISR tasks. <div>
arXiv:2512.17343v1 Announce Type: new 
Abstract: As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.17350</link>
<guid>https://arxiv.org/abs/2512.17350</guid>
<content:encoded><![CDATA[
arXiv:2512.17350v1 Announce Type: new 
Abstract: The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors</title>
<link>https://arxiv.org/abs/2512.17376</link>
<guid>https://arxiv.org/abs/2512.17376</guid>
<content:encoded><![CDATA[
arXiv:2512.17376v1 Announce Type: new 
Abstract: Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.17396</link>
<guid>https://arxiv.org/abs/2512.17396</guid>
<content:encoded><![CDATA[
arXiv:2512.17396v1 Announce Type: new 
Abstract: In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification</title>
<link>https://arxiv.org/abs/2512.17416</link>
<guid>https://arxiv.org/abs/2512.17416</guid>
<content:encoded><![CDATA[
arXiv:2512.17416v1 Announce Type: new 
Abstract: Deep neural networks are starting to show their worth in critical applications such as assisted cancer diagnosis. However, for their outputs to get accepted in practice, the results they provide should be explainable in a way easily understood by pathologists. A well-known and widely used explanation technique is occlusion, which, however, can take a long time to compute, thus slowing the development and interaction with pathologists. In this work, we set out to find a faster replacement for occlusion in a successful system for detecting prostate cancer. Since there is no established framework for comparing the performance of various explanation methods, we first identified suitable comparison criteria and selected corresponding metrics. Based on the results, we were able to choose a different explanation method, which cut the previously required explanation time at least by a factor of 10, without any negative impact on the quality of outputs. This speedup enables rapid iteration in model development and debugging and brings us closer to adopting AI-assisted prostate cancer detection in clinical settings. We propose that our approach to finding the replacement for occlusion can be used to evaluate candidate methods in other related applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments</title>
<link>https://arxiv.org/abs/2512.17432</link>
<guid>https://arxiv.org/abs/2512.17432</guid>
<content:encoded><![CDATA[
arXiv:2512.17432v1 Announce Type: new 
Abstract: Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Xiaomi MiMo-VL-Miloco Technical Report</title>
<link>https://arxiv.org/abs/2512.17436</link>
<guid>https://arxiv.org/abs/2512.17436</guid>
<content:encoded><![CDATA[
arXiv:2512.17436v1 Announce Type: new 
Abstract: We open-source \textbf{MiMo-VL-Miloco-7B} and its quantized variant \textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents</title>
<link>https://arxiv.org/abs/2512.17445</link>
<guid>https://arxiv.org/abs/2512.17445</guid>
<content:encoded><![CDATA[
arXiv:2512.17445v1 Announce Type: new 
Abstract: LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title>
<link>https://arxiv.org/abs/2512.17450</link>
<guid>https://arxiv.org/abs/2512.17450</guid>
<content:encoded><![CDATA[
arXiv:2512.17450v1 Announce Type: new 
Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</title>
<link>https://arxiv.org/abs/2512.17459</link>
<guid>https://arxiv.org/abs/2512.17459</guid>
<content:encoded><![CDATA[
arXiv:2512.17459v1 Announce Type: new 
Abstract: Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.
  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</title>
<link>https://arxiv.org/abs/2512.17488</link>
<guid>https://arxiv.org/abs/2512.17488</guid>
<content:encoded><![CDATA[
arXiv:2512.17488v1 Announce Type: new 
Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.17489</link>
<guid>https://arxiv.org/abs/2512.17489</guid>
<content:encoded><![CDATA[
arXiv:2512.17489v1 Announce Type: new 
Abstract: Current text-to-image (T2I) models have demonstrated remarkable progress in creative image generation, yet they still lack precise control over scene illuminants, which is a crucial factor for content designers aiming to manipulate the mood, atmosphere, and visual aesthetics of generated images. In this paper, we present an illuminant personalization method named LumiCtrl that learns an illuminant prompt given a single image of an object. LumiCtrl consists of three basic components: given an image of the object, our method applies (a) physics-based illuminant augmentation along the Planckian locus to create fine-tuning variants under standard illuminants; (b) edge-guided prompt disentanglement using a frozen ControlNet to ensure prompts focus on illumination rather than structure; and (c) a masked reconstruction loss that focuses learning on the foreground object while allowing the background to adapt contextually, enabling what we call contextual light adaptation. We qualitatively and quantitatively compare LumiCtrl against other T2I customization methods. The results show that our method achieves significantly better illuminant fidelity, aesthetic quality, and scene coherence compared to existing personalization baselines. A human preference study further confirms strong user preference for LumiCtrl outputs. The code and data will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.17492</link>
<guid>https://arxiv.org/abs/2512.17492</guid>
<content:encoded><![CDATA[
arXiv:2512.17492v1 Announce Type: new 
Abstract: Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2512.17495</link>
<guid>https://arxiv.org/abs/2512.17495</guid>
<content:encoded><![CDATA[
arXiv:2512.17495v1 Announce Type: new 
Abstract: Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort</title>
<link>https://arxiv.org/abs/2512.17499</link>
<guid>https://arxiv.org/abs/2512.17499</guid>
<content:encoded><![CDATA[
arXiv:2512.17499v1 Announce Type: new 
Abstract: Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer.
  Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium).
  Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa > 0.90) for all AI models and scanner pairs, including low-cost compact scanner.
  Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology.
  Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</title>
<link>https://arxiv.org/abs/2512.17504</link>
<guid>https://arxiv.org/abs/2512.17504</guid>
<content:encoded><![CDATA[
arXiv:2512.17504v1 Announce Type: new 
Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</title>
<link>https://arxiv.org/abs/2512.17514</link>
<guid>https://arxiv.org/abs/2512.17514</guid>
<content:encoded><![CDATA[
arXiv:2512.17514v1 Announce Type: new 
Abstract: Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</title>
<link>https://arxiv.org/abs/2512.17517</link>
<guid>https://arxiv.org/abs/2512.17517</guid>
<content:encoded><![CDATA[
arXiv:2512.17517v1 Announce Type: new 
Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title>
<link>https://arxiv.org/abs/2512.17532</link>
<guid>https://arxiv.org/abs/2512.17532</guid>
<content:encoded><![CDATA[
arXiv:2512.17532v1 Announce Type: new 
Abstract: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</title>
<link>https://arxiv.org/abs/2512.17541</link>
<guid>https://arxiv.org/abs/2512.17541</guid>
<content:encoded><![CDATA[
arXiv:2512.17541v1 Announce Type: new 
Abstract: We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</title>
<link>https://arxiv.org/abs/2512.17545</link>
<guid>https://arxiv.org/abs/2512.17545</guid>
<content:encoded><![CDATA[
arXiv:2512.17545v1 Announce Type: new 
Abstract: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.17547</link>
<guid>https://arxiv.org/abs/2512.17547</guid>
<content:encoded><![CDATA[
arXiv:2512.17547v1 Announce Type: new 
Abstract: 3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points</title>
<link>https://arxiv.org/abs/2512.17566</link>
<guid>https://arxiv.org/abs/2512.17566</guid>
<content:encoded><![CDATA[
arXiv:2512.17566v1 Announce Type: new 
Abstract: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis</title>
<link>https://arxiv.org/abs/2512.17573</link>
<guid>https://arxiv.org/abs/2512.17573</guid>
<content:encoded><![CDATA[
arXiv:2512.17573v1 Announce Type: new 
Abstract: Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \url{https://github.com/stonecutter-21/roomeditor}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging</title>
<link>https://arxiv.org/abs/2512.17578</link>
<guid>https://arxiv.org/abs/2512.17578</guid>
<content:encoded><![CDATA[
arXiv:2512.17578v1 Announce Type: new 
Abstract: Video snapshot compressive imaging (SCI) captures dynamic scene sequences through a two-dimensional (2D) snapshot, fundamentally relying on optical modulation for hardware compression and the corresponding software reconstruction. While mainstream video SCI using random binary modulation has demonstrated success, it inevitably results in temporal aliasing during compression. One-hot modulation, activating only one sub-frame per pixel, provides a promising solution for achieving perfect temporal decoupling, thereby alleviating issues associated with aliasing. However, no algorithms currently exist to fully exploit this potential. To bridge this gap, we propose an algorithm specifically designed for one-hot masks. First, leveraging the decoupling properties of one-hot modulation, we transform the reconstruction task into a generative video inpainting problem and introduce a stochastic differential equation (SDE) of the forward process that aligns with the hardware compression process. Next, we identify limitations of the pure diffusion method for video SCI and propose a novel framework that combines one-step regression initialization with one-step diffusion refinement. Furthermore, to mitigate the spatial degradation caused by one-hot modulation, we implement a dual optical path at the hardware level, utilizing complementary information from another path to enhance the inpainted video. To our knowledge, this is the first work integrating diffusion into video SCI reconstruction. Experiments conducted on synthetic datasets and real scenes demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Imaging AI Competitions Lack Fairness</title>
<link>https://arxiv.org/abs/2512.17581</link>
<guid>https://arxiv.org/abs/2512.17581</guid>
<content:encoded><![CDATA[
arXiv:2512.17581v1 Announce Type: new 
Abstract: Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.17601</link>
<guid>https://arxiv.org/abs/2512.17601</guid>
<content:encoded><![CDATA[
arXiv:2512.17601v1 Announce Type: new 
Abstract: Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</title>
<link>https://arxiv.org/abs/2512.17605</link>
<guid>https://arxiv.org/abs/2512.17605</guid>
<content:encoded><![CDATA[
arXiv:2512.17605v1 Announce Type: new 
Abstract: Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR</title>
<link>https://arxiv.org/abs/2512.17610</link>
<guid>https://arxiv.org/abs/2512.17610</guid>
<content:encoded><![CDATA[
arXiv:2512.17610v1 Announce Type: new 
Abstract: Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy.
  These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection.
  However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs.
  This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution</title>
<link>https://arxiv.org/abs/2512.17612</link>
<guid>https://arxiv.org/abs/2512.17612</guid>
<content:encoded><![CDATA[
arXiv:2512.17612v1 Announce Type: new 
Abstract: High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.17620</link>
<guid>https://arxiv.org/abs/2512.17620</guid>
<content:encoded><![CDATA[
arXiv:2512.17620v1 Announce Type: new 
Abstract: Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology</title>
<link>https://arxiv.org/abs/2512.17621</link>
<guid>https://arxiv.org/abs/2512.17621</guid>
<content:encoded><![CDATA[
arXiv:2512.17621v1 Announce Type: new 
Abstract: While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs</title>
<link>https://arxiv.org/abs/2512.17640</link>
<guid>https://arxiv.org/abs/2512.17640</guid>
<content:encoded><![CDATA[
arXiv:2512.17640v1 Announce Type: new 
Abstract: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Constraint In-Context Generation for Instructional Video Editing</title>
<link>https://arxiv.org/abs/2512.17650</link>
<guid>https://arxiv.org/abs/2512.17650</guid>
<content:encoded><![CDATA[
arXiv:2512.17650v1 Announce Type: new 
Abstract: The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos</title>
<link>https://arxiv.org/abs/2512.17655</link>
<guid>https://arxiv.org/abs/2512.17655</guid>
<content:encoded><![CDATA[
arXiv:2512.17655v1 Announce Type: new 
Abstract: Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</title>
<link>https://arxiv.org/abs/2512.17673</link>
<guid>https://arxiv.org/abs/2512.17673</guid>
<content:encoded><![CDATA[
arXiv:2512.17673v1 Announce Type: new 
Abstract: Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</title>
<link>https://arxiv.org/abs/2512.17675</link>
<guid>https://arxiv.org/abs/2512.17675</guid>
<content:encoded><![CDATA[
arXiv:2512.17675v1 Announce Type: new 
Abstract: Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation</title>
<link>https://arxiv.org/abs/2512.17717</link>
<guid>https://arxiv.org/abs/2512.17717</guid>
<content:encoded><![CDATA[
arXiv:2512.17717v1 Announce Type: new 
Abstract: We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses</title>
<link>https://arxiv.org/abs/2512.17724</link>
<guid>https://arxiv.org/abs/2512.17724</guid>
<content:encoded><![CDATA[
arXiv:2512.17724v1 Announce Type: new 
Abstract: The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</title>
<link>https://arxiv.org/abs/2512.17726</link>
<guid>https://arxiv.org/abs/2512.17726</guid>
<content:encoded><![CDATA[
arXiv:2512.17726v1 Announce Type: new 
Abstract: Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</title>
<link>https://arxiv.org/abs/2512.17730</link>
<guid>https://arxiv.org/abs/2512.17730</guid>
<content:encoded><![CDATA[
arXiv:2512.17730v1 Announce Type: new 
Abstract: Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</title>
<link>https://arxiv.org/abs/2512.17773</link>
<guid>https://arxiv.org/abs/2512.17773</guid>
<content:encoded><![CDATA[
arXiv:2512.17773v1 Announce Type: new 
Abstract: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</title>
<link>https://arxiv.org/abs/2512.17781</link>
<guid>https://arxiv.org/abs/2512.17781</guid>
<content:encoded><![CDATA[
arXiv:2512.17781v1 Announce Type: new 
Abstract: Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</title>
<link>https://arxiv.org/abs/2512.17782</link>
<guid>https://arxiv.org/abs/2512.17782</guid>
<content:encoded><![CDATA[
arXiv:2512.17782v1 Announce Type: new 
Abstract: Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.
  Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.
  UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras</title>
<link>https://arxiv.org/abs/2512.17784</link>
<guid>https://arxiv.org/abs/2512.17784</guid>
<content:encoded><![CDATA[
arXiv:2512.17784v1 Announce Type: new 
Abstract: Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animate Any Character in Any World</title>
<link>https://arxiv.org/abs/2512.17796</link>
<guid>https://arxiv.org/abs/2512.17796</guid>
<content:encoded><![CDATA[
arXiv:2512.17796v1 Announce Type: new 
Abstract: Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</title>
<link>https://arxiv.org/abs/2512.17817</link>
<guid>https://arxiv.org/abs/2512.17817</guid>
<content:encoded><![CDATA[
arXiv:2512.17817v1 Announce Type: new 
Abstract: While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.
  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges</title>
<link>https://arxiv.org/abs/2512.17838</link>
<guid>https://arxiv.org/abs/2512.17838</guid>
<content:encoded><![CDATA[
arXiv:2512.17838v1 Announce Type: new 
Abstract: Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&amp;D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.17851</link>
<guid>https://arxiv.org/abs/2512.17851</guid>
<content:encoded><![CDATA[
arXiv:2512.17851v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</title>
<link>https://arxiv.org/abs/2512.17852</link>
<guid>https://arxiv.org/abs/2512.17852</guid>
<content:encoded><![CDATA[
arXiv:2512.17852v1 Announce Type: new 
Abstract: Raman spectroscopy enables non-destructive, label-free molecular analysis with high specificity, making it a powerful tool for biomedical diagnostics. However, its application to biological tissues is challenged by inherently weak Raman scattering and strong fluorescence background, which significantly degrade signal quality. In this study, we present a simulation-driven denoising framework that combines a statistically grounded noise model with deep learning to enhance Raman spectra acquired under fluorescence-dominated conditions. We comprehensively modeled major noise sources. Based on this model, we generated biologically realistic Raman spectra and used them to train a cascaded deep neural network designed to jointly suppress stochastic detector noise and fluorescence baseline interference. To evaluate the performance of our approach, we simulated human skin spectra derived from real experimental data as a validation case study. Our results demonstrate the potential of physics-informed learning to improve spectral quality and enable faster, more accurate Raman-based tissue analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</title>
<link>https://arxiv.org/abs/2512.17864</link>
<guid>https://arxiv.org/abs/2512.17864</guid>
<content:encoded><![CDATA[
arXiv:2512.17864v1 Announce Type: new 
Abstract: Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InSPECT: Invariant Spectral Features Preservation of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.17873</link>
<guid>https://arxiv.org/abs/2512.17873</guid>
<content:encoded><![CDATA[
arXiv:2512.17873v1 Announce Type: new 
Abstract: Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
<link>https://arxiv.org/abs/2512.17875</link>
<guid>https://arxiv.org/abs/2512.17875</guid>
<content:encoded><![CDATA[
arXiv:2512.17875v1 Announce Type: new 
Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</title>
<link>https://arxiv.org/abs/2512.17891</link>
<guid>https://arxiv.org/abs/2512.17891</guid>
<content:encoded><![CDATA[
arXiv:2512.17891v1 Announce Type: new 
Abstract: Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
<link>https://arxiv.org/abs/2512.17897</link>
<guid>https://arxiv.org/abs/2512.17897</guid>
<content:encoded><![CDATA[
arXiv:2512.17897v1 Announce Type: new 
Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.17900</link>
<guid>https://arxiv.org/abs/2512.17900</guid>
<content:encoded><![CDATA[
arXiv:2512.17900v1 Announce Type: new 
Abstract: Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness of Vision in Open Foundation Models</title>
<link>https://arxiv.org/abs/2512.17902</link>
<guid>https://arxiv.org/abs/2512.17902</guid>
<content:encoded><![CDATA[
arXiv:2512.17902v1 Announce Type: new 
Abstract: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dexterous World Models</title>
<link>https://arxiv.org/abs/2512.17907</link>
<guid>https://arxiv.org/abs/2512.17907</guid>
<content:encoded><![CDATA[
arXiv:2512.17907v1 Announce Type: new 
Abstract: Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.
  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.
  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
<link>https://arxiv.org/abs/2512.17908</link>
<guid>https://arxiv.org/abs/2512.17908</guid>
<content:encoded><![CDATA[
arXiv:2512.17908v1 Announce Type: new 
Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</title>
<link>https://arxiv.org/abs/2512.17909</link>
<guid>https://arxiv.org/abs/2512.17909</guid>
<content:encoded><![CDATA[
arXiv:2512.17909v1 Announce Type: new 
Abstract: Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.16964</link>
<guid>https://arxiv.org/abs/2512.16964</guid>
<content:encoded><![CDATA[
arXiv:2512.16964v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</title>
<link>https://arxiv.org/abs/2512.17394</link>
<guid>https://arxiv.org/abs/2512.17394</guid>
<content:encoded><![CDATA[
arXiv:2512.17394v1 Announce Type: cross 
Abstract: Theory of Mind (ToM) -- the ability to attribute beliefs, desires, and emotions to others -- is fundamental for human social intelligence, yet remains a major challenge for artificial agents. Existing Vision-Language Models (VLMs) are increasingly applied in socially grounded tasks, but their capacity for cross-cultural ToM reasoning is largely unexplored. In this work, we introduce CulturalToM-VQA, a new evaluation benchmark containing 5095 questions designed to probe ToM reasoning across diverse cultural contexts through visual question answering. The dataset captures culturally grounded cues such as rituals, attire, gestures, and interpersonal dynamics, enabling systematic evaluation of ToM reasoning beyond Western-centric benchmarks. Our dataset is built through a VLM-assisted human-in-the-loop pipeline, where human experts first curate culturally rich images across traditions, rituals, and social interactions; a VLM then assist in generating structured ToM-focused scene descriptions, which are refined into question-answer pairs spanning a taxonomy of six ToM tasks and four graded complexity levels. The resulting dataset covers diverse theory of mind facets such as mental state attribution, false belief reasoning, non-literal communication, social norm violations, perspective coordination, and multi-agent reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry</title>
<link>https://arxiv.org/abs/2512.17505</link>
<guid>https://arxiv.org/abs/2512.17505</guid>
<content:encoded><![CDATA[
arXiv:2512.17505v1 Announce Type: cross 
Abstract: This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.17585</link>
<guid>https://arxiv.org/abs/2512.17585</guid>
<content:encoded><![CDATA[
arXiv:2512.17585v1 Announce Type: cross 
Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</title>
<link>https://arxiv.org/abs/2512.17594</link>
<guid>https://arxiv.org/abs/2512.17594</guid>
<content:encoded><![CDATA[
arXiv:2512.17594v1 Announce Type: cross 
Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</title>
<link>https://arxiv.org/abs/2512.17759</link>
<guid>https://arxiv.org/abs/2512.17759</guid>
<content:encoded><![CDATA[
arXiv:2512.17759v1 Announce Type: cross 
Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.17774</link>
<guid>https://arxiv.org/abs/2512.17774</guid>
<content:encoded><![CDATA[
arXiv:2512.17774v1 Announce Type: cross 
Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LN3DIFF++: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation</title>
<link>https://arxiv.org/abs/2403.12019</link>
<guid>https://arxiv.org/abs/2403.12019</guid>
<content:encoded><![CDATA[
arXiv:2403.12019v3 Announce Type: replace 
Abstract: The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called LN3Diff++ to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution</title>
<link>https://arxiv.org/abs/2411.07449</link>
<guid>https://arxiv.org/abs/2411.07449</guid>
<content:encoded><![CDATA[
arXiv:2411.07449v3 Announce Type: replace 
Abstract: Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted "Goldilocks zone" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</title>
<link>https://arxiv.org/abs/2411.15355</link>
<guid>https://arxiv.org/abs/2411.15355</guid>
<content:encoded><![CDATA[
arXiv:2411.15355v3 Announce Type: replace 
Abstract: Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement</title>
<link>https://arxiv.org/abs/2412.12667</link>
<guid>https://arxiv.org/abs/2412.12667</guid>
<content:encoded><![CDATA[
arXiv:2412.12667v2 Announce Type: replace 
Abstract: This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2501.15151</link>
<guid>https://arxiv.org/abs/2501.15151</guid>
<content:encoded><![CDATA[
arXiv:2501.15151v4 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are the third generation of neural networks. They have gained widespread attention in object detection due to their low power consumption and biological interpretability. However, existing SNN-based object detection methods suffer from local firing saturation, where adjacent neurons concurrently reach maximum firing rates, especially in object-centric regions. This abnormal neuron firing pattern reduces the feature discrimination capability and detection accuracy, while also increasing the firing rates that prevent SNNs from achieving their potential energy efficiency. To address this problem, we propose SpikeDet, a novel spiking object detector that optimizes firing patterns for accurate and energy-efficient detection. Specifically, we design a spiking backbone network, MDSNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better neuron firing patterns during spiking feature extraction. For the neck, to better utilize and preserve these high-quality backbone features, we introduce the Spiking Multi-direction Fusion Module (SMFM), which realizes multi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Furthermore, we propose the Local Firing Saturation Index (LFSI) to quantitatively measure local firing saturation. Experimental results validate the effectiveness of our method, with SpikeDet achieving superior performance. On the COCO 2017 dataset, it achieves 52.2% AP, outperforming previous SNN-based methods by 3.3% AP while requiring only half the power consumption. On object detection sub-tasks, including event-based GEN1, underwater URPC 2019, low-light ExDARK, and dense scene CrowdHuman datasets, SpikeDet also achieves the best performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence</title>
<link>https://arxiv.org/abs/2502.01890</link>
<guid>https://arxiv.org/abs/2502.01890</guid>
<content:encoded><![CDATA[
arXiv:2502.01890v4 Announce Type: replace 
Abstract: 3D cell segmentation methods are often hindered by \emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing</title>
<link>https://arxiv.org/abs/2502.09652</link>
<guid>https://arxiv.org/abs/2502.09652</guid>
<content:encoded><![CDATA[
arXiv:2502.09652v3 Announce Type: replace 
Abstract: Shape deviation modeling and compensation in additive manufacturing are pivotal for achieving high geometric accuracy and enabling industrial-scale production. Critical challenges persist, including generalizability across complex geometries and adaptability to position-dependent variations in batch production. Traditional methods of controlling geometric deviations often rely on complex parameterized models and repetitive metrology, which can be time-consuming yet not applicable for batch production. In this paper, we present a novel, process-agnostic approach to address the challenge of ensuring geometric precision and accuracy in position-dependent AM production. The proposed GraphCompNet presents a novel computational framework integrating graph-based neural networks with a GAN inspired training paradigm. The framework leverages point cloud representations and dynamic graph convolutional neural networks (DGCNNs) to model intricate geometries while incorporating position-specific thermal and mechanical variations. A two-stage adversarial training process iteratively refines compensated designs using a compensator-predictor architecture, enabling real-time feedback and optimization. Experimental validation across various shapes and positions demonstrates the framework's ability to predict deviations in freeform geometries and adapt to position-dependent batch production conditions, significantly improving compensation accuracy (35 to 65 percent) across the entire printing space, addressing position-dependent variabilities within the print chamber. The proposed method advances the development of a Digital Twin for AM, offering scalable, real-time monitoring and compensation capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[
arXiv:2505.15952v2 Announce Type: replace 
Abstract: With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors</title>
<link>https://arxiv.org/abs/2505.20680</link>
<guid>https://arxiv.org/abs/2505.20680</guid>
<content:encoded><![CDATA[
arXiv:2505.20680v3 Announce Type: replace 
Abstract: Continual learning (CL) enables deep networks to acquire new knowledge while avoiding catastrophic forgetting. The powerful generalization ability of pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training (CLIP) model, has inspired a range of CL methods targeting new and specialized tasks, providing rich multi-modal embeddings that support lightweight, incremental prompt tuning. Existing methods often rely on complex designs built upon specific assumptions, such as intricate regularization schemes for prompt pools, specialized routing mechanisms, or multi-stage incrementations, that introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's intrinsic capabilities. In this paper, we propose a concise CL approach for CLIP based on incremental prompt tuning that fully exploits its multi-modal structure and the stability of textual representations. Our method, Textual Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely as static classifiers, as in existing methods, but as stable anchors to guide the learning of visual prompts, thereby shaping the embedding space (i.e., TPPT-V). We show that our bidirectional supervision strategy enables more effective learning of new knowledge while reducing forgetting. To further close the vision-language gap during CL, we jointly optimizes visual and textual prompts (i.e., TPPT-VT). We also introduce a relational diversity regularization on the textual anchors to prevent embedding space collapse and mitigate correlated forgetting. Extensive experiments and analyses demonstrate the effectiveness of our proposed approach, highlighting the benefits of leveraging CLIP's intrinsic guidance for continual adaptation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title>
<link>https://arxiv.org/abs/2507.00724</link>
<guid>https://arxiv.org/abs/2507.00724</guid>
<content:encoded><![CDATA[
arXiv:2507.00724v2 Announce Type: replace 
Abstract: Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</title>
<link>https://arxiv.org/abs/2507.17860</link>
<guid>https://arxiv.org/abs/2507.17860</guid>
<content:encoded><![CDATA[
arXiv:2507.17860v3 Announce Type: replace 
Abstract: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing</title>
<link>https://arxiv.org/abs/2508.05899</link>
<guid>https://arxiv.org/abs/2508.05899</guid>
<content:encoded><![CDATA[
arXiv:2508.05899v2 Announce Type: replace 
Abstract: 3D scene generation plays a crucial role in gaming, artistic creation, virtual reality, and many other domains. However, current 3D scene design still relies heavily on extensive manual effort from creators, and existing automated methods struggle to generate open-domain scenes or support flexible editing. To address those challenges, we introduce HOLODECK 2.0, an advanced vision-language-guided framework for 3D world generation with support for interactive scene editing based on human feedback. HOLODECK 2.0 can generate diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and cyberpunk styles) that exhibit high semantic fidelity to fine-grained input descriptions, suitable for both indoor and open-domain environments. HOLODECK 2.0 leverages vision-language models (VLMs) to identify and parse the objects required in a scene and generates corresponding high-quality assets via state-of-the-art 3D generative models. Then, HOLODECK 2.0 iteratively applies spatial constraints derived from the VLMs to achieve semantically coherent and physically plausible layouts. Both human and model evaluations demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely aligned with detailed textual descriptions, consistently outperforming baselines across indoor and open-domain scenarios. Additionally, HOLODECK 2.0 provides editing capabilities that flexibly adapt to human feedback, supporting layout refinement and style-consistent object edits. Finally, we present a practical application of HOLODECK 2.0 in procedural game modeling to generate visually rich and immersive environments that can boost efficiency in game design.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</title>
<link>https://arxiv.org/abs/2508.07313</link>
<guid>https://arxiv.org/abs/2508.07313</guid>
<content:encoded><![CDATA[
arXiv:2508.07313v3 Announce Type: replace 
Abstract: Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness</title>
<link>https://arxiv.org/abs/2508.09814</link>
<guid>https://arxiv.org/abs/2508.09814</guid>
<content:encoded><![CDATA[
arXiv:2508.09814v2 Announce Type: replace 
Abstract: Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSenCLIP: A Time Series Vision-Language Model for Remote Sensing Using Single-Pixel</title>
<link>https://arxiv.org/abs/2508.11919</link>
<guid>https://arxiv.org/abs/2508.11919</guid>
<content:encoded><![CDATA[
arXiv:2508.11919v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have shown significant promise in remote sensing applications, particularly for land-use and land-cover (LULC) mapping via zero-shot classification and retrieval. However, current approaches face several key challenges, such as the dependence on caption-based supervision, which is often not available or very limited in terms of the covered semantics, and the fact of being adapted from generic VLM architectures that are suitable for very high resolution images. Consequently, these models tend to prioritize spatial context over spectral and temporal information, limiting their effectiveness for medium-resolution remote sensing imagery. In this work, we present TimeSenCLIP, a lightweight VLM for remote sensing time series, using a cross-view temporal contrastive framework to align multispectral Sentinel-2 time series with geo-tagged ground-level imagery, without requiring textual annotations. Unlike prior VLMs, TimeSenCLIP emphasizes temporal and spectral signals over spatial context, investigating whether single-pixel time series contain sufficient information for solving a variety of tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</title>
<link>https://arxiv.org/abs/2508.13911</link>
<guid>https://arxiv.org/abs/2508.13911</guid>
<content:encoded><![CDATA[
arXiv:2508.13911v2 Announce Type: replace 
Abstract: Despite advances in physics-based 3D motion synthesis, current methods face key limitations: reliance on pre-reconstructed 3D Gaussian Splatting (3DGS) built from dense multi-view images with time-consuming per-scene optimization; physics integration via either inflexible, hand-specified attributes or unstable, optimization-heavy guidance from video models using Score Distillation Sampling (SDS); and naive concatenation of prebuilt 3DGS with physics modules, which ignores physical information embedded in appearance and yields suboptimal performance. To address these issues, we propose PhysGM, a feed-forward framework that jointly predicts 3D Gaussian representation and physical properties from a single image, enabling immediate simulation and high-fidelity 4D rendering. Unlike slow appearance-agnostic optimization methods, we first pre-train a physics-aware reconstruction model that directly infers both Gaussian and physical parameters. We further refine the model with Direct Preference Optimization (DPO), aligning simulations with the physically plausible reference videos and avoiding the high-cost SDS optimization. To address the absence of a supporting dataset for this task, we propose PhysAssets, a dataset of 50K+ 3D assets annotated with physical properties and corresponding reference videos. Experiments show that PhysGM produces high-fidelity 4D simulations from a single image in one minute, achieving a significant speedup over prior work while delivering realistic renderings.Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework</title>
<link>https://arxiv.org/abs/2508.16295</link>
<guid>https://arxiv.org/abs/2508.16295</guid>
<content:encoded><![CDATA[
arXiv:2508.16295v2 Announce Type: replace 
Abstract: The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders</title>
<link>https://arxiv.org/abs/2508.18236</link>
<guid>https://arxiv.org/abs/2508.18236</guid>
<content:encoded><![CDATA[
arXiv:2508.18236v3 Announce Type: replace 
Abstract: The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeParts: a New Family of AI-Generated DeepFakes</title>
<link>https://arxiv.org/abs/2508.21052</link>
<guid>https://arxiv.org/abs/2508.21052</guid>
<content:encoded><![CDATA[
arXiv:2508.21052v2 Announce Type: replace 
Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
<link>https://arxiv.org/abs/2509.05144</link>
<guid>https://arxiv.org/abs/2509.05144</guid>
<content:encoded><![CDATA[
arXiv:2509.05144v2 Announce Type: replace 
Abstract: Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available at https://github.com/wangchaolei7/SGS-3D.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention</title>
<link>https://arxiv.org/abs/2509.09116</link>
<guid>https://arxiv.org/abs/2509.09116</guid>
<content:encoded><![CDATA[
arXiv:2509.09116v3 Announce Type: replace 
Abstract: Foundation segmentation models achieve reasonable leaf instance extraction from top-view crop images without training (i.e., zero-shot). However, segmenting entire plant individuals with each consisting of multiple overlapping leaves remains challenging. This problem is referred to as a hierarchical segmentation task, typically requiring annotated training datasets, which are often species-specific and require notable human labor. To address this, we introduce ZeroPlantSeg, a zero-shot segmentation for rosette-shaped plant individuals from top-view images. We integrate a foundation segmentation model, extracting leaf instances, and a vision-language model, reasoning about plants' structures to extract plant individuals without additional training. Evaluations on datasets with multiple plant species, growth stages, and shooting environments demonstrate that our method surpasses existing zero-shot methods and achieves better cross-domain performance than supervised methods. Implementations are available at https://github.com/JunhaoXing/ZeroPlantSeg.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi Anatomy X-Ray Foundation Model</title>
<link>https://arxiv.org/abs/2509.12146</link>
<guid>https://arxiv.org/abs/2509.12146</guid>
<content:encoded><![CDATA[
arXiv:2509.12146v2 Announce Type: replace 
Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Blind Face Restoration through Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.23339</link>
<guid>https://arxiv.org/abs/2509.23339</guid>
<content:encoded><![CDATA[
arXiv:2509.23339v2 Announce Type: replace 
Abstract: Blind Face Restoration (BFR) encounters inherent challenges in exploring its large solution space, leading to common artifacts like missing details and identity ambiguity in the restored images. To tackle these challenges, we propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the first to apply online reinforcement learning (RL) to the BFR task. LRPO leverages rewards from sampled candidates to refine the policy network, increasing the likelihood of high-quality outputs while improving restoration performance on low-quality inputs. However, directly applying RL to BFR creates incompatibility issues, producing restoration results that deviate significantly from the ground truth. To balance perceptual quality and fidelity, we propose three key strategies: 1) a composite reward function tailored for face restoration assessment, 2) ground-truth guided likelihood regularization, and 3) noise-level advantage assignment. Extensive experiments demonstrate that our proposed LRPO significantly improves the face restoration quality over baseline methods and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.11176</link>
<guid>https://arxiv.org/abs/2510.11176</guid>
<content:encoded><![CDATA[
arXiv:2510.11176v2 Announce Type: replace 
Abstract: Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2510.16442</link>
<guid>https://arxiv.org/abs/2510.16442</guid>
<content:encoded><![CDATA[
arXiv:2510.16442v2 Announce Type: replace 
Abstract: The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration</title>
<link>https://arxiv.org/abs/2510.17330</link>
<guid>https://arxiv.org/abs/2510.17330</guid>
<content:encoded><![CDATA[
arXiv:2510.17330v2 Announce Type: replace 
Abstract: License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation</title>
<link>https://arxiv.org/abs/2510.22107</link>
<guid>https://arxiv.org/abs/2510.22107</guid>
<content:encoded><![CDATA[
arXiv:2510.22107v2 Announce Type: replace 
Abstract: Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[
arXiv:2510.24830v2 Announce Type: replace 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Mesh Modeling for Anny Body</title>
<link>https://arxiv.org/abs/2511.03589</link>
<guid>https://arxiv.org/abs/2511.03589</guid>
<content:encoded><![CDATA[
arXiv:2511.03589v2 Announce Type: replace 
Abstract: Parametric body models provide the structural basis for many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms--across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling--supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic images generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[
arXiv:2511.12346v2 Announce Type: replace 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CD-DPE: Dual-Prompt Expert Network Based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution</title>
<link>https://arxiv.org/abs/2511.14014</link>
<guid>https://arxiv.org/abs/2511.14014</guid>
<content:encoded><![CDATA[
arXiv:2511.14014v3 Announce Type: replace 
Abstract: Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation</title>
<link>https://arxiv.org/abs/2511.21029</link>
<guid>https://arxiv.org/abs/2511.21029</guid>
<content:encoded><![CDATA[
arXiv:2511.21029v2 Announce Type: replace 
Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization. Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance. Project page: https://flowerdance25.github.io/ .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant symmetry-aware head pose estimation for fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[
arXiv:2512.04890v4 Announce Type: replace 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLUENet: Cluster Attention Makes Neural Networks Have Eyes</title>
<link>https://arxiv.org/abs/2512.06345</link>
<guid>https://arxiv.org/abs/2512.06345</guid>
<content:encoded><![CDATA[
arXiv:2512.06345v2 Announce Type: replace 
Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[
arXiv:2512.07984v2 Announce Type: replace 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2512.08557</link>
<guid>https://arxiv.org/abs/2512.08557</guid>
<content:encoded><![CDATA[
arXiv:2512.08557v2 Announce Type: replace 
Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-driven Assessment of Bone Density as a Biomarker Leading to the Aging Law</title>
<link>https://arxiv.org/abs/2308.02815</link>
<guid>https://arxiv.org/abs/2308.02815</guid>
<content:encoded><![CDATA[
arXiv:2308.02815v2 Announce Type: replace-cross 
Abstract: As global population aging intensifies, there is growing interest in the study of biological age. Bones have long been used to evaluate biological age, and the decline in bone density with age is a well-recognized phenomenon in adults. However, the pattern of this decline remains controversial, making it difficult to serve as a reliable indicator of the aging process. Here we present a novel AI-driven statistical method to assess the bone density, and a discovery that the bone mass distribution in trabecular bone of vertebrae follows a non-Gaussian, unimodal, and skewed distribution in CT images. The statistical mode of the distribution is defined as the measure of bone mass, which is a groundbreaking assessment of bone density, named Trabecular Bone Density (TBD). The dataset of CT images are collected from 1,719 patients who underwent PET/CT scans in three hospitals, in which a subset of the dataset is used for AI model training and generalization. Based upon the cases, we demonstrate that the pattern of bone density declining with aging exhibits a consistent trend of exponential decline across sexes and age groups using TBD assessment. The developed AI-driven statistical method blazes a trail in the field of AI for reliable quantitative computation and AI for medicine. The findings suggest that human aging is a gradual process, with the rate of decline slowing progressively over time, which will provide a valuable basis for scientific prediction of life expectancy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction</title>
<link>https://arxiv.org/abs/2405.05814</link>
<guid>https://arxiv.org/abs/2405.05814</guid>
<content:encoded><![CDATA[
arXiv:2405.05814v2 Announce Type: replace-cross 
Abstract: Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction. Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles. Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics. Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques. Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively. By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively. Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items</title>
<link>https://arxiv.org/abs/2503.22182</link>
<guid>https://arxiv.org/abs/2503.22182</guid>
<content:encoded><![CDATA[
arXiv:2503.22182v2 Announce Type: replace-cross 
Abstract: E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called "sell it before you make it", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics</title>
<link>https://arxiv.org/abs/2505.04006</link>
<guid>https://arxiv.org/abs/2505.04006</guid>
<content:encoded><![CDATA[
arXiv:2505.04006v2 Announce Type: replace-cross 
Abstract: The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided multi-stage cross-perception network for medical image segmentation</title>
<link>https://arxiv.org/abs/2506.07475</link>
<guid>https://arxiv.org/abs/2506.07475</guid>
<content:encoded><![CDATA[
arXiv:2506.07475v3 Announce Type: replace-cross 
Abstract: Medical image segmentation plays a crucial role in clinical medicine, serving as a key tool for auxiliary diagnosis, treatment planning, and disease monitoring. However, traditional segmentation methods such as U-Net are often limited by weak semantic expression of target regions, which stems from insufficient generalization and a lack of interactivity. Incorporating text prompts offers a promising avenue to more accurately pinpoint lesion locations, yet existing text-guided methods are still hindered by insufficient cross-modal interaction and inadequate cross-modal feature representation. To address these challenges, we propose the Text-guided Multi-stage Cross-perception network (TMC). TMC incorporates a Multi-stage Cross-attention Module (MCM) to enhance the model's understanding of fine-grained semantic details and a Multi-stage Alignment Loss (MA Loss) to improve the consistency of cross-modal semantics across different feature levels. Experimental results on three public datasets (QaTa-COV19, MosMedData, and Duke-Breast-Cancer-MRI) demonstrate the superior performance of TMC, achieving Dice scores of 84.65\%, 78.39\%, and 88.09\%, respectively, and consistently outperforming both U-Net-based networks and existing text-guided methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction</title>
<link>https://arxiv.org/abs/2510.23117</link>
<guid>https://arxiv.org/abs/2510.23117</guid>
<content:encoded><![CDATA[
arXiv:2510.23117v3 Announce Type: replace-cross 
Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.00324</link>
<guid>https://arxiv.org/abs/2512.00324</guid>
<content:encoded><![CDATA[
arXiv:2512.00324v2 Announce Type: replace-cross 
Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</title>
<link>https://arxiv.org/abs/2512.15774</link>
<guid>https://arxiv.org/abs/2512.15774</guid>
<content:encoded><![CDATA[
<div> Keywords: masked face detection, data augmentation, GANs, mask warping, face recognition<br /><br />Summary:<br /><br />1. The paper addresses challenges in masked face detection and recognition caused by data scarcity and distribution shifts.<br /><br />2. It proposes a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using Generative Adversarial Networks (GANs). This approach generates more realistic masked-face samples beyond what purely synthetic transformations can achieve.<br /><br />3. The method shows consistent qualitative improvements compared to using rule-based warping alone and serves as a complement to existing GAN-based masked face generation techniques such as IAMGAN.<br /><br />4. Key contributions include the introduction of a non-mask preservation loss and stochastic noise injection, both designed to stabilize the GAN training process and increase the diversity of generated samples.<br /><br />5. Experimental results validate the effectiveness of these components and provide insights for future research directions focused on data-centric augmentation strategies for improving face recognition under masked conditions. <div>
arXiv:2512.15774v1 Announce Type: new 
Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.15885</link>
<guid>https://arxiv.org/abs/2512.15885</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual reasoning, self-supervised learning, I-JEPA, vision-language alignment<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) have shown strong abilities in linking vision and language but still struggle with foundational visual reasoning tasks.  
2. The main challenge is that MLLMs primarily learn visual understanding from textual descriptions, which provide subjective and incomplete supervision, limiting the model's ability to fully grasp visual details.  
3. Additionally, multimodal instruction tuning datasets are smaller compared to massive text-only pre-training, causing MLLMs to overfit language priors while neglecting detailed visual information.  
4. To overcome these limitations, the authors present JARVIS, a framework inspired by JEPA, that integrates self-supervised visual enhancement into the MLLMs training process.  
5. JARVIS incorporates the I-JEPA learning paradigm with frozen vision foundation models as context and target encoders, training the predictor (early LLM layers) to capture structural and semantic image regularities without relying solely on language-based supervision.  
6. Experiments demonstrate that JARVIS consistently boosts performance on vision-centric benchmarks across various large language model families, without compromising multimodal reasoning capabilities.  
7. The source code for JARVIS is made publicly accessible to facilitate further research and development in this area. <div>
arXiv:2512.15885v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</title>
<link>https://arxiv.org/abs/2512.15933</link>
<guid>https://arxiv.org/abs/2512.15933</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, embodied agents, visual navigation, CityNav benchmark, Verbalization of Path (VoP)  

<br /><br />Summary:  
1. This paper introduces Sparsely Grounded Visual Navigation, a novel task aimed at evaluating the sequential decision-making capabilities of multimodal large language models (MLLMs) in complex, real-world environments.  
2. The authors develop CityNav, a benchmark consisting of navigation challenges across four diverse global cities, where agents must navigate through 50+ decision points relying solely on visual inputs and internal multimodal reasoning, without extra environment annotations or specialized model changes.  
3. Key requirements for agents include autonomous localization by interpreting city-specific cues and landmarks, spatial reasoning, and strategic route planning to reach their destinations effectively.  
4. Extensive experiments reveal that current state-of-the-art MLLMs and reasoning strategies like Chain-of-Thought and Reflection perform poorly under these demanding conditions.  
5. To improve navigation success, the paper proposes Verbalization of Path (VoP), a method that grounds the agent's internal reasoning explicitly by extracting a cognitive map, including key landmarks and directional information from the MLLMs, significantly enhancing performance. <div>
arXiv:2512.15933v1 Announce Type: new 
Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</title>
<link>https://arxiv.org/abs/2512.15940</link>
<guid>https://arxiv.org/abs/2512.15940</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D spatio-temporal reasoning, retrieval-augmented framework, vision-language models, lifelong memory, embodied AI<br /><br />Summary:  
Humans naturally perceive and reason about the world in four dimensions, using structured internal representations that encode semantics, spatial layout, and temporal dynamics. Inspired by this, the paper introduces R4, a training-free framework designed to augment vision-language models (VLMs) with structured lifelong memory in 4D spatio-temporal space. R4 continuously builds a knowledge database by anchoring object-level semantic information within metric space and time, creating a persistent world model shareable across multiple agents. During inference, natural language queries are broken down into semantic, spatial, and temporal components, which serve as keys to retrieve relevant past observations from this 4D database. These retrieved observations are then integrated into the reasoning process of VLMs. Unlike traditional retrieval-augmented generation methods, R4 conducts retrieval directly within 4D space, facilitating episodic and collaborative reasoning without the need for additional training. Experiments conducted on embodied question answering and navigation tasks demonstrate that R4 significantly enhances retrieval and reasoning involving spatio-temporal information when compared to baseline methods. Overall, this work establishes a new paradigm for embodied 4D reasoning in dynamic environments, enabling more persistent, contextual, and collaborative AI agents. <div>
arXiv:2512.15940v1 Announce Type: new 
Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</title>
<link>https://arxiv.org/abs/2512.15949</link>
<guid>https://arxiv.org/abs/2512.15949</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Perceptual Capacities, Visual Grounding, Robustness Evaluation, The Perceptual Observatory<br /><br />Summary: Recent advances in multimodal large language models (MLLMs) have led to increasingly powerful systems; however, their true perceptual capabilities remain inadequately understood. Most current models scale their language components while reusing similar vision encoders, raising concerns about whether improvements stem from genuine visual understanding or merely extensive textual knowledge. Existing evaluations focus mainly on end-task accuracy, ignoring factors like robustness, attribution fidelity, and reasoning under controlled perturbations. To address this, the authors propose The Perceptual Observatory, a comprehensive framework designed to characterize MLLMs across multiple verticals. These include simple vision tasks (such as face matching and text-in-vision comprehension), as well as local-to-global understanding tasks (like image matching, grid pointing games, and attribute localization) to test general visual grounding. The framework uses meticulously curated ground-truth datasets consisting of faces and words, which are systematically perturbed using pixel-based augmentations and diffusion-based stylized illusions to test robustness. The Perceptual Observatory moves beyond typical leaderboard accuracy metrics to provide insights into how MLLMs maintain perceptual grounding and relational structure under various perturbations, thereby establishing a principled foundation for evaluating the strengths and weaknesses of contemporary and future models. <div>
arXiv:2512.15949v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15957</link>
<guid>https://arxiv.org/abs/2512.15957</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-human behavior prediction, vision language model, context-aware, scene graphs, synthetic data<br /><br />Summary:<br /><br />1. This paper addresses the challenge of accurately predicting human behaviors in environments with multiple people, which is critical for mobile robots operating in human-populated spaces. <br /><br />2. Unlike prior work focused on egocentric views and single-human behavior, the authors focus on a third-person observer perspective for multi-human behavior prediction. <br /><br />3. The proposed method, CAMP-VLM (Context-Aware Multi-human behavior Prediction), utilizes a Vision Language Model (VLM) framework that integrates contextual features from visual inputs and spatial awareness derived from scene graphs to capture humans-scene interactions effectively. <br /><br />4. Due to the lack of real-world datasets suitable for multi-human behavior prediction from an observer view, the model was fine-tuned using synthetic human behavior data generated via a photorealistic simulation environment. <br /><br />5. The model was evaluated on both synthetic and real-world sequences, leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), and achieved up to 66.9% improvement in prediction accuracy compared to the best existing baseline methods, demonstrating strong generalization and improved predictive performance. <div>
arXiv:2512.15957v1 Announce Type: new 
Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</title>
<link>https://arxiv.org/abs/2512.15971</link>
<guid>https://arxiv.org/abs/2512.15971</guid>
<content:encoded><![CDATA[
<div> Multispectral object detection, Vision-Language Models, few-shot learning, multispectral datasets, semantic supervision<br /><br />Summary:<br /><br />Multispectral object detection plays a crucial role in applications such as autonomous driving and surveillance, where perception must remain robust under diverse illumination conditions. A major challenge in this area is the scarcity of annotated multispectral data, which limits the effective training of deep learning detectors. To address this, the authors explore the use of textual class information as a form of semantic supervision, leveraging the recent advancements in Vision-Language Models (VLMs). They adapt two prominent VLM-based detectors, Grounding DINO and YOLO-World, to work with multispectral inputs by integrating text, visual, and thermal modalities through a newly proposed mechanism. Experiments conducted on FLIR and M3FD benchmarks reveal that these VLM-based detectors significantly outperform specialized multispectral models in few-shot learning scenarios. Furthermore, the VLM-based methods achieve competitive or even superior results compared to state-of-the-art models in fully supervised settings. The study demonstrates that semantic priors learned from large-scale VLMs transfer effectively to previously unseen spectral modalities, offering a promising and data-efficient approach to multispectral object detection and perception. <div>
arXiv:2512.15971v1 Announce Type: new 
Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are vision-language models ready to zero-shot replace supervised classification models in agriculture?</title>
<link>https://arxiv.org/abs/2512.15977</link>
<guid>https://arxiv.org/abs/2512.15977</guid>
<content:encoded><![CDATA[
<div> Vision-language models, agricultural classification, zero-shot learning, model benchmarking, semantic judging<br /><br />Summary:<br /><br />1. This study evaluates the reliability of vision-language models (VLMs) for agricultural decision support by benchmarking various open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, covering 162 classes including plant disease, pest and damage identification, and plant and weed species classification.<br />2. Results reveal that zero-shot VLMs significantly underperform compared to a supervised baseline model, YOLO11, which consistently achieves much higher accuracy across all tasks.<br />3. Under multiple-choice prompting, the best VLM, Gemini-3 Pro, attains around 62% average accuracy, whereas open-ended prompting yields substantially lower performance, typically below 25% raw accuracy.<br />4. Incorporating large language model (LLM)-based semantic judging improves open-ended accuracy (e.g., from 21% to 30% for top models) and changes the relative rankings of models, indicating that the chosen evaluation methods strongly influence reported outcomes.<br />5. Among open-source options, Qwen-VL-72B shows the best results, nearing closed-source model performance under limited prompting scenarios but still trailing leading proprietary models.<br />6. Task-level analysis finds that plant and weed species classification is generally easier for VLMs than pest and damage identification, which remains the most difficult category.<br />7. The overall conclusion is that current off-the-shelf VLMs are not yet adequate as standalone systems for agricultural diagnostics but may be useful as assistive tools when combined with constrained interfaces, explicit label ontologies, and domain-aware evaluations. <div>
arXiv:2512.15977v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings</title>
<link>https://arxiv.org/abs/2512.15993</link>
<guid>https://arxiv.org/abs/2512.15993</guid>
<content:encoded><![CDATA[
<div> Keywords: robotic mowing, biodiversity, visual perception, deep learning, adaptive decision-making<br /><br />Summary:  
This paper introduces a novel robotic mowing framework designed to enhance garden biodiversity actively by leveraging visual perception combined with adaptive decision-making. Unlike traditional passive rewilding methods, the system employs deep feature-space analysis to detect and preserve visually diverse vegetation patches captured in camera images by selectively deactivating the mower blades over these areas. The core technology utilizes a ResNet50 neural network pretrained on the PlantNet300K dataset, generating ecologically meaningful embeddings that represent plant diversity without needing species-level identification. From these embeddings, a global deviation metric is derived to estimate biodiversity, which then informs a selective mowing algorithm. This algorithm dynamically switches between mowing and conservation behavior based on real-time visual biodiversity cues. The framework was successfully implemented on a commercially available robotic mower modified for the experiment and validated both in controlled mock-up lawns and real garden environments. Experimental results show a strong correlation between the dispersion of embeddings in feature space and expert biodiversity assessments, validating the use of deep visual diversity as a proxy for ecological richness. Overall, the proposed system has the potential to transform monocultural lawns into ecologically valuable, biodiverse urban green spaces through intelligent, biodiversity-aware mowing. <div>
arXiv:2512.15993v1 Announce Type: new 
Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion</title>
<link>https://arxiv.org/abs/2512.16023</link>
<guid>https://arxiv.org/abs/2512.16023</guid>
<content:encoded><![CDATA[
<div> video diffusion, action generation, robotic policy learning, cross-modal interaction, action refinement  

<br /><br />Summary:  
This paper introduces a novel method to generate video-action pairs guided by text instructions, starting from initial image observations and robot joint states. The approach addresses the common challenge of lacking action annotations for video diffusion models, enabling their comprehensive application in robotic policy learning. Unlike existing approaches limited by two-stage pipelines or single-modal diffusion adaptation, the authors propose a threefold solution: (1) extending a pretrained video diffusion model with a dedicated parallel action diffusion model, which retains pretrained video knowledge; (2) incorporating a Bridge Attention mechanism to facilitate effective cross-modal interaction between video and action modalities; and (3) designing an action refinement module that transforms coarse actions into precise control signals, particularly useful when working with low-resolution datasets. Through extensive evaluations on multiple public benchmarks and real-world datasets, the method demonstrates superior performance by generating higher-quality videos and more accurate actions compared to existing baselines. The overall framework provides a scalable way to leverage large-scale video data for advancing robotic learning tasks. <div>
arXiv:2512.16023v1 Announce Type: new 
Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.16055</link>
<guid>https://arxiv.org/abs/2512.16055</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial evaluation, end-to-end autonomous driving, real-world image generation, corner cases, traffic policy<br /><br />Summary: This paper addresses the challenge of evaluating end-to-end autonomous driving systems in real-world, safety-critical corner cases that are difficult to collect naturally. The authors introduce a closed-loop evaluation platform capable of generating adversarial interactions within real-world driving scenes to test these autonomous models. Central to the platform is a real-world image generator based on flow matching, which efficiently and stably produces realistic driving images conditioned on dynamic traffic environment information. Alongside, an adversarial traffic policy simulates challenging interactions by controlling surrounding vehicles to create corner cases that expose weaknesses in current autonomous driving systems. The platform evaluates multiple end-to-end driving models trained on real data, such as UniAD and VAD, showing that these models exhibit performance degradation in the generated adversarial scenarios. Experimental results confirm the realism of the images and the effectiveness of the adversarial policy in provoking difficult driving situations. This approach allows for systematic detection of potential model issues under realistic and complex conditions, thus providing a valuable tool to improve the safety and robustness of end-to-end autonomous driving technologies. <div>
arXiv:2512.16055v1 Announce Type: new 
Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</title>
<link>https://arxiv.org/abs/2512.16075</link>
<guid>https://arxiv.org/abs/2512.16075</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion MRI, fiber orientation distribution, high angular resolution, spherical harmonics, patch diffusion model<br /><br />Summary: This paper addresses the challenge of accurately estimating high angular resolution fiber orientation distribution (HAR-FOD) from diffusion MRI (dMRI) data. Traditional single-shell low angular resolution dMRI (LAR-FOD) methods are limited in accuracy, while multi-shell high angular resolution dMRI (HAR-FOD) provides better results but requires long scanning times. The authors propose a novel 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD data, aiming to combine accuracy with efficiency. To enhance learning, they introduce a FOD-patch adapter that incorporates prior brain anatomy, enabling more effective patch-based training. A voxel-level conditional coordinating module is designed to improve the model’s global context understanding, addressing spatial dependencies. They also develop a spherical harmonic (SH) attention module to capture complex correlations among the numerous SH coefficients representing FOD, which is critical due to the high dimensionality of the data. Experimental results demonstrate their approach achieves superior performance in HAR-FOD prediction compared to state-of-the-art techniques, highlighting its potential for improving white matter characterization while reducing scanning times. <div>
arXiv:2512.16075v1 Announce Type: new 
Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Vocabulary 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.16077</link>
<guid>https://arxiv.org/abs/2512.16077</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary 3D object detection, auto-generated classes, Semantic Score, 2D vision-language models, ScanNetV2 and SUNRGB-D datasets  

<br /><br />Summary:  
This paper introduces a new paradigm called Auto-Vocabulary 3D Object Detection (AV3DOD), which aims to automatically generate class names for detected 3D objects without any user input, differing from existing open-vocabulary methods that still rely on user-specified classes. To assess the quality of the generated class names, the authors propose a novel metric called Semantic Score (SS). The AV3DOD framework leverages 2D vision-language models (VLMs), incorporating techniques such as image captioning, pseudo 3D box generation, and feature-space semantics expansion to generate rich semantic candidates for detected objects. The method is evaluated on two prominent 3D detection benchmarks, ScanNetV2 and SUNRGB-D, where it demonstrates state-of-the-art (SOTA) performance in both object localization, measured by mean Average Precision (mAP), and semantic quality, assessed by SS. Notably, AV3DOD surpasses the previous SOTA method, CoDA, by 3.48 in overall mAP and achieves a 24.5% relative improvement in semantic score on the ScanNetV2 dataset. This work represents a significant step toward fully autonomous 3D object detection systems that do not require predefined vocabulary, enhancing both detection accuracy and semantic understanding. <div>
arXiv:2512.16077v1 Announce Type: new 
Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPX: Lightweight Hourglass Network with Global Context</title>
<link>https://arxiv.org/abs/2512.16089</link>
<guid>https://arxiv.org/abs/2512.16089</guid>
<content:encoded><![CDATA[
<div> Keywords: Human pose estimation, lightweight model, self-attention, Hourglass network, edge devices<br /><br />Summary: Human pose estimation is vital in computer vision, yet methods with state-of-the-art accuracy often require large models and high computational costs. To address this, many lightweight variants have been developed to reduce both model size and computation. However, these lighter models frequently incorporate components unsuited for efficient deployment on edge devices, or they focus too heavily on inference speed, resulting in compromised accuracy. The proposed model, LAPX, is an Hourglass network enhanced with a self-attention mechanism to better capture global contextual information, building upon the prior LAP model. LAPX not only integrates self-attention but also advances stage design and refines lightweight attention modules to balance efficiency and performance. It achieves competitive accuracy on two major benchmarks, MPII and COCO, while maintaining a small parameter count of only 2.3 million. Moreover, LAPX demonstrates real-time inference capability, confirming its suitability for use on resource-constrained edge devices. Overall, LAPX effectively addresses the trade-off between accuracy, model complexity, and inference speed in human pose estimation for edge deployment. <div>
arXiv:2512.16089v1 Announce Type: new 
Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collimator-assisted high-precision calibration method for event cameras</title>
<link>https://arxiv.org/abs/2512.16092</link>
<guid>https://arxiv.org/abs/2512.16092</guid>
<content:encoded><![CDATA[
<div> Event cameras, calibration, collimator, flickering star patterns, geometric parameters  

<br /><br />Summary:  
Event cameras offer advantages like high dynamic range and temporal resolution but calibrating them—specifically determining intrinsic and extrinsic parameters—remains challenging, especially for long-range measurements. To meet the needs of long-distance and high-precision calibration, the authors propose a novel method that uses a collimator displaying flickering star-based patterns as a calibration target. The approach begins with a linear solution for the camera parameters, utilizing the sphere motion model derived from the collimator’s movement. This initial solution is then refined through nonlinear optimization to enhance accuracy. The method was validated via extensive real-world experiments conducted under various environmental conditions. Results demonstrate that this calibration technique consistently outperforms current event camera calibration methods in both accuracy and reliability, showing strong potential for improving long-range event camera applications. <div>
arXiv:2512.16092v1 Announce Type: new 
Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
<div> TurboDiffusion, video generation acceleration, attention acceleration, step distillation, quantization<br /><br />Summary:<br /><br />TurboDiffusion is a framework designed to significantly accelerate end-to-end video diffusion model generation by 100-200 times while maintaining video quality. The acceleration is achieved through multiple key components: (1) Attention acceleration, utilizing low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computations crucial for diffusion models. (2) Step distillation, where TurboDiffusion adopts rCM (likely refined Classifier-Free Model) for efficient distillation of inference steps, reducing the number of required steps in the generation process. (3) W8A8 quantization, quantizing both model parameters and activations to 8-bit precision to speed up linear layer operations and compress the model for a smaller footprint. Additionally, the framework incorporates other engineering optimizations to further enhance speed. Experiments conducted on multiple models, including Wan2.2-I2V-14B-720P and Wan2.1-T2V variants at different resolutions and sizes, demonstrate that TurboDiffusion achieves 100-200x speedups even on a single RTX 5090 GPU. Importantly, it manages to maintain video quality comparable to slower baseline methods. The project is openly available with model checkpoints and user-friendly code at their GitHub repository: https://github.com/thu-ml/TurboDiffusion. <div>
arXiv:2512.16093v1 Announce Type: new 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Camera Calibration using a Collimator System</title>
<link>https://arxiv.org/abs/2512.16113</link>
<guid>https://arxiv.org/abs/2512.16113</guid>
<content:encoded><![CDATA[
<div> Camera calibration, collimator system, spherical motion model, angle invariance, minimal solver<br /><br />Summary:<br /><br />This paper presents a novel camera calibration method utilizing a specially designed collimator system, which offers a reliable and controllable environment for accurate calibration. The authors leverage the unique optical geometry of the collimator system to introduce an angle invariance constraint, leading to the discovery that the relative motion between the calibration target and the camera complies with a spherical motion model. This insight effectively reduces the complex original 6 degrees of freedom (6DOF) relative motion to a simpler 3 degrees of freedom (3DOF) pure rotation motion, simplifying the calibration process. Based on this spherical motion constraint, the authors develop a closed-form linear solver that handles multiple images as well as a minimal solver for calibration using just two images. Additionally, they propose an innovative algorithm capable of calibrating the camera from a single collimator image by exploiting the angle invariance constraint, thereby eliminating the need for camera motion and enabling quicker and more flexible calibration. The proposed method's effectiveness is validated through both synthetic simulations and real-world experiments, demonstrating superior performance compared to existing baseline methods. To facilitate practical adoption, the authors have made the demonstration code publicly available on GitHub. <div>
arXiv:2512.16113v1 Announce Type: new 
Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space</title>
<link>https://arxiv.org/abs/2512.16133</link>
<guid>https://arxiv.org/abs/2512.16133</guid>
<content:encoded><![CDATA[
<div> Keywords: cattle interaction detection, behavioral dataset, action latent space, contrastive learning, smart livestock management<br /><br />Summary:  
This paper presents a novel approach for automatically detecting behavioral interactions among grazing cattle using only a single image, which is critical for applications like estrus detection in smart livestock management. Unlike extensive research on human interaction detection, cattle present unique challenges due to rare occurrence of interactions and a lack of comprehensive behavioral datasets. To address this, the authors propose CattleAct, a data-efficient method that decomposes interactions into combinations of individual cattle actions. The method involves first learning an action latent space from a large-scale cattle action dataset. Rare interactions are then embedded by fine-tuning this pre-trained latent space through contrastive learning, resulting in a unified latent space that represents both individual actions and interactions. The authors also develop a practical system that integrates video and GPS data for real-world application. Experimental results from a commercial-scale pasture demonstrate that CattleAct outperforms baseline methods in accurately detecting cattle interactions. The implementation is publicly available on GitHub, promoting accessibility and potential adoption in the cattle industry for smarter livestock monitoring and management. <div>
arXiv:2512.16133v1 Announce Type: new 
Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT</title>
<link>https://arxiv.org/abs/2512.16140</link>
<guid>https://arxiv.org/abs/2512.16140</guid>
<content:encoded><![CDATA[
<div> Keywords: Dual-spectral CT, hybrid reconstruction, oblique projection modification technique, ResDynUNet++, deep learning

<br /><br />Summary:  
This paper proposes a hybrid reconstruction framework for dual-spectral computed tomography (DSCT) that combines iterative reconstruction techniques with deep learning models to improve image quality. The method consists of two complementary phases: a knowledge-driven phase and a data-driven phase. In the knowledge-driven phase, the oblique projection modification technique (OPMT) is employed to quickly generate an intermediate solution of the basis material images from the raw projection data, due to its fast convergence and reliable basis material decomposition capabilities. Following this, the data-driven phase introduces a novel neural network architecture named ResDynUNet++, which refines the intermediate solutions produced by OPMT. ResDynUNet++ enhances the standard UNet++ base by integrating residual dynamic convolution blocks that adaptively and input-specifically extract features while maintaining stable training through residual connections. This design specifically addresses challenges inherent to DSCT such as channel imbalance and large artifacts near interfaces, resulting in cleaner and more accurate reconstructed images. The framework was rigorously tested on both synthetic phantom data and real clinical datasets, demonstrating superior performance and validated efficacy compared to existing methods. This hybrid approach effectively leverages both domain knowledge and advanced deep learning for improved DSCT reconstruction outcomes. <div>
arXiv:2512.16140v1 Announce Type: new 
Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</title>
<link>https://arxiv.org/abs/2512.16143</link>
<guid>https://arxiv.org/abs/2512.16143</guid>
<content:encoded><![CDATA[
<div> few-shot 3D segmentation, foundation models, SAM segment graph, geometric features, graph neural network<br /><br />Summary:<br /><br />This paper introduces SegGraph, a novel framework designed for few-shot 3D part segmentation that leverages 2D foundation models and explicitly incorporates geometric structures. Existing methods either overlook 3D geometric cues or fail to utilize high-quality grouping information from the Segment Anything Model (SAM), resulting in poor segmentation quality such as under-segmentation and inconsistent part labeling. SegGraph addresses these issues through a SAM segment graph-based propagation approach, where segments from SAM’s masks form nodes in a graph and edges represent spatial relationships including overlap and adjacency. This graph structure allows explicit modeling of mutual segment relationships, capturing global geometric context. Nodes in the graph adaptively modulate features obtained from 2D foundation models, and the features are propagated via a graph neural network to enhance 3D feature learning. To maintain semantic consistency within segments, SegGraph maps these segment features back to 3D points using a novel view-direction-weighted fusion technique that reduces the impact of lower-quality segments. Experimental results on the PartNet-E dataset demonstrate that SegGraph surpasses previous methods by at least 6.9% mean Intersection-over-Union (mIoU), with particularly strong results on small parts and boundaries, highlighting its effectiveness in understanding geometric details. The implementation code is publicly available on GitHub. <div>
arXiv:2512.16143v1 Announce Type: new 
Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</title>
<link>https://arxiv.org/abs/2512.16164</link>
<guid>https://arxiv.org/abs/2512.16164</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised Domain Adaptation, Vision-Language Models, Prompt Tuning, Dual Alignment, Class Mapping Mechanism  

<br /><br />Summary:  
Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, addressing challenges caused by domain discrepancies. Deploying Vision-Language Models (VLMs) with prompt tuning in UDA faces key issues because existing methods focus mainly on aligning marginal distributions but overlook conditional distribution discrepancies. This oversight leads to problems such as class prototype misalignment and reduced semantic discriminability. To tackle these, the paper introduces C-DGPA (Class-Centric Dual Alignment Generative Prompt Adaptation), which synergistically optimizes both marginal and conditional distribution alignments via a novel dual-branch architecture. The marginal distribution alignment branch uses a dynamic adversarial training framework to reduce domain gaps at the feature distribution level. Meanwhile, the conditional distribution alignment branch incorporates a Class Mapping Mechanism (CMM) to align class-specific semantic prompts and prevent over-dependence on the source domain. This dual alignment strategy allows the integration of domain knowledge into prompt learning effectively, fostering domain-invariant and semantically discriminative representations. Extensive experiments conducted on the OfficeHome, Office31, and VisDA-2017 benchmarks demonstrate that C-DGPA achieves new state-of-the-art results across all evaluated datasets, validating its effectiveness and superiority in downstream UDA tasks. <div>
arXiv:2512.16164v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Closing the Domain Gap with Event Cameras</title>
<link>https://arxiv.org/abs/2512.16178</link>
<guid>https://arxiv.org/abs/2512.16178</guid>
<content:encoded><![CDATA[
<div> Event cameras, domain gap, lighting conditions, day-night difference, performance consistency<br /><br />Summary:<br /><br />1. Traditional cameras used in end-to-end driving systems face significant performance degradation when there is a domain gap, particularly caused by different lighting conditions such as day and night.<br />2. This study explores the use of event cameras as an alternative sensor to address this issue without requiring extensive adjustments or retraining.<br />3. Event cameras capture changes in the scene asynchronously and have intrinsic properties that allow them to operate consistently across varying lighting environments.<br />4. Experimental results demonstrate that event cameras maintain more stable and consistent performance across day-night lighting differences compared to traditional grayscale frames.<br />5. The domain-shift penalties observed for event cameras are generally smaller or comparable to those of grayscale cameras, making event cameras a promising sensor choice for robust cross-domain autonomous driving applications. <div>
arXiv:2512.16178v1 Announce Type: new 
Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16199</link>
<guid>https://arxiv.org/abs/2512.16199</guid>
<content:encoded><![CDATA[
<div> Keywords: Avatar4D, synthetic human motion, sports datasets, pose estimation, domain adaptation<br /><br />Summary:<br /><br />1. The paper introduces Avatar4D, a novel pipeline designed to generate customizable synthetic human motion datasets that are transferable to real-world applications without requiring manual annotations.<br />2. Avatar4D offers fine-grained control over multiple factors including body pose, appearance, camera viewpoint, and environmental context, providing greater flexibility compared to prior works which mostly handle general everyday motions.<br />3. The approach is validated specifically on sports scenarios, where human actions and movements show complex and domain-specific patterns, making motion understanding particularly challenging.<br />4. To support this validation, the authors create Syn2Sport, a large-scale synthetic dataset covering sports like baseball and ice hockey, featuring high-fidelity 4D human motion sequences with varied player appearances and diverse environments.<br />5. Several state-of-the-art pose estimation models are benchmarked on Syn2Sport, demonstrating the dataset’s utility for supervised learning, zero-shot transfer to real-world data, and generalization across different sports.<br />6. The study also assesses the alignment between synthetic and real-world data in feature space, highlighting Avatar4D’s potential to generate scalable, controllable, and transferable human motion datasets tailored for domain-specific tasks without relying on domain-specific real data. <div>
arXiv:2512.16199v1 Announce Type: new 
Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.16201</link>
<guid>https://arxiv.org/abs/2512.16201</guid>
<content:encoded><![CDATA[
<div> Radiology Report Generation, Medical Vision-Language Models, Visual Grounding, Reinforcement Learning, Disease Findings

<br /><br />Summary: Radiology Report Generation (RRG) is essential for automating healthcare workflows, improving patient assessments, and reducing the workload of medical professionals. Existing Large Medical Vision-Language Models (Med-VLMs) have made progress, but challenges persist in generating reports that are both visually grounded and clinically accurate. Current methods often depend on large labeled datasets, expensive task-specific preference data, or retrieval-based techniques, which fail to sufficiently prevent hallucinations caused by poor cross-modal alignment. To overcome these challenges, the authors propose VALOR (Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation), a reinforcement learning-based post-alignment framework featuring Group-Relative Proximal Optimization (GRPO). VALOR’s training has two stages: first, enhancing Med-VLMs with textual rewards that promote the use of clinically precise terminology, and second, aligning the vision projection module to disease findings to focus attention on diagnostically relevant image regions. Extensive experiments performed on multiple benchmarks show that VALOR significantly improves both factual accuracy and visual grounding, substantially outperforming current state-of-the-art radiology report generation methods. This work highlights the importance of better visual-linguistic alignment for reliable and clinically accurate automated report generation. <div>
arXiv:2512.16201v1 Announce Type: new 
Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Ad-hoc Categorization with Contextualized Feature Learning</title>
<link>https://arxiv.org/abs/2512.16202</link>
<guid>https://arxiv.org/abs/2512.16202</guid>
<content:encoded><![CDATA[
<div> Adaptive categorization, ad-hoc categories, CLIP, visual clustering, context tokens<br /><br />Summary:  
This paper addresses the problem of open ad-hoc categorization, where AI agents dynamically create categories based on a few labeled examples and a large amount of unlabeled data to discover underlying contexts and expand categories through semantic extension and visual clustering. The authors propose OAK, a novel model that introduces a small set of learnable context tokens input into a frozen CLIP model, trained using both CLIP's image-text alignment and a visual clustering objective from Generalized Category Discovery (GCD). OAK leverages the insight that ad-hoc and common categorization share similar perceptual mechanisms, enabling it to adaptively form meaningful categories in new contexts. Experiments on the Stanford and Clevr-4 datasets demonstrate that OAK achieves state-of-the-art accuracy and concept discovery performance, such as 87.4% novel accuracy on the Stanford Mood dataset, outperforming both CLIP and GCD by large margins (over 50%). Additionally, OAK generates interpretable saliency maps that emphasize relevant visual features according to the category type—for example, hands for Action, faces for Mood, and backgrounds for Location—enhancing transparency and trust. The approach supports adaptive, generalizable categorization suited for AI agents facing dynamic and changing visual scene understanding tasks. <div>
arXiv:2512.16202v1 Announce Type: new 
Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced 3D Shape Analysis via Information Geometry</title>
<link>https://arxiv.org/abs/2512.16213</link>
<guid>https://arxiv.org/abs/2512.16213</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D point clouds, Gaussian Mixture Models, statistical manifold, Modified Symmetric Kullback-Leibler divergence, shape analysis<br /><br />Summary:  
This paper addresses the challenges in comparing three-dimensional (3D) point clouds, which are crucial for accurate digital object representation in fields such as computer graphics, photogrammetry, computer vision, and robotics. Traditional metrics like the Hausdorff and Chamfer distances often struggle to capture the global statistical structure of point clouds and are sensitive to outliers. Likewise, existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models (GMMs) may be numerically unstable or unbounded. To overcome these limitations, the authors introduce an information geometric framework that models point clouds as GMMs situated on a statistical manifold. They prove that the space comprising these GMMs forms a valid statistical manifold and propose a new metric, the Modified Symmetric Kullback-Leibler (MSKL) divergence, which guarantees upper and lower bounds for numerical stability across all GMM comparisons. Experimental validation on datasets involving human poses (MPI-FAUST) and animal shapes (G-PCD) demonstrates that MSKL divergence yields stable, monotonic values directly correlated with geometric variations, outperforming both traditional geometric distances and previous KL divergence approximations. This approach offers a robust, theoretically sound method for point cloud shape analysis with improved numerical and geometric reliability. <div>
arXiv:2512.16213v1 Announce Type: new 
Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2512.16219</link>
<guid>https://arxiv.org/abs/2512.16219</guid>
<content:encoded><![CDATA[
<div> Keywords: novel view synthesis, diffusion models, high-quality noise, encoder-decoder network, discretized Euler inversion<br /><br />Summary:<br /><br />This paper addresses the challenge in single-view novel view synthesis (NVS) using diffusion models, focusing on the quality of the initial noise that significantly affects the quality of generated views. The authors observe that certain high-quality initial noise patterns yield better generation results but note the absence of frameworks for learning such noise. To tackle this, they first design a discretized Euler inversion method that embeds image semantic information into random Gaussian noise, enabling the creation of paired datasets of random and high-quality noise. Building on this, they propose a novel learning framework based on an encoder-decoder network (EDN) that transforms random noise directly into high-quality noise. This EDN can be integrated seamlessly with existing NVS models like SV3D and MV-Adapter. Experimental results demonstrate that the inclusion of EDN consistently improves performance across multiple datasets, validating its effectiveness and versatility. The paper contributes both a new method for noise refinement and an applicable framework for enhancing diffusion-based NVS, with publicly available code to facilitate further research and application. <div>
arXiv:2512.16219v1 Announce Type: new 
Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Compression Using Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2512.16226</link>
<guid>https://arxiv.org/abs/2512.16226</guid>
<content:encoded><![CDATA[
<div> Keywords: image compression, singular value decomposition, low-rank approximation, compression ratio, image quality<br /><br />Summary: This study focuses on the application of Singular Value Decomposition (SVD) and low-rank matrix approximations for image compression. The goal is to reduce storage and bandwidth requirements by efficiently compressing images, which form a large part of internet data. The research evaluates the compression performance through metrics like relative Frobenius error and compression ratio. Both grayscale and multichannel images are tested to verify the method's general applicability across different image types. Findings indicate that low-rank approximations often yield images that are visually close to the original, suggesting reasonable preservation of image quality. However, when compared to established compression formats such as JPEG, JPEG2000, and WEBP, the SVD-based method consistently underperforms in terms of compression efficiency at similar error levels. In fact, at low error tolerances, the compressed file size resulting from SVD can surpass that of the original image, making it impractical for real-world image compression needs. Overall, although SVD offers some visual fidelity, it is not competitive with current industry-standard codecs for effective and practical image compression. <div>
arXiv:2512.16226v1 Announce Type: new 
Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation</title>
<link>https://arxiv.org/abs/2512.16234</link>
<guid>https://arxiv.org/abs/2512.16234</guid>
<content:encoded><![CDATA[
<div> 3D human reaction generation, autoregressive framework, Bootstrap Contextual Encoding, real-time inference, motion fidelity<br /><br />Summary: This paper addresses three critical challenges in 3D human reaction generation: ensuring high motion fidelity, enabling real-time inference, and maintaining autoregressive adaptability for online scenarios. The authors propose ARMFlow, a novel MeanFlow-based autoregressive framework that captures temporal dependencies between actor and reactor motions. ARMFlow incorporates a causal context encoder and an MLP-based velocity predictor to improve generation accuracy. A key innovation is Bootstrap Contextual Encoding (BSCE), which, during training, uses generated history instead of ground-truth data, effectively reducing error accumulation in autoregressive outputs. To complement ARMFlow's online capabilities, the authors introduce ReMFlow, an offline variant that achieves state-of-the-art performance and the fastest inference speed among offline methods. ARMFlow enhances semantic alignment through a global contextual encoder, delivers high accuracy with low latency in single-step inference, and mitigates accumulated errors via BSCE. In experimental evaluations on InterHuman and InterX datasets, ARMFlow's single-step online generation surpasses existing online methods by more than 40% in FID score while matching the performance of state-of-the-art offline methods, despite relying on only partial sequence conditions. This work thus significantly advances real-time and accurate 3D human reaction generation. <div>
arXiv:2512.16234v1 Announce Type: new 
Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</title>
<link>https://arxiv.org/abs/2512.16235</link>
<guid>https://arxiv.org/abs/2512.16235</guid>
<content:encoded><![CDATA[
<div> Keywords: dermatology, AI diagnosis, family history, multi-modal framework, clinical trials  
  
<br /><br />Summary:  
This article addresses the challenge of improving dermatological diagnosis by integrating family history data with clinical imaging using AI. It highlights the global burden of skin diseases affecting 1.9 billion people and the diagnostic difficulties caused by limited specialist availability and complex presentations. The research introduces a multi-modal AI framework combining deep learning-based image analysis and structured clinical data, notably detailed family history patterns. The AI system employs interpretable convolutional neural networks linked with clinical decision trees incorporating hereditary risk factors to enhance diagnostic accuracy. Although full prospective clinical trials are proposed for future work, preliminary validation with healthcare professionals shows improved diagnostic performance, especially for hereditary conditions like melanoma, psoriasis, and atopic dermatitis. Expert feedback suggests potential benefits in early detection and personalized treatment recommendations. The AI framework is designed for seamless integration into clinical workflows while maintaining transparency through explainable AI techniques. This approach aims to support both clinical diagnosis and clinical trial validation, ultimately facilitating real-world implementation of AI-assisted dermatological care. <div>
arXiv:2512.16235v1 Announce Type: new 
Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models</title>
<link>https://arxiv.org/abs/2512.16243</link>
<guid>https://arxiv.org/abs/2512.16243</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view crowd counting, semi-supervised learning, model ranking, uncertainty estimation, limited labeled data<br /><br />Summary:<br /><br />This paper addresses the challenge of occlusion in crowd counting across large scenes by leveraging multi-view images. However, datasets for multi-view counting are limited due to the difficulty in collecting and annotating such data. To overcome this limitation, the authors explore semi-supervised frameworks that require fewer labeled multi-view images. Two novel semi-supervised methods are proposed, both based on ranking multi-view fusion models that utilize variable numbers of camera views. The first method, termed the vanilla model, ranks model predictions, enforcing the constraint that predictions with fewer views should not exceed predictions with more views. The second method ranks estimated model uncertainties, with the constraint that uncertainty should decrease as the number of views increases, guided by the prediction errors of the models. These ranking constraints are integrated into the training process to improve performance when labeled data are scarce. Experimental results demonstrate that the proposed ranking-based semi-supervised approaches outperform other existing semi-supervised crowd counting methods, providing a promising direction for handling limited multi-view labeled data in crowd counting tasks. <div>
arXiv:2512.16243v1 Announce Type: new 
Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.16266</link>
<guid>https://arxiv.org/abs/2512.16266</guid>
<content:encoded><![CDATA[
<div> Keywords: Fluorescence lifetime imaging microscopy, pixel super-resolution, deep learning, conditional generative adversarial network, image quality

<br /><br />Summary:  
1. Fluorescence lifetime imaging microscopy (FLIM) offers quantitative metabolic and molecular contrast with high translational potential for label-free, real-time diagnostics but is limited clinically by long pixel dwell times and low signal-to-noise ratio (SNR).  
2. The study introduces FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with pixel sizes up to 5 times larger than standard.  
3. FLIM_PSR_k uses a conditional generative adversarial network (cGAN) for training, providing more robust super-resolution reconstruction and significantly shorter inference times compared to diffusion model-based alternatives.  
4. The framework enables faster image acquisition and improves SNR in autofluorescence-based FLIM images, overcoming key challenges of resolution-speed trade-offs and noise limitations.  
5. Blind testing on patient-derived tumor tissue confirms reliable super-resolution with a factor of k=5, achieving a 25-fold increase in the space-bandwidth product and revealing fine architectural features otherwise lost in lower-resolution inputs, validated by statistically significant image quality improvements.  
6. By enhancing spatial resolution effectively, FLIM_PSR_k supports faster, higher-resolution, and hardware-flexible FLIM implementations, including compatibility with low-numerical-aperture and miniaturized platforms, thus enhancing FLIM’s potential for clinical translation. <div>
arXiv:2512.16266v1 Announce Type: new 
Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</title>
<link>https://arxiv.org/abs/2512.16270</link>
<guid>https://arxiv.org/abs/2512.16270</guid>
<content:encoded><![CDATA[
<div> Keywords: Text rendering, image editing, semantic consistency, multimodal models, text-guided generation<br /><br />Summary:<br /><br />This paper addresses the challenges in text rendering and editing within images, an area that has gained attention due to advances in large-scale diffusion and multimodal models but remains underexplored. The authors introduce TextEditBench, a benchmark specifically designed to evaluate text-centric regions in images, focusing not just on pixel-level changes but on reasoning-intensive scenarios requiring semantic, geometric, and contextual coherence. To better assess model performance, they propose a novel evaluation metric called Semantic Expectation (SE), which gauges a model's ability to preserve semantic consistency, contextual coherence, and alignment across modalities during text editing tasks. Through extensive experiments involving state-of-the-art image editing systems, they find that while current models can follow straightforward textual instructions, they struggle with more complex challenges such as context-dependent reasoning, maintaining physical consistency, and integrating edits in a layout-aware manner. TextEditBench thus fills a significant gap by establishing a new, focused testing ground that highlights the need to advance reasoning capabilities in text-guided image editing and multimodal generation systems. This benchmark and evaluation approach aim to push future research towards more intelligent and contextually aware text editing in visual content. <div>
arXiv:2512.16270v1 Announce Type: new 
Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFLAN: Generative Functional Layouts</title>
<link>https://arxiv.org/abs/2512.16275</link>
<guid>https://arxiv.org/abs/2512.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Floor plan generation, topological planning, geometric realization, graph neural network, Transformer  

<br /><br />Summary:  
This paper presents GFLAN, a novel generative framework designed to improve automated floor plan generation by explicitly separating the process into two stages: topological planning and geometric realization. The method addresses limitations found in prior deep learning approaches, which often fail to capture architectural reasoning such as prioritizing topological relations over geometric details, propagating functional constraints through adjacency, and forming circulation patterns from local connections. Stage A of GFLAN uses a specialized convolutional architecture with dual encoders to handle spatial context and layout state independently, sequentially placing room centroids within the building envelope based on discrete probability maps of feasible locations. Stage B builds a heterogeneous graph that links room nodes with boundary vertices and employs a Transformer-enhanced graph neural network to jointly predict precise room boundaries. By departing from traditional pixel-wise or wall-tracing methods, this two-stage approach provides a more principled and interpretable mechanism for synthesizing floor plans. The framework takes as input only the exterior boundary and front-door location, generating functional layouts that satisfy topological and geometric constraints more effectively than previous models. GFLAN represents a significant advancement in integrating combinatorial search, geometric constraints, and design functionality in automated architectural design. <div>
arXiv:2512.16275v1 Announce Type: new 
Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</title>
<link>https://arxiv.org/abs/2512.16294</link>
<guid>https://arxiv.org/abs/2512.16294</guid>
<content:encoded><![CDATA[
<div> Multi-label learning, Contrastive learning, Remote-sensing retrieval, Semantic imbalance, Adaptive weighting  

<br /><br />Summary:  
This article addresses key challenges in multi-label remote-sensing image retrieval, namely semantic overlap among land-cover categories, imbalanced label distributions, and intricate inter-class co-occurrence patterns. To overcome these issues, the authors propose Multi-Label Adaptive Contrastive Learning (MACL), an enhanced contrastive learning framework tailored for multi-label remote sensing. MACL incorporates label-aware sampling to better handle semantic overlap by focusing on relevant categories during training. It also integrates frequency-sensitive weighting to balance representation learning across both frequent and rare classes, ensuring robustness against label imbalance. Additionally, dynamic-temperature scaling is employed to adaptively adjust the contrastive loss temperature, improving the learning dynamics. Extensive experiments on three benchmark datasets—DLRSD, ML-AID, and WHDLD—demonstrate that MACL consistently outperforms baseline methods based on traditional contrastive losses. The method effectively mitigates semantic imbalance issues and achieves more reliable retrieval performance in large-scale remote sensing archives. The authors plan to release their code, pretrained models, and evaluation scripts upon acceptance, facilitating reproducibility and further research in the field. Overall, MACL presents a significant advancement for multi-label remote-sensing image retrieval by addressing critical dataset challenges. <div>
arXiv:2512.16294v1 Announce Type: new 
Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixelArena: A benchmark for Pixel-Precision Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.16303</link>
<guid>https://arxiv.org/abs/2512.16303</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, image generation, semantic segmentation, Gemini 3 Pro Image, fine-grained generative capabilities<br /><br />Summary:  
This paper addresses the evaluation of multi-modal large language models that generate images, focusing on fine-grained generation rather than just aesthetics. The authors introduce PixelArena, a novel benchmark that uses semantic segmentation tasks to objectively assess image generation at the pixel level. Their experiments reveal that the Gemini 3 Pro Image model exhibits emergent capabilities in generating semantic masks with remarkable precision in zero-shot scenarios, demonstrating advanced visual intelligence and genuine generalization in new image generation challenges. The study includes a thorough qualitative and quantitative comparison of Gemini 3 Pro Image’s performance against other models, highlighting strengths and identifying failure cases. These findings mark a significant step forward in assessing and understanding the fine-grained visual reasoning abilities of multimodal models. Furthermore, the insights gained offer valuable directions for future research in multimodality, reasoning, interpretability, and benchmarking, emphasizing the importance of objective, task-based evaluations for image generation systems. Overall, the work contributes to advancing methodologies for evaluating and improving the intelligence of models that integrate language and visual output. <div>
arXiv:2512.16303v1 Announce Type: new 
Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation</title>
<link>https://arxiv.org/abs/2512.16313</link>
<guid>https://arxiv.org/abs/2512.16313</guid>
<content:encoded><![CDATA[
<div> All-in-one video restoration, temporal modeling, degradation-agnostic features, lightweight network, LaverNet<br /><br />Summary:<br /><br />Recent advancements in video restoration have focused on developing unified models capable of handling multiple types of degradation simultaneously, known as all-in-one video restoration. However, existing approaches face significant challenges when dealing with degradations that vary over time. Firstly, the presence of degradation can dominate the temporal modeling process, causing models to focus on artifacts rather than the true video content. Secondly, current methods rely on large and complex models to achieve such restoration, which obscures the inherent difficulties of the task. To overcome these issues, the authors introduce LaverNet, a lightweight all-in-one video restoration network comprising only 362K parameters. The key innovation in LaverNet is a novel propagation mechanism that selectively passes only degradation-agnostic features across frames, effectively reducing the negative impact of degradations on temporal modeling. Despite its compact size, which is less than 1% of parameters used by existing methods, LaverNet demonstrates strong performance on benchmark datasets, achieving results comparable or even superior to larger models. This work highlights that efficient and effective all-in-one video restoration is possible with a significantly smaller network architecture. <div>
arXiv:2512.16313v1 Announce Type: new 
Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs</title>
<link>https://arxiv.org/abs/2512.16314</link>
<guid>https://arxiv.org/abs/2512.16314</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.16314v1  
Keywords: UAV, fusion localization, ridge estimation, multicollinearity, laser ranging  

<br /><br />Summary:  
This paper addresses the challenge of accurately tracking and measuring targets using multiple sensors mounted on Unmanned Aerial Vehicles (UAVs). It proposes a fusion localization method based on ridge estimation, which leverages both sequential imagery providing rich scene information and precise laser ranging data to improve localization accuracy. The study highlights the difficulty posed by multicollinearity in the design matrix's column vectors during least squares estimation, particularly under harsh conditions such as long distances, small intersection angles, and large inclination angles. Multicollinearity causes ill-conditioned problems that result in significant instability and reduced robustness of the estimates. To resolve this, ridge estimation is introduced as a solution to mitigate multicollinearity when observations are limited. Experimental results validate that the proposed fusion localization method outperforms ground localization algorithms that rely on single information sources by achieving higher accuracy. Furthermore, implementing ridge estimation not only improves accuracy but also enhances robustness, especially under constrained observation scenarios. This study demonstrates the effectiveness of combining multimodal data and applying ridge estimation techniques for UAV-based target localization in challenging conditions. <div>
arXiv:2512.16314v1 Announce Type: new 
Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2512.16325</link>
<guid>https://arxiv.org/abs/2512.16325</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality of Information, vehicular mobile crowdsensing, sensing coverage, incentive mechanism, urban monitoring  

<br /><br />Summary:  
This paper tackles the challenge of optimizing Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems, where vehicles participate dynamically and sensing coverage and reliability are intertwined challenges. The authors propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, designed to maximize sensing coverage and reliability within budget limits. A novel metric called Aggregated Sensing Quality (ASQ) is introduced to quantitatively integrate both coverage and sensing reliability, providing a comprehensive measure of QoI. QUIDS features a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and intelligently allocates incentives under uncertain conditions, thereby further improving ASQ. Evaluations conducted with real-world metropolitan data demonstrate that QUIDS enhances ASQ by 38% compared to scenarios without dispatching and by 10% over existing state-of-the-art approaches. Additionally, it reduces reconstruction map errors by 39-74% across different algorithms, indicating significant improvements in data quality. By jointly optimizing coverage and reliability through a quality-informed incentive mechanism, QUIDS enables cost-effective and high-quality urban monitoring without needing dedicated infrastructure. The system is applicable to various smart-city scenarios including traffic and environmental sensing, facilitating more effective and efficient urban data collection. <div>
arXiv:2512.16325v1 Announce Type: new 
Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Edge-to-Server Inference for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16349</link>
<guid>https://arxiv.org/abs/2512.16349</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative inference, vision-language models, edge computing, communication cost, region of interest  

<br /><br />Summary:  
This paper introduces a collaborative edge-to-server inference framework designed for vision-language models (VLMs) to minimize communication costs without sacrificing inference accuracy. Typically, edge devices send resized global images to servers for VLM processing, but resizing often loses fine details that degrade accuracy. To address this, the authors propose a two-stage approach: initially, the server processes the global image and identifies a region of interest (RoI) using the model's internal attention mechanisms. Then, by computing the min-entropy of output tokens, the system measures confidence and decides if the detailed local image of the RoI needs to be retransmitted from the edge device. If the min-entropy is above a threshold (indicating uncertainty), the server requests the detailed image, enabling refined inference by combining global and local inputs. This selective retransmission ensures that only necessary visual content is transmitted, optimizing bandwidth usage. Experimental results on multiple VLM architectures show that this framework significantly reduces communication overhead while maintaining comparable inference accuracy, validating its effectiveness in real-world deployments where preserving fine-grained visual information is critical. <div>
arXiv:2512.16349v1 Announce Type: new 
Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction</title>
<link>https://arxiv.org/abs/2512.16357</link>
<guid>https://arxiv.org/abs/2512.16357</guid>
<content:encoded><![CDATA[
<div> Keywords: Latent Diffusion Models, High Dynamic Range, Gain Map, HDR Reconstruction, Multi-exposure

<br /><br />Summary:  
This paper addresses the challenges of applying pre-trained Latent Diffusion Models (LDMs) to multi-exposure High Dynamic Range (HDR) image reconstruction. Traditional LDMs face limitations including restricted dynamic-range representation due to 8-bit latent compression, high inference costs from multi-step denoising, and the tendency to hallucinate content because of their generative nature. To overcome these, the authors propose GMODiff, a novel gain map-driven, one-step diffusion framework for HDR reconstruction. Instead of reconstructing full HDR images, GMODiff reformulates the problem as estimating a Gain Map (GM) conditionally, wherein the GM encodes the extended dynamic range while maintaining the bit depth of Low Dynamic Range (LDR) images. The approach initializes denoising from an informative regression-based estimate rather than pure noise, enabling the generation of high-quality GMs with a single denoising step. Furthermore, the method combines the strengths of regression-based models—which excel at preserving content fidelity—and LDMs, which enhance perceptual quality, by using regression priors to guide both denoising and latent decoding. This combined guidance effectively suppresses hallucinations and maintains structural accuracy. Extensive experiments show that GMODiff outperforms several state-of-the-art methods while being 100 times faster than prior LDM-based approaches. <div>
arXiv:2512.16357v1 Announce Type: new 
Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation</title>
<link>https://arxiv.org/abs/2512.16360</link>
<guid>https://arxiv.org/abs/2512.16360</guid>
<content:encoded><![CDATA[
<div> pose-driven animation, multi-character, identity correspondence, graph matching, Mask-Query Attention<br /><br />Summary: This paper addresses the challenge of consistent pose-driven character animation in multi-character scenarios, focusing particularly on the problem of maintaining correct Identity Correspondence (IC) when characters swap positions. The authors present EverybodyDance, a novel framework designed specifically to enforce IC correctness in generated animations involving multiple characters. EverybodyDance introduces the Identity Matching Graph (IMG), which represents characters in both reference and generated frames as nodes in a weighted bipartite graph; the edge weights denote affinities computed via a new Mask-Query Attention (MQA) mechanism. By framing IC correctness as a graph structural metric, the approach optimizes this criterion during training to ensure accurate identity preservation. To further enhance performance in multi-character animation, the system incorporates identity-embedded guidance, multi-scale matching strategies, and pre-classified sampling techniques, all aimed at reinforcing identity alignment. For evaluation, the authors curate the Identity Correspondence Evaluation benchmark that specifically tests IC accuracy in multi-character settings. Extensive experimental results demonstrate that EverybodyDance significantly surpasses current leading methods in both identity correspondence and visual quality of animations, marking a substantial advancement in pose-driven multi-character animation. <div>
arXiv:2512.16360v1 Announce Type: new 
Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.16371</link>
<guid>https://arxiv.org/abs/2512.16371</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Video, Factorized Video Generation, Large Language Model, Video Composition, Temporal Synthesis

<br /><br />Summary:  
This paper addresses the challenges faced by state-of-the-art Text-to-Video (T2V) diffusion models, which often struggle to generate complex scenes and follow temporal instructions accurately. The authors identify that many errors arise from the generation of an initial frame that lacks semantic correctness and logical consistency. To solve this, they propose Factorized Video Generation (FVG), a three-stage pipeline that separates the video generation process into distinct tasks. The first stage, Reasoning, involves using a Large Language Model (LLM) to rewrite the input prompt to describe only the initial scene, resolving any temporal ambiguities. Next, the Composition stage employs a Text-to-Image (T2I) model to create a high-quality, compositionally accurate anchor frame based on the refined prompt. Finally, the Temporal Synthesis stage uses a finetuned video model to animate the scene while faithfully following the prompt. This factorized approach achieves state-of-the-art performance on the T2V CompBench benchmark and improves results on the VBench2 dataset. Additionally, the method allows for a 70% reduction in sampling steps without sacrificing performance, significantly speeding up the generation process. Overall, FVG presents a practical strategy for producing more efficient, robust, and controllable video synthesis. <div>
arXiv:2512.16371v1 Announce Type: new 
Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Frequency Domain Alignment Network for Medical image segmentation</title>
<link>https://arxiv.org/abs/2512.16393</link>
<guid>https://arxiv.org/abs/2512.16393</guid>
<content:encoded><![CDATA[
<div> Adaptive Frequency Domain Alignment Network, Domain Adaptation, Medical Image Segmentation, Frequency Fusion, Cross-Domain Knowledge Transfer<br /><br />Summary:<br /><br />1. The paper addresses the scarcity of high-quality annotated medical image segmentation data, which is caused by the labor-intensive and time-consuming manual annotation process. <br />2. To overcome this limitation, the authors propose a novel domain adaptation framework named Adaptive Frequency Domain Alignment Network (AFDAN) that aligns features in the frequency domain.<br />3. AFDAN consists of three key modules: (a) an Adversarial Domain Learning Module for transferring features between source and target domains, (b) a Source-Target Frequency Fusion Module that blends frequency representations from both domains, and (c) a Spatial-Frequency Integration Module which integrates spatial and frequency features to improve segmentation performance.<br />4. The framework is designed to enable robust cross-domain knowledge transfer, thereby mitigating the impact of limited annotated data.<br />5. Extensive experiments on two datasets demonstrate AFDAN’s effectiveness, achieving a high Intersection over Union (IoU) of 90.9% for vitiligo segmentation on the new VITILIGO2025 dataset and an IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, both surpassing current state-of-the-art methods. <div>
arXiv:2512.16393v1 Announce Type: new 
Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</title>
<link>https://arxiv.org/abs/2512.16397</link>
<guid>https://arxiv.org/abs/2512.16397</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, 3D Neural Representations, Face Reconstruction, Relightable Texture, Text-driven Asset Creation

<br /><br />Summary:  
1. The paper introduces a novel method for reconstructing a neutral-pose human face from a limited set of uncalibrated images (only 11), using advanced three-dimensional neural representations.  
2. Gaussian Splatting is preferred over NeRFs due to its explicit nature, which is more conducive to applying constraints, facilitating better control over the reconstruction process.  
3. Semantic face regions are aligned using segmentation annotations, improving the coherence and accuracy of the reconstruction despite the sparse image input.  
4. The system softly constrains Gaussian primitives to an underlying triangulated surface, creating a structured 3D model that can be incorporated into standard graphics pipelines, allowing practical use in graphics applications.  
5. The accurate geometry enables transforming Gaussian Splats into texture space, treating them as view-dependent neural textures. This innovation allows high visual fidelity rendering of assets without modifying other scene assets or rendering parameters.  
6. A relightable Gaussian model disentangles texture from lighting, producing a detailed, delit high-resolution albedo texture compatible with standard graphics workflows.  
7. The approach supports training on images with varied, even incompatible, lighting conditions, increasing robustness through flexible regularization.  
8. Finally, the paper demonstrates the practical utility of the method by integrating it into a text-driven asset creation pipeline, showing its applicability in content generation and computer graphics. <div>
arXiv:2512.16397v1 Announce Type: new 
Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrepLLM: Native Boundary Representation Understanding with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16413</link>
<guid>https://arxiv.org/abs/2512.16413</guid>
<content:encoded><![CDATA[
<div> Keywords: BrepLLM, Large Language Models, 3D Boundary Representation, Cross-modal Alignment, Mixture-of-Query Experts<br /><br />Summary: Current Large Language Models (LLMs) that work with token sequences struggle to directly process 3D Boundary Representation (Brep) models, which include complex geometric and topological information. To address this, the authors propose BrepLLM, the first framework designed to enable LLMs to parse and reason over raw Brep data, effectively bridging the gap between structured 3D geometry and natural language. BrepLLM utilizes a two-stage training pipeline: first, Cross-modal Alignment Pre-training, where an adaptive UV sampling strategy converts Breps into graph representations containing geometry and topology information. A hierarchical BrepEncoder then extracts features from faces, edges, and topology to produce a global token and node token sequence, which are aligned with text embeddings from a frozen CLIP text encoder via contrastive learning. The second stage involves Multi-stage LLM Fine-tuning, which integrates the pretrained BrepEncoder into an LLM and aligns node tokens using a progressive three-step strategy: training an MLP-based semantic mapper using 2D-LLM priors, fine-tuning the LLM, and introducing a Mixture-of-Query Experts (MQE) to model geometric diversity. Additionally, the authors created the Brep2Text dataset containing 269,444 Brep-text question-answer pairs. Experiments demonstrate that BrepLLM achieves state-of-the-art performance on 3D object classification and captioning tasks. <div>
arXiv:2512.16413v1 Announce Type: new 
Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountZES: Counting via Zero-Shot Exemplar Selection</title>
<link>https://arxiv.org/abs/2512.16415</link>
<guid>https://arxiv.org/abs/2512.16415</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot object counting, exemplar selection, open-vocabulary detection, self-supervised learning, feature clustering  

<br /><br />Summary:  
The paper addresses the challenging task of zero-shot object counting (ZOC), where the goal is to count instances of previously unseen categories from class names alone. Current ZOC approaches either use open-vocabulary detectors that produce multi-instance candidates or random patch sampling which poorly isolates object instances. To overcome these limitations, the authors propose CountZES, a training-free framework that employs zero-shot exemplar selection to improve counting accuracy. CountZES operates through three complementary stages: Detection-Anchored Exemplar (DAE), which refines open-vocabulary detections to obtain precise single-instance exemplars; Density-Guided Exemplar (DGE), introducing a self-supervised method that selects exemplars based on density estimation to ensure statistical consistency and semantic compactness; and Feature-Consensus Exemplar (FCE), which uses feature-space clustering to enforce visual coherence across exemplars. This combined approach produces a diverse and complementary exemplar set balancing textual grounding, counting accuracy, and visual representativeness. Experimental results across multiple datasets, including natural, aerial, and medical images, demonstrate that CountZES outperforms existing zero-shot counting methods and generalizes well across different domains without requiring training. <div>
arXiv:2512.16415v1 Announce Type: new 
Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt</title>
<link>https://arxiv.org/abs/2512.16443</link>
<guid>https://arxiv.org/abs/2512.16443</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion, subject consistency, semantic entanglement, training-free approach, text embeddings<br /><br />Summary:<br /><br />This paper addresses the challenge of maintaining subject consistency in text-to-image diffusion models, which are proficient at generating high-quality images from natural language prompts but often produce inconsistent subjects across multiple images. Existing solutions typically involve computationally expensive fine-tuning or image conditioning that requires optimization tailored to each subject. The authors critique 1Prompt1Story, a training-free method that concatenates scene descriptions into a single prompt and rescales token embeddings; while effective, it suffers from semantic leakage where embeddings of different frames entangle, causing misalignment between the text and generated images. To overcome this, the paper proposes a novel, training-free approach that refines text embeddings from a geometric perspective to suppress unwanted semantic overlap between frames. This method effectively reduces semantic entanglement and improves subject consistency without the need for model retraining or subject-specific fine-tuning. Extensive experiments demonstrate that the proposed approach significantly enhances both subject consistency and text alignment compared to existing baselines, making it a practical solution for visual storytelling applications using diffusion-based text-to-image generation models. <div>
arXiv:2512.16443v1 Announce Type: new 
Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</title>
<link>https://arxiv.org/abs/2512.16456</link>
<guid>https://arxiv.org/abs/2512.16456</guid>
<content:encoded><![CDATA[
<div> Keywords: human motion generation, gaze priming, diffusion model, reach success, motion datasets  

<br /><br />Summary:  
This paper addresses the complex challenge of generating realistic human motion, focusing specifically on the behaviour associated with priming an object or location for picking up or putting down. This behaviour involves spotting the target object or location from a distance, termed gaze priming, followed by approaching and reaching the target. The authors curated a novel dataset consisting of 23.7K gaze-primed human motion sequences by combining data from five publicly available datasets: HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. They leverage this dataset to pre-train a text-conditioned diffusion-based motion generation model, which is further fine-tuned conditioned on either the goal pose or location. The evaluation introduces a new metric called "Prime Success" alongside the existing "Reach Success" metric to assess how well the generated motion imitates natural human movement. Experimental results highlight that, on the largest dataset HD-EPIC, the model achieved a 60% rate in prime success and an 89% rate in reach success when conditioned on the goal object location, demonstrating the model's effectiveness in synthesizing realistic gaze-primed reaching motions. <div>
arXiv:2512.16456v1 Announce Type: new 
Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.16461</link>
<guid>https://arxiv.org/abs/2512.16461</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D scene understanding, Vision-Language Models, point cloud geometry, temporal consistency, autonomous robotics<br /><br />Summary:  
1. SNOW (Scene Understanding with Open-World Knowledge) is a novel framework designed for unified 4D scene understanding in autonomous robotic systems.  
2. It addresses the limitations of Vision-Language Models (VLMs) by integrating semantic information with precise 3D geometric data and temporal dynamics to improve environment perception.  
3. The framework processes synchronized RGB images and 3D point clouds, utilizing HDBSCAN clustering to produce object-level proposals that direct the SAM2 segmentation model for more accurate region delineation.  
4. SNOW introduces Spatio-Temporal Tokenized Patch Encoding (STEP), which generates multimodal tokens encapsulating localized semantic, geometric, and temporal features.  
5. These tokens are incrementally fused into a 4D Scene Graph (4DSG), providing a structured and queryable world model that supports spatially and temporally grounded reasoning.  
6. A lightweight SLAM backend anchors STEP tokens in a global spatial reference, ensuring consistent mapping across time and enabling robust spatial grounding.  
7. Experimental evaluation demonstrates SNOW’s ability to achieve state-of-the-art performance on diverse benchmarks by leveraging structured 4D priors to enhance embodied reasoning and autonomous navigation.  
8. This work underlines the critical role of integrating open-world semantic knowledge with geometric and temporal cues for reliable real-world robotic perception and interaction. <div>
arXiv:2512.16461v1 Announce Type: new 
Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.16483</link>
<guid>https://arxiv.org/abs/2512.16483</guid>
<content:encoded><![CDATA[
arXiv:2512.16483v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.16484</link>
<guid>https://arxiv.org/abs/2512.16484</guid>
<content:encoded><![CDATA[
arXiv:2512.16484v1 Announce Type: new 
Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</title>
<link>https://arxiv.org/abs/2512.16485</link>
<guid>https://arxiv.org/abs/2512.16485</guid>
<content:encoded><![CDATA[
arXiv:2512.16485v1 Announce Type: new 
Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images</title>
<link>https://arxiv.org/abs/2512.16493</link>
<guid>https://arxiv.org/abs/2512.16493</guid>
<content:encoded><![CDATA[
arXiv:2512.16493v1 Announce Type: new 
Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16494</link>
<guid>https://arxiv.org/abs/2512.16494</guid>
<content:encoded><![CDATA[
arXiv:2512.16494v1 Announce Type: new 
Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</title>
<link>https://arxiv.org/abs/2512.16501</link>
<guid>https://arxiv.org/abs/2512.16501</guid>
<content:encoded><![CDATA[
arXiv:2512.16501v1 Announce Type: new 
Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</title>
<link>https://arxiv.org/abs/2512.16504</link>
<guid>https://arxiv.org/abs/2512.16504</guid>
<content:encoded><![CDATA[
arXiv:2512.16504v1 Announce Type: new 
Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images</title>
<link>https://arxiv.org/abs/2512.16511</link>
<guid>https://arxiv.org/abs/2512.16511</guid>
<content:encoded><![CDATA[
arXiv:2512.16511v1 Announce Type: new 
Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16523</link>
<guid>https://arxiv.org/abs/2512.16523</guid>
<content:encoded><![CDATA[
arXiv:2512.16523v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16561</link>
<guid>https://arxiv.org/abs/2512.16561</guid>
<content:encoded><![CDATA[
arXiv:2512.16561v1 Announce Type: new 
Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D Primitive-M\^ach\'e: Glueing Primitives for Persistent 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2512.16564</link>
<guid>https://arxiv.org/abs/2512.16564</guid>
<content:encoded><![CDATA[
arXiv:2512.16564v1 Announce Type: new 
Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.16567</link>
<guid>https://arxiv.org/abs/2512.16567</guid>
<content:encoded><![CDATA[
arXiv:2512.16567v1 Announce Type: new 
Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series</title>
<link>https://arxiv.org/abs/2512.16577</link>
<guid>https://arxiv.org/abs/2512.16577</guid>
<content:encoded><![CDATA[
arXiv:2512.16577v1 Announce Type: new 
Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2512.16584</link>
<guid>https://arxiv.org/abs/2512.16584</guid>
<content:encoded><![CDATA[
arXiv:2512.16584v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</title>
<link>https://arxiv.org/abs/2512.16586</link>
<guid>https://arxiv.org/abs/2512.16586</guid>
<content:encoded><![CDATA[
arXiv:2512.16586v1 Announce Type: new 
Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment</title>
<link>https://arxiv.org/abs/2512.16609</link>
<guid>https://arxiv.org/abs/2512.16609</guid>
<content:encoded><![CDATA[
arXiv:2512.16609v1 Announce Type: new 
Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.16615</link>
<guid>https://arxiv.org/abs/2512.16615</guid>
<content:encoded><![CDATA[
arXiv:2512.16615v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation</title>
<link>https://arxiv.org/abs/2512.16620</link>
<guid>https://arxiv.org/abs/2512.16620</guid>
<content:encoded><![CDATA[
arXiv:2512.16620v1 Announce Type: new 
Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeContext as Defense: Safe Image Editing in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.16625</link>
<guid>https://arxiv.org/abs/2512.16625</guid>
<content:encoded><![CDATA[
arXiv:2512.16625v1 Announce Type: new 
Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARMAE: Masked Autoencoder for SAR Representation Learning</title>
<link>https://arxiv.org/abs/2512.16635</link>
<guid>https://arxiv.org/abs/2512.16635</guid>
<content:encoded><![CDATA[
arXiv:2512.16635v1 Announce Type: new 
Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</title>
<link>https://arxiv.org/abs/2512.16636</link>
<guid>https://arxiv.org/abs/2512.16636</guid>
<content:encoded><![CDATA[
arXiv:2512.16636v1 Announce Type: new 
Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</title>
<link>https://arxiv.org/abs/2512.16670</link>
<guid>https://arxiv.org/abs/2512.16670</guid>
<content:encoded><![CDATA[
arXiv:2512.16670v1 Announce Type: new 
Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</title>
<link>https://arxiv.org/abs/2512.16685</link>
<guid>https://arxiv.org/abs/2512.16685</guid>
<content:encoded><![CDATA[
arXiv:2512.16685v1 Announce Type: new 
Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?</title>
<link>https://arxiv.org/abs/2512.16688</link>
<guid>https://arxiv.org/abs/2512.16688</guid>
<content:encoded><![CDATA[
arXiv:2512.16688v1 Announce Type: new 
Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDFoam: Signed-Distance Foam for explicit surface reconstruction</title>
<link>https://arxiv.org/abs/2512.16706</link>
<guid>https://arxiv.org/abs/2512.16706</guid>
<content:encoded><![CDATA[
arXiv:2512.16706v1 Announce Type: new 
Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry</title>
<link>https://arxiv.org/abs/2512.16710</link>
<guid>https://arxiv.org/abs/2512.16710</guid>
<content:encoded><![CDATA[
arXiv:2512.16710v1 Announce Type: new 
Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition</title>
<link>https://arxiv.org/abs/2512.16727</link>
<guid>https://arxiv.org/abs/2512.16727</guid>
<content:encoded><![CDATA[
arXiv:2512.16727v1 Announce Type: new 
Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.16740</link>
<guid>https://arxiv.org/abs/2512.16740</guid>
<content:encoded><![CDATA[
arXiv:2512.16740v1 Announce Type: new 
Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeNet: A Light Weight Model for Low Bitrate Image Compression</title>
<link>https://arxiv.org/abs/2512.16743</link>
<guid>https://arxiv.org/abs/2512.16743</guid>
<content:encoded><![CDATA[
arXiv:2512.16743v1 Announce Type: new 
Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</title>
<link>https://arxiv.org/abs/2512.16767</link>
<guid>https://arxiv.org/abs/2512.16767</guid>
<content:encoded><![CDATA[
arXiv:2512.16767v1 Announce Type: new 
Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowDet: Unifying Object Detection and Generative Transport Flows</title>
<link>https://arxiv.org/abs/2512.16771</link>
<guid>https://arxiv.org/abs/2512.16771</guid>
<content:encoded><![CDATA[
arXiv:2512.16771v1 Announce Type: new 
Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kling-Omni Technical Report</title>
<link>https://arxiv.org/abs/2512.16776</link>
<guid>https://arxiv.org/abs/2512.16776</guid>
<content:encoded><![CDATA[
arXiv:2512.16776v1 Announce Type: new 
Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R3ST: A Synthetic 3D Dataset With Realistic Trajectories</title>
<link>https://arxiv.org/abs/2512.16784</link>
<guid>https://arxiv.org/abs/2512.16784</guid>
<content:encoded><![CDATA[
arXiv:2512.16784v1 Announce Type: new 
Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title>
<link>https://arxiv.org/abs/2512.16791</link>
<guid>https://arxiv.org/abs/2512.16791</guid>
<content:encoded><![CDATA[
arXiv:2512.16791v1 Announce Type: new 
Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</title>
<link>https://arxiv.org/abs/2512.16811</link>
<guid>https://arxiv.org/abs/2512.16811</guid>
<content:encoded><![CDATA[
arXiv:2512.16811v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title>
<link>https://arxiv.org/abs/2512.16818</link>
<guid>https://arxiv.org/abs/2512.16818</guid>
<content:encoded><![CDATA[
arXiv:2512.16818v1 Announce Type: new 
Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title>
<link>https://arxiv.org/abs/2512.16826</link>
<guid>https://arxiv.org/abs/2512.16826</guid>
<content:encoded><![CDATA[
arXiv:2512.16826v1 Announce Type: new 
Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiology Report Generation with Layer-Wise Anatomical Attention</title>
<link>https://arxiv.org/abs/2512.16841</link>
<guid>https://arxiv.org/abs/2512.16841</guid>
<content:encoded><![CDATA[
arXiv:2512.16841v1 Announce Type: new 
Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</title>
<link>https://arxiv.org/abs/2512.16842</link>
<guid>https://arxiv.org/abs/2512.16842</guid>
<content:encoded><![CDATA[
arXiv:2512.16842v1 Announce Type: new 
Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2512.16853</link>
<guid>https://arxiv.org/abs/2512.16853</guid>
<content:encoded><![CDATA[
arXiv:2512.16853v1 Announce Type: new 
Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2512.16864</link>
<guid>https://arxiv.org/abs/2512.16864</guid>
<content:encoded><![CDATA[
arXiv:2512.16864v1 Announce Type: new 
Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: new 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation</title>
<link>https://arxiv.org/abs/2512.16880</link>
<guid>https://arxiv.org/abs/2512.16880</guid>
<content:encoded><![CDATA[
arXiv:2512.16880v1 Announce Type: new 
Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-PhyGs: Multi-Material Object Dynamics from Video</title>
<link>https://arxiv.org/abs/2512.16885</link>
<guid>https://arxiv.org/abs/2512.16885</guid>
<content:encoded><![CDATA[
arXiv:2512.16885v1 Announce Type: new 
Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: new 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</title>
<link>https://arxiv.org/abs/2512.16893</link>
<guid>https://arxiv.org/abs/2512.16893</guid>
<content:encoded><![CDATA[
arXiv:2512.16893v1 Announce Type: new 
Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</title>
<link>https://arxiv.org/abs/2512.16900</link>
<guid>https://arxiv.org/abs/2512.16900</guid>
<content:encoded><![CDATA[
arXiv:2512.16900v1 Announce Type: new 
Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title>
<link>https://arxiv.org/abs/2512.16905</link>
<guid>https://arxiv.org/abs/2512.16905</guid>
<content:encoded><![CDATA[
arXiv:2512.16905v1 Announce Type: new 
Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</title>
<link>https://arxiv.org/abs/2512.16906</link>
<guid>https://arxiv.org/abs/2512.16906</guid>
<content:encoded><![CDATA[
arXiv:2512.16906v1 Announce Type: new 
Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</title>
<link>https://arxiv.org/abs/2512.16907</link>
<guid>https://arxiv.org/abs/2512.16907</guid>
<content:encoded><![CDATA[
arXiv:2512.16907v1 Announce Type: new 
Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneDiff: A Benchmark and Method for Multiview Object Change Detection</title>
<link>https://arxiv.org/abs/2512.16908</link>
<guid>https://arxiv.org/abs/2512.16908</guid>
<content:encoded><![CDATA[
arXiv:2512.16908v1 Announce Type: new 
Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</title>
<link>https://arxiv.org/abs/2512.16909</link>
<guid>https://arxiv.org/abs/2512.16909</guid>
<content:encoded><![CDATA[
arXiv:2512.16909v1 Announce Type: new 
Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFTok: Bridging the Performance Gap in Discrete Tokenizers</title>
<link>https://arxiv.org/abs/2512.16910</link>
<guid>https://arxiv.org/abs/2512.16910</guid>
<content:encoded><![CDATA[
arXiv:2512.16910v1 Announce Type: new 
Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title>
<link>https://arxiv.org/abs/2512.16913</link>
<guid>https://arxiv.org/abs/2512.16913</guid>
<content:encoded><![CDATA[
arXiv:2512.16913v1 Announce Type: new 
Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</title>
<link>https://arxiv.org/abs/2512.16915</link>
<guid>https://arxiv.org/abs/2512.16915</guid>
<content:encoded><![CDATA[
arXiv:2512.16915v1 Announce Type: new 
Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaTooler-V: Adaptive Tool-Use for Images and Videos</title>
<link>https://arxiv.org/abs/2512.16918</link>
<guid>https://arxiv.org/abs/2512.16918</guid>
<content:encoded><![CDATA[
arXiv:2512.16918v1 Announce Type: new 
Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVGT: Driving Visual Geometry Transformer</title>
<link>https://arxiv.org/abs/2512.16919</link>
<guid>https://arxiv.org/abs/2512.16919</guid>
<content:encoded><![CDATA[
arXiv:2512.16919v1 Announce Type: new 
Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasyV2V: A High-quality Instruction-based Video Editing Framework</title>
<link>https://arxiv.org/abs/2512.16920</link>
<guid>https://arxiv.org/abs/2512.16920</guid>
<content:encoded><![CDATA[
arXiv:2512.16920v1 Announce Type: new 
Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
<link>https://arxiv.org/abs/2512.16921</link>
<guid>https://arxiv.org/abs/2512.16921</guid>
<content:encoded><![CDATA[
arXiv:2512.16921v1 Announce Type: new 
Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Embedding Prediction Makes Strong Vision Learners</title>
<link>https://arxiv.org/abs/2512.16922</link>
<guid>https://arxiv.org/abs/2512.16922</guid>
<content:encoded><![CDATA[
arXiv:2512.16922v1 Announce Type: new 
Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Refocusing: Flexible Defocus Control from a Single Image</title>
<link>https://arxiv.org/abs/2512.16923</link>
<guid>https://arxiv.org/abs/2512.16923</guid>
<content:encoded><![CDATA[
arXiv:2512.16923v1 Announce Type: new 
Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</title>
<link>https://arxiv.org/abs/2512.16924</link>
<guid>https://arxiv.org/abs/2512.16924</guid>
<content:encoded><![CDATA[
arXiv:2512.16924v1 Announce Type: new 
Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</title>
<link>https://arxiv.org/abs/2512.15747</link>
<guid>https://arxiv.org/abs/2512.15747</guid>
<content:encoded><![CDATA[
arXiv:2512.15747v1 Announce Type: cross 
Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?</title>
<link>https://arxiv.org/abs/2512.15748</link>
<guid>https://arxiv.org/abs/2512.15748</guid>
<content:encoded><![CDATA[
arXiv:2512.15748v1 Announce Type: cross 
Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioimageAIpub: a toolbox for AI-ready bioimaging data publishing</title>
<link>https://arxiv.org/abs/2512.15820</link>
<guid>https://arxiv.org/abs/2512.15820</guid>
<content:encoded><![CDATA[
arXiv:2512.15820v1 Announce Type: cross 
Abstract: Modern bioimage analysis approaches are data hungry, making it necessary for researchers to scavenge data beyond those collected within their (bio)imaging facilities. In addition to scale, bioimaging datasets must be accompanied with suitable, high-quality annotations and metadata. Although established data repositories such as the Image Data Resource (IDR) and BioImage Archive offer rich metadata, their contents typically cannot be directly consumed by image analysis tools without substantial data wrangling. Such a tedious assembly and conversion of (meta)data can account for a dedicated amount of time investment for researchers, hindering the development of more powerful analysis tools. Here, we introduce BioimageAIpub, a workflow that streamlines bioimaging data conversion, enabling a seamless upload to HuggingFace, a widely used platform for sharing machine learning datasets and models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</title>
<link>https://arxiv.org/abs/2512.15829</link>
<guid>https://arxiv.org/abs/2512.15829</guid>
<content:encoded><![CDATA[
arXiv:2512.15829v1 Announce Type: cross 
Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Video Planner Enables Generalizable Robot Control</title>
<link>https://arxiv.org/abs/2512.15840</link>
<guid>https://arxiv.org/abs/2512.15840</guid>
<content:encoded><![CDATA[
arXiv:2512.15840v1 Announce Type: cross 
Abstract: General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In search of truth: Evaluating concordance of AI-based anatomy segmentation models</title>
<link>https://arxiv.org/abs/2512.15921</link>
<guid>https://arxiv.org/abs/2512.15921</guid>
<content:encoded><![CDATA[
arXiv:2512.15921v1 Announce Type: cross 
Abstract: Purpose AI-based methods for anatomy segmentation can help automate characterization of large imaging datasets. The growing number of similar in functionality models raises the challenge of evaluating them on datasets that do not contain ground truth annotations. We introduce a practical framework to assist in this task. Approach We harmonize the segmentation results into a standard, interoperable representation, which enables consistent, terminology-based labeling of the structures. We extend 3D Slicer to streamline loading and comparison of these harmonized segmentations, and demonstrate how standard representation simplifies review of the results using interactive summary plots and browser-based visualization using OHIF Viewer. To demonstrate the utility of the approach we apply it to evaluating segmentation of 31 anatomical structures (lungs, vertebrae, ribs, and heart) by six open-source models - TotalSegmentator 1.5 and 2.6, Auto3DSeg, MOOSE, MultiTalent, and CADS - for a sample of Computed Tomography (CT) scans from the publicly available National Lung Screening Trial (NLST) dataset. Results We demonstrate the utility of the framework in enabling automating loading, structure-wise inspection and comparison across models. Preliminary results ascertain practical utility of the approach in allowing quick detection and review of problematic results. The comparison shows excellent agreement segmenting some (e.g., lung) but not all structures (e.g., some models produce invalid vertebrae or rib segmentations). Conclusions The resources developed are linked from https://imagingdatacommons.github.io/segmentation-comparison/ including segmentation harmonization scripts, summary plots, and visualization tools. This work assists in model evaluation in absence of ground truth, ultimately enabling informed model selection.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
arXiv:2512.15938v1 Announce Type: cross 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCR-VQGAN: A Scalable and Cost-Effective Tau PET Synthesis Approach for Alzheimer's Disease Imaging</title>
<link>https://arxiv.org/abs/2512.15947</link>
<guid>https://arxiv.org/abs/2512.15947</guid>
<content:encoded><![CDATA[
arXiv:2512.15947v1 Announce Type: cross 
Abstract: Tau positron emission tomography (PET) is a critical diagnostic modality for Alzheimer's disease (AD) because it visualizes and quantifies neurofibrillary tangles, a hallmark of AD pathology. However, its widespread clinical adoption is hindered by significant challenges, such as radiation exposure, limited availability, high clinical workload, and substantial financial costs. To overcome these limitations, we propose Multi-scale CBAM Residual Vector Quantized Generative Adversarial Network (MCR-VQGAN) to synthesize high-fidelity tau PET images from structural T1-weighted MRI scans. MCR-VQGAN improves standard VQGAN by integrating three key architectural enhancements: multi-scale convolutions, ResNet blocks, and Convolutional Block Attention Modules (CBAM). Using 222 paired structural T1-weighted MRI and tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI), we trained and compared MCR-VQGAN with cGAN, WGAN-GP, CycleGAN, and VQGAN. Our proposed model achieved superior image synthesis performance across all metrics: MSE of 0.0056 +/- 0.0061, PSNR of 24.39 +/- 4.49 dB, and SSIM of 0.9000 +/- 0.0453. To assess the clinical utility of the synthetic images, we trained and evaluated a CNN-based AD classifier. The classifier achieved comparable accuracy when tested on real (63.64%) and synthetic (65.91%) images. This result indicates that our synthesis process successfully preserves diagnostically relevant features without significant information loss. Our results demonstrate that MCR-VQGAN can offer a reliable and scalable surrogate for conventional tau PET imaging, potentially improving the accessibility and scalability of tau imaging biomarkers for AD research and clinical workflows.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes</title>
<link>https://arxiv.org/abs/2512.16085</link>
<guid>https://arxiv.org/abs/2512.16085</guid>
<content:encoded><![CDATA[
arXiv:2512.16085v1 Announce Type: cross 
Abstract: Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tri-Dynamic Preprocessing Framework for UGC Video Compression</title>
<link>https://arxiv.org/abs/2512.16101</link>
<guid>https://arxiv.org/abs/2512.16101</guid>
<content:encoded><![CDATA[
arXiv:2512.16101v1 Announce Type: cross 
Abstract: In recent years, user generated content (UGC) has become the dominant force in internet traffic. However, UGC videos exhibit a higher degree of variability and diverse characteristics compared to traditional encoding test videos. This variance challenges the effectiveness of data-driven machine learning algorithms for optimizing encoding in the broader context of UGC scenarios. To address this issue, we propose a Tri-Dynamic Preprocessing framework for UGC. Firstly, we employ an adaptive factor to regulate preprocessing intensity. Secondly, an adaptive quantization level is employed to fine-tune the codec simulator. Thirdly, we utilize an adaptive lambda tradeoff to adjust the rate-distortion loss function. Experimental results on large-scale test sets demonstrate that our method attains exceptional performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</title>
<link>https://arxiv.org/abs/2512.16123</link>
<guid>https://arxiv.org/abs/2512.16123</guid>
<content:encoded><![CDATA[
arXiv:2512.16123v1 Announce Type: cross 
Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title>
<link>https://arxiv.org/abs/2512.16126</link>
<guid>https://arxiv.org/abs/2512.16126</guid>
<content:encoded><![CDATA[
arXiv:2512.16126v1 Announce Type: cross 
Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents</title>
<link>https://arxiv.org/abs/2512.16614</link>
<guid>https://arxiv.org/abs/2512.16614</guid>
<content:encoded><![CDATA[
arXiv:2512.16614v1 Announce Type: cross 
Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.16724</link>
<guid>https://arxiv.org/abs/2512.16724</guid>
<content:encoded><![CDATA[
arXiv:2512.16724v1 Announce Type: cross 
Abstract: When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: cross 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sceniris: A Fast Procedural Scene Generation Framework</title>
<link>https://arxiv.org/abs/2512.16896</link>
<guid>https://arxiv.org/abs/2512.16896</guid>
<content:encoded><![CDATA[
arXiv:2512.16896v1 Announce Type: cross 
Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title>
<link>https://arxiv.org/abs/2512.16899</link>
<guid>https://arxiv.org/abs/2512.16899</guid>
<content:encoded><![CDATA[
arXiv:2512.16899v1 Announce Type: cross 
Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Resolution Action Recognition for Tiny Actions Challenge</title>
<link>https://arxiv.org/abs/2209.14711</link>
<guid>https://arxiv.org/abs/2209.14711</guid>
<content:encoded><![CDATA[
arXiv:2209.14711v2 Announce Type: replace 
Abstract: Tiny Actions Challenge focuses on understanding human activities in real-world surveillance. Basically, there are two main difficulties for activity recognition in this scenario. First, human activities are often recorded at a distance, and appear in a small resolution without much discriminative clue. Second, these activities are naturally distributed in a long-tailed way. It is hard to alleviate data bias for such heavy category imbalance. To tackle these problems, we propose a comprehensive recognition solution in this paper. First, we train video backbones with data balance, in order to alleviate overfitting in the challenge benchmark. Second, we design a dual-resolution distillation framework, which can effectively guide low-resolution action recognition by super-resolution knowledge. Finally, we apply model en-semble with post-processing, which can further boost per-formance on the long-tailed categories. Our solution ranks Top-1 on the leaderboard.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion</title>
<link>https://arxiv.org/abs/2401.16764</link>
<guid>https://arxiv.org/abs/2401.16764</guid>
<content:encoded><![CDATA[
arXiv:2401.16764v4 Announce Type: replace 
Abstract: Witnessing the evolution of text-to-image diffusion models, significant strides have been made in text-to-3D generation. Currently, two primary paradigms dominate the field of text-to-3D: the feed-forward generation solutions, capable of swiftly producing 3D assets but often yielding coarse results, and the Score Distillation Sampling (SDS) based solutions, known for generating high-fidelity 3D assets albeit at a slower pace. The synergistic integration of these methods holds substantial promise for advancing 3D generation techniques. In this paper, we present BoostDream, a highly efficient plug-and-play 3D refining method designed to transform coarse 3D assets into high-quality. The BoostDream framework comprises three distinct processes: (1) We introduce 3D model distillation that fits differentiable representations from the 3D assets obtained through feed-forward generation. (2) A novel multi-view SDS loss is designed, which utilizes a multi-view aware 2D diffusion model to refine the 3D assets. (3) We propose to use prompt and multi-view consistent normal maps as guidance in refinement.Our extensive experiment is conducted on different differentiable 3D representations, revealing that BoostDream excels in generating high-quality 3D assets rapidly, overcoming the Janus problem compared to conventional SDS-based methods. This breakthrough signifies a substantial advancement in both the efficiency and quality of 3D generation processes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition</title>
<link>https://arxiv.org/abs/2402.18951</link>
<guid>https://arxiv.org/abs/2402.18951</guid>
<content:encoded><![CDATA[
arXiv:2402.18951v2 Announce Type: replace 
Abstract: Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference</title>
<link>https://arxiv.org/abs/2405.14700</link>
<guid>https://arxiv.org/abs/2405.14700</guid>
<content:encoded><![CDATA[
arXiv:2405.14700v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for adapting pre-trained Vision Transformer (ViT) models to downstream applications by updating only a small subset of parameters. While current PEFT methods have achieved fine-tuning efficiency, they overlook the efficiency of computation and GPU memory during inference, falling short of practical requirements. To address this limitation, we propose Sparse-Tuning, an efficient and effective framework that leverages popular token sparsification (TS) techniques to reduce information redundancy in images and videos, thereby significantly improving computational and memory efficiency. However, TS often compromises performance due to inevitable information loss. To address this limitation, we further introduce Dense Adapters (DA) to compensate for the information losses incurred by token sparsification. DA integrates comprehensive token information from shallow layers into the retained tokens of deeper layers, ensuring minimal performance degradation. Through the integration of TS techniques and DA, Sparse-Tuning achieves a significant reduction in computation and memory overhead while maintaining performance. Empirical results on VTAB-1K, three image datasets, and two video datasets show that Sparse-Tuning reduces GFLOPs to 66\% of the original ViT-B while achieving state-of-the-art performance compared to full fine-tuning and other PEFT baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRel: Benchmarking Relation Understanding in Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2406.09121</link>
<guid>https://arxiv.org/abs/2406.09121</guid>
<content:encoded><![CDATA[
arXiv:2406.09121v3 Announce Type: replace 
Abstract: Though Multi-modal Large Language Models (MLLMs) have recently achieved significant progress, they often struggle to understand diverse and complicated inter-object relations. Specifically, the lack of large-scale and high-quality relation data has greatly hindered the progress of MLLMs in various vision-language perception tasks. We attempt to address this challenge by contributing the Multi-Modal Relation Understanding benchmark (MMRel), which features large-scale, high-quality, and diverse data on inter-object relations. MMRel has three distinctive attributes: (i) it contains 22,500 question-answer pairs spanning three distinct domains and around 400 relations, ensuring both scale and diversity; (ii) it provides manually verified, high-quality labels to ensure exceptional annotation accuracy; and (iii) it includes adversarial cases with highly unusual relations, offering a challenging setting for evaluating relation hallucination. These features make MMRel ideal for evaluating MLLMs on relation understanding, as well as for fine-tuning MLLMs to enhance relation comprehension capability. Extensive experiments on 28 MLLMs demonstrate the effectiveness of MMRel in both evaluating and enhancing MLLMs' relation understanding, and the accompanying analyses provide insights for future research. The benchmark has been made publicly available at: https://niejiahao1998.github.io/MMRel
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</title>
<link>https://arxiv.org/abs/2407.20836</link>
<guid>https://arxiv.org/abs/2407.20836</guid>
<content:encoded><![CDATA[
arXiv:2407.20836v5 Announce Type: replace 
Abstract: Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g., transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we demonstrate that adversarial attacks pose a real threat to AIGI detectors. FPBA can deliver successful black-box attacks across various detectors, generators, defense methods, and even evade cross-generator and compressed image detection, which are crucial real-world detection scenarios. Our code is available at https://github.com/onotoa/fpba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mat\'ern Kernels for Tunable Implicit Surface Reconstruction</title>
<link>https://arxiv.org/abs/2409.15466</link>
<guid>https://arxiv.org/abs/2409.15466</guid>
<content:encoded><![CDATA[
arXiv:2409.15466v3 Announce Type: replace 
Abstract: We propose to use the family of Mat\'ern kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Mat\'ern kernels have some appealing properties which make them particularly well suited for surface reconstruction -- outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scalable. Being stationary, we demonstrate that Mat\'ern kernels allow for tunable surface reconstruction in the same way as Fourier feature mappings help coordinate-based MLPs overcome spectral bias. Moreover, we theoretically analyze Mat\'ern kernels' connection to SIREN networks as well as their relation to previously employed arc-cosine kernels. Finally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\'ern kernels and conclude that especially the Laplace kernel (being part of the Mat\'ern family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Engineering Diagrams to Graphs: Digitizing P&amp;IDs with Transformers</title>
<link>https://arxiv.org/abs/2411.13929</link>
<guid>https://arxiv.org/abs/2411.13929</guid>
<content:encoded><![CDATA[
arXiv:2411.13929v2 Announce Type: replace 
Abstract: Digitizing engineering diagrams like Piping and Instrumentation Diagrams (P&amp;IDs) plays a vital role in maintainability and operational efficiency of process and hydraulic systems. Previous methods typically decompose the task into separate steps such as symbol detection and line detection, which can limit their ability to capture the structure in these diagrams. In this work, a transformer-based approach leveraging the Relationformer that addresses this limitation by jointly extracting symbols and their interconnections from P&amp;IDs is introduced. To evaluate our approach and compare it to a modular digitization approach, we present the first publicly accessible benchmark dataset for P&amp;ID digitization, annotated with graph-level ground truth. Experimental results on real-world diagrams show that our method significantly outperforms the modular baseline, achieving over 25% improvement in edge detection accuracy. This research contributes a reproducible evaluation framework and demonstrates the effectiveness of transformer models for structural understanding of complex engineering diagrams. The dataset is available under https://zenodo.org/records/14803338.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler</title>
<link>https://arxiv.org/abs/2502.20110</link>
<guid>https://arxiv.org/abs/2502.20110</guid>
<content:encoded><![CDATA[
arXiv:2502.20110v2 Announce Type: replace 
Abstract: Accurate monocular metric depth estimation (MMDE) is crucial to solving downstream tasks in 3D perception and modeling. However, the remarkable accuracy of recent MMDE methods is confined to their training domains. These methods fail to generalize to unseen domains even in the presence of moderate domain gaps, which hinders their practical applicability. We propose a new model, UniDepthV2, capable of reconstructing metric 3D scenes from solely single images across domains. Departing from the existing MMDE paradigm, UniDepthV2 directly predicts metric 3D points from the input image at inference time without any additional information, striving for a universal and flexible MMDE solution. In particular, UniDepthV2 implements a self-promptable camera module predicting a dense camera representation to condition depth features. Our model exploits a pseudo-spherical output representation, which disentangles the camera and depth representations. In addition, we propose a geometric invariance loss that promotes the invariance of camera-prompted depth features. UniDepthV2 improves its predecessor UniDepth model via a new edge-guided loss which enhances the localization and sharpness of edges in the metric depth outputs, a revisited, simplified and more efficient architectural design, and an additional uncertainty-level output which enables downstream tasks requiring confidence. Thorough evaluations on ten depth datasets in a zero-shot regime consistently demonstrate the superior performance and generalization of UniDepthV2. Code and models are available at https://github.com/lpiccinelli-eth/UniDepth
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radar-Guided Polynomial Fitting for Metric Depth Estimation</title>
<link>https://arxiv.org/abs/2503.17182</link>
<guid>https://arxiv.org/abs/2503.17182</guid>
<content:encoded><![CDATA[
arXiv:2503.17182v3 Announce Type: replace 
Abstract: We propose POLAR, a novel radar-guided depth estimation method that introduces polynomial fitting to efficiently transform scaleless depth predictions from pretrained monocular depth estimation (MDE) models into metric depth maps. Unlike existing approaches that rely on complex architectures or expensive sensors, our method is grounded in a fundamental insight: although MDE models often infer reasonable local depth structure within each object or local region, they may misalign these regions relative to one another, making a linear scale and shift (affine) transformation insufficient given three or more of these regions. To address this limitation, we use polynomial coefficients predicted from cheap, ubiquitous radar data to adaptively adjust predictions non-uniformly across depth ranges. In this way, POLAR generalizes beyond affine transformations and is able to correct such misalignments by introducing inflection points. Importantly, our polynomial fitting framework preserves structural consistency through a novel training objective that enforces local monotonicity via first-derivative regularization. POLAR achieves state-of-the-art performance across three datasets, outperforming existing methods by an average of 24.9% in MAE and 33.2% in RMSE, while also achieving state-of-the-art efficiency in terms of latency and computational cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Core-Set Selection for Data-efficient Land Cover Segmentation</title>
<link>https://arxiv.org/abs/2505.01225</link>
<guid>https://arxiv.org/abs/2505.01225</guid>
<content:encoded><![CDATA[
arXiv:2505.01225v3 Announce Type: replace 
Abstract: The increasing accessibility of remotely sensed data and their potential to support large-scale decision-making have driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models rely on large datasets. However, the common assumption that larger training datasets lead to better performance tends to overlook issues related to data redundancy, noise, and the computational cost of processing massive datasets. Effective solutions must therefore consider not only the quantity but also the quality of data. Towards this, in this paper, we introduce six basic core-set selection approaches -- that rely on imagery only, labels only, or a combination of both -- and investigate whether they can identify high-quality subsets of data capable of maintaining -- or even surpassing -- the performance achieved when using full datasets for remote sensing semantic segmentation. We benchmark such approaches against two traditional baselines on three widely used land-cover classification datasets (DFC2022, Vaihingen, and Potsdam) using two different architectures (SegFormer and U-Net), thus establishing a general baseline for future works. Our experiments show that all proposed methods consistently outperform the baselines across multiple subset sizes, with some approaches even selecting core sets that surpass training on all available data. Notably, on DFC2022, a selected subset comprising only 25% of the training data yields slightly higher SegFormer performance than training with the entire dataset. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoAPT: Mixture of Adversarial Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.17509</link>
<guid>https://arxiv.org/abs/2505.17509</guid>
<content:encoded><![CDATA[
arXiv:2505.17509v2 Announce Type: replace 
Abstract: Large pre-trained Vision Language Models (VLMs) demonstrate excellent generalization capabilities but remain highly susceptible to adversarial examples, posing potential security risks. To improve the robustness of VLMs against adversarial examples, adversarial prompt tuning methods are proposed to align the text feature with the adversarial image feature without changing model parameters. However, when facing various adversarial attacks, a single learnable text prompt has insufficient generalization to align well with all adversarial image features, which ultimately results in overfitting. To address the above challenge, in this paper, we empirically find that increasing the number of learned prompts yields greater robustness improvements than simply extending the length of a single prompt. Building on this observation, we propose an adversarial tuning method named \textbf{Mixture of Adversarial Prompt Tuning (MoAPT)} to enhance the generalization against various adversarial attacks for VLMs. MoAPT aims to learn mixture text prompts to obtain more robust text features. To further enhance the adaptability, we propose a conditional weight router based on the adversarial images to predict the mixture weights of multiple learned prompts, which helps obtain sample-specific mixture text features aligning with different adversarial image features. Extensive experiments across 11 datasets under different settings show that our method can achieve better adversarial robustness than state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAEER: Visual Attention-Inspired Emotion Elicitation Reasoning</title>
<link>https://arxiv.org/abs/2505.24342</link>
<guid>https://arxiv.org/abs/2505.24342</guid>
<content:encoded><![CDATA[
arXiv:2505.24342v2 Announce Type: replace 
Abstract: Images shared online strongly influence emotions and public well-being. Understanding the emotions an image elicits is therefore vital for fostering healthier and more sustainable digital communities, especially during public crises. We study Visual Emotion Elicitation (VEE), predicting the set of emotions that an image evokes in viewers. We introduce VAEER, an interpretable multi-label VEE framework that combines attention-inspired cue extraction with knowledge-grounded reasoning. VAEER isolates salient visual foci and contextual signals, aligns them with structured affective knowledge, and performs per-emotion inference to yield transparent, emotion-specific rationales. Across three heterogeneous benchmarks, including social imagery and disaster-related photos, VAEER achieves state-of-the-art results with up to 19% per-emotion improvements and a 12.3% average gain over strong CNN and VLM baselines. Our findings highlight interpretable multi-label emotion elicitation as a scalable foundation for responsible visual media analysis and emotionally sustainable online ecosystems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</title>
<link>https://arxiv.org/abs/2505.24862</link>
<guid>https://arxiv.org/abs/2505.24862</guid>
<content:encoded><![CDATA[
arXiv:2505.24862v4 Announce Type: replace 
Abstract: Story visualization aims to generate coherent image sequences that faithfully depict a narrative and align with character references. Despite progress in generative models, existing benchmarks are narrow in scope, often limited to short prompts, lacking character references, or single-image cases, and fail to capture real-world storytelling complexity. This hinders a nuanced understanding of model capabilities and limitations. We present \textbf{ViStoryBench}, a comprehensive benchmark designed to evaluate story visualization models across diverse narrative structures, visual styles, and character settings. The benchmark features richly annotated multi-shot scripts derived from curated stories spanning literature, film, and folklore. Large language models assist in story summarization and script generation, with all outputs human-verified to ensure coherence and fidelity. Character references are carefully curated to maintain intra-story consistency across varying artistic styles. To enable thorough evaluation, ViStoryBench introduces a set of automated metrics that assess character consistency, style similarity, prompt alignment, aesthetic quality, and generation artifacts such as copy-paste behavior. These metrics are validated through human studies, and used to benchmark a broad range of open-source and commercial models. ViStoryBench offers a multi-dimensional evaluation suite that facilitates systematic analysis and fosters future progress in visual storytelling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</title>
<link>https://arxiv.org/abs/2506.09695</link>
<guid>https://arxiv.org/abs/2506.09695</guid>
<content:encoded><![CDATA[
arXiv:2506.09695v3 Announce Type: replace 
Abstract: Early diagnosis of Alzheimer's Disease (AD), particularly at the mild cognitive impairment stage, is essential for timely intervention. However, this process faces significant barriers, including reliance on subjective assessments and the high cost of advanced imaging techniques. While deep learning offers automated solutions to improve diagnostic accuracy, its widespread adoption remains constrained due to high energy requirements and computational demands, particularly in resource-limited settings. Spiking neural networks (SNNs) provide a promising alternative, as their brain-inspired design is well-suited to model the sparse and event-driven patterns characteristic of neural degeneration in AD. These networks offer the potential for developing interpretable, energy-efficient diagnostic tools. Despite their advantages, existing SNNs often suffer from limited expressiveness and challenges in stable training, which reduce their effectiveness in handling complex medical tasks. To address these shortcomings, we introduce FasterSNN, a hybrid neural architecture that combines biologically inspired Leaky Integrate-and-Fire (LIF) neurons with region-adaptive convolution and multi-scale spiking attention mechanisms. This approach facilitates efficient, sparse processing of 3D MRI data while maintaining high diagnostic accuracy. Experimental results on benchmark datasets reveal that FasterSNN delivers competitive performance with significantly enhanced efficiency and training stability, highlighting its potential for practical application in AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</title>
<link>https://arxiv.org/abs/2506.12775</link>
<guid>https://arxiv.org/abs/2506.12775</guid>
<content:encoded><![CDATA[
arXiv:2506.12775v2 Announce Type: replace 
Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</title>
<link>https://arxiv.org/abs/2507.03558</link>
<guid>https://arxiv.org/abs/2507.03558</guid>
<content:encoded><![CDATA[
arXiv:2507.03558v3 Announce Type: replace 
Abstract: Brain stroke is a leading cause of mortality and long-term disability worldwide, underscoring the need for precise and rapid prediction techniques. Computed Tomography (CT) scan is considered one of the most effective methods for diagnosing brain strokes. Most stroke classification techniques use a single slice-level prediction mechanism, requiring radiologists to manually select the most critical CT slice from the original CT volume. Although clinical evaluations are often used in traditional diagnostic procedures, machine learning (ML) has opened up new avenues for improving stroke diagnosis. To supplement traditional diagnostic techniques, this study investigates machine learning models for early brain stroke prediction using CT scan images. This research proposes a novel machine learning approach to brain stroke detection, focusing on optimizing classification performance with pre-trained deep learning models and advanced optimization strategies. Pre-trained models, including DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are used for feature extraction. Feature engineering techniques, including BFO, PCA, and LDA, further enhance model performance. These features are then classified using machine learning algorithms, including SVC, RF, XGB, DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2, LDA, and SVC achieved the highest classification accuracy of 97.93%, significantly outperforming other model-optimizer-classifier combinations. The results underline the effectiveness of integrating lightweight pre-trained models with robust optimization and classification techniques for brain stroke diagnosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</title>
<link>https://arxiv.org/abs/2507.05859</link>
<guid>https://arxiv.org/abs/2507.05859</guid>
<content:encoded><![CDATA[
arXiv:2507.05859v3 Announce Type: replace 
Abstract: Free-Viewpoint Video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representation remains a major challenge. Existing dynamic 3D Gaussian Splatting methods couple reconstruction with optimization-dependent compression and customized motion formats, limiting generalization and standardization. To address this, we propose D-FCGS, a novel Feedforward Compression framework for Dynamic Gaussian Splatting. Key innovations include: (1) a standardized Group-of-Frames (GoF) structure with I-P coding, leveraging sparse control points to extract inter-frame motion tensors; (2) a dual prior-aware entropy model that fuses hyperprior and spatial-temporal priors for accurate rate estimation; (3) a control-point-guided motion compensation mechanism and refinement network to enhance view-consistent fidelity. Trained on Gaussian frames derived from multi-view videos, D-FCGS generalizes across diverse scenes in a zero-shot fashion. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression compared to the baseline while preserving visual quality across viewpoints. This work advances feedforward compression of dynamic 3DGS, facilitating scalable FVV transmission and storage for immersive applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</title>
<link>https://arxiv.org/abs/2507.10171</link>
<guid>https://arxiv.org/abs/2507.10171</guid>
<content:encoded><![CDATA[
arXiv:2507.10171v3 Announce Type: replace 
Abstract: Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Building Heritage Assessment Using Street-Level Imagery</title>
<link>https://arxiv.org/abs/2508.11486</link>
<guid>https://arxiv.org/abs/2508.11486</guid>
<content:encoded><![CDATA[
arXiv:2508.11486v3 Announce Type: replace 
Abstract: Registration of heritage values in buildings is important to safeguard heritage values that can be lost in renovation and energy efficiency projects. However, registering heritage values is a cumbersome process. Novel artificial intelligence tools may improve efficiency in identifying heritage values in buildings compared to costly and time-consuming traditional inventories. In this study, OpenAI's large language model GPT was used to detect various aspects of cultural heritage value in facade images. Using GPT derived data and building register data, machine learning models were trained to classify multi-family and non-residential buildings in Stockholm, Sweden. Validation against a heritage expert-created inventory shows a macro F1-score of 0.71 using a combination of register data and features retrieved from GPT, and a score of 0.60 using only GPT-derived data. The methods presented can contribute to higher-quality datasets and support decision making.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</title>
<link>https://arxiv.org/abs/2508.15216</link>
<guid>https://arxiv.org/abs/2508.15216</guid>
<content:encoded><![CDATA[
arXiv:2508.15216v2 Announce Type: replace 
Abstract: Accident prediction and timely warnings play a key role in improving road safety by reducing the risk of injury to road users and minimizing property damage. Advanced Driver Assistance Systems (ADAS) are designed to support human drivers and are especially useful when they can anticipate potential accidents before they happen. While many existing systems depend on a range of sensors such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents a more challenging but a more cost-effective and easily deployable solution. In this work, we incorporate better spatio-temporal features and aggregate them through a recurrent network to improve upon state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets show that our proposed STAGNet model achieves higher average precision and mean time-to-collision values than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</title>
<link>https://arxiv.org/abs/2509.09676</link>
<guid>https://arxiv.org/abs/2509.09676</guid>
<content:encoded><![CDATA[
arXiv:2509.09676v2 Announce Type: replace 
Abstract: Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw videos, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly fosters improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Segmentation of Polyps and Visual Explainability Analysis</title>
<link>https://arxiv.org/abs/2509.18159</link>
<guid>https://arxiv.org/abs/2509.18159</guid>
<content:encoded><![CDATA[
arXiv:2509.18159v5 Announce Type: replace 
Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.22737</link>
<guid>https://arxiv.org/abs/2509.22737</guid>
<content:encoded><![CDATA[
arXiv:2509.22737v2 Announce Type: replace 
Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Frames to Clips: Training-free Adaptive Key Clip Selection for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2510.02262</link>
<guid>https://arxiv.org/abs/2510.02262</guid>
<content:encoded><![CDATA[
arXiv:2510.02262v2 Announce Type: replace 
Abstract: Video Large Language Models (VLMs) have achieved strong performance on various vision-language tasks, yet their practical use is limited by the massive number of visual tokens produced from raw video frames, which quickly exhausts the model's context window. Existing solutions mitigate this issue by selecting a sparse set of frames, but such frame-wise selection discards essential temporal dynamics in long-form videos, leading to suboptimal reasoning about motion and event continuity. In this work, we systematically examine the role of temporal information and show that extending selection from isolated key frames to temporally coherent key clips improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we introduce frame resolution as a controllable factor in frame selection, enabling a trade-off between spatial resolution and clip length. Building on this idea, we propose an adaptive clip length module that dynamically balances these factors to ensure a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling by up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench, and MLVU, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling VLMs to real-world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.12796</link>
<guid>https://arxiv.org/abs/2510.12796</guid>
<content:encoded><![CDATA[
arXiv:2510.12796v2 Announce Type: replace 
Abstract: Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep generative priors for 3D brain analysis</title>
<link>https://arxiv.org/abs/2510.15119</link>
<guid>https://arxiv.org/abs/2510.15119</guid>
<content:encoded><![CDATA[
arXiv:2510.15119v2 Announce Type: replace 
Abstract: Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</title>
<link>https://arxiv.org/abs/2510.20322</link>
<guid>https://arxiv.org/abs/2510.20322</guid>
<content:encoded><![CDATA[
arXiv:2510.20322v3 Announce Type: replace 
Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with M\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\% additional parameters. Code is available at https://github.com/godlin-sjtu/HyperET
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</title>
<link>https://arxiv.org/abs/2510.26609</link>
<guid>https://arxiv.org/abs/2510.26609</guid>
<content:encoded><![CDATA[
arXiv:2510.26609v2 Announce Type: replace 
Abstract: Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces FARM: Fine-tuning Agricultural Regression Models, a deep learning framework designed for high-resolution, intra-field canola yield prediction. FARM leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level (30 m) yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, FARM achieves a Root Mean Squared Error (RMSE) of 0.44 and an R^2 of 0.81. Using an independent high-resolution yield monitor dataset, we further show that fine-tuning FARM on limited ground-truth labels outperforms training the same architecture from scratch, confirming the benefit of pre-training on large, upsampled county-level data for data-scarce precision agriculture. These results represent improvement over baseline architectures like 3D-CNN and DeepYield, which highlight the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, FARM offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Thinker: Interactive Thinking with Images</title>
<link>https://arxiv.org/abs/2511.04460</link>
<guid>https://arxiv.org/abs/2511.04460</guid>
<content:encoded><![CDATA[
arXiv:2511.04460v2 Announce Type: replace 
Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising "Thinking with Images" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search</title>
<link>https://arxiv.org/abs/2511.06833</link>
<guid>https://arxiv.org/abs/2511.06833</guid>
<content:encoded><![CDATA[
arXiv:2511.06833v2 Announce Type: replace 
Abstract: Recent advancements in video diffusion models have significantly enhanced audio-driven portrait animation. However, current methods still suffer from flickering, identity drift, and poor audio-visual synchronization. These issues primarily stem from entangled appearance-motion representations and unstable inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel intensity-controllable and temporally consistent talking head generation framework with diffusion noise search inference. First, we propose \textbf{an optical flow-guided temporal module (OFT)} that decouples motion features from static appearance by leveraging facial optical flow, thereby reducing visual flicker and improving temporal consistency. Second, we present an \textbf{Audio-to-Intensity (A2I) model} obtained through multimodal teacher-student knowledge distillation. By transforming audio and facial velocity features into a frame-wise intensity sequence, the A2I model enables joint modeling of audio and visual motion, resulting in more natural dynamics. This further enables fine-grained, frame-wise control of motion dynamics while maintaining tight audio-visual synchronization. Third, we introduce a \textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing explicit constraints on background coherence and motion continuity during inference-time noise search, we achieve better identity preservation and refine motion dynamics compared to the current autoregressive strategy. Extensive experiments demonstrate that ConsistTalk significantly outperforms prior methods in reducing flicker, preserving identity, and delivering temporally stable, high-fidelity talking head videos.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization</title>
<link>https://arxiv.org/abs/2511.09117</link>
<guid>https://arxiv.org/abs/2511.09117</guid>
<content:encoded><![CDATA[
arXiv:2511.09117v2 Announce Type: replace 
Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using several recent versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, two state-of-the-art (SOTA) Generative Adversarial Network (GAN) methods, as well as our Conditional GAN (cGAN) baseline. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.12142</link>
<guid>https://arxiv.org/abs/2511.12142</guid>
<content:encoded><![CDATA[
arXiv:2511.12142v2 Announce Type: replace 
Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</title>
<link>https://arxiv.org/abs/2511.14469</link>
<guid>https://arxiv.org/abs/2511.14469</guid>
<content:encoded><![CDATA[
arXiv:2511.14469v2 Announce Type: replace 
Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</title>
<link>https://arxiv.org/abs/2511.15705</link>
<guid>https://arxiv.org/abs/2511.15705</guid>
<content:encoded><![CDATA[
arXiv:2511.15705v2 Announce Type: replace 
Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeAR: Coupled Neural Asset-Renderer Stack</title>
<link>https://arxiv.org/abs/2511.18600</link>
<guid>https://arxiv.org/abs/2511.18600</guid>
<content:encoded><![CDATA[
arXiv:2511.18600v2 Announce Type: replace 
Abstract: Neural asset authoring and neural rendering have traditionally evolved as disjoint paradigms: one generates digital assets for fixed graphics pipelines, while the other maps conventional assets to images. However, treating them as independent entities limits the potential for end-to-end optimization in fidelity and consistency. In this paper, we bridge this gap with NeAR, a Coupled Neural Asset--Renderer Stack. We argue that co-designing the asset representation and the renderer creates a robust "contract" for superior generation. On the asset side, we introduce the Lighting-Homogenized SLAT (LH-SLAT). Leveraging a rectified-flow model, NeAR lifts casually lit single images into a canonical, illumination-invariant latent space, effectively suppressing baked-in shadows and highlights. On the renderer side, we design a lighting-aware neural decoder tailored to interpret these homogenized latents. Conditioned on HDR environment maps and camera views, it synthesizes relightable 3D Gaussian splats in real-time without per-object optimization. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit reconstruction, (3) unknown-lit relighting, and (4) novel-view relighting. Extensive experiments demonstrate that our coupled stack outperforms state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</title>
<link>https://arxiv.org/abs/2511.23334</link>
<guid>https://arxiv.org/abs/2511.23334</guid>
<content:encoded><![CDATA[
arXiv:2511.23334v2 Announce Type: replace 
Abstract: Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.03508</link>
<guid>https://arxiv.org/abs/2512.03508</guid>
<content:encoded><![CDATA[
arXiv:2512.03508v2 Announce Type: replace 
Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
<link>https://arxiv.org/abs/2512.04677</link>
<guid>https://arxiv.org/abs/2512.04677</guid>
<content:encoded><![CDATA[
arXiv:2512.04677v3 Announce Type: replace 
Abstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^\mathrm{3}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v3 Announce Type: replace 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^\mathrm{3}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^\mathrm{3}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^\mathrm{3}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v2 Announce Type: replace 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace-cross 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Backdoor Attacks on Neural Networks</title>
<link>https://arxiv.org/abs/2411.14516</link>
<guid>https://arxiv.org/abs/2411.14516</guid>
<content:encoded><![CDATA[
arXiv:2411.14516v2 Announce Type: replace-cross 
Abstract: Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v4 Announce Type: replace-cross 
Abstract: Adversarial examples exhibit cross-model transferability, enabling threatening black-box attacks on commercial models. Model ensembling, which attacks multiple surrogate models, is a known strategy to improve this transferability. However, prior studies typically use small, fixed ensembles, which leaves open an intriguing question of whether scaling the number of surrogate models can further improve black-box attacks. In this work, we conduct the first large-scale empirical study of this question. We show that by resolving gradient conflict with advanced optimizers, we discover a robust and universal log-linear scaling law through both theoretical analysis and empirical evaluations: the Attack Success Rate (ASR) scales linearly with the logarithm of the ensemble size $T$. We rigorously verify this law across standard classifiers, SOTA defenses, and MLLMs, and find that scaling distills robust, semantic features of the target class. Consequently, we apply this fundamental insight to benchmark SOTA MLLMs. This reveals both the attack's devastating power and a clear robustness hierarchy: we achieve 80\%+ transfer attack success rate on proprietary models like GPT-4o, while also highlighting the exceptional resilience of Claude-3.5-Sonnet. Our findings urge a shift in focus for robustness evaluation: from designing intricate algorithms on small ensembles to understanding the principled and powerful threat of scaling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StructDiff: Structure-aware Diffusion Model for 3D Fine-grained Medical Image Synthesis</title>
<link>https://arxiv.org/abs/2503.09560</link>
<guid>https://arxiv.org/abs/2503.09560</guid>
<content:encoded><![CDATA[
arXiv:2503.09560v2 Announce Type: replace-cross 
Abstract: Solving medical imaging data scarcity through semantic image generation has attracted growing attention in recent years. However, existing generative models mainly focus on synthesizing whole-organ or large-tissue structures, showing limited capability in reproducing fine-grained anatomical details. Due to the stringent requirement of topological consistency and the complex 3D morphological heterogeneity of medical data, accurately reconstructing fine-grained anatomical details remains a significant challenge. To address these limitations, we propose StructDiff, a Structure-aware Diffusion Model for fine-grained 3D medical image synthesis, which enables precise generation of topologically complex anatomies. In addition to the conventional mask-based guidance, StructDiff further introduces a paired image-mask template to guide the generation process, providing structural constrains and offering explicit knowledge of mask-to-image correspondence. Moreover, a Mask Generation Module (MGM) is designed to enrich mask diversity and alleviate the scarcity of high-quality reference masks. Furthermore, we propose a Confidence-aware Adaptive Learning (CAL) strategy based on Skip-Sampling Variance (SSV), which mitigates uncertainty introduced by imperfect synthetic data when transferring to downstream tasks. Extensive experiments demonstrate that StructDiff achieves state-of-the-art performance in terms of topological consistency and visual realism, and significantly boosts downstream segmentation performance. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
arXiv:2507.21503v3 Announce Type: replace-cross 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation</title>
<link>https://arxiv.org/abs/2508.03758</link>
<guid>https://arxiv.org/abs/2508.03758</guid>
<content:encoded><![CDATA[
arXiv:2508.03758v4 Announce Type: replace-cross 
Abstract: Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring. However, this task remains challenging due to the heterogeneous appearance, irregular morphology, and complex backgrounds associated with ulcer regions in clinical photographs. Traditional convolutional neural networks (CNNs), such as U-Net, provide strong localization capabilities but struggle to model long-range spatial dependencies due to their inherently limited receptive fields. To address this, we employ the TransUNet architecture, a hybrid framework that integrates the global attention mechanism of Vision Transformers (ViTs) into the U-Net structure. This combination allows the model to extract global contextual features while maintaining fine-grained spatial resolution. We trained the model on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset using a robust augmentation pipeline and a hybrid loss function to mitigate class imbalance. On the internal validation set, the model achieved a Dice Similarity Coefficient (F1-score) of 0.8886 using an optimized threshold of 0.4843. Crucially, to assess generalizability, we performed external validation on two independent datasets: the AZH Wound Care Center dataset (n=278) and the Medetec dataset (n=152). Without any retraining, the model achieved Dice scores of 0.6209 and 0.7850, respectively, demonstrating robust zero-shot transferability to unseen clinical domains. Furthermore, clinical utility analysis revealed a strong correlation (Pearson r = 0.9749) between predicted and ground-truth wound areas. These outcomes demonstrate that our approach effectively integrates global and local feature extraction, offering a reliable, effective, and explainable solution for automated foot ulcer assessment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Team Westwood Solution for MIDOG 2025 Challenge: An Ensemble-CNN-Based Approach For Mitosis Detection And Classification</title>
<link>https://arxiv.org/abs/2509.02600</link>
<guid>https://arxiv.org/abs/2509.02600</guid>
<content:encoded><![CDATA[
arXiv:2509.02600v3 Announce Type: replace-cross 
Abstract: This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification. On the final test set, our solution achieved an F1 score of 0.6972 for track 1 mitosis detection, and a balanced accuracy of 0.8242 for track 2 atypical mitosis classification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</title>
<link>https://arxiv.org/abs/2510.05684</link>
<guid>https://arxiv.org/abs/2510.05684</guid>
<content:encoded><![CDATA[
arXiv:2510.05684v2 Announce Type: replace-cross 
Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-localization on a 3D map by fusing global and local features from a monocular camera</title>
<link>https://arxiv.org/abs/2510.26170</link>
<guid>https://arxiv.org/abs/2510.26170</guid>
<content:encoded><![CDATA[
arXiv:2510.26170v2 Announce Type: replace-cross 
Abstract: Self-localization on a 3D map by using an inexpensive monocular camera is required to realize autonomous driving. Self-localization based on a camera often uses a convolutional neural network (CNN) that can extract local features that are calculated by nearby pixels. However, when dynamic obstacles, such as people, are present, CNN does not work well. This study proposes a new method combining CNN with Vision Transformer, which excels at extracting global features that show the relationship of patches on whole image. Experimental results showed that, compared to the state-of-the-art method (SOTA), the accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times higher than that without dynamic obstacles. Moreover, the self-localization error of our method is 20.1% smaller than that of SOTA on public datasets. Additionally, our robot using our method can localize itself with 7.51cm error on average, which is more accurate than SOTA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2511.05020</link>
<guid>https://arxiv.org/abs/2511.05020</guid>
<content:encoded><![CDATA[
arXiv:2511.05020v2 Announce Type: replace-cross 
Abstract: Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can't see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v2 Announce Type: replace-cross 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v2 Announce Type: replace-cross 
Abstract: In this paper, we present the first detailed analysis of how training hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the training hyperparameter space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[
arXiv:2511.22862v2 Announce Type: replace-cross 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at https://github.com/Luchicken/BriMPR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v2 Announce Type: replace-cross 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.02498</link>
<guid>https://arxiv.org/abs/2512.02498</guid>
<content:encoded><![CDATA[
<div> keywords: Document Layout Parsing, Vision-Language Model, dots_ocr, multilingual corpus, XDocParse<br /><br />Summary:<br /><br />Document Layout Parsing acts as an essential interface enabling AI to interpret vast amounts of structured knowledge by integrating layout detection, text recognition, and relational understanding. This is especially vital for advancing next-generation Vision-Language Models. Existing approaches often depend on fragmented, multi-stage pipelines that cause error propagation and fail to exploit joint training benefits. The paper introduces dots_ocr, a unified Vision-Language Model that jointly learns these three key tasks in an end-to-end manner, overcoming prior limitations. This innovation is supported by a scalable data engine capable of synthesizing a vast multilingual corpus, thereby enhancing the model’s robustness across varied tasks, languages, layouts, and domains. The proposed framework achieves state-of-the-art results on the OmniDocBench benchmark, demonstrating its effectiveness. Additionally, the authors present XDocParse, a new challenging benchmark spanning 126 languages to foster research in global document intelligence. On XDocParse, dots_ocr attains approximately a 10% relative improvement over previous methods, showcasing strong multilingual capabilities and establishing a new standard for document layout parsing in diverse linguistic contexts. <div>
arXiv:2512.02498v4 Announce Type: replace 
Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots_ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this benchmark, dots_ocr achieves state-of-the-art performance, delivering an approximately 10% relative improvement and demonstrating strong multilingual capability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
<div> Compositional Zero-Shot Learning, Continual Learning, Vision-Language Models, Prompt-based Learning, Knowledge Distillation<br /><br />Summary:<br /><br />This paper addresses the challenge of continual adaptation of vision-language models (VLMs) to new attributes, objects, and their unique compositions within Compositional Zero-Shot Learning (CZSL), while avoiding forgetting previously learned knowledge. Unlike classical continual learning, the task is complicated by overlapping attributes and objects across learning sessions, with only their compositions remaining unique. The authors propose PromptCCZSL, the first prompt-based framework for continual compositional zero-shot learning built on a frozen VLM backbone. PromptCCZSL uses recency-weighted multi-teacher knowledge distillation to retain prior knowledge. It employs session-aware compositional prompts for fusing multimodal features of new compositions and session-agnostic prompts for attributes and objects to maintain global semantic consistency. A Cosine Anchor Loss (CAL) further stabilizes prior knowledge retention. To promote effective current session adaptation, an Orthogonal Projection Loss (OPL) ensures distinction between new and previous embeddings, preventing overlap, while an Intra-Session Diversity Loss (IDL) encourages diversity among new embeddings for richer, discriminative representations. The authors also introduce a comprehensive evaluation protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA demonstrate that PromptCCZSL significantly outperforms prior VLM-based and non-VLM baselines, setting a new state-of-the-art in closed-world CCZSL scenarios. <div>
arXiv:2512.09172v2 Announce Type: replace 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation</title>
<link>https://arxiv.org/abs/2512.14755</link>
<guid>https://arxiv.org/abs/2512.14755</guid>
<content:encoded><![CDATA[
<div> Change detection, Synthetic Aperture Radar, Very-High-Resolution imagery, Foundation models, Label transfer<br /><br />Summary:<br /><br />1. The article addresses change detection for linear infrastructure monitoring, which requires reliable, high-resolution data with consistent acquisition intervals; however, optical Very-High-Resolution (VHR) imagery is limited by weather conditions like clouds. 2. Synthetic Aperture Radar (SAR) offers all-weather imaging capabilities but poses challenges in annotation due to its complexity. 3. The authors introduce SkyCap, a novel bitemporal VHR dataset combining optical SkySat and SAR Capella Space scenes through archive matching and precise co-registration. 4. They employ an optical-to-SAR label transfer technique to generate SAR amplitude change detection (ACD) labels without needing expert SAR annotations. 5. Continued pretraining of the SARATR-X model on this SAR data is performed to develop SAR-specific foundation models, which are benchmarked against optical foundation models on the SkyCap dataset under various preprocessing schemes. 6. Results show that the optical foundation model MTP(ViT-B+RVSA), when preprocessed with dB+Z-score normalization, achieves the best performance (F1_c = 45.06), surpassing SAR-specific models trained directly on Capella data. 7. The study also reveals a strong dependency of model performance on preprocessing alignment with pretraining data statistics and indicates that optical model rankings in change detection do not directly translate to SAR ACD tasks. 8. This work represents the first evaluation of foundation models on VHR SAR amplitude change detection, contributing valuable insights for future research in this domain. <div>
arXiv:2512.14755v1 Announce Type: new 
Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2512.14757</link>
<guid>https://arxiv.org/abs/2512.14757</guid>
<content:encoded><![CDATA[
<div> Keywords: socially compliant navigation, vision language models, reinforcement fine-tuning, semantic similarity reward, Mixture-of-Experts model<br /><br />Summary:<br /> This paper addresses the challenge of developing robots capable of socially compliant navigation in human-populated environments, emphasizing not only safety but also social norms, human comfort, and contextual appropriateness. The authors identify that while large vision language models (VLMs) have potential for this task, their high computational costs hinder real-time deployment on resource-limited robotic platforms. To overcome this, they propose SocialNav-MoE, an efficient Mixture-of-Experts VLM designed for socially compliant navigation, fine-tuned through reinforcement learning techniques. A novel semantic similarity reward (SSR) is introduced to enhance the reinforcement fine-tuning process, improving decision-making effectiveness over traditional hard-level and character-level rewards. The study further investigates the impact of various small language models (Phi, Qwen, StableLM), routing strategies, and vision encoders (CLIP and SigLIP, both frozen and fine-tuned) on navigation performance. Experimental results on the SNEI dataset demonstrate that SocialNav-MoE achieves a strong balance between navigation accuracy and computational efficiency. The findings indicate the SSR function is particularly effective in reinforcing compliance with social navigation norms. The authors plan to release their source code upon acceptance, facilitating further research and application. <div>
arXiv:2512.14757v1 Announce Type: new 
Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics</title>
<link>https://arxiv.org/abs/2512.14758</link>
<guid>https://arxiv.org/abs/2512.14758</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Music Recognition, Jianpu, Chinese Folk Songs, MusicXML, Expert System<br /><br />Summary:<br /><br />This paper addresses the underexplored area of Optical Music Recognition (OMR) for Chinese Jianpu (numbered notation) and its associated lyrics, contrasting with the predominant focus on Western staff notation in large-scale OMR research. The authors propose a modular expert-system pipeline designed to convert printed Jianpu scores with lyrics into machine-readable formats such as MusicXML and MIDI, eliminating the need for large sets of annotated training data. Their approach blends traditional computer vision techniques—including phrase correlation and skeleton analysis—with unsupervised deep-learning modules for extracting image feature embeddings, achieving a hybrid system that balances interpretability and accuracy. The method was evaluated on The Anthology of Chinese Folk Songs, covering a large-scale melody-only dataset of over 5,000 songs (more than 300,000 notes) and a curated subset including lyrics with over 1,400 songs (exceeding 100,000 notes). Experimental results demonstrate that the system attains high precision in recognizing both melodies, with a note-wise F1 score of 0.951, and aligned lyrics, with a character-wise F1 score of 0.931. This work significantly advances large-scale digitization and accessibility of Chinese Jianpu musical resources. <div>
arXiv:2512.14758v1 Announce Type: new 
Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion</title>
<link>https://arxiv.org/abs/2512.14760</link>
<guid>https://arxiv.org/abs/2512.14760</guid>
<content:encoded><![CDATA[
<div> underwater image enhancement, diffusion model, color correction, chromatic prior, cross-domain consistency loss  

<br /><br />Summary:  
Underwater images often suffer from severe degradation due to wavelength-dependent light absorption and scattering, leading to color distortion, low contrast, and loss of fine details that impair vision-based underwater applications. To tackle these issues, the paper proposes AquaDiff, a diffusion-based framework aimed at enhancing underwater images by correcting chromatic distortions while maintaining both structural and perceptual fidelity. AquaDiff incorporates a chromatic prior-guided color compensation strategy integrated within a conditional diffusion process. This process utilizes cross-attention mechanisms to dynamically fuse the degraded input images with noisy latent states during each denoising step. The model’s enhanced denoising backbone leverages residual dense blocks and multi-resolution attention modules to effectively capture global color contexts as well as local details. Moreover, AquaDiff introduces a novel cross-domain consistency loss that enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity simultaneously. Extensive experiments across multiple challenging underwater benchmarks demonstrate that AquaDiff surpasses various state-of-the-art methods, including traditional, CNN-, GAN-, and other diffusion-based techniques, by delivering superior color correction and competitive overall image quality under diverse underwater conditions. <div>
arXiv:2512.14760v1 Announce Type: new 
Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title>
<link>https://arxiv.org/abs/2512.14770</link>
<guid>https://arxiv.org/abs/2512.14770</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, Visual Question Answering, hallucination, uncertainty estimation, dual-assessment<br /><br />Summary:<br />Vision-language models (VLMs) have shown great promise in the task of Visual Question Answering (VQA), where the goal is to provide accurate answers based on visual and textual input. However, these models suffer from hallucination issues, producing confident but incorrect answers that damage trustworthiness. To counter this, the study introduces Dual-Assessment for VLM Reliability (DAVR), a novel framework designed to enhance answer reliability by providing comprehensive uncertainty estimation. DAVR employs a dual-pathway architecture: the first pathway uses dual selector modules that combine the latent features from the VLM with question-answer embeddings to evaluate the reliability of generated responses. The second pathway incorporates external reference models that conduct factual cross-checks to identify and reduce hallucinations. This two-fold approach effectively addresses both internal uncertainty and external factual accuracy. The framework was tested in the Reliable VQA Challenge at ICCV-CLVL 2025, where it achieved a top-ranking performance, recording a leading Φ₁₀₀ score of 39.64 and a 100-AUC of 97.22. These results illustrate the strong capability of DAVR in improving the trustworthiness and reliability of VLM-generated answers for visual question answering tasks. <div>
arXiv:2512.14770v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $\Phi_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</title>
<link>https://arxiv.org/abs/2512.14870</link>
<guid>https://arxiv.org/abs/2512.14870</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, VideoQA, multi-evidence integration, Minimum Required Frame-Set, compositional video understanding<br /><br />Summary:<br /><br />1. HERBench is a new Video Question Answering (VideoQA) benchmark specifically designed to evaluate the ability of Video Large Language Models (Video-LLMs) to integrate multiple, temporally separated pieces of evidence rather than relying on a single salient cue.<br /><br />2. Each of the 26,000 multiple-choice questions in HERBench requires aggregating at least three distinct non-overlapping evidential cues from different video segments, ensuring that neither language priors nor a single frame can suffice to answer correctly.<br /><br />3. HERBench includes twelve compositional tasks that examine various reasoning skills, including identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting.<br /><br />4. The authors introduce the Minimum Required Frame-Set (MRFS) metric, which quantifies the minimal number of frames a model must combine to answer a question, finding that HERBench demands significantly more integration (mean MRFS 5.5) compared to prior datasets (2.6-4.2).<br /><br />5. Evaluation of 13 state-of-the-art Video-LLMs on HERBench reveals poor performance with accuracies between 31-42%, only marginally higher than random guessing, driven by two key failure modes: retrieval deficits (missing key evidence in frame selection) and fusion deficits (failure to integrate evidence even when fully retrieved).<br /><br />6. HERBench establishes a rigorous, quantifiable challenge to push progress toward robust, compositional understanding of videos that require cross-time evidence aggregation. <div>
arXiv:2512.14870v1 Announce Type: new 
Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isolated Sign Language Recognition with Segmentation and Pose Estimation</title>
<link>https://arxiv.org/abs/2512.14876</link>
<guid>https://arxiv.org/abs/2512.14876</guid>
<content:encoded><![CDATA[
<div> Sign language recognition, ASL, pose estimation, ResNet-Transformer, computational efficiency<br /><br />Summary: This paper addresses the challenge of isolated sign language recognition (ISLR) for American Sign Language (ASL), focusing on overcoming issues such as scarce per-sign data, high variability among signers, and large computational costs. The authors propose a novel model designed to reduce computational demands while maintaining robustness to different signers. Their approach involves a multi-stage pipeline: first, a pose estimation system extracts detailed hand and face joint coordinates from video inputs to capture essential visual cues. Second, a segmentation module is employed to isolate relevant information from the video, filtering out noise and irrelevant background data. Third, the processed data is fed into a hybrid ResNet-Transformer backbone, which effectively models both spatial features and temporal dependencies inherent in sign language gestures. This combination allows the system to learn complex patterns in ASL efficiently and accurately. Overall, the work contributes an innovative approach that balances computational efficiency with high recognition performance, making ISLR more accessible and potentially benefiting the ASL community by bridging communication gaps with automated understanding systems. <div>
arXiv:2512.14876v1 Announce Type: new 
Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris</title>
<link>https://arxiv.org/abs/2512.14878</link>
<guid>https://arxiv.org/abs/2512.14878</guid>
<content:encoded><![CDATA[
<div> Keywords: animal re-identification, dermatoglyphic descriptors, cross-modal retrieval, data augmentation, ecological monitoring<br /><br />Summary:  
This study introduces an innovative approach to animal re-identification (Re-ID) by integrating precise dermatoglyphic textual descriptors, traditionally used in forensics, into ecological monitoring methods. Unlike conventional AI tools that rely primarily on image-based inputs for species with distinctive morphologies, this methodology encodes animal coat topology using human-interpretable language tags. The researchers utilized a substantial dataset consisting of 84,264 manually labeled minutiae from 3,355 images of 185 tigers (Panthera tigris) to evaluate the effectiveness of combining visual and textual data for identity retrieval across modalities. To address limitations posed by data scarcity, a novel text-image co-synthesis pipeline was developed, generating virtual individuals with dozens of life-like visuals paired with corresponding dermatoglyphic text. Benchmarking tests against real-world scenarios demonstrate that this augmentation significantly improves AI accuracy in cross-modal retrieval tasks. Importantly, this approach supports explainability by enabling textual-to-visual identity recovery based on human-verifiable matching criteria. The findings represent a substantial advancement in unifying descriptive modalities, highlighting the potential of language-guided biometrics to overcome the constraints of vision-only Re-ID systems and enhance ecological species monitoring with an interpretable and effective framework. <div>
arXiv:2512.14878v1 Announce Type: new 
Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</title>
<link>https://arxiv.org/abs/2512.14884</link>
<guid>https://arxiv.org/abs/2512.14884</guid>
<content:encoded><![CDATA[
<div> Keywords: Vibe Blending, visual concepts, latent space, Vibe Space, creative evaluation<br /><br />Summary:  
1. The paper introduces a new task called Vibe Blending, aimed at generating coherent and meaningful hybrid images by connecting distinct visual concepts through their shared "vibe" or attributes.  
2. Current generative methods face challenges in identifying and navigating nonlinear latent space paths that link distant concepts to produce smooth blends.  
3. To address this, the authors propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesic paths in feature spaces such as CLIP, enabling semantically consistent and smooth transitions between different visual concepts.  
4. The study also presents a cognitively inspired evaluation framework that integrates human judgments, reasoning from large language models (LLMs), and a geometric path-based difficulty score to measure the creativity and coherence of the generated blends.  
5. Experimental results demonstrate that Vibe Space outperforms existing methods, producing blends that humans perceive as more creative and coherent, highlighting its effectiveness for creative visual concept generation. <div>
arXiv:2512.14884v1 Announce Type: new 
Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</title>
<link>https://arxiv.org/abs/2512.14922</link>
<guid>https://arxiv.org/abs/2512.14922</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, Gleason grading, prostate cancer, robustness, PANDA-PLUS-Bench  

<br /><br />Summary:  
This article addresses the challenge of artificial intelligence foundation models potentially overfitting to specimen-specific artifacts instead of generalizable biological features in prostate cancer Gleason grading, a critical task influencing treatment decisions. To tackle this problem, the authors introduce PANDA-PLUS-Bench, a novel, expert-annotated benchmark dataset composed of nine whole slide images from unique patients with varying Gleason patterns. The dataset offers tissue patches at two resolutions (512x512 and 224x224 pixels) under eight augmentation conditions, explicitly designed to identify and quantify models' failure to distinguish biological signals from slide-level confounders. Seven foundation models were evaluated using this benchmark, revealing considerable disparities in robustness. Notably, Virchow2, despite low slide-level encoding, showed poor cross-slide accuracy, while HistoEncoder, trained specifically on prostate tissue data, achieved the best overall performance, suggesting that tissue-specific training benefits both biological feature extraction and slide-specific signature recognition. All models demonstrated measurable accuracy gaps between within-slide and cross-slide evaluations, with differences ranging from approximately 20 to 27 percentage points. To facilitate further research, the authors provide an open-source Google Colab notebook that enables standardized benchmarking of additional models against PANDA-PLUS-Bench. This work fills an essential gap by offering a dedicated resource for evaluating foundation model robustness in the clinically vital domain of prostate cancer Gleason grading. <div>
arXiv:2512.14922v1 Announce Type: new 
Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pre-trained Segmentation Models using Post-Processing</title>
<link>https://arxiv.org/abs/2512.14937</link>
<guid>https://arxiv.org/abs/2512.14937</guid>
<content:encoded><![CDATA[
<div> Gliomas, multiparametric MRI, segmentation, post-processing, sustainability<br /><br />Summary:<br /><br />1. Gliomas are the most common and lethal malignant brain tumors in adults, with survival rates under 15 months despite aggressive treatment.<br /><br />2. Accurate multiparametric MRI (mpMRI) segmentation of gliomas is essential for surgical planning, radiotherapy, and disease monitoring.<br /><br />3. Although deep learning models have improved automated segmentation accuracy, large-scale pretrained models often generalize poorly, leading to systematic errors such as false positives, label swaps, and slice discontinuities.<br /><br />4. Challenges are exacerbated by unequal GPU access and the high environmental cost associated with training large-scale models.<br /><br />5. The authors propose adaptive post-processing techniques to refine glioma segmentation results from large pretrained tumor segmentation models.<br /><br />6. These techniques were tested on BraTS 2025 segmentation challenges, improving the ranking metric by 14.9% in the sub-Saharan Africa challenge and 0.9% in the adult glioma challenge.<br /><br />7. This work encourages a shift from creating increasingly complex model architectures towards efficient, clinically relevant post-processing methods that are precise, computationally equitable, and more sustainable. <div>
arXiv:2512.14937v1 Announce Type: new 
Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</title>
<link>https://arxiv.org/abs/2512.14938</link>
<guid>https://arxiv.org/abs/2512.14938</guid>
<content:encoded><![CDATA[
<div> Keywords: TalkVerse, audio-driven talking video generation, large-scale dataset, DiT model, zero-shot video dubbing<br /><br />Summary:  
The paper introduces TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation, designed to enable fair and reproducible comparison across methods. The dataset contains 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours, curated from over 60k hours of video using a transparent pipeline including scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations such as 2D skeletons and structured visual/audio-style captions. Leveraging this dataset, the authors present a reproducible 5 billion parameter DiT baseline based on Wan2.2-5B. Their model utilizes a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context to achieve minute-long video generation with low drift. The approach delivers lip-sync and visual quality comparable to the 14B Wan-S2V model but requires 10 times lower inference cost. To improve storytelling in long videos, an MLLM director module is integrated to rewrite prompts based on audio and visual cues. Additionally, the model supports zero-shot video dubbing through controlled latent noise injection. The dataset, training recipes, and 5B model checkpoints are open-sourced to facilitate further research in audio-driven human video generation. <div>
arXiv:2512.14938v1 Announce Type: new 
Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Puzzle Curriculum GRPO for Vision-Centric Reasoning</title>
<link>https://arxiv.org/abs/2512.14944</link>
<guid>https://arxiv.org/abs/2512.14944</guid>
<content:encoded><![CDATA[
<div> Keywords: Puzzle Curriculum GRPO, Vision Language Models, reinforcement learning, self-supervised puzzles, reasoning consistency<br /><br />Summary:<br /><br />This paper addresses key challenges in reinforcement learning (RL) approaches for Vision Language Models (VLMs), specifically issues with dependence on expensive annotations or external verifiers, flat and sparse reward structures, and the logical inconsistency between chain-of-thought reasoning and final answers. The authors propose Puzzle Curriculum GRPO (PC-GRPO), a supervision-free RL method that utilizes Verifiable Rewards (RLVR) without requiring labels or external verification. PC-GRPO introduces three self-supervised puzzle environments—PatchFit, Rotation (binary rewards), and Jigsaw (graded partial credit)—to provide meaningful rewards and reduce sparsity. To improve training, it employs a difficulty-aware curriculum which dynamically adjusts sample weights, focusing on medium difficulty puzzles to maximize learning. The paper also monitors Reasoning-Answer Consistency (RAC) during post-training, revealing RAC initially improves but then declines; their curriculum delays this drop, and implementing consistency-enforcing reward mechanisms further enhances RAC, which correlates with better downstream task accuracy. Experiments conducted on Qwen-7B and Qwen-3B VLMs across multiple benchmarks demonstrate that PC-GRPO significantly improves reasoning abilities, training stability, and final task performance. Overall, this work offers a scalable, interpretable, and annotation-free reinforcement learning framework for advancing visual reasoning in VLMs. <div>
arXiv:2512.14944v1 Announce Type: new 
Abstract: Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities</title>
<link>https://arxiv.org/abs/2512.14961</link>
<guid>https://arxiv.org/abs/2512.14961</guid>
<content:encoded><![CDATA[
<div> Keywords: person recognition, trimodal system, multi-task learning, cross-attention fusion, robustness to modality loss  

<br /><br />Summary: This paper addresses the challenges of person recognition in real-world scenarios where audio, visual, or behavioral data modalities are often missing or degraded. The authors propose a Trimodal person identification framework that integrates voice, face, and gesture modalities to improve robustness. Their method employs multi-task learning to independently process each modality and utilizes cross-attention alongside gated fusion mechanisms to enable effective interaction across these modalities. A key innovation is a confidence-weighted fusion strategy that dynamically adjusts to missing or low-quality data, ensuring strong performance even when only one or two modalities are available. The system is evaluated on a newly introduced interview-based multimodal dataset named CANDOR, marking the first benchmark for this dataset, where it achieves an impressive 99.18% Top-1 accuracy. Additionally, the model is tested on the well-known VoxCeleb1 dataset, attaining 99.92% accuracy in bimodal settings. The results highlight the proposed system's superiority over traditional unimodal and late-fusion approaches. The code and data accompanying this work have been made publicly available, promoting further research and application development in robust multimodal person recognition systems. <div>
arXiv:2512.14961v1 Announce Type: new 
Abstract: Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where is the Watermark? Interpretable Watermark Detection at the Block Level</title>
<link>https://arxiv.org/abs/2512.14994</link>
<guid>https://arxiv.org/abs/2512.14994</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, image watermarking, discrete wavelet transform, interpretability, robustness

<br /><br />Summary:  
This paper addresses the challenges posed by generative AI in creating realistic digital content, emphasizing concerns about authenticity, ownership, and misuse. Traditional image watermarking schemes typically provide global detection scores without clarifying where or how watermarks exist within images, limiting transparency and interpretability. To overcome this, the authors propose a post-hoc image watermarking method that combines localized embedding with region-level interpretability, allowing more detailed insight into watermarked areas. Their approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy, enabling generation of detection maps that identify watermarked or altered regions. Experiments demonstrate that the method achieves strong robustness against common image transformations such as cropping, even up to half the image, while maintaining sensitivity to semantic manipulations. Importantly, the watermark remains imperceptible to users, preserving image quality. Compared to prior post-hoc watermarking methods, this technique provides improved interpretability of detections without sacrificing robustness. Overall, the work presents a compelling balance between invisible watermark embedding, reliable tampering detection, and meaningful interpretability, enhancing user trust and utility in watermark verification for digitally generated images. <div>
arXiv:2512.14994v1 Announce Type: new 
Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</title>
<link>https://arxiv.org/abs/2512.14998</link>
<guid>https://arxiv.org/abs/2512.14998</guid>
<content:encoded><![CDATA[
<div> Keywords: precision livestock farming, social behavior, pose estimation, interaction classification, computer vision<br /><br />Summary:<br /><br />1. The study addresses the challenge of accurately assessing social interactions among livestock in commercial barns, which is critical for monitoring herd welfare. Traditional methods relying on static proximity thresholds fail to distinguish between affiliative (friendly) and agonistic (aggressive) behaviors in complex environments.<br /><br />2. The authors introduce a novel pose-based computational framework that leverages spatiotemporal geometry of anatomical keypoints rather than simple distance or pixel appearance cues to classify social interactions.<br /><br />3. The system integrates a series of computer vision components: YOLOv11 for object detection achieving a mean average precision (mAP@0.50) of 96.24%, a supervised individual identification module with 98.24% accuracy, ByteTrack for multi-object tracking with 81.96% accuracy, and ZebraPose for 27-point anatomical keypoint estimation.<br /><br />4. A support vector machine classifier trained on dynamic pose-derived distance features successfully differentiates affiliative from agonistic behaviors, reaching 77.51% accuracy on annotated clips from a commercial dairy barn.<br /><br />5. Compared to proximity-only baselines, the pose-based approach shows substantial improvement in discriminating social behaviors, especially affiliative interactions, confirming its potential for real-time, automated social network analysis in precision livestock farming using commodity hardware. <div>
arXiv:2512.14998v1 Announce Type: new 
Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</title>
<link>https://arxiv.org/abs/2512.15006</link>
<guid>https://arxiv.org/abs/2512.15006</guid>
<content:encoded><![CDATA[
<div> Video Question Generation, Expert Knowledge, Question Quality, EgoExoAsk Dataset, Question-to-Answer Retrieval<br /><br />Summary:<br /><br />1. This paper focuses on improving Video Question Generation (VQG) by emphasizing the quality of questions generated to elicit unseen knowledge from human experts rather than just evaluating answerability. <br /><br />2. It introduces a novel evaluation protocol that simulates a question-answering communication process with human experts, using a question-to-answer retrieval mechanism to quantitatively assess question effectiveness. <br /><br />3. To support this approach, the authors have created EgoExoAsk, a new dataset consisting of 27,666 question-answer pairs derived from expert commentary annotations on the Ego-Exo4D video dataset. <br /><br />4. The EgoExoAsk training set is used to train the retriever model, and the benchmark evaluation is conducted on validation segments of Ego-Exo4D videos, ensuring a realistic and challenging testing scenario. <br /><br />5. Experimental results show that the proposed evaluation metric correlates with model configurations accessing richer contextual information, validating the effectiveness of the protocol. The EgoExoAsk dataset and resources are publicly available for further research and continuous improvement of VQG models. <div>
arXiv:2512.15006v1 Announce Type: new 
Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Agnostic Preference Optimization for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.15009</link>
<guid>https://arxiv.org/abs/2512.15009</guid>
<content:encoded><![CDATA[
<div> Preference optimization, medical image segmentation, model-agnostic, Dropout, boundary adherence<br /><br />Summary: Preference optimization provides a scalable supervision paradigm based on relative preference signals but has been limited in medical image segmentation due to model-specific designs and low-diversity prediction sampling. This paper introduces MAPO (Model-Agnostic Preference Optimization), a novel training framework that leverages Dropout-driven stochastic segmentation hypotheses to generate preference-consistent gradients without requiring direct ground-truth supervision. MAPO is designed to be fully architecture- and dimensionality-agnostic, enabling compatibility with various segmentation pipelines including both 2D and 3D CNNs as well as Transformer-based models. Extensive evaluations on a wide range of medical datasets demonstrate that MAPO consistently improves boundary adherence in segmentation results, which is critical for medical accuracy. Furthermore, it notably reduces overfitting, leading to better generalization on unseen data. Additionally, MAPO achieves more stable optimization dynamics compared to traditional supervised training methods, contributing to more reliable and robust model performance. Through these advantages, MAPO targets key challenges in medical image segmentation, offering a flexible and effective alternative supervision strategy that can be broadly adopted across different architectures and data dimensionalities. <div>
arXiv:2512.15009v1 Announce Type: new 
Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</title>
<link>https://arxiv.org/abs/2512.15048</link>
<guid>https://arxiv.org/abs/2512.15048</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, super-resolution, multi-view attention, epipolar constraint, auxiliary view selection<br /><br />Summary:<br /><br />1. The paper addresses the limitation of 3D Gaussian Splatting (3DGS) models trained on low-resolution images, which underperform when rendering high-resolution outputs. <br />2. Existing single-image super-resolution methods for 3DGS lack cross-view consistency and fail to effectively fuse information from multiple views, while video-based SR methods require sequential frames and are not suitable for unstructured multi-view data.<br />3. The authors propose MVGSR, a novel framework designed to enhance 3DGS super-resolution by integrating multi-view information while maintaining geometric consistency and high-frequency detail.<br />4. A key contribution is the Auxiliary View Selection Method based on camera poses, enabling flexible handling of arbitrarily organized multi-view datasets without reliance on temporal continuity or frame ordering.<br />5. MVGSR incorporates, for the first time, an epipolar-constrained multi-view attention mechanism that selectively aggregates consistent information from auxiliary views, improving fidelity and geometric coherence.<br />6. Extensive experiments demonstrate that MVGSR outperforms prior state-of-the-art methods in both object-centric and scene-level 3DGS super-resolution benchmarks, validating the effectiveness of the proposed strategies. <div>
arXiv:2512.15048v1 Announce Type: new 
Abstract: Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement</title>
<link>https://arxiv.org/abs/2512.15055</link>
<guid>https://arxiv.org/abs/2512.15055</guid>
<content:encoded><![CDATA[
<div> Keywords: event camera, LED markers, high-frequency deformation, noise filtering, monocular measurement  

<br /><br />Summary:  
This paper addresses the challenge of measuring high-frequency deformations in large-scale structures, which are often caused by complex loads and difficult to capture using traditional high-speed cameras due to harsh lighting and cost limitations. It introduces a novel measurement method leveraging an event camera in combination with blinking LED markers. The approach begins by filtering observation noise based on the unique characteristics of the event stream generated by LED blinking and utilizing spatiotemporal correlation to enhance signal quality. Subsequently, the method distinguishes between motion-induced events and those generated by LED blinking, enabling accurate extraction of LED markers even at high speeds. This separation is crucial for capturing fast-moving markers without confusion from structural motion. Finally, the monocular event camera system is used to measure the high-frequency planar deformations of the observed structure. Experiments validate the accuracy and effectiveness of the proposed method, demonstrating its potential as a low-cost, robust alternative to traditional high-speed cameras for deformation measurement under challenging conditions. <div>
arXiv:2512.15055v1 Announce Type: new 
Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</title>
<link>https://arxiv.org/abs/2512.15066</link>
<guid>https://arxiv.org/abs/2512.15066</guid>
<content:encoded><![CDATA[
<div> Keywords: ultrasound segmentation, wavelet filtering, memory bank, long video tracking, small object detection<br /><br />Summary:<br /><br />1. The paper addresses challenges in medical ultrasound video analysis, focusing on accurate lesion and organ segmentation essential for disease diagnosis and surgical planning.<br />2. Ultrasound videos suffer from low contrast and noisy backgrounds causing segmentation errors, especially at object boundaries and for small objects.<br />3. To overcome these issues, the authors propose a memory bank-based wavelet filtering and fusion network (MWNet) utilizing an encoder-decoder architecture to extract fine spatial features and high-frequency information.<br />4. Innovations include memory-based wavelet convolution capturing category and adjacent details, cascaded wavelet compression for multiscale frequency fusion and receptive field expansion, and a long short-term memory bank with cross-attention for tracking objects in long videos.<br />5. An HF-aware feature fusion module using adaptive wavelet filters is incorporated in the decoder to leverage boundary-sensitive high-frequency details.<br />6. Extensive benchmarking on four ultrasound video datasets (including thyroid nodules, thyroid gland, and heart datasets) shows significant improvements over state-of-the-art methods.<br />7. The method excels particularly in segmenting small thyroid nodules in long-duration videos.<br />8. The implementation code is publicly available at https://github.com/XiAooZ/MWNet, promoting reproducibility and further research. <div>
arXiv:2512.15066v1 Announce Type: new 
Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMMD: A pose-guided multi-view multi-modal diffusion for person generation</title>
<link>https://arxiv.org/abs/2512.15069</link>
<guid>https://arxiv.org/abs/2512.15069</guid>
<content:encoded><![CDATA[
<div> Keywords: human image generation, multimodal diffusion, pose control, multi-view synthesis, text-conditioned

<br /><br />Summary: Generating consistent and photorealistic human images with controllable pose and appearance is critical for various applications such as virtual try-on, image editing, and digital human creation. The paper introduces Pose-guided Multi-view Multimodal Diffusion (PMMD), a novel diffusion-based framework that synthesizes person images by conditioning on multiple visual references, pose maps, and text prompts. PMMD features a multimodal encoder that effectively integrates visual views, pose information, and semantic text descriptions, minimizing cross-modal discrepancies and enhancing identity preservation. To improve local detail while maintaining the global structure of synthesized images, the authors propose a Residual Cross-View Attention (ResCVA) module. Additionally, a cross-modal fusion module is designed to incorporate image semantics and textual cues cohesively throughout the denoising diffusion process. Experimental results on the DeepFashion MultiModal dataset demonstrate that PMMD surpasses existing methods in terms of pose consistency, garment detail preservation, and controllability over appearance. The proposed framework successfully addresses common challenges like occlusions, garment style drift, and pose misalignment. The authors have made the project’s code and resources publicly available, facilitating further research and development in multimodal image synthesis with controllable human appearance and pose. <div>
arXiv:2512.15069v1 Announce Type: new 
Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Parser Technical Report</title>
<link>https://arxiv.org/abs/2512.15098</link>
<guid>https://arxiv.org/abs/2512.15098</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-Parser, document parsing, multi-expert architecture, GPU load balancing, large-scale cloud deployment

<br /><br />Summary: This technical report presents Uni-Parser, an advanced document parsing engine designed specifically for industrial-scale processing of scientific literature and patents. The system distinguishes itself from traditional pipeline-based methods by utilizing a modular, loosely coupled multi-expert architecture that maintains precise cross-modal alignments among text, equations, tables, figures, and chemical structures. This design also ensures easy extensibility to accommodate new modalities. Uni-Parser integrates adaptive GPU load balancing, distributed inference capabilities, and dynamic orchestration of modules, allowing for configurable modes that support either a holistic parsing approach or modality-specific parsing. Optimized for deployment on large-scale cloud infrastructure, Uni-Parser can process up to 20 PDF pages per second using eight NVIDIA RTX 4090D GPUs, delivering cost-efficient inference on a massive scale. Such scalability enables various downstream applications, including literature retrieval, summarization, and the extraction of specialized data such as chemical structures, reaction schemes, and bioactivity information. Additionally, Uni-Parser supports the curation of large corpora essential for training the next generation of large language models and AI-driven scientific discovery systems, thereby facilitating advancements in AI4Science research and applications. <div>
arXiv:2512.15098v1 Announce Type: new 
Abstract: This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets</title>
<link>https://arxiv.org/abs/2512.15110</link>
<guid>https://arxiv.org/abs/2512.15110</guid>
<content:encoded><![CDATA[
<div> Keywords: Nano Banana Pro, text-to-image generation, low-level vision, zero-shot evaluation, visual quality<br /><br />Summary:<br /><br />1. The paper evaluates Nano Banana Pro, a cutting-edge text-to-image generation model, for its capability in traditional low-level vision tasks.<br />2. A comprehensive zero-shot evaluation was performed on 14 different low-level vision tasks across 40 diverse datasets using simple text prompts without any fine-tuning.<br />3. Nano Banana Pro was benchmarked against state-of-the-art specialist models designed specifically for these tasks.<br />4. Results show a performance dichotomy: Nano Banana Pro outperforms specialists in subjective visual quality, often creating plausible high-frequency details that specialists miss.<br />5. However, it underperforms on standard reference-based quantitative metrics due to the stochastic and generative nature of the model, which challenges pixel-level consistency.<br />6. The study concludes that while Nano Banana Pro is a promising zero-shot all-rounder for low-level vision, bridging the gap to achieve the high fidelity and accuracy of dedicated domain specialists remains an open challenge. <div>
arXiv:2512.15110v1 Announce Type: new 
Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding</title>
<link>https://arxiv.org/abs/2512.15126</link>
<guid>https://arxiv.org/abs/2512.15126</guid>
<content:encoded><![CDATA[
<div> 3D animation, proxy representation, appearance synthesis, motion control, low-power platforms<br /><br />Summary:<br /><br />This paper addresses the challenges in 3D animation production, which is traditionally labor-intensive, requires expert knowledge, and is computationally expensive. It highlights that existing AI-generated content (AIGC) approaches either replicate the expensive full 3D pipelines or use video-synthesis methods that compromise 3D control and interactivity. The authors focus on single-image 3D animation generation and identify a key trade-off between rendering quality and 3D control as a fundamental bottleneck. To overcome this, they propose a lightweight 3D animation framework that separates geometric control from appearance synthesis. The core innovation is a 2D-3D aligned proxy representation employing a coarse 3D estimate to guide structural aspects, while learned image-space generative models handle detailed appearance and view synthesis. This design allows for precise 3D-aware motion control analogous to classical systems but without the need for accurate geometry or costly optimization. Additionally, the method supports coherent background animation naturally. Experimental results show the framework delivers efficient animation generation on low-power devices and surpasses video-based 3D animation methods in preserving identity, maintaining geometric and textural consistency, and offering fine-grained, interactive user control. <div>
arXiv:2512.15126v1 Announce Type: new 
Abstract: 3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.
  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Borrowing from anything: A generalizable framework for reference-guided instance editing</title>
<link>https://arxiv.org/abs/2512.15138</link>
<guid>https://arxiv.org/abs/2512.15138</guid>
<content:encoded><![CDATA[
<div> Keywords: reference-guided instance editing, semantic disentanglement, Spatial Alignment Module, Adaptive Residual Scaling Module, Progressive Attention Fusion  

<br /><br />Summary:  
This paper addresses the challenge of semantic entanglement in reference-guided instance editing, where intrinsic appearance features of a reference are mixed with extrinsic attributes, complicating the editing process. The authors propose GENIE, a Generalizable Instance Editing framework designed to achieve explicit disentanglement between intrinsic and extrinsic information. GENIE incorporates three main components: first, a Spatial Alignment Module (SAM) corrects spatial misalignments between the reference and the target, ensuring better correspondence. Second, an Adaptive Residual Scaling Module (ARSM) identifies and amplifies salient intrinsic cues in the reference image while suppressing irrelevant extrinsic attributes, effectively learning what information should be borrowed. Third, a Progressive Attention Fusion (PAF) mechanism governs how the borrowed appearance is rendered onto the target, preserving the target's original structure. The framework is tested extensively on the challenging AnyInsertion dataset, where it demonstrates state-of-the-art performance in terms of fidelity and robustness. By explicitly disentangling features and addressing how to transfer and apply appearance details properly, GENIE sets a new benchmark in instance editing tasks that rely on reference guidance. This work significantly advances capabilities in controlled and semantically faithful image editing. <div>
arXiv:2512.15138v1 Announce Type: new 
Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</title>
<link>https://arxiv.org/abs/2512.15153</link>
<guid>https://arxiv.org/abs/2512.15153</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Action Assessment, Chain-of-Thought Explanation, Video Dataset, Fitness and Martial Arts, Explainable Model<br /><br />Summary: This paper introduces a novel task called Human Action Form Assessment (AFA), aimed at evaluating whether human actions are performed in a standard manner and providing detailed, reasonable feedback for improvement. Existing video understanding methods mainly focus on identifying what and where the action occurs but lack the capability to assess action quality or standardization. To address the scarcity of datasets labeled for action standardization and explainability, the authors present a new large-scale dataset named CoT-AFA, which includes diverse fitness and martial arts videos with multi-level annotations supporting comprehensive analysis. A key innovation is the integration of a Chain-of-Thought explanation paradigm, where feedback is not isolated but involves a full reasoning process—from action step identification to outcome analysis and concrete solution proposals. The paper also proposes the Explainable Fitness Assessor framework that employs two parallel streams and a dynamic gating mechanism to combine visual and semantic inputs, enhancing assessment and explanation capabilities. Experimental results show significant improvements over baseline methods, including a 16.0% increase in CIDEr for explanation generation, 2.7% higher accuracy in action classification, and 2.1% boost in quality assessment. The dataset and code are publicly available, offering strong potential for advancing explainable human action evaluation research. <div>
arXiv:2512.15153v1 Announce Type: new 
Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.15160</link>
<guid>https://arxiv.org/abs/2512.15160</guid>
<content:encoded><![CDATA[
<div> spatial cognition, multimodal reasoning, reinforcement learning, 3D perception, video keyframe selection<br /><br />Summary: The paper introduces EagleVision, a novel dual-stage framework aimed at improving spatial intelligence in vision-language models by addressing key challenges in spatial Chain-of-Thought (CoT) reasoning. First, it tackles the problem of building a global spatial perception within strict token budgets by employing a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact, geometry- and semantics-aware set of keyframes from long video sequences. Second, EagleVision formalizes spatial CoT as BEV-grounded pose querying, where the agent iteratively predicts object or camera poses on a bird’s-eye view (BEV) plane and retrieves nearest real video frames to verify these predictions. Third, the framework is trained purely through reinforcement learning guided by a spatial grounding reward, which quantitatively measures consistency between predicted poses and observed views, therefore explicitly associating 3D hypotheses with visual evidence. EagleVision demonstrates significant improvements over existing open-source vision-language models on the VSI-Bench benchmark, showcasing strong and generalizable spatial understanding. By integrating macro-level perception with micro-level verification and a well-designed reward function, EagleVision advances multimodal reasoning methods in spatial tasks without relying on black-box 3D reconstructions or compromising evidence traceability. <div>
arXiv:2512.15160v1 Announce Type: new 
Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis</title>
<link>https://arxiv.org/abs/2512.15171</link>
<guid>https://arxiv.org/abs/2512.15171</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal classification, renal biopsy, ultra-scale learning, glomerular disease, feature fusion

<br /><br />Summary: The paper presents CMUS-Net, a novel cross-modal ultra-scale learning network designed for automatic classification of multiple glomerular diseases using three types of renal biopsy images obtained from different modalities: transmission electron microscopy (TEM), optical microscopy (OM), and immunofluorescence microscopy (IM). The main challenge addressed is the significant scale difference between nanoscale TEM image features and microscale OM/IM features, which complicates effective feature fusion in existing models. CMUS-Net bridges this gap by integrating multiple ultrastructural information. A sparse multi-instance learning module is introduced to effectively aggregate features from TEM images, while a cross-modal scale attention module enhances feature interaction and boosts the extraction of pathological semantic information. Additionally, the framework employs multiple loss functions to balance the importance of different modalities, leading to improved classification accuracy. Evaluation on an in-house dataset demonstrates high performance, with an accuracy of 95.37±2.41%, AUC of 99.05±0.53%, and F1-score of 95.32±2.41%. The method excels over other multi-modal and multi-scale approaches and shows strong generalization in membranous nephropathy staging. The approach aligns with standard renal biopsy diagnostic procedures and marks the first instance of automatic multi-disease classification using tri-modal, dual-scale renal biopsy images. The related code is publicly available on GitHub. <div>
arXiv:2512.15171v1 Announce Type: new 
Abstract: Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving</title>
<link>https://arxiv.org/abs/2512.15181</link>
<guid>https://arxiv.org/abs/2512.15181</guid>
<content:encoded><![CDATA[
<div> Keywords: automated driving, object detection, safety evaluation, criticality metrics, DeepAccident dataset<br /><br />Summary: This paper addresses the paramount goal of ensuring safety in automated driving through accurate environmental perception. It emphasizes the need for safety-specific performance metrics in evaluating object detection systems, particularly focusing on distinguishing relevant objects from non-relevant ones via criticality or relevance metrics. The authors conduct the first comprehensive analysis of criticality metrics for safety evaluation by reviewing existing literature and identifying a range of applicable metrics. The effectiveness of these metrics is empirically validated using the DeepAccident dataset, which contains diverse safety-critical scenarios. To further enhance evaluation accuracy, the paper proposes two novel application strategies: bidirectional criticality rating and multi-metric aggregation. These strategies collectively improve the criticality classification accuracy by up to 100%. The findings highlight the significant potential of these approaches to advance the safety evaluation framework for object detection systems in automated vehicles, ultimately contributing to safer autonomous driving technologies. <div>
arXiv:2512.15181v1 Announce Type: new 
Abstract: Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Calibrated Detection of Authentic Multimedia Content</title>
<link>https://arxiv.org/abs/2512.15182</link>
<guid>https://arxiv.org/abs/2512.15182</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, generative models, resynthesis framework, adversarial robustness, inversion techniques  

<br /><br />Summary:  
Generative models can create highly realistic synthetic content, known as deepfakes, which pose significant challenges to verifying digital media authenticity. Existing deepfake detection methods are unreliable primarily due to two major issues: first, distinguishing inauthentic content after the fact is often impossible, particularly for memorized samples, resulting in an unbounded false positive rate (FPR). Second, current detectors lack robustness since adversaries can easily adapt to them using limited computational resources, thus evading detection with near-perfect accuracy. To overcome these limitations, the authors propose a novel resynthesis framework designed to determine if a content sample is authentically original or if its authenticity can be plausibly denied. Their approach focuses on a high-precision and low-recall operating point aimed at defending against compute-restricted adversaries. The first key contribution of their work is demonstrating that their calibrated resynthesis method reliably verifies authentic samples while maintaining controllable and low false positive rates. The second contribution shows that their method achieves robustness to adversarial attacks under the same computational constraints, outperforming prior detection methods which fail under such conditions. Additionally, their approach is versatile, supporting multiple data modalities and integrating state-of-the-art inversion techniques for enhanced performance and generalizability. <div>
arXiv:2512.15182v1 Announce Type: new 
Abstract: Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment</title>
<link>https://arxiv.org/abs/2512.15186</link>
<guid>https://arxiv.org/abs/2512.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: RAW image enhancement, low-light imaging, multi-scale processing, green channel guidance, real-time performance<br /><br />Summary:  
This paper addresses the challenge of enhancing low-light RAW images efficiently and effectively. Firstly, the authors highlight that RAW images outperform sRGB images in low-light enhancement but existing methods typically process multi-scale information sequentially, limiting model efficiency and speed. Secondly, current approaches often neglect the unique advantages of the green channel in RAW data, which contains richer information for image reconstruction. To tackle these issues, the authors propose ERIENet, an efficient RAW Image Enhancement Network designed to process multi-scale features in parallel using a novel channel-aware residual dense block. This architecture reduces computational cost and boosts speed, enabling real-time processing. Additionally, ERIENet incorporates a green channel guidance branch specifically to leverage the abundant green channel information, improving reconstruction accuracy without significantly increasing parameter count or computation. Experiments conducted on standard low-light enhancement datasets demonstrate that ERIENet outperforms state-of-the-art methods in both image quality and efficiency. Notably, it achieves a high processing speed of over 146 frames per second for 4K-resolution images on a single NVIDIA GeForce RTX 3090 GPU with 24GB memory, making it suitable for practical, real-time applications in low-light imaging scenarios. <div>
arXiv:2512.15186v1 Announce Type: new 
Abstract: RAW images have shown superior performance than sRGB images in many image processing tasks, especially for low-light image enhancement. However, most existing methods for RAW-based low-light enhancement usually sequentially process multi-scale information, which makes it difficult to achieve lightweight models and high processing speeds. Besides, they usually ignore the green channel superiority of RAW images, and fail to achieve better reconstruction performance with good use of green channel information. In this work, we propose an efficient RAW Image Enhancement Network (ERIENet), which parallelly processes multi-scale information with efficient convolution modules, and takes advantage of rich information in green channels to guide the reconstruction of images. Firstly, we introduce an efficient multi-scale fully-parallel architecture with a novel channel-aware residual dense block to extract feature maps, which reduces computational costs and achieves real-time processing speed. Secondly, we introduce a green channel guidance branch to exploit the rich information within the green channels of the input RAW image. It increases the quality of reconstruction results with few parameters and computations. Experiments on commonly used low-light image enhancement datasets show that ERIENet outperforms state-of-the-art methods in enhancing low-light RAW images with higher effiency. It also achieves an optimal speed of over 146 frame-per-second (FPS) for 4K-resolution images on a single NVIDIA GeForce RTX 3090 with 24G memory.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2512.15211</link>
<guid>https://arxiv.org/abs/2512.15211</guid>
<content:encoded><![CDATA[
<div> Infrared image fusion, visible image fusion, no-reference metrics, noise trap, Target-Background Contrast (TBC)<br /><br />Summary:<br /><br />Infrared and visible image fusion is crucial for low-altitude UAV reconnaissance, enhancing target detection and tracking by combining thermal saliency with background texture details. Traditional no-reference quality metrics like Entropy (EN) and Average Gradient (AG) perform poorly in complex low-light conditions because they cannot distinguish between actual image details and high-frequency sensor noise. This limitation results in a "Noise Trap," where noisy images receive misleadingly high scores, adversely affecting fusion algorithm performance. To overcome this challenge, the authors propose the Target-Background Contrast (TBC) metric, inspired by Weber's Law. TBC shifts focus from global image statistics to the relative contrast between salient targets and the background, effectively penalizing noise while rewarding clear target visibility. Experimental results on the DroneVehicle dataset demonstrate that TBC aligns more closely with human visual perception under low-light and noisy conditions, providing a more reliable and accurate quality assessment standard for infrared-visible image fusion in UAV applications. This work addresses a critical limitation in existing evaluation metrics and offers a practical solution for improving image fusion outcomes in challenging environments. <div>
arXiv:2512.15211v1 Announce Type: new 
Abstract: Infrared and visible image fusion is a pivotal technology in low-altitude UAV reconnaissance missions, providing high-quality data support for downstream tasks such as target detection and tracking by integrating thermal saliency with background texture details.However, traditional no-reference metrics fail(Specifically,like Entropy (EN) and Average Gradient (AG)) in complex low-light environments. They often misinterpret high-frequency sensor noise as valid detail. This creates a "Noise Trap," paradoxically assigning higher scores to noisy images and misguiding fusion algorithms.To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Experiments on the DroneVehicle dataset demonstrate that TBC aligns better with human perception and provides a reliable standard for low-altitude scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Camera to World: A Plug-and-Play Module for Human Mesh Transformation</title>
<link>https://arxiv.org/abs/2512.15212</link>
<guid>https://arxiv.org/abs/2512.15212</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D human mesh, camera rotation, world coordinate system, Mesh-Plug, root joint orientation  

<br /><br />Summary:  
1. The paper addresses the challenge of reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images, which is difficult due to the unknown camera rotation information.  
2. Existing methods typically assume zero camera rotation and reconstruct meshes in the camera coordinate system, but this leads to errors when the mesh is transformed to the world coordinate system.  
3. To solve this, the authors propose Mesh-Plug, a plug-and-play module designed to accurately convert human meshes from camera coordinates to world coordinates by estimating the camera rotation.  
4. The approach is human-centered, leveraging both RGB images and depth maps rendered from the initial mesh in order to predict camera pitch angle without relying on environmental cues.  
5. Mesh-Plug first trains a camera rotation prediction module focused on the human body’s spatial configuration, then uses these estimated parameters in a mesh adjustment module that refines the root joint orientation and body pose simultaneously.  
6. Extensive experiments on benchmark datasets SPEC-SYN and SPEC-MTP demonstrate that Mesh-Plug outperforms current state-of-the-art methods, achieving more accurate alignment of 3D human meshes in the world coordinate system. <div>
arXiv:2512.15212v1 Announce Type: new 
Abstract: Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal</title>
<link>https://arxiv.org/abs/2512.15221</link>
<guid>https://arxiv.org/abs/2512.15221</guid>
<content:encoded><![CDATA[
<div> lens flare, nighttime, transformer, frequency domain, flare removal  

<br /><br />Summary:  
Lens flare is a common artifact in nighttime photography caused by strong light sources scattering within camera lenses, resulting in hazy streaks, halos, and glare that degrade image quality. Existing methods struggle to effectively remove nonuniform scattered flares, limiting their performance in complex real-world lighting conditions. To solve this problem, the authors propose SLCFormer, a spectral-local context transformer framework designed specifically for nighttime lens flare removal. SLCFormer includes two main components: the Frequency Fourier and Excitation Module (FFEM), which captures global contextual information in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM), which enhances local spatial structures and directional features for precise flare removal. Additionally, they develop a ZernikeVAE-based scatter flare generation pipeline that synthesizes physically realistic flare artifacts with spatially varying point spread functions (PSFs), combining principles of optical physics with data-driven learning. Experiments conducted on the Flare7K++ dataset show that SLCFormer achieves state-of-the-art performance with superior quantitative metrics and visual quality compared to existing methods. Furthermore, the method generalizes robustly to real nighttime scenes containing complex flare patterns, demonstrating its practical effectiveness and adaptability. <div>
arXiv:2512.15221v1 Announce Type: new 
Abstract: Lens flare is a common nighttime artifact caused by strong light sources scattering within camera lenses, leading to hazy streaks, halos, and glare that degrade visual quality. However, existing methods usually fail to effectively address nonuniform scattered flares, which severely reduces their applicability to complex real-world scenarios with diverse lighting conditions. To address this issue, we propose SLCFormer, a novel spectral-local context transformer framework for effective nighttime lens flare removal. SLCFormer integrates two key modules: the Frequency Fourier and Excitation Module (FFEM), which captures efficient global contextual representations in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM) for local structural enhancement and directional features in the spatial domain for precise flare removal. Furthermore, we introduce a ZernikeVAE-based scatter flare generation pipeline to synthesize physically realistic scatter flares with spatially varying PSFs, bridging optical physics and data-driven training. Extensive experiments on the Flare7K++ dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in both quantitative metrics and perceptual visual quality, and generalizing robustly to real nighttime scenes with complex flare artifacts.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Null-LoRA: Low-Rank Adaptation on Null Space</title>
<link>https://arxiv.org/abs/2512.15233</link>
<guid>https://arxiv.org/abs/2512.15233</guid>
<content:encoded><![CDATA[
<div> keywords: Null-LoRA, parameter-efficient fine-tuning, low-rank adaptation, null space, large-scale models<br /><br />Summary: Parameter-efficient fine-tuning methods such as LoRA have become prominent for adapting large-scale models efficiently to downstream tasks by modifying a limited number of parameters. Traditional approaches perform low-rank adaptation across the entire parameter space, which may include redundant or ineffective updates. Inspired by the existence of non-trivial null spaces in pre-trained models, the authors propose Null-space based Low-Rank Adaptation (Null-LoRA), a novel technique that freezes certain parts of the low-rank matrices to reduce redundancy and improve the effective rank of updates. By restricting the incremental updates entirely within the null space, Null-LoRA maximizes the use of parameter updates specific to new tasks, enhancing parameter efficiency. Extensive experiments on image-text retrieval and visual question answering benchmarks demonstrate that Null-LoRA outperforms state-of-the-art fine-tuning methods while requiring fewer parameters. This method thus represents a significant advancement in parameter-efficient transfer learning by leveraging the structural properties of pre-trained models to optimize adaptation without increasing complexity. <div>
arXiv:2512.15233v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</title>
<link>https://arxiv.org/abs/2512.15249</link>
<guid>https://arxiv.org/abs/2512.15249</guid>
<content:encoded><![CDATA[
<div> Medical AI, intersectional bias, diagnostic certainty, fairness, multimodal vision-language models  

<br /><br />Summary:  
This study addresses the challenge of intersectional biases in medical AI systems, specifically multimodal vision-language models (VLMs), which tend to show reduced confidence and accuracy in diagnosing marginalized patient subgroups. Such biases result from demographically skewed data and varied diagnostic certainty distributions, often causing higher missed diagnosis rates among these groups. Traditional fairness interventions either fail to fully close these gaps or degrade overall diagnostic performance by forcing statistical parity. To overcome these issues, the authors propose a novel training framework named Cross-Modal Alignment Consistency (CMAC-MMD) that standardizes diagnostic certainty across intersectional subgroups without needing sensitive demographic data during inference, thus preserving patient privacy. The framework was evaluated on large-scale datasets: 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), as well as 10,000 fundus images for glaucoma detection (Harvard-FairVLMed). Performance was stratified by intersectional attributes such as age, gender, and race. Results showed that CMAC-MMD substantially reduced the missed diagnosis gap (difference in True Positive Rate, ΔTPR) while improving overall diagnostic accuracy (AUC): in dermatology, ΔTPR dropped from 0.50 to 0.26 and AUC rose from 0.94 to 0.97; in glaucoma detection, ΔTPR decreased from 0.41 to 0.31 with a slight AUC improvement from 0.71 to 0.72. This work presents a scalable and privacy-preserving approach to build equitable, high-stakes clinical AI systems that maintain or improve accuracy alongside reducing bias. <div>
arXiv:2512.15249v1 Announce Type: new 
Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $\Delta$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $\Delta$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15254</link>
<guid>https://arxiv.org/abs/2512.15254</guid>
<content:encoded><![CDATA[
<div> Counting, Vision-Language Models, Object Enumeration, Visual Scenes, Benchmark

<br /><br />Summary:  
This study addresses the challenge of counting objects in visual scenes, a significant problem in computer vision. Traditional methods depend on specialized counting architectures trained on datasets with fixed object categories. However, the emergence of large-scale multimodal vision-language models (VLMs) offers a more flexible option for open-set object counting. The authors systematically compare state-of-the-art specialized counting models with generalist VLMs using two established counting datasets and a newly designed benchmark that allows fine-grained control over visual test properties. Results indicate that many VLMs can perform approximate enumeration of objects, sometimes equaling or outperforming specialized models. A key discovery is that counting accuracy improves when VLMs are prompted to create intermediate outputs, such as object locations and verbal labels, before enumerating items. Despite these advances, no model currently demonstrates reliable counting performance in highly complex visual scenes. This highlights the necessity for continued research aimed at developing AI systems capable of dependable object counting in real-world environments. <div>
arXiv:2512.15254v1 Announce Type: new 
Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</title>
<link>https://arxiv.org/abs/2512.15261</link>
<guid>https://arxiv.org/abs/2512.15261</guid>
<content:encoded><![CDATA[
<div> Pan-sharpening, Multimodal Diffusion Transformer, Cross-modal fusion, Zero-shot super-resolution, Multimodal interleaved scanning<br /><br />Summary:<br /><br />1. The paper addresses pan-sharpening, aiming to generate high-resolution multispectral (HRMS) images by effectively fusing a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) image.<br />2. Traditional CNN-based methods rely on channel-wise concatenation and fixed convolutions, which lack adaptability to varying spatial and spectral features, while existing cross-attention approaches, though enabling global interaction, suffer from high computational cost and diluted fine-grained correspondences.<br />3. The authors leverage recent Multimodal Diffusion Transformer (MMDiT) advances, which use in-context conditioning rather than cross-attention, to enable more direct and efficient cross-modal information exchange.<br />4. The proposed framework, MMMamba, is built on the Mamba architecture, ensuring linear computational complexity and strong cross-modal fusion capability, and it uniquely supports image super-resolution tasks in a zero-shot manner.<br />5. Additionally, the paper introduces a novel multimodal interleaved (MI) scanning mechanism that facilitates effective interactions between PAN and MS modalities, with extensive experiments demonstrating the framework’s superior performance over current state-of-the-art methods across several benchmarks and tasks. <div>
arXiv:2512.15261v1 Announce Type: new 
Abstract: Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.15310</link>
<guid>https://arxiv.org/abs/2512.15310</guid>
<content:encoded><![CDATA[
<div> Zero Shot Weakly Supervised Semantic Segmentation, SynthSeg Agents, Large Language Models, Synthetic Training Data, Semantic Segmentation

<br /><br />Summary:  
1. This paper introduces Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), a novel approach aiming to perform semantic segmentation without using any real training images, relying solely on synthetic data generated through Large Language Models (LLMs).  
2. The proposed framework, SynthSeg Agents, consists of two main components: the Self Refine Prompt Agent and the Image Generation Agent. The former iteratively creates diverse and semantically rich image prompts using refinement, memory, and exploration techniques, guided by CLIP similarity metrics and diversity filtering.  
3. The Image Generation Agent utilizes Vision Language Models (VLMs) to convert these prompts into synthetic images. High-quality images are selected through a frozen CLIP-based scoring model, ensuring semantic relevance and image fidelity.  
4. After synthetic data generation, a ViT-based classifier is trained to relabel the entire dataset with enhanced semantic precision, improving the training data's quality further.  
5. Experiments on benchmark datasets PASCAL VOC 2012 and COCO 2014 demonstrate that SynthSeg Agents achieve competitive semantic segmentation performance without using any real images during training, showcasing the potential for cost-efficient and scalable approaches driven by LLM-based agents. <div>
arXiv:2512.15310v1 Announce Type: new 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation</title>
<link>https://arxiv.org/abs/2512.15311</link>
<guid>https://arxiv.org/abs/2512.15311</guid>
<content:encoded><![CDATA[
<div> LiDAR image representation, cross-modality distillation, Bird's-Eye-View segmentation, panoramic camera, autonomous driving<br /><br />Summary:<br /><br />This paper introduces a novel cross-modality distillation framework designed specifically for single-panoramic-camera Bird's-Eye-View (BEV) segmentation, aiming to improve perception in autonomous driving systems. The authors propose a unique LiDAR image representation that fuses range, intensity, and ambient data channels, combined with a voxel-aligned view transformer to maintain spatial details while enabling efficient BEV processing. A Teacher network that fuses high-capacity LiDAR and camera inputs extracts rich spatial and semantic features, which are distilled into a lightweight Student network relying solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate the Teacher model outperforms existing camera-based BEV methods by a 25.6% IoU margin. The Student network, after distillation, achieves an 8.5% IoU improvement along with state-of-the-art inference speed of 31.2 FPS, making it suitable for real-time applications. Further evaluations on KITTI-360 with fisheye cameras validate the framework’s generalizability across different camera setups. Overall, this approach effectively reduces sensor complexity and deployment costs, providing a practical, efficient, and robust solution for BEV segmentation in real-world autonomous driving contexts. <div>
arXiv:2512.15311v1 Announce Type: new 
Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</title>
<link>https://arxiv.org/abs/2512.15315</link>
<guid>https://arxiv.org/abs/2512.15315</guid>
<content:encoded><![CDATA[
<div> Motion artifacts, MRI quality, supervised contrastive learning, interpretability, affinity scores<br /><br />Summary:  
Motion artifacts significantly degrade MRI image quality, leading to increased patient recalls and inefficiencies. Existing automated quality assessment techniques typically offer only binary decisions (artifact present or not) and lack interpretability, making it challenging for clinicians to understand the severity of motion artifacts. To address this, the authors propose AutoMAC-MRI, an explainable framework designed to grade motion artifacts across diverse MRI contrasts and orientations. The method leverages supervised contrastive learning to develop a discriminative representation of motion severity in the feature space. Within this space, AutoMAC-MRI calculates grade-specific affinity scores, which measure an image’s proximity to defined motion severity grades, providing transparency and interpretability for the assigned grades. The framework was evaluated using over 5000 expert-annotated brain MRI slices collected from multiple contrasts and views. Experimental results demonstrate strong alignment between the affinity scores and expert labels, validating the scores as an interpretable indicator of motion severity. By combining accurate grade prediction with per-grade affinity scoring, AutoMAC-MRI facilitates inline MRI quality control, which has the potential to reduce unnecessary rescans and enhance workflow efficiency in clinical settings. <div>
arXiv:2512.15315v1 Announce Type: new 
Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.15319</link>
<guid>https://arxiv.org/abs/2512.15319</guid>
<content:encoded><![CDATA[
<div> Few-shot anomaly detection, prototypical learning, context-aware segmentation, feature adaptation, FSAD performance<br /><br />Summary:<br /><br />Few-shot anomaly detection (FSAD) aims to identify anomalies within a specific category using a very limited number of normal samples. Existing FSAD approaches depend heavily on pre-trained features, but often neglect the domain gap between those features and the target anomaly detection scenario. To tackle this, the paper proposes PCSNet, a Prototypical Learning Guided Context-Aware Segmentation Network designed to reduce this domain gap and improve the descriptiveness of features in target settings, thereby boosting FSAD performance. PCSNet consists of two key components: a Prototypical Feature Adaptation (PFA) sub-network and a Context-Aware Segmentation (CAS) sub-network. The PFA module generates prototypical features that ensure normal class features remain compact and distinctly separated from anomalies, aided by a pixel-level disparity classification loss that highlights subtle anomalies. The CAS sub-network provides pixel-wise anomaly localization, utilizing pseudo anomalies during training to enhance detection accuracy. Experimental evaluation on benchmark datasets MVTec and MPDD demonstrates that PCSNet achieves impressive image-level AUROC scores of 94.9% and 80.2%, respectively, in an 8-shot setup. Additionally, real-world tests in automotive plastic part inspection validate PCSNet’s effectiveness in practical applications with limited training data. The authors provide the source code for reproducibility at their GitHub repository. <div>
arXiv:2512.15319v1 Announce Type: new 
Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MECAD: A multi-expert architecture for continual anomaly detection</title>
<link>https://arxiv.org/abs/2512.15323</link>
<guid>https://arxiv.org/abs/2512.15323</guid>
<content:encoded><![CDATA[
<div> MECAD, continual anomaly detection, multi-expert architecture, coreset selection, incremental learning  

<br /><br />Summary:  
This paper introduces MECAD, a new approach for continual anomaly detection utilizing a multi-expert architecture. The system dynamically assigns experts to specific object classes by analyzing feature similarity, which allows for more specialized detection tailored to different categories. To retain knowledge of previously encountered classes, MECAD incorporates efficient memory management strategies. Key to its incremental learning capability is the use of an optimized coreset selection method coupled with a specialized replay buffer, enabling model updates without full retraining. Experimental evaluation on the MVTec AD dataset, which includes 15 diverse object categories, shows that the optimal configuration with five experts achieves an average AUROC score of 0.8259. This performance not only demonstrates strong anomaly detection ability but also significantly reduces knowledge degradation compared to single-expert models. The framework optimally balances computational efficiency, retention of specialized knowledge, and adaptability. Such a balance makes MECAD particularly suitable for industrial applications where product types and related anomaly patterns continuously evolve, requiring systems that can learn incrementally and maintain high detection performance over time. <div>
arXiv:2512.15323v1 Announce Type: new 
Abstract: In this paper we propose MECAD, a novel approach for continual anomaly detection using a multi-expert architecture. Our system dynamically assigns experts to object classes based on feature similarity and employs efficient memory management to preserve the knowledge of previously seen classes. By leveraging an optimized coreset selection and a specialized replay buffer mechanism, we enable incremental learning without requiring full model retraining. Our experimental evaluation on the MVTec AD dataset demonstrates that the optimal 5-expert configuration achieves an average AUROC of 0.8259 across 15 diverse object categories while significantly reducing knowledge degradation compared to single-expert approaches. This framework balances computational efficiency, specialized knowledge retention, and adaptability, making it well-suited for industrial environments with evolving product types.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.15326</link>
<guid>https://arxiv.org/abs/2512.15326</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge distillation, anomaly detection, image restoration, masked reverse knowledge distillation, overgeneralization  

<br /><br />Summary:  
This paper addresses the overgeneralization issue in knowledge distillation-based image anomaly detection and localization, which arises due to similarities between input and supervisory signals. To overcome this, the authors propose a novel approach called masked reverse knowledge distillation (MRKD). MRKD leverages two key techniques: image-level masking (ILM) and feature-level masking (FLM). ILM aids in capturing global contextual information by distinguishing input signals from supervisory signals, effectively transforming image reconstruction into an image restoration task. FLM further enhances the approach by injecting synthetic feature-level anomalies, ensuring that the learned representations retain sufficient local details. Together, these strategies strengthen the network's capacity to capture image context while reducing the likelihood of overgeneralization. Experimental results on the MVTec anomaly detection dataset demonstrate that MRKD achieves outstanding performance, with an image-level AU-ROC of 98.9%, pixel-level AU-ROC of 98.4%, and AU-PRO of 95.3%. Additionally, comprehensive ablation studies confirm the effectiveness of MRKD in mitigating overgeneralization, highlighting its superiority over existing methods in anomaly detection and localization tasks. <div>
arXiv:2512.15326v1 Announce Type: new 
Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-based module for accurately reading linear scales in a laboratory</title>
<link>https://arxiv.org/abs/2512.15327</link>
<guid>https://arxiv.org/abs/2512.15327</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-based models, quantitative measurements, linear scale reading, syringe measurement, robotic autonomy<br /><br />Summary:<br />1. Vision-based models have rapidly advanced and can perform tasks such as object detection, image classification, and instance segmentation with high accuracy.<br />2. However, models capable of extracting precise quantitative measurements from images, akin to human visual interpretation, remain uncommon.<br />3. For robots to operate fully autonomously in laboratory settings, they must develop basic skills including navigation, object handling, sample preparation, and reading measurements from instruments.<br />4. The paper proposes a human-inspired method to read measurements from linear scales, using syringes and measuring cylinders as test cases.<br />5. To handle syringes in random orientations, image transformations are applied to correct the orientation.<br />6. The system isolates the region containing the linear scale to enhance efficiency and robustness.<br />7. Feature extraction identifies key elements such as major markers, digit labels, and the level indicator's position.<br />8. These features are combined to compute the final measurement reading.<br />9. The system's readings were compared with human readings of the same instances, showing accurate correspondence, validating the approach for robotic autonomous measurement reading tasks. <div>
arXiv:2512.15327v1 Announce Type: new 
Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</title>
<link>https://arxiv.org/abs/2512.15340</link>
<guid>https://arxiv.org/abs/2512.15340</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D conversational head generation, causal modeling, multimodal fusion, TIMAR, diffusion head  

<br /><br />Summary:  
This paper introduces TIMAR (Turn-level Interleaved Masked AutoRegression), a novel causal framework designed for generating 3D conversational head movements that capture natural human interaction dynamics. 1) It addresses the bidirectional dynamics of human conversation by modeling speech and nonverbal cues such as head nods, gaze shifts, and facial expressions in an interleaved, turn-level manner rather than treating talking and listening as separate processes. 2) TIMAR utilizes multimodal fusion within each conversational turn, integrating audio and visual context effectively, and applies turn-level causal attention to maintain temporal coherence and accumulate conversational history. 3) A lightweight diffusion-based prediction module generates continuous and expressive 3D head dynamics that reflect both coordination between interlocutors and individual expressive variability. 4) The model was evaluated on the DualTalk benchmark, where it demonstrated significant improvements by reducing Fréchet Distance and mean squared error by 15-30%, indicating better realism and accuracy in head movement synthesis. 5) Furthermore, TIMAR maintains its performance on out-of-distribution data, highlighting robust generalization. The authors plan to release the source code on GitHub, facilitating further research and applications in expressive avatars and interactive robotics. <div>
arXiv:2512.15340v1 Announce Type: new 
Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fr\'echet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</title>
<link>https://arxiv.org/abs/2512.15347</link>
<guid>https://arxiv.org/abs/2512.15347</guid>
<content:encoded><![CDATA[
<div> Group Relative Policy Optimization, Reward Clustering, Optimal Variance Filtering, Proactive GRPO, Trajectory Pruning

<br /><br />Summary:  
This paper addresses the challenge of computational inefficiency in Group Relative Policy Optimization (GRPO) caused by large group sizes. First, the authors identify a "reward clustering" phenomenon where many trajectories tend to converge around the group mean reward, which limits optimization improvements. Second, they propose Optimal Variance Filtering (OVF), a heuristic method that selects a high-variance subset of trajectories, which can achieve better performance than using the whole unfiltered group. However, OVF as a static, post-sampling filter still incurs computational waste by sampling many trajectories that are ultimately discarded. To tackle this, the authors introduce Pro-GRPO, a dynamic framework that incorporates latent feature-based trajectory pruning directly into the sampling process through early termination of clustered reward trajectories. This reduces unnecessary computations. Additionally, Pro-GRPO uses an "Expand-and-Prune" strategy: it initially increases the sampling group size to enhance diversity and then applies multi-step OVF on latent representations to avoid heavy computational costs. Extensive experiments on diffusion- and flow-based generative models show that Pro-GRPO is a general and efficient approach to improve GRPO optimization while minimizing computational overhead. <div>
arXiv:2512.15347v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</title>
<link>https://arxiv.org/abs/2512.15369</link>
<guid>https://arxiv.org/abs/2512.15369</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic segmentation, bridges, infrastructure inspection, domain gap, deep learning

<br /><br />Summary:  
This article introduces a novel dataset specifically designed for 3D semantic segmentation of bridges, addressing a critical need in infrastructure inspection and maintenance. The dataset includes high-resolution 3D scans of diverse bridge structures from multiple countries, with detailed semantic labeling for each structural component. Its primary goal is to facilitate accurate and automated segmentation of bridge parts, thereby improving structural health monitoring practices. To assess the dataset's usefulness, the authors evaluate the performance of three state-of-the-art 3D deep learning architectures on this task. Additionally, the paper investigates the domain gap caused by varying sensor technologies used to acquire the data, providing insights into how sensor diversity affects model accuracy. The study quantifies this domain gap and observes that performance degradation can be as high as 11.4% in mean Intersection over Union (mIoU) metrics when models are tested across data from different sensors. Despite this, all tested architectures demonstrate robust performance on the new dataset, indicating its practical value for advancing automated bridge inspection methods. This work contributes to improving reliability and automation in monitoring the structural integrity of bridges, which carries significant importance for public safety and infrastructure maintenance. <div>
arXiv:2512.15369v1 Announce Type: new 
Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Recognition in Signers</title>
<link>https://arxiv.org/abs/2512.15376</link>
<guid>https://arxiv.org/abs/2512.15376</guid>
<content:encoded><![CDATA[
<div> keywords: sign language, emotion recognition, cross-lingual, dataset, hand motion  

<br /><br />Summary:  
This paper tackles two main challenges in recognizing emotions in sign language: the overlap between grammatical and affective facial expressions, and the scarcity of data for model training. To address these, the authors introduce eJSL, a new benchmark dataset for Japanese Sign Language emotion recognition, alongside BOBSL, a large British Sign Language dataset with subtitles, enabling a cross-lingual approach. The eJSL dataset consists of 1,092 video clips, featuring two signers expressing 78 distinct utterances across seven emotional states. The study empirically shows that leveraging textual emotion recognition from spoken language data helps mitigate the issue of limited sign language data. Additionally, it demonstrates that selecting appropriate temporal segments significantly impacts recognition performance. The incorporation of hand motion information further enhances the accuracy of emotion recognition in signers. Overall, the paper establishes a stronger baseline for emotion recognition in sign language than current spoken language large language models (LLMs), highlighting the benefits of cross-modal and cross-lingual strategies combined with fine-tuned temporal and motion cues. <div>
arXiv:2512.15376v1 Announce Type: new 
Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</title>
<link>https://arxiv.org/abs/2512.15386</link>
<guid>https://arxiv.org/abs/2512.15386</guid>
<content:encoded><![CDATA[
arXiv:2512.15386v1 Announce Type: new 
Abstract: Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[
arXiv:2512.15396v1 Announce Type: new 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</title>
<link>https://arxiv.org/abs/2512.15410</link>
<guid>https://arxiv.org/abs/2512.15410</guid>
<content:encoded><![CDATA[
arXiv:2512.15410v1 Announce Type: new 
Abstract: Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</title>
<link>https://arxiv.org/abs/2512.15423</link>
<guid>https://arxiv.org/abs/2512.15423</guid>
<content:encoded><![CDATA[
arXiv:2512.15423v1 Announce Type: new 
Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-GUI Technical Report</title>
<link>https://arxiv.org/abs/2512.15431</link>
<guid>https://arxiv.org/abs/2512.15431</guid>
<content:encoded><![CDATA[
arXiv:2512.15431v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning</title>
<link>https://arxiv.org/abs/2512.15433</link>
<guid>https://arxiv.org/abs/2512.15433</guid>
<content:encoded><![CDATA[
arXiv:2512.15433v1 Announce Type: new 
Abstract: Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence</title>
<link>https://arxiv.org/abs/2512.15445</link>
<guid>https://arxiv.org/abs/2512.15445</guid>
<content:encoded><![CDATA[
arXiv:2512.15445v1 Announce Type: new 
Abstract: Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</title>
<link>https://arxiv.org/abs/2512.15480</link>
<guid>https://arxiv.org/abs/2512.15480</guid>
<content:encoded><![CDATA[
arXiv:2512.15480v1 Announce Type: new 
Abstract: Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting</title>
<link>https://arxiv.org/abs/2512.15488</link>
<guid>https://arxiv.org/abs/2512.15488</guid>
<content:encoded><![CDATA[
arXiv:2512.15488v1 Announce Type: new 
Abstract: Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</title>
<link>https://arxiv.org/abs/2512.15505</link>
<guid>https://arxiv.org/abs/2512.15505</guid>
<content:encoded><![CDATA[
arXiv:2512.15505v1 Announce Type: new 
Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.15508</link>
<guid>https://arxiv.org/abs/2512.15508</guid>
<content:encoded><![CDATA[
arXiv:2512.15508v1 Announce Type: new 
Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</title>
<link>https://arxiv.org/abs/2512.15512</link>
<guid>https://arxiv.org/abs/2512.15512</guid>
<content:encoded><![CDATA[
arXiv:2512.15512v1 Announce Type: new 
Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations</title>
<link>https://arxiv.org/abs/2512.15524</link>
<guid>https://arxiv.org/abs/2512.15524</guid>
<content:encoded><![CDATA[
arXiv:2512.15524v1 Announce Type: new 
Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</title>
<link>https://arxiv.org/abs/2512.15528</link>
<guid>https://arxiv.org/abs/2512.15528</guid>
<content:encoded><![CDATA[
arXiv:2512.15528v1 Announce Type: new 
Abstract: Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain</title>
<link>https://arxiv.org/abs/2512.15531</link>
<guid>https://arxiv.org/abs/2512.15531</guid>
<content:encoded><![CDATA[
arXiv:2512.15531v1 Announce Type: new 
Abstract: The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BLANKET: Anonymizing Faces in Infant Video Recordings</title>
<link>https://arxiv.org/abs/2512.15542</link>
<guid>https://arxiv.org/abs/2512.15542</guid>
<content:encoded><![CDATA[
arXiv:2512.15542v1 Announce Type: new 
Abstract: Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.15560</link>
<guid>https://arxiv.org/abs/2512.15560</guid>
<content:encoded><![CDATA[
arXiv:2512.15560v1 Announce Type: new 
Abstract: The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation</title>
<link>https://arxiv.org/abs/2512.15564</link>
<guid>https://arxiv.org/abs/2512.15564</guid>
<content:encoded><![CDATA[
arXiv:2512.15564v1 Announce Type: new 
Abstract: Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</title>
<link>https://arxiv.org/abs/2512.15577</link>
<guid>https://arxiv.org/abs/2512.15577</guid>
<content:encoded><![CDATA[
arXiv:2512.15577v1 Announce Type: new 
Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title>
<link>https://arxiv.org/abs/2512.15581</link>
<guid>https://arxiv.org/abs/2512.15581</guid>
<content:encoded><![CDATA[
arXiv:2512.15581v1 Announce Type: new 
Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision</title>
<link>https://arxiv.org/abs/2512.15599</link>
<guid>https://arxiv.org/abs/2512.15599</guid>
<content:encoded><![CDATA[
arXiv:2512.15599v1 Announce Type: new 
Abstract: We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition</title>
<link>https://arxiv.org/abs/2512.15603</link>
<guid>https://arxiv.org/abs/2512.15603</guid>
<content:encoded><![CDATA[
arXiv:2512.15603v1 Announce Type: new 
Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multi-view Camera Calibration from Dense Matches</title>
<link>https://arxiv.org/abs/2512.15608</link>
<guid>https://arxiv.org/abs/2512.15608</guid>
<content:encoded><![CDATA[
arXiv:2512.15608v1 Announce Type: new 
Abstract: Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images</title>
<link>https://arxiv.org/abs/2512.15618</link>
<guid>https://arxiv.org/abs/2512.15618</guid>
<content:encoded><![CDATA[
arXiv:2512.15618v1 Announce Type: new 
Abstract: With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</title>
<link>https://arxiv.org/abs/2512.15621</link>
<guid>https://arxiv.org/abs/2512.15621</guid>
<content:encoded><![CDATA[
arXiv:2512.15621v1 Announce Type: new 
Abstract: Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Physically-Based Sky-Modeling For Image Based Lighting</title>
<link>https://arxiv.org/abs/2512.15632</link>
<guid>https://arxiv.org/abs/2512.15632</guid>
<content:encoded><![CDATA[
arXiv:2512.15632v1 Announce Type: new 
Abstract: Accurate environment maps are a key component for rendering photorealistic outdoor scenes with coherent illumination. They enable captivating visual arts, immersive virtual reality, and a wide range of engineering and scientific applications. Recent works have extended sky-models to be more comprehensive and inclusive of cloud formations but, as we demonstrate, existing methods fall short in faithfully recreating natural skies. Though in recent years the visual quality of DNN-generated High Dynamic Range Imagery (HDRI) has greatly improved, the environment maps generated by DNN sky-models do not re-light scenes with the same tones, shadows, and illumination as physically captured HDR imagery. In this work, we demonstrate progress in HDR literature to be tangential to sky-modelling as current works cannot support both photorealism and the 22 f-stops required for the Full Dynamic Range (FDR) of outdoor illumination. We achieve this by proposing AllSky, a flexible all-weather sky-model learned directly from physically captured HDRI which we leverage to study the input modalities, tonemapping, conditioning, and evaluation of sky-models. Per user-controlled positioning of the sun and cloud formations, AllSky expands on current functionality by allowing for intuitive user control over environment maps and achieves state-of-the-art sky-model performance. Through our proposed evaluation, we demonstrate existing DNN sky-models are not interchangeable with physically captured HDRI or parametric sky-models, with current limitations being prohibitive of scalability and accurate illumination in downstream applications
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</title>
<link>https://arxiv.org/abs/2512.15635</link>
<guid>https://arxiv.org/abs/2512.15635</guid>
<content:encoded><![CDATA[
arXiv:2512.15635v1 Announce Type: new 
Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization</title>
<link>https://arxiv.org/abs/2512.15644</link>
<guid>https://arxiv.org/abs/2512.15644</guid>
<content:encoded><![CDATA[
arXiv:2512.15644v1 Announce Type: new 
Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift</title>
<link>https://arxiv.org/abs/2512.15647</link>
<guid>https://arxiv.org/abs/2512.15647</guid>
<content:encoded><![CDATA[
arXiv:2512.15647v1 Announce Type: new 
Abstract: Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
<link>https://arxiv.org/abs/2512.15649</link>
<guid>https://arxiv.org/abs/2512.15649</guid>
<content:encoded><![CDATA[
arXiv:2512.15649v1 Announce Type: new 
Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Synthetic Augmentation further improves Corruption Robustness</title>
<link>https://arxiv.org/abs/2512.15675</link>
<guid>https://arxiv.org/abs/2512.15675</guid>
<content:encoded><![CDATA[
arXiv:2512.15675v1 Announce Type: new 
Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</title>
<link>https://arxiv.org/abs/2512.15693</link>
<guid>https://arxiv.org/abs/2512.15693</guid>
<content:encoded><![CDATA[
arXiv:2512.15693v1 Announce Type: new 
Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</title>
<link>https://arxiv.org/abs/2512.15701</link>
<guid>https://arxiv.org/abs/2512.15701</guid>
<content:encoded><![CDATA[
arXiv:2512.15701v1 Announce Type: new 
Abstract: Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</title>
<link>https://arxiv.org/abs/2512.15702</link>
<guid>https://arxiv.org/abs/2512.15702</guid>
<content:encoded><![CDATA[
arXiv:2512.15702v1 Announce Type: new 
Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</title>
<link>https://arxiv.org/abs/2512.15707</link>
<guid>https://arxiv.org/abs/2512.15707</guid>
<content:encoded><![CDATA[
arXiv:2512.15707v1 Announce Type: new 
Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View Foundation Models</title>
<link>https://arxiv.org/abs/2512.15708</link>
<guid>https://arxiv.org/abs/2512.15708</guid>
<content:encoded><![CDATA[
arXiv:2512.15708v1 Announce Type: new 
Abstract: Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</title>
<link>https://arxiv.org/abs/2512.15711</link>
<guid>https://arxiv.org/abs/2512.15711</guid>
<content:encoded><![CDATA[
arXiv:2512.15711v1 Announce Type: new 
Abstract: We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15713</link>
<guid>https://arxiv.org/abs/2512.15713</guid>
<content:encoded><![CDATA[
arXiv:2512.15713v1 Announce Type: new 
Abstract: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Pursuit of Pixel Supervision for Visual Pre-training</title>
<link>https://arxiv.org/abs/2512.15715</link>
<guid>https://arxiv.org/abs/2512.15715</guid>
<content:encoded><![CDATA[
arXiv:2512.15715v1 Announce Type: new 
Abstract: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatia: Video Generation with Updatable Spatial Memory</title>
<link>https://arxiv.org/abs/2512.15716</link>
<guid>https://arxiv.org/abs/2512.15716</guid>
<content:encoded><![CDATA[
arXiv:2512.15716v1 Announce Type: new 
Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
arXiv:2512.14706v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
arXiv:2512.14712v1 Announce Type: cross 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
arXiv:2512.14732v1 Announce Type: cross 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</title>
<link>https://arxiv.org/abs/2512.14735</link>
<guid>https://arxiv.org/abs/2512.14735</guid>
<content:encoded><![CDATA[
arXiv:2512.14735v1 Announce Type: cross 
Abstract: This paper proposes PyFi, a novel framework for pyramid-like financial image understanding that enables vision language models (VLMs) to reason through question chains in a progressive, simple-to-complex manner. At the core of PyFi is PyFi-600K, a dataset comprising 600K financial question-answer pairs organized into a reasoning pyramid: questions at the base require only basic perception, while those toward the apex demand increasing levels of capability in financial visual understanding and expertise. This data is scalable because it is synthesized without human annotations, using PyFi-adv, a multi-agent adversarial mechanism under the Monte Carlo Tree Search (MCTS) paradigm, in which, for each image, a challenger agent competes with a solver agent by generating question chains that progressively probe deeper capability levels in financial visual reasoning. Leveraging this dataset, we present fine-grained, hierarchical, and comprehensive evaluations of advanced VLMs in the financial domain. Moreover, fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on the pyramid-structured question chains enables these models to answer complex financial questions by decomposing them into sub-questions with gradually increasing reasoning demands, yielding average accuracy improvements of 19.52% and 8.06%, respectively, on the dataset. All resources of code, dataset and models are available at: https://github.com/AgenticFinLab/PyFi .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[
arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</title>
<link>https://arxiv.org/abs/2512.14797</link>
<guid>https://arxiv.org/abs/2512.14797</guid>
<content:encoded><![CDATA[
arXiv:2512.14797v1 Announce Type: cross 
Abstract: Advanced Ovarian Cancer (AOC) is often diagnosed at an advanced stage with peritoneal carcinosis (PC). Fagotti score (FS) assessment at diagnostic laparoscopy (DL) guides treatment planning by estimating surgical resectability, but its subjective and operator-dependent nature limits reproducibility and widespread use. Videos of patients undergoing DL with concomitant FS assessments at a referral center were retrospectively collected and divided into a development dataset, for data annotation, AI training and evaluation, and an independent test dataset, for internal validation. In the development dataset, FS-relevant frames were manually annotated for anatomical structures and PC. Deep learning models were trained to automatically identify FS-relevant frames, segment structures and PC, and predict video-level FS and indication to surgery (ItS). AI performance was evaluated using Dice score for segmentation, F1-scores for anatomical stations (AS) and ItS prediction, and root mean square error (RMSE) for final FS estimation. In the development dataset, the segmentation model trained on 7,311 frames, achieved Dice scores of 70$\pm$3% for anatomical structures and 56$\pm$3% for PC. Video-level AS classification achieved F1-scores of 74$\pm$3% and 73$\pm$4%, FS prediction showed normalized RMSE values of 1.39$\pm$0.18 and 1.15$\pm$0.08, and ItS reached F1-scores of 80$\pm$8% and 80$\pm$2% in the development (n=101) and independent test datasets (n=50), respectively. This is the first AI model to predict the feasibility of cytoreductive surgery providing automated FS estimation from DL videos. Its reproducible and reliable performance across datasets suggests that AI can support surgeons through standardized intraoperative tumor burden assessment and clinical decision-making in AOC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</title>
<link>https://arxiv.org/abs/2512.14880</link>
<guid>https://arxiv.org/abs/2512.14880</guid>
<content:encoded><![CDATA[
arXiv:2512.14880v1 Announce Type: cross 
Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</title>
<link>https://arxiv.org/abs/2512.14989</link>
<guid>https://arxiv.org/abs/2512.14989</guid>
<content:encoded><![CDATA[
arXiv:2512.14989v1 Announce Type: cross 
Abstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gaussian Parameterization for Direct Atomic Structure Identification in Electron Tomography</title>
<link>https://arxiv.org/abs/2512.15034</link>
<guid>https://arxiv.org/abs/2512.15034</guid>
<content:encoded><![CDATA[
arXiv:2512.15034v1 Announce Type: cross 
Abstract: Atomic electron tomography (AET) enables the determination of 3D atomic structures by acquiring a sequence of 2D tomographic projection measurements of a particle and then computationally solving for its underlying 3D representation. Classical tomography algorithms solve for an intermediate volumetric representation that is post-processed into the atomic structure of interest. In this paper, we reformulate the tomographic inverse problem to solve directly for the locations and properties of individual atoms. We parameterize an atomic structure as a collection of Gaussians, whose positions and properties are learnable. This representation imparts a strong physical prior on the learned structure, which we show yields improved robustness to real-world imaging artifacts. Simulated experiments and a proof-of-concept result on experimentally-acquired data confirm our method's potential for practical applications in materials characterization and analysis with Transmission Electron Microscopy (TEM). Our code is available at https://github.com/nalinimsingh/gaussian-atoms.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</title>
<link>https://arxiv.org/abs/2512.15047</link>
<guid>https://arxiv.org/abs/2512.15047</guid>
<content:encoded><![CDATA[
arXiv:2512.15047v1 Announce Type: cross 
Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</title>
<link>https://arxiv.org/abs/2512.15061</link>
<guid>https://arxiv.org/abs/2512.15061</guid>
<content:encoded><![CDATA[
arXiv:2512.15061v1 Announce Type: cross 
Abstract: This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization</title>
<link>https://arxiv.org/abs/2512.15111</link>
<guid>https://arxiv.org/abs/2512.15111</guid>
<content:encoded><![CDATA[
arXiv:2512.15111v1 Announce Type: cross 
Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.15195</link>
<guid>https://arxiv.org/abs/2512.15195</guid>
<content:encoded><![CDATA[
arXiv:2512.15195v1 Announce Type: cross 
Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Preprocessing for Image Compression with Pre-trained Diffusion Models</title>
<link>https://arxiv.org/abs/2512.15270</link>
<guid>https://arxiv.org/abs/2512.15270</guid>
<content:encoded><![CDATA[
arXiv:2512.15270v1 Announce Type: cross 
Abstract: Preprocessing is a well-established technique for optimizing compression, yet existing methods are predominantly Rate-Distortion (R-D) optimized and constrained by pixel-level fidelity. This work pioneers a shift towards Rate-Perception (R-P) optimization by, for the first time, adapting a large-scale pre-trained diffusion model for compression preprocessing. We propose a two-stage framework: first, we distill the multi-step Stable Diffusion 2.1 into a compact, one-step image-to-image model using Consistent Score Identity Distillation (CiD). Second, we perform a parameter-efficient fine-tuning of the distilled model's attention modules, guided by a Rate-Perception loss and a differentiable codec surrogate. Our method seamlessly integrates with standard codecs without any modification and leverages the model's powerful generative priors to enhance texture and mitigate artifacts. Experiments show substantial R-P gains, achieving up to a 30.13% BD-rate reduction in DISTS on the Kodak dataset and delivering superior subjective visual quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preprocessing Framework for Video Machine Vision under Compression</title>
<link>https://arxiv.org/abs/2512.15331</link>
<guid>https://arxiv.org/abs/2512.15331</guid>
<content:encoded><![CDATA[
arXiv:2512.15331v1 Announce Type: cross 
Abstract: There has been a growing trend in compressing and transmitting videos from terminals for machine vision tasks. Nevertheless, most video coding optimization method focus on minimizing distortion according to human perceptual metrics, overlooking the heightened demands posed by machine vision systems. In this paper, we propose a video preprocessing framework tailored for machine vision tasks to address this challenge. The proposed method incorporates a neural preprocessor which retaining crucial information for subsequent tasks, resulting in the boosting of rate-accuracy performance. We further introduce a differentiable virtual codec to provide constraints on rate and distortion during the training stage. We directly apply widely used standard codecs for testing. Therefore, our solution can be easily applied to real-world scenarios. We conducted extensive experiments evaluating our compression method on two typical downstream tasks with various backbone networks. The experimental results indicate that our approach can save over 15% of bitrate compared to using only the standard codec anchor version.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[
arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</title>
<link>https://arxiv.org/abs/2512.15411</link>
<guid>https://arxiv.org/abs/2512.15411</guid>
<content:encoded><![CDATA[
arXiv:2512.15411v1 Announce Type: cross 
Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbol{\pi}_{0}$, $\boldsymbol{\pi}_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoFlow: Solution Flow Models for One-Step Generative Modeling</title>
<link>https://arxiv.org/abs/2512.15657</link>
<guid>https://arxiv.org/abs/2512.15657</guid>
<content:encoded><![CDATA[
arXiv:2512.15657v1 Announce Type: cross 
Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[
arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</title>
<link>https://arxiv.org/abs/2312.09245</link>
<guid>https://arxiv.org/abs/2312.09245</guid>
<content:encoded><![CDATA[
arXiv:2312.09245v3 Announce Type: replace 
Abstract: Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for High-Quality Radiance Fields Reconstruction</title>
<link>https://arxiv.org/abs/2406.20066</link>
<guid>https://arxiv.org/abs/2406.20066</guid>
<content:encoded><![CDATA[
arXiv:2406.20066v2 Announce Type: replace 
Abstract: NeRF-based methods reconstruct 3D scenes by building a radiance field with implicit or explicit representations. While NeRF-based methods can perform novel view synthesis (NVS) at arbitrary scale, the performance in high-resolution novel view synthesis (HRNVS) with low-resolution (LR) optimization often results in oversmoothing. On the other hand, single-image super-resolution (SR) aims to enhance LR images to HR counterparts but lacks multi-view consistency. To address these challenges, we propose Arbitrary-Scale Super-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel view synthesis (SRNVS). We propose an attention-based VoxelGridSR model to directly perform 3D super-resolution (SR) on the optimized volume. Our model is trained on diverse scenes to ensure generalizability. For unseen scenes trained with LR views, we then can directly apply our VoxelGridSR to further refine the volume and achieve multi-view consistent SR. We demonstrate quantitative and qualitatively that the proposed method achieves significant performance in SRNVS.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title>
<link>https://arxiv.org/abs/2410.01609</link>
<guid>https://arxiv.org/abs/2410.01609</guid>
<content:encoded><![CDATA[
arXiv:2410.01609v2 Announce Type: replace 
Abstract: Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes \textbf{SynJAC} (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2411.07167</link>
<guid>https://arxiv.org/abs/2411.07167</guid>
<content:encoded><![CDATA[
arXiv:2411.07167v2 Announce Type: replace 
Abstract: Facial landmark detection is a fundamental problem in computer vision for many downstream applications. This paper introduces a new facial landmark detector based on vision transformers, which consists of two unique designs: Dual Vision Transformer (D-ViT) and Long Skip Connections (LSC). Based on the observation that the channel dimension of feature maps essentially represents the linear bases of the heatmap space, we propose learning the interconnections between these linear bases to model the inherent geometric relations among landmarks via Channel-split ViT. We integrate such channel-split ViT into the standard vision transformer (i.e., spatial-split ViT), forming our Dual Vision Transformer to constitute the prediction blocks. We also suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby preventing useful information from being discarded by intermediate supervision. Extensive experiments are conducted to evaluate the performance of our proposal on the widely used benchmarks, i.e., WFLW, COFW, and 300W, demonstrating that our model outperforms the previous SOTAs across all three benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Robust and Accurate Adversarial Camouflage Generation against Vehicle Detectors</title>
<link>https://arxiv.org/abs/2411.10029</link>
<guid>https://arxiv.org/abs/2411.10029</guid>
<content:encoded><![CDATA[
arXiv:2411.10029v2 Announce Type: replace 
Abstract: Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, End-to-End Neural Renderer Plus (E2E-NRP), which can accurately optimize and project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the E2E-NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA-final outperforms existing methods in both simulation and real-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>If you can describe it, they can see it: Cross-Modal Learning of Visual Concepts from Textual Descriptions</title>
<link>https://arxiv.org/abs/2411.15611</link>
<guid>https://arxiv.org/abs/2411.15611</guid>
<content:encoded><![CDATA[
arXiv:2411.15611v2 Announce Type: replace 
Abstract: Humans can visualize new and unknown concepts from their natural language description, based on their experience and previous knowledge. Insipired by this, we present a way to extend this ability to Vision-Language Models (VLMs), teaching them novel concepts by only using a textual description. We refer to this approach as Knowledge Transfer (KT). Our hypothesis is that the knowledge of a pre-trained VLM can be re-used to represent previously unknown concepts. Provided with a textual description of the novel concept, KT works by aligning relevant features of the visual encoder, obtained through model inversion, to its text representation. Differently from approaches relying on visual examples or external generative models, KT transfers knowledge within the same VLM by injecting visual knowledge directly from the text. Through an extensive evaluation on several VLM tasks, including classification, segmentation, image-text retrieval, and captioning, we show that: 1) KT can efficiently introduce new visual concepts from a single textual description; 2) the same principle can be used to refine the representation of existing concepts; and 3) KT significantly improves the performance of zero-shot VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do MLLMs Exhibit Human-like Perceptual Behaviors? HVSBench: A Benchmark for MLLM Alignment with Human Perceptual Behavior</title>
<link>https://arxiv.org/abs/2412.09603</link>
<guid>https://arxiv.org/abs/2412.09603</guid>
<content:encoded><![CDATA[
arXiv:2412.09603v3 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) excel at many vision tasks, it is unknown if they exhibit human-like perceptual behaviors. To evaluate this, we introduce HVSBench, the first large-scale benchmark with over 85,000 samples designed to test MLLM alignment with the human visual system (HVS). The benchmark covers 13 categories across 5 key fields: Prominence, Subitizing, Prioritizing, Free-Viewing, and Searching. Our comprehensive evaluation reveals a significant perceptual gap: even state-of-the-art MLLMs achieve only moderate results. In contrast, human participants demonstrate strong performance, significantly outperforming all models. This underscores the high quality of HVSBench and the need for more human-aligned AI. We believe our benchmark will be a critical tool for developing the next generation of explainable MLLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MS-Temba: Multi-Scale Temporal Mamba for Understanding Long Untrimmed Videos</title>
<link>https://arxiv.org/abs/2501.06138</link>
<guid>https://arxiv.org/abs/2501.06138</guid>
<content:encoded><![CDATA[
arXiv:2501.06138v3 Announce Type: replace 
Abstract: Temporal Action Detection (TAD) in untrimmed videos poses significant challenges, particularly for Activities of Daily Living (ADL) requiring models to (1) process long-duration videos, (2) capture temporal variations in actions, and (3) simultaneously detect dense overlapping actions. Existing CNN and Transformer-based approaches, struggle to jointly capture fine-grained detail and long-range structure at scale. State-space Model (SSM) based Mamba offers powerful long-range modeling, but naive application to TAD collapses fine-grained temporal structure and fails to account for the challenges inherent to TAD. To this end, we propose Multi-Scale Temporal Mamba (MS-Temba), which extends Mamba to TAD with newly introduced dilated SSMs. Each Temba block, comprising dilated SSMs coupled with our proposed additional losses, enables the learning of discriminative representations across temporal scales. A lightweight Multi-scale Mamba Fuser then unifies these multi-scale features via SSM-based aggregation, yielding precise action-boundary localization. With only 17M parameters, MS-Temba achieves state-of-the-art performance on densely labeled ADL benchmarks TSU & Charades, and further generalizes to long-form video summarization, setting new state-of-the-art results on TVSum & SumMe.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[
arXiv:2501.13439v2 Announce Type: replace 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization</title>
<link>https://arxiv.org/abs/2504.13460</link>
<guid>https://arxiv.org/abs/2504.13460</guid>
<content:encoded><![CDATA[
arXiv:2504.13460v4 Announce Type: replace 
Abstract: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v2 Announce Type: replace 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation</title>
<link>https://arxiv.org/abs/2505.24431</link>
<guid>https://arxiv.org/abs/2505.24431</guid>
<content:encoded><![CDATA[
arXiv:2505.24431v2 Announce Type: replace 
Abstract: 3D point cloud anomaly detection is essential for robust vision systems but is challenged by pose variations and complex geometric anomalies. Existing patch-based methods often suffer from geometric fidelity issues due to discrete voxelization or projection-based representations, limiting fine-grained anomaly localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel framework that integrates 3D anomaly detection and repair by learning a continuous, pose-invariant shape representation. PASDF leverages a Pose Alignment Module for canonicalization and a SDF Network to dynamically incorporate pose, enabling implicit learning of high-fidelity anomaly repair templates from the continuous SDF. This facilitates precise pixel-level anomaly localization through an Anomaly-Aware Scoring Module. Crucially, the continuous 3D representation in PASDF extends beyond detection, facilitating in-situ anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate state-of-the-art performance, achieving high object-level AUROC scores of 80.2% and 90.0%, respectively. These results highlight the effectiveness of continuous geometric representations in advancing 3D anomaly detection and facilitating practical anomaly region repair. The code is available at https://github.com/ZZZBBBZZZ/PASDF to support further research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</title>
<link>https://arxiv.org/abs/2506.02976</link>
<guid>https://arxiv.org/abs/2506.02976</guid>
<content:encoded><![CDATA[
arXiv:2506.02976v3 Announce Type: replace 
Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title>
<link>https://arxiv.org/abs/2506.09677</link>
<guid>https://arxiv.org/abs/2506.09677</guid>
<content:encoded><![CDATA[
arXiv:2506.09677v2 Announce Type: replace 
Abstract: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binarization-Aware Adjuster: A Theoretical Framework for Bridging Continuous Optimization and Discrete Inference with Application to Edge Detection</title>
<link>https://arxiv.org/abs/2506.12460</link>
<guid>https://arxiv.org/abs/2506.12460</guid>
<content:encoded><![CDATA[
arXiv:2506.12460v2 Announce Type: replace 
Abstract: In machine learning, discrete decision-making tasks exhibit a fundamental inconsistency between training and inference: models are optimized using continuous-valued outputs, yet evaluated through discrete predictions. This discrepancy arises from the non-differentiability of discretization operations, weakening the alignment between optimization objectives and practical decision outcomes. To address this, we present a theoretical framework for constructing a Binarization-Aware Adjuster (BAA) that integrates binarization behavior directly into gradient-based learning. Central to the approach is a Distance Weight Function (DWF) that dynamically modulates pixel-wise loss contributions based on prediction correctness and proximity to the decision boundary, thereby emphasizing decision-critical regions while de-emphasizing confidently correct samples. Furthermore, a self-adaptive threshold estimation procedure is introduced to better match optimization dynamics with inference conditions. As one of its applications, we implement experiments on the edge detection (ED) task, which also demonstrate the effectiveness of the proposed method experimentally. Beyond binary decision tasks and ED, the proposed framework provides a general strategy for aligning continuous optimization with discrete evaluation and can be extended to multi-valued decision processes in broader structured prediction problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness</title>
<link>https://arxiv.org/abs/2507.02314</link>
<guid>https://arxiv.org/abs/2507.02314</guid>
<content:encoded><![CDATA[
arXiv:2507.02314v3 Announce Type: replace 
Abstract: Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Navigation Refinement: Achieving Lane-Level Guidance by Associating Standard-Definition and Online Perception Maps</title>
<link>https://arxiv.org/abs/2507.07487</link>
<guid>https://arxiv.org/abs/2507.07487</guid>
<content:encoded><![CDATA[
arXiv:2507.07487v3 Announce Type: replace 
Abstract: Lane-level navigation is critical for geographic information systems and navigation-based tasks, offering finer-grained guidance than road-level navigation by standard definition (SD) maps. However, it currently relies on expansive global HD maps that cannot adapt to dynamic road conditions. Recently, online perception (OP) maps have become research hotspots, providing real-time geometry as an alternative, but lack the global topology needed for navigation. To address these issues, Online Navigation Refinement (ONR), a new mission is introduced that refines SD-map-based road-level routes into accurate lane-level navigation by associating SD maps with OP maps. The map-to-map association to handle many-to-one lane-to-road mappings under two key challenges: (1) no public dataset provides lane-to-road correspondences; (2) severe misalignment from spatial fluctuations, semantic disparities, and OP map noise invalidates traditional map matching. For these challenges, We contribute: (1) Online map association dataset (OMA), the first ONR benchmark with 30K scenarios and 2.6M annotated lane vectors; (2) MAT, a transformer with path-aware attention to aligns topology despite spatial fluctuations and semantic disparities and spatial attention for integrates noisy OP features via global context; and (3) NR P-R, a metric evaluating geometric and semantic alignment. Experiments show that MAT outperforms existing methods at 34 ms latency, enabling low-cost and up-to-date lane-level navigation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation</title>
<link>https://arxiv.org/abs/2507.18625</link>
<guid>https://arxiv.org/abs/2507.18625</guid>
<content:encoded><![CDATA[
arXiv:2507.18625v2 Announce Type: replace 
Abstract: Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts</title>
<link>https://arxiv.org/abs/2508.06032</link>
<guid>https://arxiv.org/abs/2508.06032</guid>
<content:encoded><![CDATA[
arXiv:2508.06032v2 Announce Type: replace 
Abstract: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model (obtained by fine-tuning a T2I model on 3D human texture maps) for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments, separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks, and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
arXiv:2508.07981v4 Announce Type: replace 
Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation</title>
<link>https://arxiv.org/abs/2508.08179</link>
<guid>https://arxiv.org/abs/2508.08179</guid>
<content:encoded><![CDATA[
arXiv:2508.08179v2 Announce Type: replace 
Abstract: Human motion generation has found widespread applications in AR/VR, film, sports, and medical rehabilitation, offering a cost-effective alternative to traditional motion capture systems. However, evaluating the fidelity of such generated motions is a crucial, multifaceted task. Although previous approaches have attempted at motion fidelity evaluation using human perception or physical constraints, there remains an inherent gap between human-perceived fidelity and physical feasibility. Moreover, the subjective and coarse binary labeling of human perception further undermines the development of a robust data-driven metric. We address these issues by introducing a physical labeling method. This method evaluates motion fidelity by calculating the minimum modifications needed for a motion to align with physical laws. With this approach, we are able to produce fine-grained, continuous physical alignment annotations that serve as objective ground truth. With these annotations, we propose PP-Motion, a novel data-driven metric to evaluate both physical and perceptual fidelity of human motion. To effectively capture underlying physical priors, we employ Pearson's correlation loss for the training of our metric. Additionally, by incorporating a human-based perceptual fidelity loss, our metric can capture fidelity that simultaneously considers both human perception and physical alignment. Experimental results demonstrate that our metric, PP-Motion, not only aligns with physical laws but also aligns better with human perception of motion fidelity than previous work.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World</title>
<link>https://arxiv.org/abs/2509.16674</link>
<guid>https://arxiv.org/abs/2509.16674</guid>
<content:encoded><![CDATA[
arXiv:2509.16674v3 Announce Type: replace 
Abstract: Text-based Pedestrian Retrieval (TPR) deals with retrieving specific target pedestrians in visual scenes according to natural language descriptions. Although existing methods have achieved progress under constrained settings, interactive retrieval in the open-world scenario still suffers from limited model generalization and insufficient semantic understanding. To address these challenges, we propose FitPro, an open-world interactive zero-shot TPR framework with enhanced semantic comprehension and cross-scene adaptability. FitPro has three innovative components: Feature Contrastive Decoding (FCD), Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval (QHR). The FCD integrates prompt-guided contrastive decoding to generate high-quality structured pedestrian descriptions from denoised images, effectively alleviating semantic drift in zero-shot scenarios. The ISM constructs holistic pedestrian representations from multi-view observations to achieve global semantic modeling in multi-turn interactions, thereby improving robustness against viewpoint shifts and fine-grained variations in descriptions. The QHR dynamically optimizes the retrieval pipeline according to query types, enabling efficient adaptation to multi-modal and multi-view inputs. Extensive experiments on five public datasets and two evaluation protocols demonstrate that FitPro significantly overcomes the generalization limitations and semantic modeling constraints of existing methods in interactive retrieval, paving the way for practical deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title>
<link>https://arxiv.org/abs/2509.21979</link>
<guid>https://arxiv.org/abs/2509.21979</guid>
<content:encoded><![CDATA[
arXiv:2509.21979v3 Announce Type: replace 
Abstract: Visual language models (VLMs) have the potential to transform medical workflows. However, the deployment is limited by sycophancy. Despite this serious threat to patient safety, a systematic benchmark remains lacking. This paper addresses this gap by introducing a Medical benchmark that applies multiple templates to VLMs in a hierarchical medical visual question answering task. We find that current VLMs are highly susceptible to visual cues, with failure rates showing a correlation to model size or overall accuracy. we discover that perceived authority and user mimicry are powerful triggers, suggesting a bias mechanism independent of visual data. To overcome this, we propose a Visual Information Purification for Evidence based Responses (VIPER) strategy that proactively filters out non-evidence-based social cues, thereby reinforcing evidence based reasoning. VIPER reduces sycophancy while maintaining interpretability and consistently outperforms baseline methods, laying the necessary foundation for the robust and secure integration of VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation</title>
<link>https://arxiv.org/abs/2510.03769</link>
<guid>https://arxiv.org/abs/2510.03769</guid>
<content:encoded><![CDATA[
arXiv:2510.03769v3 Announce Type: replace 
Abstract: The increasing size and complexity of medical imaging datasets, particularly in 3D formats, present significant barriers to collaborative research and transferability. This study investigates whether the ZFP compression technique can mitigate these challenges without compromising the performance of automated cerebrovascular segmentation, a critical first step in intracranial aneurysm detection. We apply ZFP in both its error tolerance and fixed-rate modes to a large scale, and one of the most recent, datasets in the literature, 3D medical dataset containing ground-truth vascular segmentations. The segmentation quality on the compressed volumes is rigorously compared to the uncompressed baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance mode--while maintaining a high degree of fidelity, with the mean Dice coefficient remaining high at 0.87656. These results demonstrate that ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration across the community.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.15022</link>
<guid>https://arxiv.org/abs/2510.15022</guid>
<content:encoded><![CDATA[
arXiv:2510.15022v2 Announce Type: replace 
Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like Civit.ai, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our method generates diverse outputs across a wide range of domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset</title>
<link>https://arxiv.org/abs/2510.15742</link>
<guid>https://arxiv.org/abs/2510.15742</guid>
<content:encoded><![CDATA[
arXiv:2510.15742v2 Announce Type: replace 
Abstract: Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping</title>
<link>https://arxiv.org/abs/2510.26569</link>
<guid>https://arxiv.org/abs/2510.26569</guid>
<content:encoded><![CDATA[
arXiv:2510.26569v2 Announce Type: replace 
Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall. The dataset and code are available at https://github.com/ostadabbas/AdSum204.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification</title>
<link>https://arxiv.org/abs/2511.05170</link>
<guid>https://arxiv.org/abs/2511.05170</guid>
<content:encoded><![CDATA[
arXiv:2511.05170v2 Announce Type: replace 
Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abstract 3D Perception for Spatial Intelligence in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.10946</link>
<guid>https://arxiv.org/abs/2511.10946</guid>
<content:encoded><![CDATA[
arXiv:2511.10946v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)</title>
<link>https://arxiv.org/abs/2511.12061</link>
<guid>https://arxiv.org/abs/2511.12061</guid>
<content:encoded><![CDATA[
arXiv:2511.12061v2 Announce Type: replace 
Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching</title>
<link>https://arxiv.org/abs/2511.12998</link>
<guid>https://arxiv.org/abs/2511.12998</guid>
<content:encoded><![CDATA[
arXiv:2511.12998v2 Announce Type: replace 
Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</title>
<link>https://arxiv.org/abs/2511.18806</link>
<guid>https://arxiv.org/abs/2511.18806</guid>
<content:encoded><![CDATA[
arXiv:2511.18806v2 Announce Type: replace 
Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search</title>
<link>https://arxiv.org/abs/2511.18929</link>
<guid>https://arxiv.org/abs/2511.18929</guid>
<content:encoded><![CDATA[
arXiv:2511.18929v3 Announce Type: replace 
Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across plausible futures. To facilitate this study, we propose HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Augmented Contrastive Learning With Soft Mixture of Experts for Blind Super-Resolution of Planetary Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.20045</link>
<guid>https://arxiv.org/abs/2511.20045</guid>
<content:encoded><![CDATA[
arXiv:2511.20045v2 Announce Type: replace 
Abstract: Blind Super-Resolution (BSR) in planetary remote sensing constitutes a highly ill-posed inverse problem, characterized by unknown degradation patterns and a complete absence of ground-truth supervision. Existing unsupervised approaches often struggle with optimization instability and distribution shifts, relying on greedy strategies or generic priors that fail to preserve distinct morphological semantics. To address these challenges, we propose History-Augmented Contrastive Mixture of Experts (HAC-MoE), a novel unsupervised framework that decouples kernel estimation from image reconstruction without external kernel priors. The framework is founded on three key innovations: (1) A Contrastive Kernel Sampling mechanism that mitigates the distribution bias inherent in random Gaussian sampling, ensuring the generation of plausible kernel priors via similarity constraints; (2) A History-Augmented Contrastive Learning strategy that leverages historical model states as negative self-priors. We provide a theoretical analysis demonstrating that this mechanism induces strong convexity in the feature space, thereby stabilizing the unsupervised optimization trajectory and preventing overfitting; and (3) A Morphology-Aware Soft Mixture-of-Experts (MA-MoE) estimator that dynamically modulates spectral-spatial features to adaptively reconstruct diverse planetary topographies. To facilitate rigorous evaluation, we introduce Ceres-50, a benchmark dataset encapsulating diverse geological features under realistic degradation simulations. Extensive experiments demonstrate that HAC-MoE achieves state-of-the-art performance in reconstruction quality and kernel estimation accuracy, offering a solution for scientific observation in data-sparse extraterrestrial environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</title>
<link>https://arxiv.org/abs/2511.22039</link>
<guid>https://arxiv.org/abs/2511.22039</guid>
<content:encoded><![CDATA[
arXiv:2511.22039v2 Announce Type: replace 
Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.03424</link>
<guid>https://arxiv.org/abs/2512.03424</guid>
<content:encoded><![CDATA[
arXiv:2512.03424v2 Announce Type: replace 
Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding. The code will be released at https://github.com/L1277471578/DM3D.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
arXiv:2512.05277v2 Announce Type: replace 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2403.13522</link>
<guid>https://arxiv.org/abs/2403.13522</guid>
<content:encoded><![CDATA[
arXiv:2403.13522v3 Announce Type: replace-cross 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pretraining to Privacy: Federated Ultrasound Foundation Model with Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2411.16380</link>
<guid>https://arxiv.org/abs/2411.16380</guid>
<content:encoded><![CDATA[
arXiv:2411.16380v2 Announce Type: replace-cross 
Abstract: Ultrasound imaging is widely used in clinical diagnosis due to its non-invasive nature and real-time capabilities. However, traditional ultrasound diagnostics relies heavily on physician expertise and is often hampered by suboptimal image quality, leading to potential diagnostic errors. While artificial intelligence (AI) offers a promising solution to enhance clinical diagnosis by detecting abnormalities across various imaging modalities, existing AI methods for ultrasound face two major challenges. First, they typically require vast amounts of labeled medical data, raising serious concerns regarding patient privacy. Second, most models are designed for specific tasks, which restricts their broader clinical utility. To overcome these challenges, we present UltraFedFM, an innovative privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively pre-trained using federated learning across 16 distributed medical institutions in 9 countries, leveraging a dataset of over 1 million ultrasound images covering 19 organs and 10 ultrasound modalities. This extensive and diverse data, combined with a secure training framework, enables UltraFedFM to exhibit strong generalization and diagnostic capabilities. It achieves an average area under the receiver operating characteristic curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level ultrasonographers (4-8 years of experience) and matches the performance of expert-level sonographers (10+ years of experience) in the joint diagnosis of 8 common systemic diseases.c These findings indicate that UltraFedFM can significantly enhance clinical diagnostics while safeguarding patient privacy, marking a significant advancement in AI-driven ultrasound imaging for future clinical applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedicoSAM: Robust Improvement of SAM for Medical Imaging</title>
<link>https://arxiv.org/abs/2501.11734</link>
<guid>https://arxiv.org/abs/2501.11734</guid>
<content:encoded><![CDATA[
arXiv:2501.11734v2 Announce Type: replace-cross 
Abstract: Medical image segmentation is an important analysis task in clinical practice and research. Deep learning has massively advanced the field, but current approaches are mostly based on models trained for a specific task. Training such models or adapting them to a new condition is costly due to the need for (manually) labeled data. The emergence of vision foundation models, especially Segment Anything, offers a path to universal segmentation for medical images, overcoming these issues. Here, we study how to improve Segment Anything for medical images by comparing different finetuning strategies on a large and diverse dataset. We evaluate the finetuned models on a wide range of interactive and (automatic) semantic segmentation tasks. We find that the performance can be clearly improved for interactive segmentation. However, semantic segmentation does not benefit from pretraining on medical images. Our best model, MedicoSAM, is publicly available at https://github.com/computational-cell-analytics/medico-sam. We show that it is compatible with existing tools for data annotation and believe that it will be of great practical value.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Camera Meets Mobile Embodied Perception: Abstraction, Algorithm, Acceleration, Application</title>
<link>https://arxiv.org/abs/2503.22943</link>
<guid>https://arxiv.org/abs/2503.22943</guid>
<content:encoded><![CDATA[
arXiv:2503.22943v4 Announce Type: replace-cross 
Abstract: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory</title>
<link>https://arxiv.org/abs/2507.18183</link>
<guid>https://arxiv.org/abs/2507.18183</guid>
<content:encoded><![CDATA[
arXiv:2507.18183v2 Announce Type: replace-cross 
Abstract: Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Registering the 4D Millimeter Wave Radar Point Clouds Via Generalized Method of Moments</title>
<link>https://arxiv.org/abs/2508.02187</link>
<guid>https://arxiv.org/abs/2508.02187</guid>
<content:encoded><![CDATA[
arXiv:2508.02187v2 Announce Type: replace-cross 
Abstract: 4D millimeter wave radars (4D radars) are new emerging sensors that provide point clouds of objects with both position and radial velocity measurements. Compared to LiDARs, they are more affordable and reliable sensors for robots' perception under extreme weather conditions. On the other hand, point cloud registration is an essential perception module that provides robot's pose feedback information in applications such as Simultaneous Localization and Mapping (SLAM). Nevertheless, the 4D radar point clouds are sparse and noisy compared to those of LiDAR, and hence we shall confront great challenges in registering the radar point clouds. To address this issue, we propose a point cloud registration framework for 4D radars based on Generalized Method of Moments. The method does not require explicit point-to-point correspondences between the source and target point clouds, which is difficult to compute for sparse 4D radar point clouds. Moreover, we show the consistency of the proposed method. Experiments on both synthetic and real-world datasets show that our approach achieves higher accuracy and robustness than benchmarks, and the accuracy is even comparable to LiDAR-based frameworks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v2 Announce Type: replace-cross 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[
arXiv:2510.12691v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[
arXiv:2511.01140v2 Announce Type: replace-cross 
Abstract: Medical imaging often operates under limited labeled data, especially in rare disease and low resource clinical environments. Existing multimodal and meta learning approaches improve performance in these settings but lack a theoretical explanation of why or when they succeed. This paper presents a unified theoretical framework for few shot multimodal medical imaging that jointly characterizes sample complexity, uncertainty quantification, and interpretability. Using PAC learning, VC theory, and PAC Bayesian analysis, we derive bounds that describe the minimum number of labeled samples required for reliable performance and show how complementary modalities reduce effective capacity through an information gain term. We further introduce a formal metric for explanation stability, proving that explanation variance decreases at an inverse n rate. A sequential Bayesian interpretation of Chain of Thought reasoning is also developed to show stepwise posterior contraction. To illustrate these ideas, we implement a controlled multimodal dataset and evaluate an additive CNN MLP fusion model under few shot regimes, confirming predicted multimodal gains, modality interference at larger sample sizes, and shrinking predictive uncertainty. Together, the framework provides a principled foundation for designing data efficient, uncertainty aware, and interpretable diagnostic models in low resource settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline</title>
<link>https://arxiv.org/abs/2512.13731</link>
<guid>https://arxiv.org/abs/2512.13731</guid>
<content:encoded><![CDATA[
<div> Keywords: Mathematical Expression Recognition, complex expressions, datasets, Structured Mathematical Language, CMERNet<br /><br />Summary:<br />Mathematical Expression Recognition (MER) has advanced in recognizing simple expressions but struggles with complex expressions that involve many tokens and multi-line layouts. To address this, the authors introduce CMER-Bench, a benchmark dataset that categorizes mathematical expressions into three difficulty levels: easy, moderate, and complex. They use this benchmark to evaluate current MER models as well as multimodal large language models (MLLMs), finding that performance significantly declines on complex expressions due to a lack of complex samples in existing datasets. To overcome this limitation, the authors create two new large-scale datasets, MER-17M and CMER-3M, which focus on complex mathematical expressions and provide diverse samples for robust model training. Furthermore, they develop a novel expression tokenizer and propose a new representation called Structured Mathematical Language, which explicitly captures the hierarchical and spatial structures of mathematical expressions, going beyond traditional LaTeX representations. Building on these innovations, the authors design CMERNet, an encoder-decoder based model trained on the CMER-3M dataset. Despite having only 125 million parameters, CMERNet significantly outperforms existing MER models and general-purpose MLLMs on CMER-Bench, demonstrating superior accuracy and robustness in recognizing complex mathematical expressions. <div>
arXiv:2512.13731v1 Announce Type: new 
Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title>
<link>https://arxiv.org/abs/2512.13739</link>
<guid>https://arxiv.org/abs/2512.13739</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence Generated Content, journalism, semantic alignment, human-in-the-loop, cultural specificity  

<br /><br />Summary: This paper addresses the challenges of using Artificial Intelligence Generated Content (AIGC) to assist image production in journalism, focusing on issues such as misinformation, authenticity, semantic fidelity, and interpretability. It emphasizes that many AIGC tools function as opaque "black boxes," complicating efforts to ensure content accuracy and semantic alignment. The study explores methods for controllable image production specifically tailored for special coverage in journalism. Two experiments are conducted with a Chinese media agency to test these methods. The first experiment evaluates cross-platform adaptability through standardized prompts used across three different scenes, uncovering variations in semantic alignment, cultural specificity, and visual realism caused by both training data biases and platform filtering mechanisms. The second experiment develops a modular human-in-the-loop pipeline integrating components for precise segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulation (Style-LoRA, Prompt-to-Prompt). This pipeline incorporates editorial checks such as CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials to maintain fidelity and traceability. The paper proposes a human-AI collaborative framework for AIGC-assisted image production in journalism and suggests evaluation metrics including Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA). <div>
arXiv:2512.13739v1 Announce Type: new 
Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2512.13742</link>
<guid>https://arxiv.org/abs/2512.13742</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical image classification, Large language models, Clinical reasoning, MobileCoAtNet, Endoscopic images<br /><br />Summary:<br /><br />This study addresses the gap between medical image classification and clinical reasoning by integrating deep learning (DL) models with large language models (LLMs). The authors introduce MobileCoAtNet, a hybrid model tailored for endoscopic images, achieving high accuracy in classifying eight stomach-related diseases. They leverage the classifier’s outputs to prompt multiple LLMs to generate structured clinical explanations involving causes, symptoms, treatments, lifestyle, and follow-up care. To evaluate these explanations, two expert-verified benchmarks were developed as gold standards. Thirty-two LLMs were assessed against these benchmarks, revealing that stronger image classification enhances the quality of LLM-generated narratives, but none reach human-level consistency or stability. The best-performing LLMs still show variability in reasoning when input prompts change, highlighting current limitations. The research demonstrates the potential of combining DL with LLMs to create useful clinical narratives, while cautioning that existing LLMs are not yet reliable for critical medical decisions. The proposed framework clarifies LLMs’ constraints and suggests directions for developing safer and more stable clinical reasoning systems. Additionally, the authors have made the source code and datasets publicly available for further research and validation. <div>
arXiv:2512.13742v1 Announce Type: new 
Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making</title>
<link>https://arxiv.org/abs/2512.13747</link>
<guid>https://arxiv.org/abs/2512.13747</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, medical decision making, Alzheimer's disease classification, MIMIC-CXR, vision-text reasoning<br /><br />Summary:<br /><br />1. Advanced multimodal large language models (MLLMs) have shown strong zero-shot capabilities in vision-language tasks but face significant challenges in the biomedical domain, particularly with fundamental Medical Decision Making (MDM) tasks.<br />2. The study evaluates MLLMs on two difficult datasets: a three-stage Alzheimer's disease classification task, where categories differ subtly, and the MIMIC-CXR dataset involving classification of 14 overlapping chest radiograph conditions.<br />3. Empirical results reveal that text-only reasoning consistently surpasses vision-only or combined vision-text approaches, with multimodal inputs occasionally performing worse than text alone.<br />4. To address these limitations, three strategies are tested: (a) in-context learning using reason-annotated exemplars, (b) employing vision captioning followed by text-only inference, and (c) few-shot fine-tuning of the vision encoder with classification supervision.<br />5. The findings demonstrate that current MLLMs lack robust grounded visual understanding in medical contexts and highlight promising directions to enhance multimodal medical decision-making systems. <div>
arXiv:2512.13747v1 Announce Type: new 
Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning</title>
<link>https://arxiv.org/abs/2512.13752</link>
<guid>https://arxiv.org/abs/2512.13752</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, autoregressive model, unified multimodal understanding, task-progressive learning, image representation<br /><br />Summary:<br /><br />This paper introduces STAR, a novel framework designed for unified multimodal learning, addressing the challenges of simultaneously achieving both multimodal understanding and generation. The method decomposes the learning process into three progressive stages: understanding, generation, and editing, which helps isolate and optimize each task effectively. A key innovation is the use of a STacked AutoRegressive (AR) scheme, where the base AR model's parameters are frozen while additional isomorphic AR modules are sequentially stacked, preventing interference between tasks and enabling scalability of capabilities. To enhance image representation granularity, STAR employs a high-capacity vector quantization (VQ) mechanism. Furthermore, an implicit reasoning mechanism is integrated to improve the model's ability to generate high-quality content, especially under complex scenarios. Experimental validation demonstrates that STAR surpasses previous models by achieving state-of-the-art results on multiple benchmarks: GenEval with a score of 0.91, DPG-Bench with 87.44, and ImgEdit with 4.34. These results collectively confirm STAR’s effectiveness in providing a unified framework that balances multimodal comprehension and generative performance without compromising on either aspect. <div>
arXiv:2512.13752v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-aware UNet and super-resolution deep residual networks for spatial downscaling</title>
<link>https://arxiv.org/abs/2512.13753</link>
<guid>https://arxiv.org/abs/2512.13753</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite data, spatial downscaling, deep learning, tropospheric ozone, temporal encoding  

<br /><br />Summary:  
This article addresses the challenge of improving the spatial resolution of satellite data on atmospheric pollutants, specifically for tropospheric ozone, which is often limited by coarse spatial resolution. The study investigates two popular deep learning architectures for spatial downscaling: the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet. Both architectures are enhanced with a lightweight temporal module designed to incorporate the observation time into the model, leveraging either sinusoidal or radial basis function (RBF) encoding methods. This temporal information is fused with spatial features within the networks. The extended models are evaluated through a case study focused on ozone downscaling over Italy. Results show that these time-aware extensions improve the downscaling performance, providing more accurate high-resolution ozone fields compared to baseline models without temporal modules. Additionally, the temporal modules contribute to faster convergence during training while only modestly increasing computational complexity. The findings demonstrate the value of integrating temporal encoding in deep learning architectures aimed at spatial downscaling of environmental satellite data, enhancing their application for local-scale environmental analysis and decision-making. <div>
arXiv:2512.13753v1 Announce Type: new 
Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</title>
<link>https://arxiv.org/abs/2512.13796</link>
<guid>https://arxiv.org/abs/2512.13796</guid>
<content:encoded><![CDATA[
<div> Gaussian splatting, novel view synthesis, surfels, neural field, textured primitives<br /><br />Summary:<br /><br />This paper addresses the limitations of Gaussian splatting in novel view synthesis, particularly its need for millions of primitives to accurately model textured scenes even when geometry is simple. The authors propose a new representation that decouples geometry and appearance to create a more compact and efficient model. Geometry is represented using surfels, while appearance is modeled with a combination of a global neural field and per-primitive colors. This approach textures a fixed number of primitives per pixel, ensuring low computational overhead. Compared to 3D Gaussian splatting, the proposed method delivers comparable perceptual quality while drastically reducing resource usage—using 9.7 times fewer primitives and 5.5 times less memory on outdoor scenes, and 31 times fewer primitives and 3.7 times less memory on indoor scenes. Additionally, the method renders at twice the speed of existing textured primitive techniques while improving visual quality. Overall, the representation offers a significant improvement in efficiency and rendering speed without sacrificing visual fidelity, making it highly suitable for applications involving complex textured scene synthesis. <div>
arXiv:2512.13796v1 Announce Type: new 
Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\times$ fewer primitives and $5.5\times$ less memory on outdoor scenes and using $31\times$ fewer primitives and $3.7\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title>
<link>https://arxiv.org/abs/2512.13834</link>
<guid>https://arxiv.org/abs/2512.13834</guid>
<content:encoded><![CDATA[
<div> Keywords: VajraV1, real-time object detection, YOLO, mAP, COCO validation set<br /><br />Summary:<br /><br />This technical report introduces VajraV1, a new model architecture for real-time object detection that builds upon and enhances existing YOLO-based detectors. VajraV1 incorporates effective design elements from previous YOLO versions (YOLOv10 to YOLOv13) to improve both accuracy and inference speed. The model is evaluated on the COCO validation dataset, where it shows significant performance gains across multiple variants. VajraV1-Nano achieves a mean Average Precision (mAP) of 44.3%, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7%, while maintaining comparable latency to YOLOv12-N and YOLOv11-N. The Small variant of VajraV1 reaches 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium attains 52.7% mAP, slightly better by 0.2% than YOLOv12-M. The Large model variant achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. Finally, VajraV1-Xlarge delivers a state-of-the-art 56.2% mAP, outperforming all existing real-time object detectors. Overall, VajraV1 demonstrates significant improvements in detection accuracy across different model sizes while maintaining competitive inference speeds, positioning it as a leading real-time object detection architecture. <div>
arXiv:2512.13834v1 Announce Type: new 
Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoLingo: Motion-Language Alignment for Text-to-Motion Generation</title>
<link>https://arxiv.org/abs/2512.13840</link>
<guid>https://arxiv.org/abs/2512.13840</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-motion, diffusion, latent space, cross-attention, human motion generation<br /><br />Summary: We introduce MoLingo, an advanced text-to-motion (T2M) model that generates realistic human motion through denoising in a continuous latent space. Unlike previous methods that perform latent diffusion either on the entire latent at once or auto-regressively over multiple latents, this work focuses on optimizing diffusion specifically for continuous motion latents. The paper addresses two main challenges: first, building a semantically aligned latent space where motion representations that share similar textual meanings remain close together, improving the efficiency and effectiveness of diffusion. This is achieved by training a semantic-aligned motion encoder using frame-level text labels. Second, it investigates how to inject text conditioning optimally so that the generated motion aligns closely with the text description. The study compares single-token conditioning against a multi-token cross-attention mechanism, concluding that cross-attention significantly improves both motion realism and text-motion correspondence. By combining semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, MoLingo establishes a new state of the art on standard human motion generation benchmarks and user studies. The authors plan to publicly release their code and models to support further research and applications. <div>
arXiv:2512.13840v1 Announce Type: new 
Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging</title>
<link>https://arxiv.org/abs/2512.13855</link>
<guid>https://arxiv.org/abs/2512.13855</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Segmentation Models, Parameter-Efficient Fine-Tuning, Telescopic Adapters, Medical Imaging, Transformer Layers<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting Vision Language Segmentation Models (VLSMs), such as CLIPSeg, to medical imaging domains, where conventional fine-tuning demands extensive computational resources. The authors identify limitations in existing Parameter-Efficient Fine-Tuning (PEFT) methods, which apply uniform adapter sizes to all transformer layers, resulting in inefficient parameter use and suboptimal adaptation. To solve this, they propose Telescopic Adapters, a novel PEFT framework that scales adapter capacity progressively from shallow to deep transformer layers, based on layer depth and semantic relevance. This depth-aware scaling hypothesizes that deeper layers require greater adaptation capacity, a claim validated through comprehensive ablation studies. Implemented by integrating lightweight bottleneck modules into both vision and text encoders of CLIPSeg, the approach trains only 613k parameters, reducing the parameter count by 244 times compared to full fine-tuning. Experiments conducted on five diverse medical datasets—including polyp segmentation, skin lesion detection, and breast ultrasound imaging—demonstrate superior segmentation performance using Telescopic Adapters. The method enables effective VLSM adaptation in resource-constrained clinical settings while maintaining competitive accuracy, establishing a new efficient paradigm for medical imaging model fine-tuning. <div>
arXiv:2512.13855v1 Announce Type: new 
Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13869</link>
<guid>https://arxiv.org/abs/2512.13869</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV human detection, synthetic data, domain adaptation, diffusion model, hierarchical alignment<br /><br />Summary: Training object detectors for UAV-based human detection is challenged by constantly changing target distributions and limited labeled real-world images. To address this, synthetic simulators generate annotated data with low cost but suffer from a domain gap between synthetic and real images. The paper proposes Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework aimed at narrowing this domain gap while preserving original synthetic labels. CFHA involves (1) Global Style Transfer, which uses a diffusion model to align the color, illumination, and texture of synthetic images to real image style based on a small real reference set; (2) Local Refinement, employing a super-resolution diffusion model to enhance photorealistic details of small objects like humans while maintaining shape and boundary integrity; and (3) Hallucination Removal, filtering out synthetic human instances whose visual attributes mismatch real-world data to refine appearance closer to the target distribution. Experimental results on UAV Sim2Real detection benchmarks demonstrate significant improvements in detection accuracy, with up to +14.1 mAP50 gain on the Semantic-Drone benchmark. Ablation studies validate the complementary roles of global and local alignment stages and emphasize the importance of hierarchical domain adaptation. The framework and code are publicly available at the provided GitHub repository. <div>
arXiv:2512.13869v1 Announce Type: new 
Abstract: Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \href{https://github.com/liwd190019/CFHA}{this url}.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13874</link>
<guid>https://arxiv.org/abs/2512.13874</guid>
<content:encoded><![CDATA[
<div> any-horizon reasoning, multi-turn reasoning, video understanding, reinforcement learning, synthetic data generation  

<br /><br />Summary:  
The paper introduces SAGE, an agent system designed to mimic human flexible video reasoning by supporting any-horizon video understanding, meaning it can handle both long and short videos effectively by performing multi-turn reasoning for complex videos and single-turn reasoning for simpler tasks. To develop this system, the authors create a synthetic data generation pipeline using Gemini-2.5-Flash, which is used to train SAGE-MM, the critical orchestrator module of SAGE. They also propose a reinforcement learning (RL) post-training method that is crucial for enabling the any-horizon reasoning capability of SAGE-MM. To evaluate the system's performance in realistic scenarios, the authors curate SAGE-Bench, a benchmark comprising videos averaging over 700 seconds with real-world entertainment use cases. Experimental results demonstrate that SAGE, along with the data pipeline and RL training, significantly improves video reasoning performance, achieving up to 6.1% gains on open-ended video reasoning tasks overall, and up to 8.2% improvement specifically on videos longer than 10 minutes. This work sets a new direction toward resource-efficient and flexible video understanding models that better align with natural human reasoning across varied video durations. <div>
arXiv:2512.13874v1 Announce Type: new 
Abstract: As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Route-DETR: Pairwise Query Routing in Transformers for Object Detection</title>
<link>https://arxiv.org/abs/2512.13876</link>
<guid>https://arxiv.org/abs/2512.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: DETR, object detection, adaptive routing, decoder self-attention, query competition  

<br /><br />Summary:  
Detection Transformer (DETR) is an end-to-end object detection framework that removes the need for hand-crafted components like non-maximum suppression but faces challenges due to inefficient query competition, where multiple queries focus on the same object and cause redundant computations. Route-DETR is proposed to solve this by using adaptive pairwise routing within decoder self-attention layers, effectively differentiating between competing queries (aimed at the same object) and complementary queries (aimed at different objects). It incorporates dual routing mechanisms: suppressor routes that reduce duplication by modulating attention between competing queries, and delegator routes that promote exploration of diverse regions. These mechanisms are realized through learnable low-rank attention biases allowing asymmetric interactions among queries. A dual-branch training approach applies these routing biases only during training, keeping inference unchanged to avoid extra computational cost. Evaluations on COCO and Cityscapes datasets show that Route-DETR consistently improves results across multiple DETR baselines, achieving a +1.7% mean average precision (mAP) gain over the DINO baseline on ResNet-50 and reaching a state-of-the-art 57.6% mAP with Swin-L backbone. This confirms Route-DETR’s effectiveness in addressing query competition in DETR for enhanced object detection performance. <div>
arXiv:2512.13876v1 Announce Type: new 
Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI</title>
<link>https://arxiv.org/abs/2512.13902</link>
<guid>https://arxiv.org/abs/2512.13902</guid>
<content:encoded><![CDATA[
<div> Prostate segmentation, MRI, K-Nearest Neighbor attention, Cross Stage Partial, deep learning<br /><br />Summary:  
This paper addresses the challenge of real-time prostate MRI segmentation on clinical workstations, where computational load and memory constraints limit performance. The authors propose KLO-Net, a novel dynamic K-Nearest Neighbor (K-NN) attention U-Net combined with a Cross Stage Partial (CSP) encoder to improve efficiency and segmentation accuracy. Unlike traditional K-NN attention, the dynamic K-NN attention mechanism in KLO-Net adaptively determines the number of attention connections for each spatial location within a slice, enhancing model flexibility and localization. The CSP blocks integrated into the architecture reduce memory consumption and computational overhead, making the model more suitable for real-time application. To validate the effectiveness of their approach, the authors conduct comprehensive experiments and ablation studies on two public MRI datasets—PROMISE12 and PROSTATEx. The results show that KLO-Net achieves a favorable balance of computational efficiency and accurate prostate gland segmentation compared to existing methods. The study highlights the benefits of combining adaptive attention mechanisms with efficient network design to tackle anatomical variability and resource constraints in clinical prostate MRI segmentation tasks. <div>
arXiv:2512.13902v1 Announce Type: new 
Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes</title>
<link>https://arxiv.org/abs/2512.13950</link>
<guid>https://arxiv.org/abs/2512.13950</guid>
<content:encoded><![CDATA[
<div> Keywords: SVBRDF, deep generative models, texture atlas, multiview coherence, UNet<br /><br />Summary:<br /><br />1. The paper addresses the integration of deep generative models in digital content creation, focusing on the synthesis of realistic RGB images aligned with 3D scene geometry for texturing purposes.<br /><br />2. It explores the use of SVBRDF (Spatially Varying Bidirectional Reflectance Distribution Function) prediction networks, which estimate material parameters from RGB images, facilitating detailed appearance modeling.<br /><br />3. By combining conditional image generation and SVBRDF prediction, the authors propose a pipeline to rapidly generate SVBRDF maps from multiple views that can be merged into a coherent SVBRDF texture atlas of a scene.<br /><br />4. The paper discusses challenges such as multiview incoherence caused by single-view SVBRDF prediction, which can lead to inconsistent textures across views in the final atlas.<br /><br />5. Additional information derived from generated RGB images, conditioned on various modalities, can improve SVBRDF estimation beyond what is possible from standard photographs.<br /><br />6. Through a comparative study of different neural network architectures and conditioning strategies, the authors identify designs that achieve both high accuracy and multiview coherence.<br /><br />7. Interestingly, their findings show that a relatively simple and standard UNet architecture performs competitively with more complex models in this context.<br /><br />8. The work offers insights for practitioners aiming to build fast and coherent appearance modeling pipelines leveraging recent advances in generative image synthesis and material prediction.<br /><br />9. The project page provides additional resources and evaluation results: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation <div>
arXiv:2512.13950v1 Announce Type: new 
Abstract: Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.13953</link>
<guid>https://arxiv.org/abs/2512.13953</guid>
<content:encoded><![CDATA[
<div> Keywords: unbranding, trademark removal, text-to-image diffusion, brand recognition, vision language models<br /><br />Summary:<br /><br />The paper introduces the task of unbranding, aimed at the fine-grained removal of trademarks and subtle structural brand features from images generated by text-to-image diffusion models, while maintaining semantic coherence. The motivation stems from growing concerns over unauthorized reproduction of trademarked content, which extends beyond explicit logos to implicit brand identifiers like distinctive design elements. To support research in this area, the authors create a comprehensive benchmark dataset specifically designed for unbranding. They also identify limitations in existing brand detectors that mostly focus on explicit logos and do not handle abstract trade dress features such as the shape of product containers. To address this evaluation gap, the paper proposes a novel metric based on Vision Language Models (VLMs) using a question-answering approach to detect both explicit and implicit brand characteristics in images. The study also highlights the trend that newer, higher-fidelity text-to-image models (e.g., SDXL, FLUX) tend to generate brand identifiers more readily compared to older models, underscoring the increasing importance of unbranding techniques. Experimental results validated by the proposed VLM metric demonstrate that unbranding is a distinct and practically relevant problem that calls for specialized removal methods. The project page provides further resources and data for ongoing research. <div>
arXiv:2512.13953v1 Announce Type: new 
Abstract: The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation</title>
<link>https://arxiv.org/abs/2512.13970</link>
<guid>https://arxiv.org/abs/2512.13970</guid>
<content:encoded><![CDATA[
<div> Keywords: marine obstacle detection, diffusion models, data augmentation, segmentation robustness, synthetic samples<br /><br />Summary: Marine obstacle detection is challenging due to adverse environmental conditions like sun glitter, fog, and dynamic wave patterns, which degrade image quality. To address limited training data caused by scarcity and repetitive marine datasets, the authors propose a novel sample expansion pipeline that enhances training data diversity without retraining diffusion models. This pipeline consists of two main components: a class-aware style bank that generates high-entropy, semantically meaningful prompts, and an adaptive annealing sampler that introduces controlled perturbations during early conditioning. A COD-guided proportional controller dynamically regulates these perturbations to boost sample diversity while preserving the semantic layout fidelity. The synthetic images produced at inference time by this method improve segmentation robustness across different marine obstacle benchmarks. Adding these controlled synthetic samples to the training data consistently enhances segmentation performance across multiple network backbones. Additionally, this approach increases visual variation particularly in rare and texture-sensitive classes, addressing the limitations of traditional mask-conditioned diffusion models which tend to output low-diversity images under low-entropy conditioning. This contribution represents an effective strategy for generating diverse, semantically aligned synthetic training data to improve marine obstacle detection. <div>
arXiv:2512.13970v1 Announce Type: new 
Abstract: Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAI-Driven Diagnosis of Generalization Failure in State-Space Cerebrovascular Segmentation Models: A Case Study on Domain Shift Between RSNA and TopCoW Datasets</title>
<link>https://arxiv.org/abs/2512.13977</link>
<guid>https://arxiv.org/abs/2512.13977</guid>
<content:encoded><![CDATA[
<div> Keywords: domain shift, Explainable AI, cerebrovascular segmentation, State-Space Models, attention mechanism<br /><br />Summary:<br /><br />1. The article addresses the critical issue of domain shift in deploying deep learning models for medical imaging, highlighting how models that perform well on their training data catastrophically fail on external datasets, thus limiting trustworthy AI applications.  
2. The authors propose that overcoming this challenge requires moving beyond simple performance metrics and incorporating Explainable AI (XAI) methods to deeply understand model behavior, particularly in medical image analysis.  
3. They focus on diagnosing the generalization failure of the State-Space Model UMamaba on cerebrovascular segmentation, comparing two datasets: the Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT), which differ notably in Z-resolution and background noise.  
4. A dramatic drop in model performance was documented, with the Dice score declining from 0.8604 on the Source dataset to 0.2902 on the Target dataset, indicating severe generalization failure.  
5. The core contribution is the use of Seg-XRes-CAM, an XAI technique, to quantify the model’s attention by measuring the overlap between its attention maps and both the Ground Truth and the model’s own incorrect prediction masks. Quantitative results show that the model’s attention shifted away from true anatomical vessels (IoU ~ 0.101) but remained aligned with its erroneous predictions (IoU ~ 0.282), demonstrating reliance on spurious correlations. This insight confirms the value of XAI in diagnosing dataset bias and model failure in emerging neural architectures. <div>
arXiv:2512.13977v1 Announce Type: new 
Abstract: The clinical deployment of deep learning models in medical imaging is severely hindered by domain shift. This challenge, where a high-performing model fails catastrophically on external datasets, is a critical barrier to trustworthy AI. Addressing this requires moving beyond simple performance metrics toward deeper understanding, making Explainable AI (XAI) an essential diagnostic tool in medical image analysis. We present a rigorous, two-phase approach to diagnose the generalization failure of state-of-the-art State-Space Models (SSMs), specifically UMamaba, applied to cerebrovascular segmentation. We first established a quantifiable domain gap between our Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT) datasets, noting significant differences in Z-resolution and background noise. The model's Dice score subsequently plummeted from 0.8604 (Source) to 0.2902 (Target). In the second phase, which is our core contribution, we utilized Seg-XRes-CAM to diagnose the cause of this failure. We quantified the model's focus by measuring the overlap between its attention maps and the Ground Truth segmentations, and between its attention maps and its own Prediction Mask. Our analysis proves the model failed to generalize because its attention mechanism abandoned true anatomical features in the Target domain. Quantitative metrics confirm the model's focus shifted away from the Ground Truth vessels (IoU~0.101 at 0.3 threshold) while still aligning with its own wrong predictions (IoU~0.282 at 0.3 threshold). This demonstrates the model learned spurious correlations, confirming XAI is a powerful diagnostic tool for identifying dataset bias in emerging architectures.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FocalComm: Hard Instance-Aware Multi-Agent Perception</title>
<link>https://arxiv.org/abs/2512.13982</link>
<guid>https://arxiv.org/abs/2512.13982</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaborative perception, hard instance mining, feature-level fusion, pedestrian detection, autonomous driving<br /><br />Summary: Multi-agent collaborative perception (CP) enhances autonomous driving safety by improving 3D perception, especially for vulnerable road users like pedestrians. Existing CP methods often focus on vehicle detection metrics and struggle with detecting smaller, safety-critical objects such as pedestrians, where errors can be dangerous. Current approaches typically rely on exchanging full feature sets among agents, which is inefficient and can lead to higher false negatives. To address these challenges, the paper introduces FocalComm, a novel CP framework that prioritizes the exchange of hard-instance-oriented features among collaborative agents. FocalComm incorporates two main innovations: first, a learnable progressive hard instance mining (HIM) module that enables each agent to extract features specifically targeting hard instances; second, a query-based feature-level fusion technique that dynamically weights and integrates these challenging features during collaboration. Experimental results demonstrate that FocalComm significantly outperforms state-of-the-art CP methods on two real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaboration settings. Notably, FocalComm achieves considerable improvements in pedestrian detection within the V2X-Real dataset, highlighting its potential to enhance safety in autonomous driving applications by reducing detection failures of critical small objects. <div>
arXiv:2512.13982v1 Announce Type: new 
Abstract: Multi-agent collaborative perception (CP) is a promising paradigm for improving autonomous driving safety, particularly for vulnerable road users like pedestrians, via robust 3D perception. However, existing CP approaches often optimize for vehicle detection performance metrics, underperforming on smaller, safety-critical objects such as pedestrians, where detection failures can be catastrophic. Furthermore, previous CP methods rely on full feature exchange rather than communicating only salient features that help reduce false negatives. To this end, we present FocalComm, a novel collaborative perception framework that focuses on exchanging hard-instance-oriented features among connected collaborative agents. FocalComm consists of two key novel designs: (1) a learnable progressive hard instance mining (HIM) module to extract hard instance-oriented features per agent, and (2) a query-based feature-level (intermediate) fusion technique that dynamically weights these identified features during collaboration. We show that FocalComm outperforms state-of-the-art collaborative perception methods on two challenging real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaborative setups. FocalComm also shows a strong performance gain in pedestrian detection in V2X-Real.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repurposing 2D Diffusion Models for 3D Shape Completion</title>
<link>https://arxiv.org/abs/2512.13991</link>
<guid>https://arxiv.org/abs/2512.13991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape completion, diffusion models, Shape Atlas, point clouds, 2D representation

<br /><br />Summary:  
The paper proposes a novel framework that adapts existing 2D diffusion models for the task of 3D shape completion from incomplete point clouds. Recognizing the limitation caused by the scarcity of high-quality 3D datasets and the modality gap between 3D inputs and 2D latent diffusion spaces, the authors introduce the Shape Atlas, a compact 2D representation of 3D geometry designed to bridge this gap. This representation allows the full generative power of pretrained 2D diffusion models to be leveraged effectively for 3D shape reconstruction tasks. The approach also aligns the conditioning modalities between input and output, enabling more effective and accurate shape completion results. The unified 2D formulation facilitates learning from limited 3D datasets and successfully preserves intricate detail in completed shapes. Experimental validation on benchmark datasets, PCN and ShapeNet-55, demonstrates the efficacy and competitiveness of the proposed method. Beyond shape completion, the framework's practical application extends to generating artist-created meshes from the completed point clouds, showcasing its potential utility in creative workflows and digital content creation environments. <div>
arXiv:2512.13991v1 Announce Type: new 
Abstract: We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.14008</link>
<guid>https://arxiv.org/abs/2512.14008</guid>
<content:encoded><![CDATA[
<div> Masked discrete diffusion models, inference speed, token truncation, register tokens, LaViDa-O

<br /><br />Summary: Masked Discrete Diffusion Models (MDMs) are effective across multimodal tasks such as image understanding, generation, and editing but suffer from slow inference speeds due to processing redundant masked tokens at every sampling step. This paper introduces Sparse-LaViDa, a new framework designed to speed up MDM sampling by dynamically truncating unnecessary masked tokens during inference. To preserve the quality of generated outputs despite truncation, specialized register tokens are introduced to compactly represent these removed tokens. The framework also addresses the training-inference mismatch by designing a specialized attention mask that aligns the training procedure with the truncated sampling strategy used during inference. Sparse-LaViDa builds upon the state-of-the-art unified MDM LaViDa-O and demonstrates an up to 2x speedup across multiple domains, including text-to-image generation, image editing, and mathematical reasoning. Importantly, this acceleration is achieved without sacrificing generation quality, making Sparse-LaViDa an efficient and effective enhancement for masked discrete diffusion models. <div>
arXiv:2512.14008v1 Announce Type: new 
Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.14017</link>
<guid>https://arxiv.org/abs/2512.14017</guid>
<content:encoded><![CDATA[
<div> Key frame sampling, long video QA, multi-scene annotations, sampling quality metric, question-video relevance  

<br /><br />Summary:  
This paper introduces KFS-Bench, the first dedicated benchmark for key frame sampling in long video question answering (QA). It uniquely features multi-scene annotations that enable direct and robust evaluation of sampling strategies, overcoming the limitations of prior approaches that only assessed frame selection quality indirectly through QA accuracy. Key frame sampling is critical for efficient understanding of long-form videos, as selecting informative frames allows multimodal large language models (MLLMs) to enhance both accuracy and computational efficiency in QA tasks. KFS-Bench provides ground-truth annotations identifying multiple disjoint scenes required for each question, facilitating precise analysis of how different sampling methods capture essential content throughout long videos. Using this benchmark, the authors conduct a comprehensive study of existing key frame sampling techniques, revealing that sampling precision alone is insufficient, and that scene coverage and sampling balance are also crucial factors influencing QA outcomes. Based on these insights, they propose a novel sampling quality metric that correlates strongly with QA accuracy by jointly considering these factors. Additionally, they develop a new sampling method that leverages question-video relevance to balance sampling diversity and question-frame similarity, thereby improving coverage of relevant scenes. This adaptively balanced approach achieves superior performance in both key frame selection quality and QA effectiveness. The benchmark and code are publicly available on GitHub. <div>
arXiv:2512.14017v1 Announce Type: new 
Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Perspective of Scene Understanding in Autonomous Robots</title>
<link>https://arxiv.org/abs/2512.14020</link>
<guid>https://arxiv.org/abs/2512.14020</guid>
<content:encoded><![CDATA[
<div> Deep learning, scene understanding, autonomous robots, object detection, visual SLAM  

<br /><br />Summary:  
This paper reviews deep learning applications in scene understanding specifically targeted at autonomous robots. It covers key innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM, highlighting how these advances contribute to improved environmental perception. The review emphasizes the superiority of learning-based methods over traditional geometric models, particularly in handling challenges like occlusions and textureless surfaces for real-time depth perception. It stresses the enhancement of semantic reasoning to facilitate better understanding and interpretation of surroundings, which is critical for autonomous operation. Integration of these perception modules is shown to improve robot effectiveness in dynamic and unstructured environments, leading to more robust decision-making, navigation, and interaction capabilities. Finally, the paper identifies existing challenges and outlines future research directions aiming to further advance learning-based scene understanding, ensuring autonomous robots become increasingly reliable and capable in complex real-world scenarios. <div>
arXiv:2512.14020v1 Announce Type: new 
Abstract: This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers</title>
<link>https://arxiv.org/abs/2512.14026</link>
<guid>https://arxiv.org/abs/2512.14026</guid>
<content:encoded><![CDATA[
<div> Multi-modal learning, Self-Supervised Learning, Cross-tabular, Prototype-guided mixture-of-linear layer, Alzheimer's disease<br /><br />Summary:<br /><br />1. This paper addresses the challenge of integrating medical images and heterogeneous tabular data through multi-modal learning, which has become crucial for clinical decision-making. <br />2. Self-Supervised Learning (SSL) methods have been effective for pretraining on large unlabeled datasets but typically struggle with heterogeneous tabular data due to rigid tabular modeling, limiting their ability to transfer knowledge across diverse data cohorts. <br />3. The authors propose CITab, a novel SSL framework that enables cross-tabular multi-modal feature representation learning by incorporating semantic awareness through the integration of column headers as semantic cues, enhancing transferability and scalability. <br />4. A novel prototype-guided mixture-of-linear layer (P-MoLin) module is introduced, which specializes tabular feature learning, allowing the model to effectively handle heterogeneity within tabular data and to explore underlying medical concepts. <br />5. Extensive experiments on Alzheimer's disease diagnosis across three public cohorts with 4,461 subjects demonstrate that CITab outperforms existing state-of-the-art methods, highlighting its potential for scalable and effective cross-tabular multi-modal learning in medical applications. <div>
arXiv:2512.14026v1 Announce Type: new 
Abstract: Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding</title>
<link>https://arxiv.org/abs/2512.14028</link>
<guid>https://arxiv.org/abs/2512.14028</guid>
<content:encoded><![CDATA[
<div> Keywords: structured light, 3D imaging, neural feature matching, depth refinement, synthetic data<br /><br />Summary:<br /><br />1. This paper addresses the challenge of active 3D imaging in single-shot structured light systems commonly used in devices like Apple Face ID and Intel RealSense.<br />2. Traditional pixel-domain correspondence matching methods suffer from robustness issues in difficult scenarios such as occlusions, detailed fine structures, and non-Lambertian surfaces.<br />3. The authors propose a novel learning-based framework that performs correspondence matching in feature space using neural features extracted from projected patterns and captured infrared images.<br />4. To leverage geometric priors, the framework constructs cost volumes in feature space, significantly improving performance compared to pixel-domain methods.<br />5. A depth refinement module is introduced, incorporating strong priors from large-scale monocular depth estimation models to enhance fine details and global structural coherence.<br />6. A physically-based rendering pipeline was developed to synthesize nearly one million structured light pattern and image pairs featuring diverse objects and materials in indoor settings to train the model.<br />7. Experiments show the method generalizes well to real indoor scenes, works across multiple pattern types without retraining, and outperforms commercial structured light systems as well as passive stereo RGB depth estimation approaches.<br />8. The project webpage is available for further details and resources. <div>
arXiv:2512.14028v1 Announce Type: new 
Abstract: We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</title>
<link>https://arxiv.org/abs/2512.14032</link>
<guid>https://arxiv.org/abs/2512.14032</guid>
<content:encoded><![CDATA[
<div> Keywords: neural SLAM, RGB-D, Scene Coordinate Regression, real-time mapping, implicit map representation<br /><br />Summary:<br /><br />1. The paper introduces a novel neural RGB-D SLAM system that learns an implicit map of scenes in real time using Scene Coordinate Regression (SCR) as the core representation. <br /><br />2. SCR is explored for the first time in neural SLAM pipelines, leveraging a lightweight network that maps 2D image features directly to 3D global coordinates, offering advantages in memory efficiency, speed, and privacy preservation.<br /><br />3. The proposed system is the first to achieve strict real-time performance in neural implicit RGB-D SLAM by using a dedicated SCR-based architecture tailored for this task.<br /><br />4. Key design elements for integrating SCR into a live SLAM pipeline are presented, resulting in a flexible framework that supports both sparse and dense features and maintains reliable operation in dynamic environments without special adaptations.<br /><br />5. The approach is evaluated on established synthetic and real-world datasets, demonstrating competitive performance compared to current state-of-the-art methods.<br /><br />The project code and further details are available at the given GitHub repository. <div>
arXiv:2512.14032v1 Announce Type: new 
Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</title>
<link>https://arxiv.org/abs/2512.14039</link>
<guid>https://arxiv.org/abs/2512.14039</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, texture parameterization, adaptive sampling, anisotropic parameterization, rendering efficiency<br /><br />Summary: This paper addresses key inefficiencies in textured 3D Gaussian Splatting used for appearance modeling and related tasks. It identifies two major issues with current methods: (1) textures are defined in canonical space, causing inefficient sampling and wasting texture capacity on regions that contribute little to the final rendering, and (2) uniform texture parameterization is applied across all Gaussians regardless of their visual complexity, leading to over-parameterization and unnecessary memory usage. To tackle these problems, the authors propose ASAP Textured Gaussians, which incorporates two strategies: adaptive sampling based on the Gaussian density distribution, enabling more efficient use of texture resources by focusing sampling on visually important areas, and an error-driven anisotropic parameterization that adaptively assigns texture parameters according to rendering error, allocating more detail where needed and saving resources elsewhere. This method significantly improves the balance between quality and computational efficiency, allowing for high-fidelity rendering with substantially fewer texture parameters, making it a practical advancement for memory-constrained applications in 3D graphics and rendering. <div>
arXiv:2512.14039v1 Announce Type: new 
Abstract: Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning</title>
<link>https://arxiv.org/abs/2512.14040</link>
<guid>https://arxiv.org/abs/2512.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: ChartAgent, Tool-Integrated Reasoning, chart understanding, multimodal large language models, visual parsing<br /><br />Summary:<br /><br />1. Charts are widely used for data analysis and communication due to their high information density and intuitive readability, yet current multimodal large language models (MLLMs) struggle with chart understanding when key numerical annotations are missing.<br /><br />2. To overcome these limitations, the paper introduces ChartAgent, a novel chart understanding framework based on Tool-Integrated Reasoning (TIR), which mimics human cognition by breaking down complex chart analysis into a series of observable and replayable steps.<br /><br />3. ChartAgent incorporates an extensible and modular tool library consisting of over a dozen specialized tools including key element detection, instance segmentation, and optical character recognition (OCR), allowing dynamic orchestration to systematically parse diverse chart types.<br /><br />4. The framework provides transparency and verifiability by consolidating intermediate outputs into a structured Evidence Package, enabling traceable and reproducible support for its final analytical conclusions.<br /><br />5. Experimental results demonstrate that ChartAgent significantly enhances robustness and reliability in settings with sparse or missing annotations, offering a practical approach toward trustworthy and extensible chart understanding systems. <div>
arXiv:2512.14040v1 Announce Type: new 
Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14044</link>
<guid>https://arxiv.org/abs/2512.14044</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, autonomous driving, object hallucination, reinforcement learning, visual grounding<br /><br />Summary:<br /><br />The paper addresses the challenge of deploying Vision-Language Models (VLMs) in safety-critical autonomous driving (AD) environments, focusing on the prevalent issue of object hallucination caused by ungrounded, text-based Chain-of-Thought (CoT) reasoning. It identifies two key limitations in existing multi-modal CoT approaches: the separation of perception and reasoning phases that hinders end-to-end optimization, and the dependence on costly, dense localization annotations. To overcome these, the authors propose OmniDrive-R1, a unified end-to-end VLM framework that integrates perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) process. A central innovation is the reinforcement-driven visual grounding mechanism, allowing the model to autonomously focus and zoom on important image regions for detailed analysis. This is realized via a novel two-stage reinforcement learning training pipeline and a new Clip-GRPO algorithm. Clip-GRPO introduces an annotation-free grounding reward based on process consistency between visual attention and textual reasoning, removing the need for dense labels and avoiding reliance on external tools. Experiments on the DriveLMM-o1 dataset demonstrate substantial performance gains, with OmniDrive-R1 elevating reasoning scores from 51.77% to 80.35% and answer accuracy from 37.81% to 73.62% compared to the Qwen2.5VL-7B baseline. <div>
arXiv:2512.14044v1 Announce Type: new 
Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SELECT: Detecting Label Errors in Real-world Scene Text Data</title>
<link>https://arxiv.org/abs/2512.14050</link>
<guid>https://arxiv.org/abs/2512.14050</guid>
<content:encoded><![CDATA[
<div> Label error detection, scene text recognition, multi-modal training, sequence label misalignment, data corruption<br /><br />Summary:<br /><br />1. The paper introduces SELECT (Scene tExt Label Errors deteCTion), a novel method designed to detect label errors in real-world scene text datasets by leveraging multi-modal training techniques combining image and text data. 2. SELECT utilizes an image-text encoder alongside a character-level tokenizer to handle challenges such as variable-length sequence labels, sequence misalignment, and character-level errors, leading to improved accuracy over existing approaches. 3. The authors propose a new error introduction method called Similarity-based Sequence Label Corruption (SSLC), which simulates realistic label errors during training by corrupting labels considering both sequence length changes and the visual similarity between characters. 4. SSLC aids in training SELECT to better generalize to various real-world label error scenarios, providing a more robust detection mechanism. 5. Experiments demonstrate that SELECT effectively identifies label errors in diverse scene text recognition datasets and enhances the accuracy of scene text recognition (STR) models, illustrating its practical value and contribution to improving data quality in the field. <div>
arXiv:2512.14050v1 Announce Type: new 
Abstract: We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices</title>
<link>https://arxiv.org/abs/2512.14052</link>
<guid>https://arxiv.org/abs/2512.14052</guid>
<content:encoded><![CDATA[
<div> Keywords: HyperVL, multimodal large language model, Visual Resolution Compressor, Dual Consistency Learning, on-device inference  

<br /><br />Summary:  
The article presents HyperVL, an efficient multimodal large language model designed specifically for on-device inference, addressing the challenges of high computational and memory demands commonly seen in current models. HyperVL introduces an image-tiling strategy to limit peak memory usage during processing of high-resolution inputs. It employs two novel techniques: first, a Visual Resolution Compressor (VRC) that adaptively determines the optimal encoding resolution to avoid unnecessary computation; second, Dual Consistency Learning (DCL), which aligns multi-scale Vision Transformer (ViT) encoders within a common framework, allowing dynamic switching between visual branches while sharing a unified large language model (LLM). Extensive experiments show that HyperVL achieves state-of-the-art performance relative to other models of similar size across various benchmarks. Additionally, HyperVL significantly reduces latency and power consumption on real mobile devices, proving its suitability and practicality for on-device multimodal applications. The study highlights that despite the advances of small-parameter models, traditional ViT encoders still pose bottlenecks in latency and memory when processing high-resolution images, which HyperVL effectively overcomes through its innovations. Overall, HyperVL represents a key step forward in making powerful multimodal LLMs more accessible and efficient for edge-computing environments. <div>
arXiv:2512.14052v1 Announce Type: new 
Abstract: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</title>
<link>https://arxiv.org/abs/2512.14056</link>
<guid>https://arxiv.org/abs/2512.14056</guid>
<content:encoded><![CDATA[
<div> Keywords: talking face editing, speech-conditional facial motion infilling, FacEDiT, diffusion transformer, FacEDiTBench<br /><br />Summary:<br />1. This paper proposes a unified view of talking face editing and face generation as subtasks of speech-conditional facial motion infilling, rather than treating them separately.<br />2. The authors introduce FacEDiT, a novel speech-conditional Diffusion Transformer model trained with flow matching that synthesizes masked facial motions conditioned on both surrounding motions and speech input.<br />3. FacEDiT’s design is inspired by masked autoencoders and supports localized editing operations such as substitution, insertion, and deletion, while ensuring smooth transitions and lip synchronization with the unedited regions.<br />4. To improve boundary continuity and lip-sync quality, the model incorporates biased attention mechanisms and temporal smoothness constraints.<br />5. The paper also presents FacEDiTBench, the first dedicated dataset and benchmark for talking face editing, featuring varied edit types, lengths, and new evaluation metrics.<br />6. Extensive experiments demonstrate that talking face editing and generation naturally emerge from the proposed speech-conditional motion infilling framework, with FacEDiT achieving accurate, speech-aligned, identity-preserving, and visually smooth edits while generalizing well to full talking face generation tasks. <div>
arXiv:2512.14056v1 Announce Type: new 
Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</title>
<link>https://arxiv.org/abs/2512.14058</link>
<guid>https://arxiv.org/abs/2512.14058</guid>
<content:encoded><![CDATA[
<div> Daylight-linked controls, indoor workplane illuminance, multimodal deep learning, temporal-spatial features, dynamic indoor scenes  

<br /><br />Summary:  
This study focuses on improving indoor daylight prediction to enhance daylight-linked controls (DLCs), which can significantly reduce energy consumption in buildings. Unlike previous methods that primarily target static scenes, the proposed approach uses a multimodal deep learning framework capable of real-time prediction of indoor workplane illuminance distributions. The model uniquely extracts image features exclusively from the side-lit window areas instead of the entire interior, allowing its application in dynamically occupied indoor spaces without intrusive monitoring. A comprehensive field experiment was conducted in a test room located in Guangzhou, China, where 17,344 image samples were gathered for training and validating the model. The results demonstrate outstanding prediction accuracy, with an R² greater than 0.98 and RMSE below 0.14 on test data drawn from the same distribution as the training set. The model also showed robust temporal generalization by achieving an R² above 0.82 and RMSE less than 0.17 on data from previously unseen days. These findings highlight the model’s potential for real-time indoor daylight monitoring and control in real-world, dynamically changing environments, advancing the practical deployment of energy-saving DLC systems. <div>
arXiv:2512.14058v1 Announce Type: new 
Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.14061</link>
<guid>https://arxiv.org/abs/2512.14061</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based super-resolution, feature modulation, generative prior activation, text-prompt alignment, one-step inference

<br /><br />Summary: This paper addresses three major limitations in recent diffusion-based one-step image super-resolution methods: loss of fidelity due to compression encoding of low-quality inputs, weak region-discriminative activation of generative priors, and poor alignment between text prompts and semantic regions. To overcome these challenges, the authors introduce CODSR, a controllable one-step diffusion network designed to improve image super-resolution. First, the proposed LQ-guided feature modulation module incorporates information from the original uncompressed low-quality inputs to provide high-fidelity conditioning during the diffusion process, mitigating fidelity loss. Second, a region-adaptive generative prior activation method is developed to improve perceptual richness while preserving local structural details, ensuring better visual quality without sacrificing accuracy. Third, a text-matching guidance strategy is employed to fully exploit text prompts, aligning them more effectively with their corresponding semantic regions in the output image. Extensive experiments demonstrate that CODSR not only achieves superior perceptual quality but also maintains competitive fidelity compared to state-of-the-art methods, all while enabling efficient one-step inference for practical use. <div>
arXiv:2512.14061v1 Announce Type: new 
Abstract: Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2512.14068</link>
<guid>https://arxiv.org/abs/2512.14068</guid>
<content:encoded><![CDATA[
<div> block-wise discrete diffusion, vision-language understanding, training efficiency, stability, noise scheduling<br /><br />Summary:  
The paper introduces SDAR-VL, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU). Block-wise discrete diffusion provides a balance between parallel generation and causal dependency modeling, but prior approaches suffered from high training costs, slow convergence, and instability, limiting their practical use compared to autoregressive (AR) baselines. To address these challenges, the authors propose an integrated framework composed of three key components: (1) Asynchronous Block-wise Noise Scheduling, which diversifies supervision within each training batch; (2) Effective Mask Ratio Scaling, ensuring unbiased loss normalization when stochastic masking is applied; and (3) a Progressive Beta Noise Curriculum that progressively increases mask coverage while preserving data corruption diversity. Evaluations on 21 diverse benchmarks, including single-image, multi-image, and video tasks, demonstrate that SDAR-VL significantly improves training efficiency, convergence stability, and overall task performance over conventional block diffusion methods. Furthermore, SDAR-VL establishes a new state of the art among diffusion-based vision-language models, and matches or surpasses strong AR baselines such as LLaVA-OneVision and the global diffusion baseline LLaDA-V under comparable experimental settings. This work thus confirms block-wise diffusion as a practical and effective backbone for vision-language understanding. <div>
arXiv:2512.14068v1 Announce Type: new 
Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</title>
<link>https://arxiv.org/abs/2512.14087</link>
<guid>https://arxiv.org/abs/2512.14087</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, plant reconstruction, hierarchical representation, structure and appearance disentanglement, multi-view images  

<br /><br />Summary:  
1. The paper proposes GaussianPlant, a novel method that jointly recovers both the appearance and internal structure of botanical plants from multi-view images using 3D Gaussian Splatting (3DGS).  
2. Traditional 3DGS is effective for reconstructing appearance but lacks explicit structural representation like branching patterns, which limits its use in applications such as plant phenotyping.  
3. GaussianPlant introduces a hierarchical 3DGS representation that disentangles structure and appearance by utilizing structure primitives (StPs) and appearance primitives (ApPs).  
4. StPs explicitly model plant geometry, representing branches as cylinders and leaves as disks, with attribute optimization that distinguishes between branches and leaves in a self-organized manner.  
5. ApPs represent the visual appearance bound to each StP, allowing the method to reconstruct both the plant’s geometry and its appearance simultaneously.  
6. StPs and ApPs are jointly optimized leveraging re-rendering losses and gradient flow between appearance and structure components, ensuring accurate reconstruction fidelity.  
7. Experimental results demonstrate that GaussianPlant achieves high-fidelity appearance reconstruction and precise structural recovery, enabling effective extraction of branch structures and leaf instances for practical plant phenotyping tasks. <div>
arXiv:2512.14087v1 Announce Type: new 
Abstract: We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes</title>
<link>https://arxiv.org/abs/2512.14092</link>
<guid>https://arxiv.org/abs/2512.14092</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical workflow, scene graphs, prototype learning, graph neural networks, explainability<br /><br />Summary:<br /><br />1. Purpose: The paper addresses the challenge of detailed surgical recognition in AI, hindered by annotation costs, limited data, and lack of interpretability. It proposes ProtoFlow, a framework that employs dynamic scene graph prototypes to model complex surgical workflows in an explainable and robust way.<br /><br />2. Methods: ProtoFlow uses a graph neural network encoder-decoder combined with self-supervised pretraining to learn rich representations, followed by prototype-based fine-tuning. This approach identifies and refines core prototypes representing clinically significant, recurring surgical interaction patterns, enabling explainable workflow analysis.<br /><br />3. Results: Evaluated on the CAT-SG dataset, ProtoFlow surpasses conventional GNN baselines in accuracy and exhibits remarkable robustness in data-scarce, few-shot learning conditions, maintaining strong performance even when trained on a single surgical video. Qualitative analysis confirms that learned prototypes correspond to distinct surgical sub-techniques and provide interpretable insights into workflow deviations and rare complications.<br /><br />4. Conclusion: ProtoFlow combines robust learning with built-in explainability, advancing transparent, reliable, and data-efficient AI models for surgery. This progress could facilitate clinical AI adoption in surgical training, real-time support, and workflow optimization. <div>
arXiv:2512.14092v1 Announce Type: new 
Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality-Aware Framework for Video-Derived Respiratory Signals</title>
<link>https://arxiv.org/abs/2512.14093</link>
<guid>https://arxiv.org/abs/2512.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: respiratory rate estimation, remote photoplethysmography, signal quality, machine learning, adaptive fusion<br /><br />Summary:<br /><br />1. The article addresses the challenge of unreliable video-based respiratory rate (RR) estimation caused by inconsistent signal quality across different extraction methods.<br />2. The authors propose a quality-aware predictive framework that integrates multiple heterogeneous signal sources derived from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines.<br />3. Ten signals are extracted and analyzed using four spectral estimation techniques: Welch’s method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection.<br />4. Segment-level quality indices are computed and fed into machine learning models to predict the estimation accuracy or select the most reliable signals dynamically.<br />5. This approach facilitates adaptive signal fusion and quality-based segment filtering to improve RR estimation.<br />6. The framework was evaluated on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI), demonstrating reduced RR estimation errors compared to individual methods.<br />7. Performance improvements varied depending on dataset characteristics, indicating adaptability of the method.<br />8. Overall, the study highlights the effectiveness of quality-driven predictive modeling in achieving scalable and generalizable video-based respiratory monitoring solutions. <div>
arXiv:2512.14093v1 Announce Type: new 
Abstract: Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</title>
<link>https://arxiv.org/abs/2512.14095</link>
<guid>https://arxiv.org/abs/2512.14095</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D human-object interaction, zero-shot generation, video diffusion models, anchor-based prior distillation, Neural Radiance Fields

<br /><br />Summary:  
This paper addresses the challenge of text-driven 4D human-object interaction (HOI) generation, which is traditionally limited by the lack of large-scale datasets. To improve scalability, previous methods have leveraged zero-shot 4D HOI generation using pre-trained image diffusion models but faced limitations in capturing detailed interaction cues across diverse scenarios. The proposed framework, AnchorHOI, advances 4D HOI generation by integrating hybrid priors, specifically introducing video diffusion models in addition to image diffusion models. A key difficulty lies in directly optimizing the high-dimensional 4D HOI data, particularly for accurate human pose and motion representation. AnchorHOI overcomes this challenge through an anchor-based prior distillation strategy, employing a two-step generation process guided by interaction-aware anchors. Two specially designed anchors are introduced: anchor Neural Radiance Fields (NeRFs) for detailed and expressive interaction composition, and anchor keypoints to ensure realistic motion synthesis. Experimental results indicate that AnchorHOI significantly outperforms previous approaches, offering enhanced diversity and better generalization in generated 4D human-object interactions. This framework thus presents a novel and effective solution to the zero-shot 4D HOI generation problem by leveraging advanced priors and structured guidance mechanisms. <div>
arXiv:2512.14095v1 Announce Type: new 
Abstract: Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration</title>
<link>https://arxiv.org/abs/2512.14096</link>
<guid>https://arxiv.org/abs/2512.14096</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Classifier-Free Guidance, Optimization, Adaptive caching, Evolutionary algorithms  

<br /><br />Summary: Diffusion models are currently the leading technique for generating high-quality images but are computationally intensive due to their iterative denoising process. One common enhancement, Classifier-Free Guidance (CFG), improves generation quality and control but doubles computation by requiring both conditional and unconditional forward passes at each timestep. The paper introduces OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a two-stage framework designed to accelerate diffusion transformers (DiT) by optimizing CFG usage. The first stage uses evolutionary algorithms to strategically select which timesteps to skip and determine appropriate guidance scales, successfully eliminating up to 82% of unconditional passes. The second stage implements adaptive rank allocation that adjusts calibration efforts for each transformer block, thereby preserving caching efficiency despite variable guidance scales. This method addresses the challenge that variable guidance introduces denoising deviations, which degrade traditional caching methods assuming constant CFG scales. Experimental results show that OUSAC achieves significant computational savings—53% on DiT-XL/2 with a 15% quality improvement, 60% savings and 16.1% quality gain on PixArt-alpha, and a 5x speedup on FLUX models—while also improving metrics like the CLIP Score compared to standard 50-step baselines. Overall, OUSAC effectively reduces computational costs while enhancing image generation quality in diffusion models. <div>
arXiv:2512.14096v1 Announce Type: new 
Abstract: Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models</title>
<link>https://arxiv.org/abs/2512.14099</link>
<guid>https://arxiv.org/abs/2512.14099</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view image generation, discrete diffusion, visual tokens, cross-view consistency, masked token prediction<br /><br />Summary:<br /><br />1. This paper addresses the challenge of generating consistent multi-view images from a single input image and text description, which is difficult due to the need to maintain geometric consistency across viewpoints.<br /><br />2. Unlike prior methods that depend on 3D-aware architectures or specialized diffusion models requiring extensive multi-view data and geometric priors, the authors propose ViewMask-1-to-3, which leverages discrete diffusion for multi-view image synthesis.<br /><br />3. ViewMask-1-to-3 treats multi-view synthesis as a discrete sequence modeling problem, representing each viewpoint via visual tokens obtained through MAGVIT-v2 tokenization.<br /><br />4. The model achieves progressive generation of multiple viewpoints by masked token prediction using text input and iterative token unmasking, employing simple random masking and self-attention to maintain cross-view consistency.<br /><br />5. The method eliminates complex geometric constraints and specialized attention mechanisms, showing state-of-the-art performance on the GSO and 3D-FUTURE datasets according to PSNR, SSIM, and LPIPS metrics, while maintaining architectural simplicity. <div>
arXiv:2512.14099v1 Announce Type: new 
Abstract: Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</title>
<link>https://arxiv.org/abs/2512.14102</link>
<guid>https://arxiv.org/abs/2512.14102</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing, Text-to-Image Retrieval, Neurosymbolic AI, Large Language Models, First-Order Logic<br /><br />Summary:<br /><br />1. This paper addresses challenges in text-to-image retrieval in remote sensing (RS), focusing on explainability and handling complex spatial relationships, which remain problematic in current large vision-language models (RS-LVLMS).<br />2. The authors propose RUNE (Reasoning Using Neurosymbolic Entities), a novel approach that integrates Large Language Models (LLMs) with neurosymbolic AI to reason explicitly over detected entities and First-Order Logic (FOL) expressions derived from textual queries.<br />3. Unlike RS-LVLMS that use implicit joint embeddings, RUNE enhances interpretability and performance by performing explicit logic-based reasoning, improving retrieval accuracy especially on complex queries.<br />4. For scalability, the method decomposes logic to operate on conditioned subsets of entities, reducing execution time compared to purely neural methods.<br />5. Foundation models are employed only for generating FOL expressions; the main reasoning process is handled by a dedicated neurosymbolic inference module.<br />6. The authors repurpose the DOTA object detection dataset by augmenting it with more complex queries to evaluate their approach.<br />7. Two new metrics are introduced—Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU)—to assess performance relative to query intricacy and image quality.<br />8. Experimental results demonstrate that RUNE surpasses state-of-the-art RS-LVLMS in performance, robustness, and explainability.<br />9. A practical use case on post-flood satellite image retrieval illustrates RUNE’s potential for real-world remote sensing applications. <div>
arXiv:2512.14102v1 Announce Type: new 
Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach</title>
<link>https://arxiv.org/abs/2512.14113</link>
<guid>https://arxiv.org/abs/2512.14113</guid>
<content:encoded><![CDATA[
<div> unlearning, CLIP, zero-shot classification, domain-specific forgetting, multimodal nullspace  

<br /><br />Summary:  
This paper addresses the challenge of unlearning specific object classes in pretrained models like CLIP without additional data, retraining, or negative impact on unrelated tasks. The authors introduce a novel, training- and data-free unlearning framework that supports three key forgetting paradigms: (1) global unlearning, which removes selected objects across all visual domains, (2) domain-specific knowledge removal that targets certain domains (e.g., removing sketches while keeping photo recognition intact), and (3) selective complete unlearning within chosen domains only. Their approach exploits a multimodal nullspace by integrating text prompts with synthesized visual prototypes generated from CLIP’s joint embedding space, allowing precise removal of undesired class information while preserving other knowledge. This methodology overcomes the constraints of traditional retraining-based unlearning techniques, delivering a flexible and computationally efficient solution. The proposed framework enhances control over model forgetting, making it applicable in real-world scenarios where selective knowledge removal is critical without compromising overall model performance or requiring costly retraining efforts. <div>
arXiv:2512.14113v1 Announce Type: new 
Abstract: Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or "unlearning") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction</title>
<link>https://arxiv.org/abs/2512.14114</link>
<guid>https://arxiv.org/abs/2512.14114</guid>
<content:encoded><![CDATA[
<div> Keywords: document image enhancement, binarization, GAN, multi-scale feature extraction, Haar wavelet transformation<br /><br />Summary: Document image enhancement and binarization are crucial preprocessing steps in improving the accuracy and efficiency of optical character recognition (OCR) systems, especially for degraded color documents. Traditional approaches use separate generative adversarial networks (GANs) for different color channels to eliminate shadows and noise, but this leads to increased training and inference times. To overcome this, the authors propose MFE-GAN, an efficient GAN-based framework that integrates multi-scale feature extraction (MFE) using Haar wavelet transformation (HWT) and normalization to preprocess images before feeding them into the GAN. This approach reduces both training and inference times without compromising performance. The paper introduces novel generator and discriminator architectures along with specialized loss functions that further enhance the model's effectiveness. Comprehensive ablation studies validate the contributions of each component. Experimental evaluations on Benchmark, Nabuco, and CMATERdb datasets show that MFE-GAN significantly lowers computational costs while achieving performance comparable to state-of-the-art methods. The implementation is publicly accessible at https://ruiyangju.github.io/MFE-GAN, providing a practical solution for efficient document image preprocessing in real-world OCR systems. <div>
arXiv:2512.14114v1 Announce Type: new 
Abstract: Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</title>
<link>https://arxiv.org/abs/2512.14121</link>
<guid>https://arxiv.org/abs/2512.14121</guid>
<content:encoded><![CDATA[
<div> Keywords: SportsGPT, MotionDTW, KISMAM, SportsRAG, interpretable sports motion assessment<br /><br />Summary: This paper introduces SportsGPT, a novel framework that leverages Large Language Models (LLMs) combined with motion analysis techniques to provide interpretable sports motion assessment and professional training guidance. First, the authors present MotionDTW, a two-stage time series alignment algorithm designed to accurately extract keyframes from skeleton-based motion sequences by comparing athlete movements with high-quality target models. Next, they develop the Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM), which produces interpretable metrics such as "insufficient extension" by contrasting extracted keyframes against target models, enabling precise diagnostic insights. Finally, the framework includes SportsRAG, a retrieval-augmented generation (RAG) model based on Qwen3, which utilizes a 6 billion token domain-specific knowledge base containing QA pairs to prompt the LLM to deliver expert training guidance tailored to the athlete's performance. Experimental results show that MotionDTW notably outperforms traditional alignment methods by achieving lower temporal error and higher Intersection over Union (IoU) scores. Ablation studies further validate the effectiveness of KISMAM and SportsRAG, demonstrating that SportsGPT exceeds the diagnostic accuracy and professionalism of general large language models. Overall, SportsGPT establishes a closed-loop system from raw motion time-series input to actionable, interpretable sports training advice, addressing current gaps in intelligent sports analysis solutions. <div>
arXiv:2512.14121v1 Announce Type: new 
Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent Instance Field for Dynamic Scene Understanding</title>
<link>https://arxiv.org/abs/2512.14126</link>
<guid>https://arxiv.org/abs/2512.14126</guid>
<content:encoded><![CDATA[
<div> Keywords: Consistent Instance Field, deformable 3D Gaussians, spatio-temporal representation, novel-view panoptic segmentation, open-vocabulary 4D querying<br /><br />Summary:<br /><br />1. The paper introduces Consistent Instance Field, a novel continuous and probabilistic spatio-temporal representation designed for dynamic scene understanding, aiming to address limitations of previous discrete tracking or view-dependent feature-based methods.<br /><br />2. The approach disentangles object visibility from persistent object identity by modeling each space-time coordinate with two components: an occupancy probability and a conditional instance distribution.<br /><br />3. A key innovation is the use of deformable 3D Gaussians embedded with instance information, which jointly encode radiance and semantic features. These are learned directly from input RGB images and instance masks using differentiable rasterization techniques.<br /><br />4. The method includes new mechanisms to calibrate identities per Gaussian and to resample Gaussians towards semantically important regions, enabling consistent instance representations across both space and time.<br /><br />5. Experimental validation on the HyperNeRF and Neu3D datasets shows that the proposed method significantly outperforms current state-of-the-art approaches in tasks such as novel-view panoptic segmentation and open-vocabulary 4D querying, demonstrating its robustness and practical effectiveness in dynamic scene analysis. <div>
arXiv:2512.14126v1 Announce Type: new 
Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models</title>
<link>https://arxiv.org/abs/2512.14137</link>
<guid>https://arxiv.org/abs/2512.14137</guid>
<content:encoded><![CDATA[
<div> Selective Unlearning, Multimodal Models, Nullspace Projection, CLIP, Privacy Preservation  

<br /><br />Summary:  
This paper presents a novel closed-form method for selective unlearning within multimodal models, focusing on pretrained models like CLIP. The proposed technique uses nullspace projection to remove specific target class information embedded in the final projection layer, eliminating the need for retraining or accessing images from the forget set. By calculating an orthonormal basis for the subspace spanned by the target text embeddings and projecting these directions out, the method significantly reduces the alignment between image features and undesired classes. Unlike conventional unlearning methods that depend on iterative fine-tuning and extensive data curation, this approach is computationally efficient and precise. It achieves a marked decline in zero-shot classification performance on undesired target classes while maintaining the model’s overall multimodal understanding. Additionally, experiments show that partial projection allows balancing complete unlearning and retention of useful information, overcoming challenges related to model decontamination and privacy preservation. This approach provides an effective, efficient alternative for managing unwanted knowledge in large pretrained multimodal models without compromising general capabilities. <div>
arXiv:2512.14137v1 Announce Type: new 
Abstract: We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing</title>
<link>https://arxiv.org/abs/2512.14140</link>
<guid>https://arxiv.org/abs/2512.14140</guid>
<content:encoded><![CDATA[
<div> Keywords: sketch editing, image editing, instruction-guided edits, style preservation, mixture-of-experts<br /><br />Summary:  
This paper introduces SketchAssist, an interactive assistant designed to enhance digital sketch editing by combining high-level semantic instruction-guided edits with precise, local line-guided redrawing, while maintaining the integrity of unrelated sketch regions and overall composition. To support scalable training, the authors develop a controllable data generation pipeline that creates realistic multi-step edit sequences by starting from attribute-free base sketches, adding attributes in sequences, and further augmenting style diversity using a style-preserving attribute-removal model applied across diverse sketches. SketchAssist builds on this dataset through a unified editing framework derived from DiT-based editors, innovatively repurposing RGB image channels to encode multi-modal inputs. This design enables seamless switching between instruction-based global edits and line-based local redraw tasks within a single interface. A novel task-guided mixture-of-experts mechanism is integrated into LoRA layers, which uses textual and visual cues to route information and specialize network behavior per editing mode. Extensive experiments demonstrate that SketchAssist achieves state-of-the-art performance in both instruction adherence and style/structure preservation, outperforming recent baseline methods. Overall, this work delivers an effective, controllable toolset and dataset for practical sketch creation and revision, addressing key challenges in preserving style-sensitive line art structures during complex edits. <div>
arXiv:2512.14140v1 Announce Type: new 
Abstract: Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models</title>
<link>https://arxiv.org/abs/2512.14141</link>
<guid>https://arxiv.org/abs/2512.14141</guid>
<content:encoded><![CDATA[
<div> Keywords: performance anti-patterns, machine learning models, PyTorch traces, large language models, computer vision  

<br /><br />Summary:  
This paper addresses the challenge of identifying performance anti-patterns in machine learning (ML) models, which is vital for efficient training and inference but usually requires specialized expertise. Current approaches used by large tech companies depend on dedicated ML infrastructure engineers and resource-intensive workflows that are inaccessible to many computer vision researchers. The authors highlight the difficulty of pinpointing problematic segments within lengthy execution traces, a task that remains laborious and is not effectively automated by existing ML models, including large language models (LLMs). To overcome this, they introduce the first benchmark dataset designed specifically to evaluate and enhance ML models' capabilities in detecting anti-patterns in traces. This dataset comprises over 600 PyTorch traces collected from a variety of computer vision tasks—classification, detection, segmentation, and generation—across multiple hardware platforms. The paper proposes a novel iterative method involving a lightweight ML model for initial detection of anti-pattern segments, followed by a large language model for detailed classification and targeted feedback. Experimental results demonstrate that this combined approach outperforms unsupervised clustering and rule-based statistical methods, while effectively mitigating LLMs’ limitations in context length and reasoning efficiency. <div>
arXiv:2512.14141v1 Announce Type: new 
Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World</title>
<link>https://arxiv.org/abs/2512.14158</link>
<guid>https://arxiv.org/abs/2512.14158</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, object detection, multi-trigger, inter-object interaction, CIS-BA  

<br /><br />Summary:  
1. The paper addresses the vulnerability of object detection models, especially in autonomous driving, to backdoor attacks that exploit static, single-trigger, single-object mappings.  
2. A novel attack paradigm called CIS-BA is proposed, which shifts the trigger design from static object features to continuous inter-object interaction patterns, capturing how objects co-occur and interact within a scene.  
3. CIS-BA introduces space triggers modeled in a continuous interaction space, enabling multi-trigger, multi-object attacks that maintain robustness through invariant geometric relationships.  
4. To realize this framework, the authors design CIS-Frame, which analyzes interactions to construct space triggers, formalizes them as class-geometry constraints for training data poisoning, and integrates the backdoor into the detector during training.  
5. CIS-Frame supports both single-object attacks (such as misclassification and disappearance) and simultaneous multi-object attacks, creating complex coordinated effects adaptable across varying interaction states.  
6. Experimental results on MS-COCO and real-world videos demonstrate that CIS-BA achieves over 97% attack success in complex environments and maintains more than 95% effectiveness under dynamic multi-trigger scenarios while bypassing three state-of-the-art defense methods.  
7. This work expands the scope of backdoor attacks into interaction-intensive scenarios and offers new insights into securing object detection systems against sophisticated coordinated backdoor threats. <div>
arXiv:2512.14158v1 Announce Type: new 
Abstract: Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.14162</link>
<guid>https://arxiv.org/abs/2512.14162</guid>
<content:encoded><![CDATA[
arXiv:2512.14162v1 Announce Type: new 
Abstract: Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.14177</link>
<guid>https://arxiv.org/abs/2512.14177</guid>
<content:encoded><![CDATA[
arXiv:2512.14177v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</title>
<link>https://arxiv.org/abs/2512.14180</link>
<guid>https://arxiv.org/abs/2512.14180</guid>
<content:encoded><![CDATA[
arXiv:2512.14180v1 Announce Type: new 
Abstract: Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity</title>
<link>https://arxiv.org/abs/2512.14196</link>
<guid>https://arxiv.org/abs/2512.14196</guid>
<content:encoded><![CDATA[
arXiv:2512.14196v1 Announce Type: new 
Abstract: Between $15\,\%$ and $45\,\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\,\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</title>
<link>https://arxiv.org/abs/2512.14200</link>
<guid>https://arxiv.org/abs/2512.14200</guid>
<content:encoded><![CDATA[
arXiv:2512.14200v1 Announce Type: new 
Abstract: Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos</title>
<link>https://arxiv.org/abs/2512.14217</link>
<guid>https://arxiv.org/abs/2512.14217</guid>
<content:encoded><![CDATA[
arXiv:2512.14217v1 Announce Type: new 
Abstract: Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.14222</link>
<guid>https://arxiv.org/abs/2512.14222</guid>
<content:encoded><![CDATA[
arXiv:2512.14222v1 Announce Type: new 
Abstract: Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14225</link>
<guid>https://arxiv.org/abs/2512.14225</guid>
<content:encoded><![CDATA[
arXiv:2512.14225v1 Announce Type: new 
Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients</title>
<link>https://arxiv.org/abs/2512.14232</link>
<guid>https://arxiv.org/abs/2512.14232</guid>
<content:encoded><![CDATA[
arXiv:2512.14232v1 Announce Type: new 
Abstract: The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</title>
<link>https://arxiv.org/abs/2512.14234</link>
<guid>https://arxiv.org/abs/2512.14234</guid>
<content:encoded><![CDATA[
arXiv:2512.14234v1 Announce Type: new 
Abstract: Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title>
<link>https://arxiv.org/abs/2512.14235</link>
<guid>https://arxiv.org/abs/2512.14235</guid>
<content:encoded><![CDATA[
arXiv:2512.14235v1 Announce Type: new 
Abstract: Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding</title>
<link>https://arxiv.org/abs/2512.14236</link>
<guid>https://arxiv.org/abs/2512.14236</guid>
<content:encoded><![CDATA[
arXiv:2512.14236v1 Announce Type: new 
Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs</title>
<link>https://arxiv.org/abs/2512.14257</link>
<guid>https://arxiv.org/abs/2512.14257</guid>
<content:encoded><![CDATA[
arXiv:2512.14257v1 Announce Type: new 
Abstract: Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance</title>
<link>https://arxiv.org/abs/2512.14266</link>
<guid>https://arxiv.org/abs/2512.14266</guid>
<content:encoded><![CDATA[
arXiv:2512.14266v1 Announce Type: new 
Abstract: Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\circ$ field of view driver attention dataset, containing $\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</title>
<link>https://arxiv.org/abs/2512.14273</link>
<guid>https://arxiv.org/abs/2512.14273</guid>
<content:encoded><![CDATA[
arXiv:2512.14273v1 Announce Type: new 
Abstract: Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</title>
<link>https://arxiv.org/abs/2512.14274</link>
<guid>https://arxiv.org/abs/2512.14274</guid>
<content:encoded><![CDATA[
arXiv:2512.14274v1 Announce Type: new 
Abstract: Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SS4D: Native 4D Generative Model via Structured Spacetime Latents</title>
<link>https://arxiv.org/abs/2512.14284</link>
<guid>https://arxiv.org/abs/2512.14284</guid>
<content:encoded><![CDATA[
arXiv:2512.14284v1 Announce Type: new 
Abstract: We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition</title>
<link>https://arxiv.org/abs/2512.14309</link>
<guid>https://arxiv.org/abs/2512.14309</guid>
<content:encoded><![CDATA[
arXiv:2512.14309v1 Announce Type: new 
Abstract: Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region</title>
<link>https://arxiv.org/abs/2512.14312</link>
<guid>https://arxiv.org/abs/2512.14312</guid>
<content:encoded><![CDATA[
arXiv:2512.14312v1 Announce Type: new 
Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title>
<link>https://arxiv.org/abs/2512.14320</link>
<guid>https://arxiv.org/abs/2512.14320</guid>
<content:encoded><![CDATA[
arXiv:2512.14320v1 Announce Type: new 
Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Attention Guided Defense Against Malicious Edits</title>
<link>https://arxiv.org/abs/2512.14333</link>
<guid>https://arxiv.org/abs/2512.14333</guid>
<content:encoded><![CDATA[
arXiv:2512.14333v1 Announce Type: new 
Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</title>
<link>https://arxiv.org/abs/2512.14336</link>
<guid>https://arxiv.org/abs/2512.14336</guid>
<content:encoded><![CDATA[
arXiv:2512.14336v1 Announce Type: new 
Abstract: Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>